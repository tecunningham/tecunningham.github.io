---
title: The Influence of AI on Content Moderation and Communication
execute:
  echo: false
  cache: true # caches chunk output
date: 2023-06-06
#fontsize: 10pt
fig-align: center
reference-location: margin
citation-location: margin
format:
   html:
      embed-resources: true
      html-math-method:
         method: mathjax
         url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js"
         #     ^ this forces SVG instead of CHTML, otherwise xypic renders weird
      include-in-header:
         - text: |
            <script>window.MathJax = {
               loader: { load: ["https://cdn.jsdelivr.net/gh/sonoisa/XyJax-v3@3.0.1/build/xypic.js"]},
               tex: {packages: {'[+]': ['xypic','bm']},
                     macros: {  bm: ["\\boldsymbol{#1}", 1],
                                ut: ["\\underbrace{#1}_{\\text{#2}}", 2],
                                utt: ["\\underbrace{#1}_{\\substack{\\text{#2}\\\\\\text{#3}}}", 3] }
               }
            };
            </script>
engine: knitr
editor:
   render-on-save: true
---
<style>
    h1 {  border-bottom: 4px solid black;}
    h2 {  border-bottom: 1px solid #ccc;}
</style>

***Tom Cunningham***[^note]

First version June 6 2023. ***Still a draft, don't circulate!***

[^note]: tom.cunningham@gmail, [\@testingham](https://twitter.com/testingham), resident fellow at the [Integrity Institute](https://integrityinstitute.org). I worked at FB for 5 years, and Twitter for 1 year, but this note entirely based on public information.

<!-- 
ONLINE: http://tecunningham.github.io/2023-06-06-effect-of-ai-on-communication.html

SEE ALSO:
   - 2021-08-16 technology and adversarial situations / does technology help the center or periphery?
   - 2022-03-26-moderation-models: Producer Incentives -- has a model of producer incentives
   - 2022-01-03-models-of-communication-and-spam.md
   - 2023-04-16 shallow and deep quality
   - 2020-12-17-social-media-master.md    -- has a table of diff types of decentralized media and exploitations
-->

**It is difficult to anticipate the effect of AI on communication.** AI models have already had big effects on content moderation (detecting spam, bots, misinformation), but they are also starting to have big effects on content production (obfuscation, generating synthetic spam, deepfakes).

**In this note I will try to make some predictions about the equilibrium impact of AI.** Below I discuss and give predictions about the impact on a variety of outcomes such as the prevalence of policy-violating content on platforms (e.g. porn, hate speech), the prevalence of bots, the prevalence and influence of synthetic media (e.g. deepfakes), and the shift of overall communication between platforms.

**It is useful to distinguish between "internal" and "external" properties of content:**

1. **For "internal" properties, AI is likely to benefit the receiver.** I define an "internal" property as something that is a function solely of the content: whether an image contains nudity, whether text contains hate speech, whether a joke is funny. AI classifiers are rapidly approaching human-level accuracy for these properties and this means that platforms (and governments) will be able to near-perfectly filter out content that violates their rules, even when senders have access to the same technology.

2. **For "external" properties, AI is likely to benefit the sender.** An "external" property of a message depends on some fact outside the content itself: e.g. whether an image was computer-generated, or whether a headline is true. Platforms will get better at predicting external properties but they will be outpaced by motivated actors who can manipulate their fakes until they become indistinguishable from genuine articles (and manipulate lies so they're indistinguishable from the truth).

3. **AI will increase the demand for intermediaries.** As a consequence of both of the points above individual users will see relatively more value from moving to platforms which have protection against those adversaries (spammers, scammers, short sellers, trolls, state actors). Thus it seems likely that there will be a further consolidation onto platforms that are large and tightly controlled.

**The remainder of this note:**

1. A series of specific predictions about different aspects of communication systems.

2. A deeper discussion of the distinction between "internal" vs "external" properties.

3. A simple model of equilibrium between a platform who is trying to filter content, and a producer who is trying to obfuscate content. The model applies to "internal" properties, and has the prediction that a perfect classifier will drive prevalence of violating content to zero.

I also have a short appendices on the history of forged documents, and on relevant forecasting questions from Metaculus.


#           Predictions

**(1) The prevalence of policy-violating content on platforms will approach zero.** AI models will keep getting better at detecting nudity, obscenity, hate speech, slur words, incitement, illegal acts, violence. Producers will also get better at obfuscating their content to avoid the classifiers. However as the AI models become arbitrarily close to human judgment then fewer and fewer exceptions will slip through, i.e. fewer items which a human recognizes as violating but the computer does not. Below I give a more formal argument for this ("Venn diagram model"). This prediction applies just to "self contained" categories of content where the ground truth is human interpretation without reference to outside information. The prediction does not apply to deepfakes or misinformation, discussed below.

   This prediction is consistent with recent historical trends. Meta reports that the prevalence of nudity, bullying, hate speech, and graphic content have each declined over 2017-2022 by a factor of between 2 and 5.[^metaDecline]
   
   [^metaDecline]: See Meta's [Community Standards report](https://transparency.fb.com/data/community-standards-enforcement/) and [my visualization of this data](http://tecunningham.github.io/2023-01-31-social-media-suspensions-data.html#meta-facebook-instagram).

   This prediction also applies to censorship. Authoritarian governments often use crude pattern-matching to identify content that expresses, for example, anti-government sentiment. Citizens get around this by using indirect language. Better AI will allow governments to classify every post with reliability as high as if they had a human read every individual post, and so indirect language or obfuscation will no longer be an effective workaround.

   This prediction would fail if there were substantial limits on the performance of AI, e.g. if there were ways of obfuscating content that AI will remain unable to recognize. It seems plausible that AI systems will remain less flexible than humans in general, but they are trained on enormous quantities of data, so although a novel means of obfuscation would confuse them in the short-run they would be likely to learn to recognize it very quickly.

   An important qualification is that even if human-evaluted policy-violating content goes to zero there will likely remain a lot of content that is *interpreted* in a violating way. This can occur in equilibrium when a community uses context or pragmatics to change the significance of a message, e.g. using double entendres, substituting "globalist" for "jewish", saying the opposite of what is meant (e.g. saying excessively positive things about a hated ethnic group, or a pro-anorexia community speaking as if they were anti-anorexia), using euphemisms for illegal acts, using emojis of eggplants and peaches, using photos without nudity but which can be read as pornographic.[^selective] In each of these cases a human evaluator might judge the content to be non-violating because they interpret it in a non-violating way, yet the typical audience for these posts interprets them in a violating way. This is fundamentally a limitation of treating the policy's ground truth as the significance of a message when interpreted without context.

   <!-- need some discussion of  why you can't just evaluate with context-->

   <!-- Old Soviet joke: A man hands out leaflets on Red Square, and the KGB arrest him. But when they get him to the station, they find that the leaflets are all blank. And he says "Well, everyone knows what the problem is, so why bother writing it down?" -->

   [^selective]: A related phenomena is people using selective truths to give a false impression. E.g. it is common for anti-vaccination groups to avoid posting false claims about vaccines and instead selectively post negative claims among the pool of true claims. It is common for people with a bias against some ethnic group to avoid posting false claims but selectively post negative claims. Because the pool of true claims is enormous it is easy to give a false impression without any post being false.

**(2) The prevalence of database-matched violating content on platforms will approach zero.** Many platforms try to detect content that matches content in a database, e.g. databases of illegal sexual media (CSAM), IP-protected content (Content ID), or terrorist recruitment content (GIFCT). Content producers often obfuscate their content to confuse the matching algorithms, e.g. by adding noise. Improvements to AI will help both parties but it seems likely to help platforms relatively more: as AI recognition approaches a human level it will be impossible to upload a recognizable variant of a known piece of violating content without the platform automatically labelling it as such.

**(3) Platforms will not be able to identify bots from their behavior.** Most online platforms struggle with automated users (bots) who are disruptive in a variety of ways. One way of protecting against bots is with behavioral tests, e.g. a CAPTCHA test asking users an image-recognition task[^captcha], or by using on-platform behavior to detect whether a user is human. However improvements in AI mean that computers can now recognize most images as well as humans, and can learn to imitate human-style behavior patterns, so it seems likely these behavioral tests will become ineffective against sophisticated actors.

   This does not imply that the prevalence of bots will increase. All platforms need some defense against bots so they will have to rely relatively more on other forms of authentication: monetary payment, offline identity credentials (government ID, credit card number), hard-to-fake metadata (e.g. unique IP address, device ID), or 3rd-party authentication (e.g. from Google, Apple).^[The 3rd-party identity providers will themselves have to rely on some other ground truth when accepting signups.] Thus the barriers to signing up for a service (and especially posting on it) will become higher.

   [^captcha]: CAPTCHA stands for Completely Automated Public Turing test to tell Computers and Humans Apart.

**(4) Platforms will find it hard to discriminate between real and fake media.** There are a number of properties where the ground truth depends on the relationship between the content and the real world:

   - Whether some media (audio, photo, video) has been manipulated or synthesized.
   - Whether text was written by a human or a computer (e.g. by GPT).
   - Whether text was written by a specific person.

   Better AI will make it easier to predict each of these properties all else equal. However better AI will also make it easier for computers to obfuscate or synthesize fake content. It seems likely that the latter effect will dominate: it will gradually become possible to camouflage computer-generated content such that neither a computer nor a human could tell them apart. If the content-producer has access to the platforms' model then they can keep perturbing their fake media until it is labelled as non-fake.

   It seems likely that prevalence of fake media will increase on unmoderated platforms, though much of that increase will likely to be replacing simple false assertions without any veridical evidence. However the major platforms have strong reason to reduce the spread of fake media[^ranking], and can control it with indirect means, e.g. Meta and YouTube dramatically decreased the prevalence of misinformation over 2016-2020 not primarily through better detection of whether a claim is false, but by a host of indirect measures. Thus I do not expect overall prevalence of fake factual media to substantially increase on the major platforms.

   [^ranking]: See discussion in my note on [how platforms rank](http://tecunningham.github.io/2023-04-28-ranking-by-engagement.html). 
   
**(5) Fake media (deepfakes) will not have a substantial influence on politics.**  As it becomes cheaper to manipulate and synthesize media then people are likely to become more skeptical and rely relatively more on the *provenance* of information. Thus although synthetic media will likely circulate I do not think it will have a substantial influence on beliefs in equilibrium.

   It is already fairly easy to forge or alter documents, or edit video in a misleading way, and as a consequence mainstream media organizations typically do not publish leaked materials unless they have either a chain or provenance for the leaks or independent confirmation of their content. Influential forgeries have been historically much rarer than influential true leaks: over the past 25 years I estimate perhaps 10% of all influential leaks were based on forged materials.[^twoCases][^videoLeaks]

   A great deal of false claims already circulate on the internet and it's not clear that the quality of the faked media is an important constraint on the volume. circulates, especially in the losely moderated part of the internet (e.g. by email, on Telegram, 4chan, Truth Social, WhatsApp, Twitter). It is not clear that an . it's not uncommon to find a clip of an interview with a politician edited to make it appear that they are admitting to a crime or secret agenda.[^Snopes] If people already take what they see at face value then adding deepfakes seems unlikely to change their opinions substantially. Alternatively if people are skeptical and look for corroborating sources then, again, deepfakes would be unpersuasive. It seems that deepfakes would only be influential if there are a significant population who are exposed to many lies but are not persuaded because the documentary evidence is not sufficiently strong (photos, audio, videos).

   [^twoCases]: I know of two forged documents that were widely taken as true in the last 25 years, from around 15 substantial leaks that I could find: (1) the "yellowcake" letters from Iraq to Niger, cited in the 2002 US case for war against Iraq; (2) a fake G W Bush military transcript reported on by CBS and Dan Rather in 2004. It's notable both that these cases are somewhat rare, and that each was passed through a chain of apparently reputable parties.

   [^videoLeaks]: This argument implies that, prior to AI, anonymously leaked video would be more likely to be published and circulated than anonymously leaked documents, because video is harder to fake. In fact I cannot think of many cases of influential anonymous leaks of videos. When Trump's "Access Hollywood" tape was leaked to the Washington Post they got confirmation before publishing it. In fact maybe leaked video has always been untrustworthy because it has always been easy to make deceptive edits.

   [^Snopes]: Snopes.com has an enormous database of both false claims and misleadingly manipulated media that has circulated since 1994. A typical [recent example](https://www.factcheck.org/2021/03/scicheck-video-targets-gates-with-old-clip-misleading-edit/) is an edit of a Bill Gates interview to make it appear he wants to use vaccination to reduce population growth.

   <!-- **The informative value of photos and videos will decline.** Photos, audio and videos will become less persuasive. For example suppose police could alter surveillance camera footage to remove or add someone just by typing an instruction to a video-editing program. -->

   <!-- Circulation of fake information: we have already adapted (1) skepticism ("don't believe what u read on the internet"); (2) platforms get burned by it so try to reduce distribution. Not sure whether adding audio/video would add to this. -->

**(6) Communication will migrate towards large closed platforms.** It seems likely that smaller platforms who cannot afford sophisticated defences will become overrun with AI-created bots, spam, obfuscated violating content, and fake media. This would imply that consumers will tend to migrate to larger closed platforms with more effective defences, and which have more restriction on participation. This continues a general movement over the last 20 years of communication moving from small open platforms (independent email, small forums, mailing lists, independent websites) to large closed platforms (large email providers, large social media platforms).

   It also seems likely that people will come to rely more on established sources of truth because they can trust veridical evidence less, e.g. they will rely relatively more on Wikipedia, Community Notes, and mainstream recognized media sources.

   It may be that people come to rely more on cryptographic signing to verify authenticity.  This seems unlikely to me: it is generally more efficient for an intermediary to verify authenticity of senders (AKA filter out frauds) than for users to do it themselves. I think we've seen that in other domains: (1) PGP signing of email has been less important than email providers filtering spam and phishing; (2) SSL certificates in browsers have been less important than browsers giving warnings for suspected phishing sites (e.g. Google's [safe browsing](https://safebrowsing.google.com) database of sites with phishing or malware is used to give warnings in Chrome and Safari).

<!-- **(7) Cryptographic signing won't get widely adopted.** AI will make it easier to trick people by spoofing, e.g. spoof videos by celebrities advertising a cryptocurrency, spoof phone calls from your cousin asking for money. One response could be greater adoption of cryptographic signing to verify identity: you would have a public key for the celebrity, and for your cousin, and you would verify incoming messages against that key.

   I don't expect that adoption of cryptographic signing will markedly increase, in short because (...) -->

**(7) Things will get weird.** Much of our common-sense understanding of media will be violated when we routinely use AI models to manipulate and synthesize artefacts. Some examples:

   - **Entertainment content may become overwhelmingly synthetic.** A classifier that can detect whether a photo is pretty can also generate a synthetic photo that is pretty, and a classifier that can detect whether a joke is funny should also be able to generate funny jokes.^[I think language models haven't yet been very good at jokes because they generate one word at a time (autoregressive), while jokes typically have a logical structure such that the setup is probable given the punchline, but not the other way around. When we get language models which generate text using different statistical algorithms (e.g. diffusion instead of autoregressive generation) then it seems likely they'll be able to create good jokes.] On average people spend around 3 hours per day watching entertainment (TV, YouTube, TikTok, Instagram). It seems likely that trained models will be able to synthesize content that is highly engaging though it's hard to anticipate what it will look like.

   - **Producers will synthesize content to sit on the *edge* of a category.** If platforms take action whenever content passes some threshold then adversarial actors will generate or perturb content such that it sits right below the threshold. If a platform removes a photo whenever more than 50% of raters would say it depicts nudity then producers would upload photos which 49% of raters would say depicts nudity. People would upload movies which *almost* look like an existing IP-protected movie, and students might submit essays that are close to existing sources but don't quite trigger the plagiarism detector.
   
   <!-- - **Producers will produce ambiguous / bistable  safcontent.** -->
   <!-- - Will synthesize CSAM. -->

**TO ADD:** (1) discussion of whether cryptographic and blockchain tech will become more useful; (2) discussion of the value of documentary evidence in more mundane situations -- e.g. using photos and videos as proof in ordinary disputes or criminal trials; (3) using AI to generate content, e.g. letters to the editor, journal submissions: can no longer use superficial coherence as a proxy for underlying quality.


<!-- TO ADD: limits on the classifiers, types of content which they cannot recognize or cannot synthesize to human level. E.g. chess with new rules. -->


#           Internal vs External Properties [UNFINISHED]

**A common thread unifies many of the predictions above: internal vs external properties.** I say an "internal" property of a piece of content is one where the ground truth is human judgment of that content without reference to any external facts. An "external" property depend on some external fact, e.g. about provenance of the content (who created it, how it was made), or about the accuracy of what the content claims or depicts.



| property                                             | internal/external | detection method                  | obfuscation methods  |
| ---------------------------------------------------- | ----------------- | --------------------------------- | -------------------- |
| whether text contains a curse/slur                   | internal          | string matching                   | misspell, neologisms |
| whether text contains criticism of government        | internal          | string matching                   | indirect language    |
| whether text is match against database               | internal          | plagiarism detection (e.g. Chegg) | misspell, reword     |
| whether photo contains nudity                        | internal          |                                   | add noise, transform |
| whether media is match against database              | internal          | PhotoDNA                          | add noise, transform |
| whether image contains a specific watermark          |                   |                                   |                      |
| whether a message is signed by a specific public key | internal          |                                   |                      |
|                                                      |                   |                                   |                      |
| whether a picture looks good                         | internal          |                                   |                      |
| whether a joke is funny                              | internal          |                                   |                      |
| whether user will engage (like, comment, share)      | internal          |                                   |                      |
|                                                      |                   |                                   |                      |
| whether email is mass and unsolicited (spam)         | external          | naive Bayes                       |                      |
| whether message is from a real human (vs bot)        | external          | naive Bayes                       |                      |
| whether artwork is by a specific person              | external          |                                   |                      |
| whether writing is by a specific person              | external          | stylometry               |                      |
| whether media has been altered                       | external          |                                   |                      |
| whether media is synthetic (deepfake)                | external          |                                   |                      |
| whether text is generated by an LLM                  | external          |                                   |                      |
| whether user behavior is by a bot                    | external          | CAPTCHA                           |                      |
| whether an article is misinformation                 | external          |                                   |                      |
| whether a photo is misleading                        | external          |                                   |                      |
| whether content is retentive                         | external          |                                   |                      |

<!-- 
**On-demand filtering is not the primary tool.** Even if we have highly imperfect ways of detecting a property, whether internal or external, still we can drive down prevalence through using metadata and repeated interactions, and arguably that's been the primary reason for the decline.

##          Observations on Internal Properties

- **Historically it was feasible to do filtering with human judgment.** The technology of mass media means that a small amount of content was shown to a large audience (books, magazines, radio, television, movies), which made it feasible to have a human censor who manually reviewed the majority of content. 
- **Filtering with crude classifier.** Motivated senders will be able to cheaply obfuscate their content to get around the classifier.
- **Filtering with good classifier.** Receiver will be able to filter perfectly, content will be eliminated. 
- **Selecting with crude classifier.** Suppose you want to select posts that are pretty, funny, clickable, etc.. In these cases the incentives are more likely to be aligned: I can't think of many cases where a sender wishes to be classified as having some good property, but also prefers to not actually have that property.
- **Selecting with good classifier.** This should eliminate the Goodhart problem.

##          Observations on External Properties

- **Controlling on external properties typically uses metadata.** The primary method for controlling spam is not using content-based classifiers but instead maintaining blacklists of servers: Reiley and Rao (2012) say "[t]he single most effective weapon in the spam-blocking arsenal turns out to be blacklisting an email server." Similarly for misinformation: the most effective measures have not been proactive classifiers (typically low precision, e.g. 10%) or third-party-fact checking (typically high latency). Instead it has been discouragements to posting misinformation, and changes in the overall ranking system.
- **Cryptographically signing messages.** It still means you must trust *someone*. Digital signing always had limited uptake for email and for SSL on websites: the attention tradeoff implies that it's better if an intermediary does the validation. 

##          A model of both
- There's a set of features 1,...,n, a model can be defined as the number of features that you condition on. We can then characterize three parties with an integer representing the number of features: (1) human, (2) sender, (3) receiver.
- ground truth is all features - as classifier gets better u get squeezed ; 
- sender has to hide a "1" from the classifier , gradually get squeezed ; 
- other adversarial : putting police around town to prevent crime (don't post it publicly), auditing tax returns , applying antibiotics ; (there's some dynamics : try to wipe out population). 
- key is whether sender has a constraint in state space ; but conditional independence won't hold 
- human affairs : jigsaw puzzle it's all there ; vs human things it's all empirical vs theory ;
- -->


#           Model of AI and Internal Properties (Venn Model)

**In short: *improvements in AI are likely to reduce overall prevalence of violating content on platforms*.** 

**Better models will help both sides in this war:** they will help platforms detect violating content, but they also also help producers camouflage their violating content. I argue below that the net effect will likely be to help platforms, the basic argument is that if both sides had access to a *perfect* classifier then there would be zero violating content on the platform.^[Yann Lecunn made a similar argument in a [tweet](https://twitter.com/ylecun/status/1664651158562979841) from June 2023: "AI is part of the solution here, not part of the problem! It is because of progress in natural language understanding in multiple languages (due to self-supervised transformers) that, for example, hate speech can be detected and taken down in hundreds of languages.". Although I think the wording is misleading by implying that AI being part of the solution precludes it from also being part of the problem. Also worth noting that LeCun misstates FB's own statistics: he says "82% of hate speech is taken down automatically by AI before anyone sees it." In fact it's 82% of *detected* hate speech (the share of true hate speech detected by AI is necessarily smaller but FB does not report that figure), and the 82% refers to the share that is actioned before it is *reported*, not actioned before it is *seen*.]

**Prevalence of violating content has fallen.** Overall prevalence of violating content has shrunk dramatically over the past 5-10 years on the major social media platforms, and this can largely be attributed to improvements in classifiers.

**Model: a Venn-diagram model of content moderation with classifiers.** I describe a game between a platform and a producer, each of whom has access to a classifier. We want to know the effect on total violating content that survives filtering by the platform. Conclusions:

   1. If a plaform gets a better classifier then less violating content gets through.
   2. If a producer gets a better classifier then the effect is ambiguous, but probably more violating content will get through. In practice this means producers using an AI model to rephrase text, add noise to photos, or synthesize video, such that the content remains violating but the platform's classifier does not pick it up.
   3. If both platform and producer get a better classifier then probably less violating content will get through.
   4. A lot of content will sit *just on the threshold* for getting filtered.


##           Model

**The diagram below illustrates three sets of posts:** ^[I'm assuming that the universe is all possible posts, e.g. all possible strings of text, or sets of pixels. Then sets $V$ and $\hat{V}$ and $M$ will be enormous, but the actually existing posts are only a tiny tiny fraction of the set of possible posts. When calculating prevalence we use cardinality of these sets, implying (1) posts are drawn uniformly from the sets, and (2) there are no identical posts, i.e. each post is drawn at most 1 time.]

- **$V$: posts that are violating** -- according to expert human raters. I'm concentrating on "self-contained" rules, e.g. nudity, hate speech, holocaust denial, where the violation depends only on the direct content of the post. I'm not considering categories like misinformation, deepfakes, or spam where violation depends on whether the content is true, or the provenance of the content.
- **$\hat{V}$: posts that are classified as violating** -- by the platform's classifier, can parameterize with a threshold ($\hat{V}(k)$). The red segments below represent the false positives and false negatives of the classifier:
- **$M$: posts that the producer wants to send** -- i.e., posts that express their message, $M$. We can classify some producers as "good" (only wants to send non-violating $M\cap V=\varnothing$), or as bad (only want to send violating $M\cap V=M$).


```{tikz}
\begin{tikzpicture}[scale=2]
   \draw (-1,-1)--(1,-1)--(1,1)--(-1,1)--(-1,-1);
   \fill[red!50!white] (0,.1) circle (.5);
   \fill[red!50!white] (-.1,-.1) circle (.5);
   \begin{scope}
      \clip (0,.1) circle (.5);
      \fill[white] (-.1,-.1) circle (.5);
   \end{scope}
   \draw (0,.1) circle (.5);
      \draw[dashed] (0,.6) -- (0,1.2) node[above,align=center] {violating posts\\($V$)};
   \draw (-.1,-.1) circle (.5);
      \draw[dashed] (-.6,-.1) -- (-1.2,-.1) node[left,align=center] {classified as violating\\by platform ($\hat{V}$)};
   \draw (.1,-.2) circle (.5);
      \draw[dashed] (.6,-.2) -- (1.2,-.2) node[right,align=center] {messages the producer\\wishes to send ($M$)};
\end{tikzpicture}
```

##           Implications of this Model

Suppose that we remove all detected violating posts (i.e. $\hat{V}$). In the case illustrated above the posts removed ($M\cap\hat{V}$) would be the majority of the producer's posts, and of those that remain only a small share are violating. We can make a few observations given this setup:


1. **Improvements to the platform's classifier will reduce prevalence of violating content.** If we hold fixed the producer's production, then as the classifier $\hat{V}$ gets closer to $V$ we should generally expect fewer false-positives and fewer false-negatives, so overall a better outcome for the platform (there are cases where this wouldn't happen but they seem unusual.)

2. **Improvements to producer's classifier will *probably* increase prevalence of violating content.** Suppose that producers have their own estimate of the platform's classifier, $\tilde{V}$, and they will produce posts uniformly from the set they believe will not be removed, i.e. from the set $M \backslash \tilde{V}$. If the producer learns the true classifier $\tilde{V}=\hat{V}$ then they will reduce their rate of deleted posts, which is good for the producer, but the effect on the platform can be ambiguous:
   
   - **For good producers:** For producer who do not wish to produce violating content ($M\cap V=\varnothing$) knowledge of the classifier will prevent them from producing false positives, and they will produce more true negatives, which is good for the platform.
   - **For bad producers:** For producers who *only* want to produce violating content ($M\cap V=M$) they will increase their rate of violating but non-detected posts (false negatives), which is bad for the platform.
   - **For neutral producers:** For neutral producers (where $M$ is neither a subset nor mutually exclusive with being violating), the net effect is ambiguous.

   In fact the net effect seems likely to be negative because in most cases people who produce violating content do it on purpose, e.g. those who produce nudity and hate speech and holocaust denial do it intentionally, and they couldn't achieve their purpose without violating the policy.

   **What this looks like in practice:** tricking classifiers by misspelling words, using roundabout phrasing, inverting colours in a photo, adding noise to a video. Earlier classifiers had difficulty with these manipulations but deep neural nets are much better at recognizing the deep content and are harder to confuse.

   **Qualification.** In the argument above I'm holding fixed the platform's classifier $\hat{V}$. However if the producer changes the posts they produce then the performance of that classifier will change, e.g. precision might fall, and so the full effect should include an adjustment to that classifier. Could add an equilibrium condition that the classifier's threshold $k$ will be set such that precision will be $p$ ($\frac{|\hat{V}(k)\cap V \cap M|}{|\hat{V}(k)\cap M|}=p$). I don't think this would change the qualitative conclusion.
   
3. **Improvements to a classifier used by both platform and producer will lower prevalence of violating content.** Suppose both parties used the same classifier ($\tilde{V}=\hat{V}$), and that classifier was perfect ($\tilde{V}=\hat{V}=V$). Then there will be no violating posts shown: negative producers won't post anything, and neutral and positive producers will only produce non-violating posts. This argument applies to the limit of having a perfect model, but it seems reasonable to presume that on the path to that limit the rate of violating posts will generally decline.

4. **Content will cluster just below the classifier threshold.** So far we have discussed everything in binary terms, thought realistically most things are continuous. If we action posts in a binary manner, e.g. filtering posts with a classifier score above some threshold, then producers will have an incentive to produce posts which are just below the threshold for filtering.

<!-- **Followup: "open" policies.** This discussion has been predicated on there being an unambiguous ground truth, $V$. However in some cases violation of the policy can depend on facts outside of the content itself. E.g. whether a post is misinformation (depends on facts in the world), whether a link is exaggerating or leads to an ads-farm (depends on the content of the link), whether a post provokes hate speech (depends on user response to the post), whether a post is inauthentic (depends on whether author is who they claim to be). I think the analysis will be a bit different then. -->

#           Appendix: Historical Observations on Forgeries

**Forgeries usually purport to be leaks.** The typical influential forged document purports to be a private communication that reveals a lie or hidden motive.

**Influential leaks of political documents since 1997:**
 
|                                                     |      |                   |                                                |
| --------------------------------------------------- | ---- | ----------------- | ---------------------------------------------- |
| Tripp Tapes                                         | 1997 | audio             | Linda Tripp to Kenneth Starr                   |
| [**FORGERY**] Iraq letters to Niger ("yellowcake")  | 2002 | documents         | Unknown to Italian intelligence to CIA         |
| [**FORGERY**] Bush military transcripts ("Killian") | 2004 | fax of 1970s memo | Unknown to retired colonel to Dan Rather / CBS |
| Abu Ghraib photos                                   | 2004 | photos            | Unnkown to CBS                                 |
| Baghdad Airstrike ("Collateral Murder")             | 2007 | video             | Chelsea Manning to Wikileaks                   |
| US Iraq war logs                                    | 2010 | digital docs      | Chelsea Manning to Wikileaks                   |
| US Diplomatic cables                                | 2010 | digital docs      | Chelsea Manning to Wikileaks                   |
| Romney Fundraiser Tape ("47%")                      | 2012 | audio             | Bartender to Mother Jones                      |
| NSA Surveillance Leaks                              | 2013 | digital docs      | Edward Snowden to the Guardian, WaPo           |
| DNC emails                                          | 2016 | emails            | Unknown to Wikileaks                           |
| Podesta emails                                      | 2016 | emails            | Unknown to Wikileaks                           |
| Colin Powell emails                                 | 2016 | emails            | Unknown to DCLeaks                             |
| Panama papers                                       | 2016 | documents         | Unknown to Süddeutsche Zeitung                 |
| Donald Trump Access Hollywood Tape                  | 2016 | video             | Unknown to Washington Post                     |
| [**MIXED**] Macron emails                           | 2017 | emails            | Unknown to Pastebin                            |
| China Cables                                        | 2019 | digital docs      | Unknown to the ICIJ                            |
| Hunter Biden laptop                                 | 2020 | docs,audio,video  | computer shop to Giuliani to NY Post           |
| Los Angeles Council call ("changuito")              | 2022 | audio             | Unknown to Reddit to LA Times                  |


**Why are forgerties not more common?** I can think of three possible reasons:

1. It's difficult to forge credible documents -- e.g. even a simple memo is hard to fake because there are lots of small details like the letterforms and formatting and jargon used.
2. It's easy to forge credible documents but intelligence agencies and the media won't believe them without independent confirmation.
3. It's easy to forge credible documents and for them to get coverage but not many people are motivated to try.

**Older examples of forgeries.**

|                                                           |      |                                                               |
| --------------------------------------------------------- | ---- | ------------------------------------------------------------- |
| Mark Antony's will                                        | 33BC | read out by Octavian in the senate (disputed whether forgery) |
| Dreyfus letters sharing military info w Germany           | 1894 | fabricated by French military                                 |
| Protocols of Elders of Zion (Jewish plans for domination) | 1903 |                                                               |
| Castle Document, letter to British govt in Dublin         | 1916 | unclear source, contributed to 1916 Easter rising             |
| Zinoviev letter from Russia to UK Labour party            | 1924 | unclear source                                                |

**Notes.** 

- The Macron email leaks seemed to include both real and fake content ([ref](https://www.csis.org/analysis/successfully-countering-russian-electoral-interference)). In fact one report says that Macron's team sent each other outrageous implausible emails as a pre-emptive defense to make any subsequent leaks seem less credible ([ref](https://www.lawfareblog.com/macron-leaks-are-they-real-and-it-russia)).

- The Steele dossier isn't really a forgery: it doesn't purport to have a different author than its true author. The problem with the dossier is that the author makes knowingly false claims of fact. This .


#           Appendix: Forecasts from Metaculus [UNFINISHED]

**Each questions asks the probability of significant intentional damage caused with the use of AI.** 

**I think a reasonable starting point would be 50% of the historical base-rate of each type of damage.** Assume (1) AI will help attackers and defenders about equally, so total successful attacks will be constant; (2) AI will be used in 50% of successful attacks. Then we'd expect the future rate of AI-assisted successful attacks to be 50% of the historical rate of total successful attacks.

**There have been many substantial prior technological shifts:** postal service, photographs, newspapers, telegraph, telephone, television, internet. Each helped both defenders and attackers.

<!-- ## Will a deepfake cause damage & make the front page of a major news source in 2023?    10%

I can think of two cases of forged paper documents that were widely taken at face value in the US since 2000. The first seemed to be pretty influential, the second less so.

1. Fake letters from Iraq to Niger arranging for the acquisition of uraniam ("yellowcake" letters). These were acquired through the Italian intelligence agency, I think it's unclear who created them, and were prominently cited in US's justification for their invasion of Iraq.
2. A fake G W Bush military transcript that was reported in 2004 (the "Killian" documents).

Some other influential documents were based on lies, rather than forgeries, e.g. the Steele dossier.

The technology to create "deepfakes" started to emerge around 2018.
(Nancy Pelosi video)

## Will a deepfake be blamed by G20 politician for an election loss?         2025

## Will AI be used in an attack on infrastructure costing >$1B?           2025

## Will AI be used in a theft of intellectual property cost >$10M?        2025

## Will AI cause a stock exchange to halt trading for >24 hours?          2025

## Will AI be used in a major attack on voting systems in G20?            2025
 -->

|                                                                        | deadline | Metaculus        | me  |
| ---------------------------------------------------------------------- | -------- | ---------------- | --- |
| Will a deepfake cause damage & make front page of a major news source? | 2023     |                  |     |
| Will a deepfake be blamed by G20 politician for election loss?         | 2025     |                  |     |
| Will AI be used in an attack on infrastructure costing >$1B?           | 2025     |                  |     |
| Will AI be used in a theft of intellectual property cost >$10M?        | 2025     |                  |     |
| Will AI cause a stock exchange to halt trading for >24 hours?          | 2025     |                  |     |
| Will AI be used in a major attack on voting systems in G20?            | 2025     |                  |     |
| -----                                                                  |          |                  |     |
| Will a deepfake about politicial running for office get 2M+ views?     | 2018     | (resolved false) |     |

**Links:**

[In 2023 will a successful deepfake attempt causing real damage make the front page of a major news source?](https://www.metaculus.com/questions/14277/deepfake-attempt-in-major-news-in-2023/)

[Will a politician claim they lost a major election due to a "deepfake" image, video, or audio recording in a G20 country before 2025?](https://www.metaculus.com/questions/17180/deepfake-costs-election-before-2025/)

[Will a infrastructure disaster costing >$1B in a G20 country be widely attributed to an AI cyberattack before 2025?](https://www.metaculus.com/questions/17179/infrastructure-disaster-from-ai-before-2025/)

[Will a theft of >$10M of intellectual property be widely attributed to an AI cyberattack before 2025?](https://www.metaculus.com/questions/17178/ip-theft-from-ai-before-2025/)

[Will a stock exchange halt trading for >24 hours with a cause widely attributed to AI before 2025?](https://www.metaculus.com/questions/17176/ai-takes-down-stock-exchange-before-2025/)

[Will a major attack on voting systems in a G20 country be widely attributed to an AI before 2025?](https://www.metaculus.com/questions/17177/election-hack-before-2025/)

[Will a "Deepfake" video about a national U.S. political candidate running for office in 2018 get 2M+ views?](https://www.metaculus.com/questions/1335/will-a-deepfake-video-about-a-national-us-political-candidate-running-for-office-in-2018-get-2m-views/)

<!-- 
#           Model of AI and External Properties

**General:**

$$\begin{aligned}
   P(spam) &= \mu \\
   P(spam|\bm{x}) &= \frac{P(\bm{x}|spam)P(spam)}{P(\bm{x})} \\
      &= \frac{P(\bm{x}|spam)\mu}{P(\bm{x}|spam)\mu + P(\bm{x}|-spam)(1-\mu)}
\end{aligned}$$

**Now with conditionally independent binary features:**
$$\begin{aligned}
   P(x_i=1|spam) &= p \\ 
   P(x_i=1|nonspam) &= .5 \\
   P(spam|x_i=1) &= \frac{p\mu}{p\mu+.5(1-\mu)}\\
   P(spam|x_i=0) &= \frac{(1-p)\mu}{(1-p)\mu+.5(1-\mu)}\\\\
   P(m\text{ of }n|spam) &= \frac{p^m(1-p)^{n-m}\mu}{p^m(1-p)^{n-m}\mu+.5^m(1-\mu)}
\end{aligned}$$

**Claims:**

1. If receiver can add extra feature to their filter then discrimination gets better (obviously).
2. If sender can randomize some feature then discrimination gets much worse (in short run) and a little worse (in medium run).

**Want:**

1. **Internal property:** adding more features to both sides will make discrimination perfect, but in this baseline model it won't affect anything.
2. **External property:** adding more features on both sides will make discrimination worse. Yes, can do this, just distinguish between $n_s$, $n_r$, and $n_h$. As the sender and receiver models get better then they impinge on $n_h$. -->

<!-- 
#           References

##       Bender, Gebru, McMillan-Major, Mitchell (2021, FAccT) ["On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?"](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922)

- Discuss environmental cost.
> "LMs are not performing natural language understanding (NLU), and only have success in tasks that can be approached by manipulating linguistic form [14]. "

> "the tendency of human interlocutors to impute meaning where there is none can mislead both NLP researchers and the general public into taking synthetic text as meaningful."

> "Combined with the ability of LMs to pick up on both subtle biases and overtly abusive language patterns in training data, this leads to risks of harms, including encountering derogatory language and experiencing discrimination at the hands of others who reproduce racist, sexist, ableist, extremist or other harmful ideologies rein- forced through interactions with synthetic language."

**Coherence in the Eye of the Beholder.** It produces "apparently" coherent text but not really coherent.

**Risks and Harms.** Generally absorbing hegemonic worldview. E.g. (1) assuming doctor will be male, nurse female; (2) outputting abusive language; (3) could generate meaningless text & used, e.g., to recruit terrorists. / Especially this point: *apparent* fluency will mislead people into thinking that there's some genuine content. *"the human tendency to attribute meaning to text, in combination with large LMs’ ability to learn patterns of forms that humans associate with various bi- ases and other harmful attitudes, leads to risks of real-world harm."*

**Proposal.** Value-sensitive design. -->