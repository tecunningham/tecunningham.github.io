---
title: Recursive Self-Improvement
author: Tom Cunningham
date: today
draft: true
engine: knitr
execute:
  echo: false
  warning: false
  error: false
  cache: true # caches chunk output
editor:
  render-on-save: true
---
<style>
   h1 {  border-bottom: 4px solid black; }
   h2 {  border-bottom: 1px solid gray; padding-bottom: 0px; color: black; }
   dl {display: grid;}
   dt {grid-column-start: 1; width: 4cm;}
   dd {grid-column-start: 2; margin-left: 2em;}
</style>

#           Summary

TL;DR: an explosion is AI capabilities is *possible* but it's intrinsically difficult to forecasat.

: Define an explosion as a significant acceleration of the historical rates of progress in AI, e.g. measured by effective compute required for a given level of accuracy (e.g. perplexity).

Progress in AI is rapid but smooth.

: We can measure progress in various ways, algorithmic progress has accelerated over the last 10-15 years. Epoch estimate that algorithmic efficiency has been growing at 3X/year between 2012-2024, measured by effective compute.

Forecasters expect an 8-20% chance of an explosion.
: They define an explosion as a 3X speedup in the historical rate of progress by some measure, from the METR/FRI survey.

AI has already been accelerating AI research.

: AI has been self-accelerating for decades: (1) automatic differentiation; (2) bayesian optimization of hyperparameters; (3) neural architecture search; (4) LLM coding autocomplete and chatbots; (5) LLM coding agents.

There's good theoretical reason to expect an explosion.

: (...)

AI usefulness for optimization depends on features of the setup.

: Hassabis says AI will make progress wherever there's (1) combinatorial search space; (2) clear feedback; (3) lots of data or an automatic validator.

      However there's clearly a fourth condition: that the data has some lower-dimensional latent structure. There are many problems that satisfy the first 3 but where we don't expect substantial progress from autonomous NNs: (A) the telephone book; (B) mapping out stars in the sky; (C) documenting the genome.


AI speedups to discovery depends on the shape of the underlying landscape.

: (abc)


#           Models

AK model of recursive growth.

: Suppose we have $Y=AK^\alpha$, and also we use some share $s$ of output on R&D, which increases productivity $A=sY^\beta$, then if $\beta+\alpha>1$ we get a continuously increasing growth rate. Aghion, Jones and Jones (2017) elaborate on this model, where AI gradually is able to take over human tasks, & they get a similar condition.

: The critical question is whether the returns to R&D effort are sufficiently steep. Some classic papers: Bloom et al. "are we running out of ideas"; and Erdil. 

Landscape model.
: asdf



#           Figure / Evals

Q: what evals would be useful? Suppose we can plot the following:

```{tikz}
\begin{tikzpicture}[scale=1.2]
    % Draw axes
    \draw[->] (0,0) -- (5,0) node[right,align=left] {trainer-model ability\\(time horizon)};
    \draw[->] (0,0) -- (0,5) node[midway,above,rotate=90] {trained-model ability};
    
    \draw[thick, blue] (0,0) plot[domain=0:4, samples=100] (\x, {2*sqrt(\x)});
    
    \foreach \val/\pos in {2/0, 8/1, 16/2, 32/3, 64/4} {
        \draw (\pos,0) -- (\pos,-0.1);
        \node[below] at (\pos,-0.15) {\val};
    }
   
    %\draw[red] (0,2)--(4.5,2) node[right]{1 week};
    \draw[gray, very thin, dashed] (0,0) grid (4.5,4.5);
\end{tikzpicture}
```

We could plot data from a variety of different experiments on this plot, & ask some interesting questions:

1. At what time horizon do we forecast models beating human-level ability at training models?
2. Do we see grokking, i.e. discontinuous jumps in model-training ability?
3. Are some models better than others at model-training?


```{tikz}
\begin{tikzpicture}[scale=1.2]
    % Draw axes
    \draw[->] (0,0) -- (5,0) node[midway,below] {trainer-model ability};
    \draw[->] (0,0) -- (0,5) node[midway,above,rotate=90] {trained-model ability};

    \draw[thick,red] (0,0) -- (5,5);
    
    \draw[thick, blue] (0,0) -- (2,0) -- (5,6);
    \draw[gray, very thin, dashed] (0,0) grid (4.5,4.5); % grid
\end{tikzpicture}
```


#           References

FRI/METR forecasts of AI progress.
: (...)

Kortum (1997) ["Research, Patenting, and Technological Change"](https://egc.yale.edu/sites/default/files/Kortum_1997.pdf)
: He assumes that technological progress comes from taking random draws from a distribution (undirected search). Then growth over time will be characterized by the extreme value distribution of the underlying distribution, & returns to experience will depend on the thickeness of tails. Alternative assumptions:

      - Bounded: growth slows as it approaches a ceiling.
      - Exponential (thin tails): $A=\ln n$
      - Pareto (thick tails): $A = n^\gamma$

Erdil, Besiroglu, Ho (2024) ["Estimating Idea Production: A Methodological Survey"](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4814445)
: They discuss the difficulty of estimating the returns to R&D on productivity across a few different domains. They have nice estimates for chess inputs & outputs. They also estimate algorithmic progress:

      - Computer vision: doubling time 9 months 2012-2022.
      - RL sample efficiency: doubling time 11 months, 2015-2019.
      - SAT solvers: doubling time 2 years, 1997-2018.
      - Linear programming: doubling time 6 years, 1998-2018.

      They include a nice derivation of the critical threshold for hyperbolic growth:

      $$\begin{aligned}
         Y &= AK^\alpha \\
         \dot{K} &\propto Y && \text{(constant savings rate)}\\
         \dot{A}/A &\propto A^{-\beta}K^\lambda && \text{(productivity growth)}
      \end{aligned}$$

      You'll get steady-state growth if and only if $\lambda/\beta+\alpha=1$. You'll get hyperbolic growth if $\lambda/\beta+\alpha>1$.


Aghion, Jones, and Jones (2019) "Artificial Intelligence and Economic Growth"
: They give conditions under which automating R&D will cause an explosion in GDP growth (i.e. a steadily increasing rate of growth, AKA hyperbolic growth).


<!-- $$\begin{aligned}
         Y_t &= A_t^{\sigma} K_t^{\alpha} L^{1-\alpha} \\
         \dot A_t &= K_t^{\beta} S^{\lambda} A_t^{\phi}
      \end{aligned}$$

      In task-based automation interpretations, ($\alpha$) and ($\beta$) reflect the fractions of tasks automated in goods and idea production. Aghion, Jones and Jones analyze when AI-driven automation can yield accelerating growth (“Type I”) or even a singularity (“Type II”). in their Cobb–Douglas setup a key threshold is ($\gamma = \frac{\sigma}{1-\alpha}\cdot\frac{\beta}{1-\phi}$), with a Type II singularity when ($\gamma>1$). -->

David Owen (2024) ["Automation of AI R&D: Researcher Perspectives"](https://epoch.ai/files/Interviewing_AI_researchers_on_automation_of_AI_R_D.pdf)

Eric Drexler (2025) ["The Reality of Recursive Self-Improvement"](https://aiprospects.substack.com/p/the-reality-of-recursive-improvement)
: He argues that . Automating steps in the optimization loop:
      - Automating differentiation.
      - Neural architecture search.
      - Bayesian optimization.

      > "The fundamental mechanism is systemic friction reduction — aggregate improvements expand possibilities by enabling faster progress and more ambitious goals
      
      > "In hyperparameter optimization, advances in Bayesian and multi-fidelity methods often achieve order-of-magnitude savings compared to naive grid search. What once required thousands of full model trainings can now be accomplished with fewer and more intelligent probes. As daunting costs of innovation fall, research becomes faster and more ambitious.

      > "The trajectory toward comprehensive AI capabilities makes these developments predictable, not in detail, but in outline.



Eric Drexler (Aug 2025) ["The Reality of Recursive Self-Improvement"](https://aiprospects.substack.com/p/the-reality-of-recursive-improvement)

:  Automating steps in the optimization loop:

      - Automating differentiation.
      - Neural architecture search.
      - Bayesian optimization.

:  > "The fundamental mechanism is systemic friction reduction — aggregate improvements expand possibilities by enabling faster progress and more ambitious goals

    > "In hyperparameter optimization, advances in Bayesian and multi-fidelity methods often achieve order-of-magnitude savings compared to naive grid search. What once required thousands of full model trainings can now be accomplished with fewer and more intelligent probes. As daunting costs of innovation fall, research becomes faster and more ambitious.

    > "The trajectory toward comprehensive AI capabilities makes these developments predictable, not in detail, but in outline.




#       We need more bottom-up modelling of AI's economic effect [UNFINISHED]

There are two approaches to modelling AI's effect.

: From a lot of conversations about recursive self-improvement I realized it's useful to distinguish between two qualitatively different ways of thinking about AI's ability to do work:

      1. _Top down:_ AI replaces each of the human subtasks.
      2. _Bottom-up:_ AI just does the entire procedure from first principles.
   
      I think 80% of discussion of economic impacts was of the top-down type.

What would bottom-up modelling look like?
:    There are some papers with "model organisms" of recursive self-improvement: Grefenstette (1986) [genetic algorithm to learn parameters for genetic algorithsm](https://ui.adsabs.harvard.edu/abs/1986ITSMC..16..122G/abstract); [AutoML-Zero](https://arxiv.org/pdf/2003.03384) (2020); Schmidhuber (2003) [Godel Machines](https://arxiv.org/abs/cs/0309048?utm_source=chatgpt.com). 

What are the implications for recursive self-improvement?
: AI treats it as a pure optimization problem, already it's better at chip design, algorithm design.


Some related discussion:
: Nathan Lambert recapitulates some discussions from the Curve about recursive self-improvement [here](https://www.interconnects.ai/p/thoughts-on-the-curve).

    This is related to the Bresnahan/systems view of AI. He talks about the first wave of ML models: "The transition to ICT-based production has largely proceeded at the production system level, not the task level."[^bresnahan] 


[^bresnahan]: "My empirical conclusion about these applications is that the lazy idea of AI – that is, of computer systems that are able to perform productive tasks previously done by humans– is irrelevant to understanding how these technologies create value. Here “irrelevant” does not mean that substitution of machine for human tasks is less important than other determinants of the value in use of AITs. It means irrelevant: task-level substitution of machine for human plays no role in these highly valuable systems."


#          Overflow / Offcuts

Some optimization landscapes that were already conquered.
: - _People used to compile tables by doing manual calculations_ -- digits of pi, values for pi, tolerance ranges of metals, random numbers.
   - _Simplex optimization_ -- very many optimization problems can now be brute-forced. We already knew the algorithms, but it was slow to compute by hand. E.g. simplex.
   - (see a ChatGPT literature review I made on 2026-01-04)


| Domain / Problem                                                                                    | Pre-computer human method (what people did before; typical scale)                                                                                                          | Computer-era breakthrough (year + system/organization)                                                                                                                                                                                                 | What changed (order-of-magnitude scale/speed; new capability)                                                                                                                                               | Key sources (2–3)                                                                                                                                                                                                                            |
| --------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Census tabulation (large-scale counting & cross-tabs)                                               | Manual tally sheets + hand addition; limited ability to do many cross-tabulations; 1880 U.S. census tabulation famously took **>8 years**                                  | **1890**: U.S. Census Bureau adopts **Hollerith punched‑card electric tabulators**                                                                                                                                                                     | Mechanized “brute-force counting” (sort + count across huge card decks); basic totals moved from years to months; punched‑card tabulation became routine for decades (into the 1950s)                       | U.S. Census Bureau on Hollerith tabulation ([Census.gov][1]); IBM “Punch cards” history & 1880 vs 1890 timing ([Wikipedia][2]); Census.gov history note on long use into 1950s ([Census.gov][3])                                             |
| Cryptanalysis: **Enigma** daily key recovery (systematic key search under constraints)              | Human cryptanalysts built hypotheses (“cribs”), did hand checks / desk-calculator work; throughput limited and time‑sensitive                                              | **1940**: **Bletchley Park** deploys the (British) **Bombe** (Turing/Welchman concept; electro‑mechanical search)                                                                                                                                      | Turned a largely manual, fragile workflow into a mechanized search over Enigma settings consistent with a crib—key recovery became operationally scalable (when cribs/traffic conditions allowed)           | NSA history of the cryptanalytic bombe ; National Museum of Computing “Bombe Machine”                                                                                                                                                        |
| Cryptanalysis: **Lorenz SZ40/42 (“Tunny”)** wheel-setting search                                    | Before Colossus, attacks relied on laborious statistical/correlation work with earlier aids + significant human effort                                                     | **1944**: **Colossus** at Bletchley Park (Tommy Flowers / Post Office Research Station), programmable electronic machine for Tunny analysis                                                                                                            | High-speed automated counting/correlation tests greatly accelerated wheel-setting searches and made the method operational at wartime scale (often described as the first programmable electronic computer) | National Museum of Computing “The Colossus Computer” ([Investors.com][4]); National Museum of Computing “Colossus: the world’s first programmable computer” ([Computer Science][5])                                                          |
| Operations research: **Linear programming (LP)** optimization (simplex)                             | Desk calculators + accounting machines; even “toy” LPs were expensive—e.g., the 77‑variable diet LP took ~**120 person‑days** on desk calculators                          | **1952**: **Project SCOOP** (USAF/DoD context) uses **UNIVAC** in the Pentagon era to run LP; early practitioners report solving problems with **hundreds** of variables/constraints                                                                   | LP becomes a “push-button” computational optimization workflow rather than a bespoke hand calculation; feasible problem sizes jump from tens to hundreds (and later far more)                               | Dantzig biography (desk calculators/IBM accounting equipment context) ; Gill note on 77‑variable diet LP taking ~120 man‑days by hand ; Saul Gass interview recounting UNIVAC’s 1952 arrival + early LP scale (e.g., 250 eqns/500 vars)      |
| Operations research: **Integer programming** via cutting planes (exact discrete optimization)       | Small integer decisions often handled by hand enumeration or ad hoc reasoning; scaling was rapidly prohibitive                                                             | **1958–1960**: **Ralph Gomory** (RAND) introduces/implements **cutting‑plane methods**; early work implemented on **IBM 704**                                                                                                                          | Created a systematic exact method: solve LP relaxations, generate valid cuts, iterate—turning discrete optimization into a repeatable computational procedure                                               | Gomory’s 1958 “algorithm for integer solutions to linear programs” ; Gomory retrospective noting IBM 704 + FORTRAN implementation (summer 1958) ; RAND report on mixed‑integer algorithm (1960)                                              |
| Combinatorial optimization: **TSP** solved to optimality at modern scale (branch‑and‑cut)           | Humans could do only small instances exactly; early exact “proof” instances were dozens of cities (often with special structure/hand work)                                 | **2005–2006** (computation reported): **Concorde** branch‑and‑cut solves **pla85900 (85,900 cities)** to optimality; formal certification described in later paper                                                                                     | Exact TSP moves from tens/hundreds → **tens of thousands** with machine‑checkable certificates; “exact + certified” becomes routine for benchmark sets (TSPLIB)                                             | Concorde project page (largest TSPLIB solved = 85,900) ; pla85900 page (2005/06 computation context) ; Applegate et al. on certifying optimality for 85,900‑city TSP                                                                         |
| Combinatorial search: **SAT / automated theorem proving** (DPLL/DP → modern CDCL)                   | Human proof search/case analysis; limited scale for complex logical formulas and circuit constraints                                                                       | **1960**: Davis–Putnam procedure framed for machine proof search; **2001**: **Chaff** delivers **1–2 orders‑of‑magnitude** speedups vs prior complete solvers                                                                                          | SAT becomes a practical “black-box” combinatorial search engine; real instances from verification can reach **millions of variables / clauses**                                                             | Davis & Putnam (1960) original procedure PDF ; Chaff paper reporting 1–2 orders‑of‑magnitude speedups ; Zhang et al. noting verification instances with millions of variables/clauses                                                        |
| Mathematical case analysis: **Four‑color theorem** (finite unavoidable set + reducibility checking) | Purely human proofs stalled for a century; manual reducibility checking did not scale to the needed case set                                                               | **1976–1977**: **Appel & Haken** (Univ. of Illinois) complete the proof by reducing to a large finite set and checking reducibility by computer (papers published 1977)                                                                                | Enabled exhaustive checking of **thousands** of configurations—often cited as the first major theorem proved with extensive computer assistance; counts reported vary (e.g., 1,936 vs later 1,482)          | Illinois exhibit overview (timeline + “finite but large number of cases” + computer role) ; Appel–Haken Part I (1977) paper (primary) ; note on 1,482‑member unavoidable set (secondary)                                                     |
| Scientific least squares / estimation: **orbit determination & trajectory prediction**              | “Human computers” using desk calculators (e.g., Friden-era workflows) integrated trajectories and iteratively corrected orbits from tracking data—slow and labor intensive | **Early 1960s**: **Project Mercury** uses **IBM 7090** for trajectory computation and real‑time tracking/prediction; **1963** report documents orbit‑determination/prediction programs; JPL deep‑space navigation evolves orbit determination programs | Orbit estimation becomes fast enough for mission operations (frequent updates, higher data volume, more systematic differential correction/least‑squares style pipelines)                                   | IBM history on Mercury using 7090s for trajectory parameters + real‑time tracking/prediction ; NASA/NTRS “Orbit Determination and Prediction, and Computer Programs” (1963) ; JPL navigation history (orbit determination program evolution) |
| Cryptanalysis: **DES** exhaustive key search (demonstration attack)                                 | By hand, exhaustive search is impossible; early “massive software” efforts showed feasibility but still took many days                                                     | **1998**: EFF builds **DES Cracker (“Deep Crack”)** hardware; cracks RSA’s DES Challenge II in **<3 days**; built for **< $250,000**                                                                                                                   | Made brute‑force key search demonstrably practical/affordable; shifted the security narrative for DES and reinforced the need for stronger standards                                                        | EFF project page (cost < $250k; <3 days; beat 39‑day prior record) ; *Cracking DES* book (technical design + purpose) ; NIST AES development history citing EFF’s “Cracking DES”                                                             |
| Game solving: **Checkers** (exhaustive retrograde analysis + proof search)                          | Human masters analyzed openings/endgames, but no proof of the game-theoretic value; endgame knowledge limited to manageable subsets                                        | **2007**: Schaeffer et al. (Chinook / Univ. of Alberta) announce **checkers is solved** (perfect play → draw) using retrograde analysis and related proof techniques                                                                                   | Replaced human analysis with a computer‑verified solution and large computed databases; established a definitive game-theoretic result                                                                      | *Science* “Checkers Is Solved” (primary) ; hosted PDF copy (same article)                                                                                                                                                                    |
| Game-tree search: **Chess** (Deep Blue)                                                             | Humans explored variations mentally; even strong humans can’t enumerate anywhere near machine scale; early engines were far weaker due to limited search                   | **1997**: **IBM Deep Blue** defeats Kasparov; architecture combines parallelism + custom chess chips; reports include ~**200 million positions/sec** scale                                                                                             | Brute-force/alpha–beta style search at massive throughput became sufficient for world‑champion performance, making deep tactical exploration routine for machines                                           | IBM history page (200M positions/sec; brute-force framing) ; Hsu (IEEE Micro, 1999) on Deep Blue chips & system evolution ; Campbell et al. technical overview (Deep Blue system/search rates)                                               |

[1]: https://www.census.gov/about/history/bureau-history/census-innovations/technology/hollerith-machine.html?utm_source=chatgpt.com "The Hollerith Machine"
[2]: https://en.wikipedia.org/wiki/Cryptanalysis_of_the_Lorenz_cipher?utm_source=chatgpt.com "Cryptanalysis of the Lorenz cipher"
[3]: https://www.census.gov/about/history/stories/monthly/2016/january-2016.html?utm_source=chatgpt.com "January 2016: Herman Hollerith and Mechanical Tabulation"
[4]: https://www.investors.com/news/management/leaders-and-success/herman-hollerith-pioneered-data-processing/?utm_source=chatgpt.com "Herman Hollerith Put A Punch Into Data Processing"
[5]: https://cs.stanford.edu/people/eroberts/courses/soco/projects/2008-09/colossus/history.html?utm_source=chatgpt.com "The History of the Lorenz Cipher and the Colossus Machine"



#              Davidson, Halperin, Houlden, Korinek "When Does Automating Research Produce Explosive Growth?"


##       summary

- Goal: forecast when we're likely to get explosive intelligence growth.

- Summary: once you automate a sufficient number of tasks (do them with capital) then it'll kick off an intelligence explosion.

- Basic model:
   - If $\dot{S}_t=S_t^{1-\beta}$, so you get explosion if $\beta<0$, steady-state growth if $\beta=0$, and gradual slowing if $\beta>1$.
   - Now if you start automating inputs, the effective exponent gradually gets higher.

- Multi-sector version.
   - You gradually automate some of the things, so they can be done with capital.
   - You have spillovers between different processes.

- Estimates of $\beta$:
   - Bloom overall $\beta=3$
   - For software R&D they estimate $\beta=3$
   - they say in software $\beta=0.1$

- NOTE: $\beta$

- Questions:
   - Any historical domain where we've seen regimes of $\beta<0$?


##       my observations


- Q: how to think about the growth effect of uplift vs automation?
 
- Automation makes things *free*, rather than being produced by capital.



##       tom houlden questions

Q: is this graphical system novel?

Q: is there a nice metaphor/analogy to think about automation/explosion?




#                chart


```mermaid
flowchart LR
  RD["R&D"] --> algorithms
  RD_compute["R&D compute"] --> algorithms
  algorithms --> intelligence
  compute --> intelligence
  data --> intelligence
  intelligence -->|??| economic_value["economic value"]
  intelligence -->|??| RD
```


