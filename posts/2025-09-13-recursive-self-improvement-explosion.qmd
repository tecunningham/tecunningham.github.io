---
title: Recursive Self-Improvement
author: Tom Cunningham
date: today
draft: true
editor:
  render-on-save: true
---
<style>
   h1 {  border-bottom: 4px solid black; }
   h2 {  border-bottom: 1px solid gray; padding-bottom: 0px; color: black; }
   dl {display: grid;}
   dt {grid-column-start: 1; width: 4cm;}
   dd {grid-column-start: 2; margin-left: 2em;}
</style>

#           Summary

TL;DR: an explosion is AI capabilities is *possible* but it's intrinsically difficult to forecasat.

: Define an explosion as a significant acceleration of the historical rates of progress in AI, e.g. measured by effective compute required for a given level of accuracy (e.g. perplexity).

Progress in AI is rapid but smooth.

: We can measure progress in various ways, algorithmic progress has accelerated over the last 10-15 years. Epoch estimate that algorithmic efficiency has been growing at 3X/year between 2012-2024, measured by effective compute.

Forecasters expect an 8-20% chance of an explosion.
: They define an explosion as a 3X speedup in the historical rate of progress by some measure.

AI has already been accelerating AI research.

: AI has been self-accelerating for decades: (1) automatic differentiation; (2) bayesian optimization of hyperparameters; (3) neural architecture search; (4) LLM coding autocomplete and chatbots; (5) LLM coding agents.

There's good theoretical reason to expect an explosion.

: If AI increases AI, then it will.

AI speedups to discovery depends on the.


#           Models

AK model of recursive growth.

: Suppose we have $Y=AK^\alpha$, and also we use some share $s$ of output on R&D, which increases productivity $A=sY^\beta$, then if $\beta+\alpha>1$ we get a continuously increasing growth rate. Aghion, Jones and Jones (2017) elaborate on this model, where AI gradually is able to take over human tasks, & gets a similar condition.

: The critical question is whether the returns to R&D effort are sufficiently steep. Some classic papers: Bloom et al. "are we running out of ideas"; and Erdil. 



#           References

FRI/METR forecasts of AI progress.
: 

Kortum (1997) ["Research, Patenting, and Technological Change"](https://egc.yale.edu/sites/default/files/Kortum_1997.pdf)
: He assumes that technological progress comes from taking random draws from a distribution (undirected search). Then growth over time will be characterized by the extreme value distribution of the underlying distribution, & returns to experience will depend on the thickeness of tails:
   - Bounded: (hits a ceiling)
   - Exponential (thin tails): $A=\ln n$
   - Pareto (thick tails): $A = n^\gamma$

Erdil, Besiroglu, Ho (2024) ["Estimating Idea Production: A Methodological Survey"](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4814445)
: They discuss the difficulty of estimating the returns to R&D on productivity across a few different domains. They have nice estimates for chess inputs & outputs. They also estimate algorithmic progress:

      - Computer vision: doubling time 9 months 2012-2022.
      - RL sample efficiency: doubling time 11 months, 2015-2019.
      - SAT solvers: doubling time 2 years, 1997-2018.
      - Linear programming: doubling time 6 years, 1998-2018.

      They include a nice derivation of the critical threshold for hyperbolic growth:

      $$\begin{aligned}
         Y &= AK^\alpha \\
         \dot{K} &= \propto Y && \text{(constant savings rate)}\\
         \dot{A}/A \propto A^{-\beta}K^[\lambda] && \text{(productivity growth)}
      \end{aligned}$$

      You'll get steady-state growth if and only if $\lambda/\beta+\alpha=1$. You'll get hyperbolic growth if $\lambda/\beta+\alpha>1$.


Aghion, Jones, and Jones (2019) "Artificial Intelligence and Economic Growth"
: They give conditions under which automating R&D will cause an explosion in GDP growth (i.e. a steadily increasing rate of growth, AKA hyperbolic growth).

Eric Drexler (2025) ["The Reality of Recursive Self-Improvement"](https://aiprospects.substack.com/p/the-reality-of-recursive-improvement)
: He argues that . Automating steps in the optimization loop:
      - Automating differentiation.
      - Neural architecture search.
      - Bayesian optimization.

      > "The fundamental mechanism is systemic friction reduction — aggregate improvements expand possibilities by enabling faster progress and more ambitious goals
      
      > "In hyperparameter optimization, advances in Bayesian and multi-fidelity methods often achieve order-of-magnitude savings compared to naive grid search. What once required thousands of full model trainings can now be accomplished with fewer and more intelligent probes. As daunting costs of innovation fall, research becomes faster and more ambitious.

      > "The trajectory toward comprehensive AI capabilities makes these developments predictable, not in detail, but in outline.



