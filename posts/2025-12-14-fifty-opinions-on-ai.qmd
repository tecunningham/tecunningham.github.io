---
title: Fifty Opinions on Economics of AI
draft: true
bibliography: ai.bib
reference-location: section
---
<style>
   h2 { text-align: center; border-bottom: 8px solid black; padding-below: 0px; margin-below: 0px; }
   dl {display: grid; grid-template-columns: max-content auto;
       }
   dt {grid-column-start: 1; width: 5cm;  padding-bottom: 30px;
      margin-right: 0px;
      padding-right: 5px;
      border-top: 1px solid black; }
   dd {grid-column-start: 2; margin-left: 2em;  padding-bottom: 30px; 
      margin-left: 0px;
      padding-left: 5px;
      border-top: 1px solid black; }
</style>

<!-- TODO: ADD ALL CURVE OPINIONS -->

I do my best to give reasons, but some have just bubbled up from my unconscious & now I can't find reasons to dislodge them. I have tried to write not about the discourse but about the world. It's always tempting to complain about other peoples' mistakes, I've given in to that temptation in the last section.

##    Today's impact of AI

The welfare impact in 2025 is around 1% of GDP.
: The impact is through (A) using chatbots to solve everyday problems (home production); (B) increased productivity at work.

      The former doesn't show up in measured GDP at all, the latter will only partly be reflected in GDP because it primarily increases productivity in services, and much of the GDP of services is imputed from the input costs (i.e. wages paid to service-providers).

The welfare impact has been increasing at around 4X/year.
: Adoption has been growing around 2X/year, and the value/user has been growing around 2X/year. This growth can be decomposed into growth in capabilities and diffusion, I'd say it's been roughly 50-50.

##    What LLMs do

Neural nets extract the low-dimensional structure of the world.
: Our pre-conscious brains extract low-dimensional representations from high-dimensional inputs. We have finally we've taught computers to do the same thing (AKA the manifold hypothesis, @bengio2013representation).

LLMs imitate human answers to questions.
: A simple model of LLMs is that they provide answers to questions, based on a large training set of questions & answers. This has a number of implications:

      1. LLMs will be used for *new-to-you* questions, i.e. questions that you have not encountered before, but someone else has.

      2. LLMs will be used for tasks which are *routine* in the sense that there's relatively little variation, and input space is relatively low-dimensional.

      3. The performance of an LLM on a task depends not on the intrinsic properties of that task, but just on the density of similar tasks in the training data (@vonwerra2025jaggeddata).

      4. LLMs exhibit superhuman performance ("transcendence") when the task requires combining the answers to two questions (@cunningham2023imitation, @abreu2025taxonomytranscendence).

LLMs are worse at generalization than humans.
: The model above treated knowledge as binary: either you know the answer or you don't. We can extend this to extrapolation, and it seems clear that

      1. LLMs are less sample-efficient learners than humans
      2. LLMs are relatively worse at questions that are farther from their training data (compared to humans).

We are moving from human-supervised to world-supervised AI.
: LLMs very quickly caught up with human abilities because they're trained on human judgments (human generated text, paid rater judgment, LLM user preferences). This led to a plateau around expert-human level.

      However we can hook them up directly to the real world & they can learn from that, & so we are starting to see them break out of that plateau. (A nice analogy: suppose you play chess by looking up each position in the history of recorded games, and otherwise playing randomly; then you will rapidly become a competent player, but then hit a ceiling).

      The properties you need to make progress on world-supervised problems: (1) cheap real-world feedback; (2) .

Unlike humans, computers can use their representations to synthesize new artefacts.
: Human judgment has a notable asymmetry between recognition and production. There are many properties of objects which we can immediately recognize, but it's far more difficult to synthesize a new object that has that property. We can judge whether a joke is funny, or a poem rhymes, or a picture looks realistic, but we find it far harder to create a funny joke, a rhyming poem, or a realistic picture. This is sometimes attributed to the hierarchical and feed-foward nature of how the brain processes information.

      Artificial neural nets do not suffer from the same degree of asymmetry. They can be reversed to synthesize new objects that satisfy a given property. This means computers will be able to do qualitatively different things that humans cannot do, even with the same amount of knowledge.

LLM capabilities are highly correlated.
: Many studies find that benchmark scores are highly correlated across LLMs. This could be due for *supply* reasons or *demand* reasons.

LLM capabilities are qualitatively different from human capabilities.
: The crispest representation is that LLMs can pass all exams, and pass all interview questions, but they cannot do the associated jobs. There is still a substantial bottleneck. Loosely I would characterize the bottleneck as failure to generalize, difficulty with off-distribution tasks. Concretely this manifests in weakness in unusual situations, interactive decision-making, or continual learning.

##    Economic Effects of AI

LLMs make private knowledge public.
: We can apply the question-answering model of LLMs described above to economic equilibrium. Suppose each person has a set of questions which they know the answer to, & this determines their economic capabilities (i.e. they earn rents off their private knowledge). If we treat LLMs as pooling all that private knowledge to make it public, this has clear implications: (A) greater welfare; (B) lower inequality; (C) less trade (i.e. lower GDP).

Market power is not a big deal.
: I think you could get a good approximation of the static economic effect of AI assuming that the market is competitive, i.e. the competition questions are second-order. The market today is highly competitive: there's a small gap in capabilities among the frontier labs, and open models are, at worst, 12 months behind. It's possible that one lab will jump ahead, but it seems reasonable to treat the market as competitive as an approximation. 

AI companies will capture a small share of the surplus.
: This follows from the above. It's easy to fall into the trap of thinking "a lot of value is being created, so the AI companies must be getting that value."

Baumol effects are not a big deal.
: ABC

We cannot direct technical change.
: .

Jobs don't decompose tasks with observable outputs.
: It's notable that the vast majority of labor is paid by *inputs* by *outputs*. I.e. work is mostly paid by the hour, or by the month, not paid by output (performance pay, piece rate). This implies that observing peoples' output is difficult, and so it's likely that .

: Why is observing output difficult? (...)

Demand will shift towards intelligence over knowledge.
: LLMs make knowledge cheap.

AI will benefit poorer countries relatively more.
: AI has a relatively larger effect on the cost of producing services than on physical goods, so it will change the terms of trade in favor of poorer countries.

GDP as it's currently measured will fall.
: A consequence: tax revenues will fall without changes to how taxes are levied.

Real wages will fall.
: 

The relative value of land will rise.
: 

Occupations with additively separable outputs will get replaced.
: 

Demand for human-produced goods is limited.
: 

AI will significantly advance science.
: My very basic model of science: over thousands of years we've been gradually uncovering deeper and simpler latent structures of the world, progressively adding them to the textbooks. In retrospect those structures are obvious, it required the flash of insight.

##    Safety Effects

##    Social Effects

It will dissolve the glue that holds everything together.
: abc

Preferences will change in hard-to-predict ways.
: Tastes are labile -- what you choose as an adult is highly influenced by what you saw people choosing when you were a child. AI suddenly opens up the landscape of what we can get, but it's hard to predict where we will end up because choices themselves have their own effects.

Computers will be able to make better art than humans.

:  The success of an artwork is often judged by its ability to satisfy multiple different criteria simultaneously, for example a good picture both (a) depicts a specific scene, and (b) is a harmonious arrangement of colours. A good poem both (a) tells a story, (b) rhymes, and (c) scans. A good piece of music is like a sudoku puzzle -- simultaneously satisfying many constraints. It's easy for humans to recognize when an artwork successfully satisfies multiple criteria but hard to create a new artwork that satisfies those criteria. Neural nets allow computers to learn to represent the characteristics of artworks, and so it seems plausible they would become better than humans in being able to find permutations that satisfy them.^[Computers have been better than humans at solving well-specified combinatorial problems for 70 years. The difference with newer generations of algorithms is that they can now learn categories that humans learn unconsciously: harmony, rhythm, representation, semantics, etc.]

The effect on adversarial problems: help the bad guys more than the good guys.

:  In adversarial situations like spam-detection computers will help both spammers and spam-detectors. However they seem likely to shift the balance in favor of the spammers because of the relative advantage computers have at synthesizing over recognizing content (compared to humans). Humans can intuitively discriminate between real and fake artefacts, but that ability will become less valuable when computers have learned the cues that humans use to make those discriminations.

Communication will rely relatively more on *provenance* than on *content*.

:  When evaluating a message we can use signals either from the content or the provenance (i.e. where the message come from). Many communication systems already rely heavily on provenance, e.g. spam detection relies heavily on the identity of the sender. As generative ML improves I think we should expect equilibrium to move even further in the direction of provenance and reputation and away from content (see my [post](https://tecunningham.github.io/posts/2023-06-06-effect-of-ai-on-communication.html)).

The impact of AI on a domain will depend on the statistical structure of that domain.
: (see ai-big-picture and )

Giving people more power could be good or bad.
: The basic argument in favor of AI is that it allows us to better achieve our ends ("solves hard problems"). In turn, historically having a longer lever has led to better things â€“ more people, living longer, & more comfortably.

Our nominal ends are not our actual ends.
: Strauss, Berlin, MacIntyre.

##    What Research is Needed

We are constrained more on theory than on data.
: We have many projects which are collecting data on AI impacts, we can organize them into a waterfall, from top to bottom:

      1. **Data on AI capabilities** -- benchmarks that have representative tasks across the economy  - e.g. GDP-val (@patwardhan2025gdpval), APEX.
      2. **Data on AI uplift** -- effect on productivity, e.g. @becker2025uplift.
      3. **Data on AI adoption** -- adoption by occupations, by industry, by demographic, E.g. @bick2024rapid.
      4. **Data on AI usage** -- what types of economic tasks are LLMs used for, e.g. @handa2025economicindex, @chatterji2025chatgpt.
      5. **Data on AI economic effects** -- changes in hiring and wages by occupation, e.g. @brynjolfsson2025canaries.

      Each of these is relatively *unopinionated*, they try to canvas AI impacts in general. But just collecting data isn't very useful without theory.


Someone should organize a conference of junior people working on transformative AI.
: 

We need to unbundle labor.
: .

Randomized trials have limited value.
: 

We need more work on the offense-defense balance

: There are dozens of cases where there's some offense-defense balance, and it's no immediately clear how AI will affect that balance. Some examples that came up in the Curve:

      - hacking
      - ransomware
      - spearfishing
      - media manipulation
      - drone assassinations
      - drone warfare. 
   
    In each case it's clear that AI could help both sides, but arguable how the equilibrium will be affected.

We should have some common theory.
: It seems wasteful to treat each of these problems independently, there ought to be some general principles we can apply on how AI will affect offense-defense balance.

      The closest I know is @garfinkel2019offensedefense. The argue that when both sides get sufficiently strong then this will generally tend to favor the defender:

      > "we offer a general formalization of the offense-defense balance in terms of contest success functions. Simple models of ground invasions and cyberattacks that exploit software vulnerabilities suggest that, in both cases, growth in investments will favor offense when investment levels are sufficiently low and favor defense when they are sufficiently high."

      I also have a [note from 2023](https://tecunningham.github.io/posts/2023-06-06-effect-of-ai-on-communication.html), which argues that AI will favor the defender for "internal" properties (where human judgment is the ground truth), but favor the attacker for "external" properties (where external reality is the ground truth).

      This seems an incredibly fertile area for economic theory but I have seen very little enagement from economists.

##    Opinions on the Discourse



Discourse is tangled because we don't have a theory of capabilities.
: Many disagreements .

Most disagreement about economic growth is really about capabilities growth.
: 

We have no canonical model of AI's impact on the economy.
: 

LLMs expose some holes in classical economics.
: We can only explain the success of NNs & LLMs by talking about the low-dimensional and high-dimensional structure of the world.

Knife-edge explosion conditions are bogus.
: sadf

Having explicit forecasts would be very useful.
: What do we expect AI to do to these things?

      - Wages & employment, across sectors and tenure.
      - The price of land, the value of the capital stock.
      - Incomes across different countries.

      I feel it's like early 2020 and COVID: if we're trying to make a decision about whether to announce a lock-down it should be based on a clear idea about the counterfactual, which includes a lot of equilibrium effects.

