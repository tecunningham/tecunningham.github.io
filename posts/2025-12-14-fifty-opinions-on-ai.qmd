---
title: Fifty Opinions on Economics of AI
draft: true
bibliography: ai.bib
reference-location: section
---
<style>
   h2 { text-align: center; border-bottom: 0px; padding-below: 0px; margin-below: 0px; }
   dl {display: grid; grid-template-columns: max-content auto;
       }
   dt {grid-column-start: 1; width: 5cm;  padding-bottom: 30px;
      margin-right: 0px;
      padding-right: 5px;
      border-top: 1px solid black; }
   dd {grid-column-start: 2; margin-left: 2em;  padding-bottom: 30px; 
      margin-left: 0px;
      padding-left: 5px;
      border-top: 1px solid black; }
</style>

<!-- TODO: ADD ALL CURVE OPINIONS -->

I do my best to give reasons, but some have just bubbled up from my unconscious & now I can't find reasons to dislodge them. I have tried to write not about the discourse but about the world. It's always tempting to complain about other peoples' mistakes, I've given in to that temptation in the last section.

##    Today's impact of AI

The welfare impact is around 1/2% of GDP.
: The impact is through (A) using chatbots to solve everyday problems (home production); (B) increased productivity at work. The former doesn't show up in GDP at all, the latter will only partly be reflected in GDP because it primarily increases productivity in services, and we don't have a good way of measuring the quantity or quality of services produced (we impute it from the wages paid to the service-providers).

The welfare impact has been increasing at around 4X/year.
: Adoption has been growing around 2X/year, and the value/user has been growing around 2X/year. This growth can be decomposed into growth in capabilities and diffusion, I'd say it's been roughly 50-50.

##    What LLMs do

Neural nets extract the low-dimensional structure of the world.
: Our pre-conscious brains extract low-dimensional representations from high-dimensional inputs. We have finally we've taught computers to do the same thing (AKA the manifold hypothesis, @bengio2013representation).

LLMs make private knowledge public.
: Chatbots can be trained to produce answers to questions, given some corpus of existing questions with known answers (a mix of pretraining, paid raters, & user preference data). 

      Now that computers can perceive the latent low-dimensional structure of the world, we can train them to answer factual questions. However they will be limited just to the facts that included in their training set (or those which they can retrieve using tools, and common sense.

      This changes the access to knowledge & we can make some basic predictions. Suppose each person has a set of questions which they know the answer to, & this determines their economic capabilities. People have different knowledge-sets, & so they trade. We can treat LLMs as pooling all that private knowledge to make it public. The implications: (A) greater welfare; (B) lower inequality; (C) less trade.

LLMs are worse at generalization than humans.
: We can extend the knowledge model above. It seems clear that LLMs (1) are less sample-efficient learners than humans; (2) are relatively worse at questions that are farther from their training data. Suppose you encounter a new question, you don't know the answer to.

LLMs are temporarily stuck at a ceiling of human ability.
: LLMs very quickly caught up with human abilities because they're trained on human judgments (human generated text, paid rater judgment, LLM user preferences). However we can hook them up directly to the real world & they can learn from that, & so we should expect them to surpass the ceiling soon. (A nice analogy: suppose you play chess by looking up each position in the history of recorded games, and otherwise playing randomly; then you will quickly get to a pretty reasonable level of skill, but you hit a ceiling).

Unlike humans, computers can use their representations to synthesize new artefacts.
: Human judgment has a notable asymmetry between recognition and production. There are many properties of objects which we can immediately recognize, but it's far more difficult to synthesize a new object that has that property. We can judge whether a joke is funny, or a poem rhymes, or a picture looks realistic, but we find it far harder to create a funny joke, a rhyming poem, or a realistic picture. This is sometimes attributed to the hierarchical and feed-foward nature of how the brain processes information.

      Artificial neural nets do not suffer from the same degree of asymmetry. They can be reversed to synthesize new objects that satisfy a given property. This means computers will be able to do qualitatively different things that humans cannot do, even with the same amount of knowledge.

LLM capabilities are highly correlated.
: Many studies find that benchmark scores are highly correlated across LLMs. This could be due for *supply* reasons or *demand* reasons.

LLM capabilities are qualitatively different from human capabilities.
: The crispest representation is that LLMs can pass all exams, and pass all interview questions, but they cannot do the associated jobs. There is still a substantial bottleneck. Loosely I would characterize the bottleneck as failure to generalize, difficulty with off-distribution tasks. Concretely this manifests in weakness in unusual situations, interactive decision-making, or continual learning.

We cannot direct technical change.
: .



##    Future Economic Effects

Market power is not a big deal.
: I think you could get a good approximation of the static economic effect of AI assuming that the market is competitive, i.e. the competition questions are second-order. The market today is highly competitive: there's a small gap in capabilities among the frontier labs, and open models are, at worst, 12 months behind. It's possible that one lab will jump ahead, but it seems reasonable to treat the market as competitive as an approximation. 

AI companies will capture a small share of the surplus.
: This follows from the above. It's easy to fall into the trap of thinking "a lot of value is being created, so the AI companies must be getting that value."

Baumol effects are not a big deal.
: ABC

Demand will shift towards intelligence over knowledge.
: LLMs make knowledge cheap.

AI will benefit poorer countries relatively more.
: AI has a relatively larger effect on the cost of producing services than on physical goods, so it will change the terms of trade in favor of poorer countries.

GDP as it's currently measured will fall.
: A consequence: tax revenues will fall without changes to how taxes are levied.

Real wages will fall.
: 

The relative value of land will rise.
: 

Occupations with additively separable outputs will get replaced.
: 

Demand for human-produced goods is limited.
: 

AI will significantly advance science.
: My very basic model of science: over thousands of years we've been gradually uncovering deeper and simpler latent structures of the world, progressively adding them to the textbooks. In retrospect those structures are obvious, it required the flash of insight.

##    Social Effects

It will dissolve the glue that holds everything together.
: abc

Preferences will change in hard-to-predict ways.
: Tastes are labile -- what you choose as an adult is highly influenced by what you saw people choosing when you were a child. AI suddenly opens up the landscape of what we can get, but it's hard to predict where we will end up because choices themselves have their own effects.

Computers will be able to make better art than humans.

:  The success of an artwork is often judged by its ability to satisfy multiple different criteria simultaneously, for example a good picture both (a) depicts a specific scene, and (b) is a harmonious arrangement of colours. A good poem both (a) tells a story, (b) rhymes, and (c) scans. A good piece of music is like a sudoku puzzle -- simultaneously satisfying many constraints. It's easy for humans to recognize when an artwork successfully satisfies multiple criteria but hard to create a new artwork that satisfies those criteria. Neural nets allow computers to learn to represent the characteristics of artworks, and so it seems plausible they would become better than humans in being able to find permutations that satisfy them.^[Computers have been better than humans at solving well-specified combinatorial problems for 70 years. The difference with newer generations of algorithms is that they can now learn categories that humans learn unconsciously: harmony, rhythm, representation, semantics, etc.]

The effect on adversarial problems: help the bad guys more than the good guys.

:  In adversarial situations like spam-detection computers will help both spammers and spam-detectors, but they seem likely to shift the balance in favor of the spammers because of the relative advantage computers have at synthesizing over recognizing content (compared to humans). Humans can intuitively discriminate between real and fake artefacts, but that ability will become less valuable when computers have learned the cues that humans use to make those discriminations.

The effect on communication: greater reliance on *provenance* than on *content*.

:  When evaluating a message we can use signals either from the content or the provenance (i.e. where the message come from). Many communication systems already rely heavily on provenance, e.g. spam detection relies heavily on the identity of the sender. As generative ML improves I think we should expect equilibrium to move even further in the direction of provenance and reputation and away from content (see my [post](https://tecunningham.github.io/posts/2023-06-06-effect-of-ai-on-communication.html)).

The impact of AI on a domain will depend on the statistical structure of that domain.
: (see ai-big-picture and )

Giving people more power could be good or bad.
: The basic argument in favor of AI is that it allows us to better achieve our ends ("solves hard problems"). In turn, historically having a longer lever has led to better things â€“ more people, living longer, & more comfortably.

Our nominal ends are not our actual ends.
: Strauss, Berlin, MacIntyre.

##    What Research is Needed

We are constrained more on theory than on data.
: 

Someone should organize a conference of junior people working on transformative AI.
: 

We need to unbundle labor.
: .

Randomized trials have limited value.
: 

We need more work on offense-defense balance.
: 

##    Opinions on the Discourse

Discourse is tangled because we don't have a theory of capabilities.
: Many disagreements .

Most disagreement about economic growth is really about capabilities growth.
: 

We have no canonical model of AI's impact on the economy.
: 

LLMs expose some holes in classical economics.
: We can only explain the success of NNs & LLMs by talking about the low-dimensional and high-dimensional structure of the world.

Knife-edge explosion conditions are bogus.
: sadf

