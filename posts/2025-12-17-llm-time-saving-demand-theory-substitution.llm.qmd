---
title: "LLM Time-Saving, Demand Theory, and Task Activation"
author: "Tom Cunningham (METR)"
date: today
draft: true
citation: true
reference-location: document
bibliography: ai.bib
format:
  html:
    toc: true
    toc-depth: 3
execute:
  echo: false
  warning: false
  error: false
  cache: true
---

<!-- https://tecunningham.github.io/posts/2025-12-17-llm-time-saving-demand-theory-substitution.llm.html -->

<style>
  dl {display: grid;}
  dt {grid-column-start: 1; width: 18em;}
  dd {grid-column-start: 2; margin-left: 1.25em;}
</style>

## Results-first summary

Speedups do not mechanically translate into aggregate productivity. Once LLMs change the relative time costs of tasks, optimal time allocation changes, and the aggregate lift depends on substitution and (often) task activation.

Motivating questions:

Overall lift is not pinned down by a share and a conditional speedup alone
: If tasks that get sped up initially take share $s$ of time and get speedup factor $A$ when you do them, then an arithmetic share calculation gives $1+s(A-1)$. That expression is correct only for a knife-edge estimand where you hold the task mix fixed. If you let the person reoptimize, $s$ is endogenous and the total lift depends on substitution.

Allowing reallocation makes the estimand a demand-theory object
: The object of interest is the best attainable output index under a unit time endowment,
  $$
  v(p)\;=\;\max_{x\ge 0} u(x)\quad\text{s.t.}\quad \sum_i p_i x_i\le 1,
  $$
  where $p_i$ is time per unit of task output and $u(x)$ aggregates task outputs into an overall notion of productivity. LLM speedups $\beta_i$ shift time prices to $p_i'=p_i/\beta_i$, and the productivity gain is $v(p')/v(p)$.

Substitutability governs how much time share moves when a task becomes cheap
: In a continuous model, if tasks are close substitutes then time shifts strongly toward the sped-up tasks and the aggregate lift can be much larger than $1+s(A-1)$. If tasks are strong complements, time shares are stable and the aggregate lift is closer to the share calculation. In parametric cases this is summarized by an elasticity; outside parametric cases it is a demand-curve object.

Observing post-LLM shares can identify the elasticity in a CES benchmark
: In a two-good CES setting, observing both pre and post time shares for an AI-affected bundle, together with the relevant speedup, identifies the substitution elasticity via a logit-share formula. This makes post-LLM shares especially informative when you want a point estimate.

With unit-demand task choice, you can bound the lift using pre and post task baskets
: If tasks are lumpy and you either do a task or not, and you observe the time required for each task pre and post AI, then you can bound the aggregate lift by evaluating the pre-AI chosen basket at post-AI time prices (a Laspeyres-style bound) and the post-AI chosen basket at post-AI time prices (a Paasche-style bound). The discrete section formalizes this.

## Setup: time prices and speedups

Setup in time-allocation form
: Let $t=(t_1,\dots,t_n)$ be time allocated to tasks with $\sum_i t_i=1$. Let $A=(A_1,\dots,A_n)$ be task-specific productivity, so that the effective inputs to production are $(A_1 t_1,\dots,A_n t_n)$. Let the output index be $y(A_1 t_1,\dots,A_n t_n)$, where $y$ is increasing in each argument.

Equivalent time-price form
: Define task outputs as $x_i=A_i t_i$. Then the time constraint becomes $\sum_i (1/A_i)x_i\le 1$, so $p_i\equiv 1/A_i$ is time per unit of task output. An LLM that multiplies productivity on task $i$ by $\beta_i$ maps to $A_i'=\beta_i A_i$ and $p_i'=p_i/\beta_i$. In what follows I use the time-price notation because it lines up directly with expenditure functions and price-index results.

Two margins matter
: The continuous model treats $x$ as divisible and focuses on intensive reallocation. The discrete model treats the task set as lumpy (setup time or unit demand) and focuses on which tasks get activated.

Continuous intensive margin means smooth substitution
: Choose continuous $x_i$ and allow interior reallocation.

Discrete extensive margin means task activation and selection
: Choose which tasks to do at all (unit demand or setup costs).

I treat these separately because the logic, formulas, and data requirements diverge.

## Estimation cheat sheet

This section is a “lookup table” for turning *measured* speedups into a productivity lift $v(p')/v(p)$ under common assumptions.

### Two-good reduction (AI-affected bundle vs the rest)

For many back-of-envelope calculations, you can collapse tasks into:

- good 2: the set of tasks whose time cost drops by $\beta$ when AI is allowed,
- good 1: everything else (normalized).

Let $s_0$ be the pre-AI time share on good 2 (i.e. $s_0 \equiv p_2 x_2 / (p_1 x_1+p_2 x_2)$ evaluated at baseline prices), and suppose only $p_2$ changes to $p_2' = p_2/\beta$.

Then common benchmarks are:

| Assumption about substitution | Implied output gain $v(p')/v(p)$ | Notes |
|---|---|---|
| Perfect complements (Amdahl-style) | $\dfrac{1}{(1-s_0)+s_0/\beta}$ | Fixed proportions; AI only relaxes the bottleneck |
| Cobb-Douglas ($\varepsilon=1$) | $\beta^{s_0}$ | Share is constant; log change is $s_0\ln\beta$ |
| CES elasticity $\varepsilon$ | $\left((1-s_0)+s_0\,\beta^{\varepsilon-1}\right)^{\frac{1}{\varepsilon-1}}$ | Requires $\varepsilon$ (or enough data to infer it) |
| Perfect substitutes | $\beta$ | You can reallocate everything to the sped-up bundle |

In the multi-task case, a common log-index approximation is the Tornqvist/Divisia form
$$
\Delta\ln v \;\approx\; \sum_i \bar s_i\,\ln \beta_i,\qquad \bar s_i \equiv \tfrac12(s_i^0+s_i^1),
$$
which uses average (pre/post) time shares. @caves1982indexnumbers

Anthropic-style numbers as a sanity check
: If $s_0=0.10$ and $\beta=5$ (80% time reduction when AI is used), the implied gain ranges from:

- complements bound: $1/(0.9+0.02)\approx 1.087$ (8.7%),
- Cobb-Douglas: $5^{0.1}\approx 1.174$ (17.4%),
- CES with $\varepsilon=2$: $0.9+0.1\cdot 5 = 1.4$ (40%),
- substitutes bound: $5$ (400%).

The spread here is the point: large conditional speedups do *not* translate to a unique aggregate lift without additional structure or data.

### If you observe pre and post time shares, you can infer an elasticity (in CES)

In the same two-good CES setting, let $s_1$ be the post-AI time share on good 2. Then
$$
\operatorname{logit}(s_1)-\operatorname{logit}(s_0)=(\varepsilon-1)\ln \beta,
$$
so
$$
\varepsilon \;=\; 1 + \frac{\operatorname{logit}(s_1)-\operatorname{logit}(s_0)}{\ln\beta}.
$$

Here $\operatorname{logit}(s)\equiv \ln\!\left(\frac{s}{1-s}\right)$.

This is often the cleanest way to use both pre and post shares: estimate $\varepsilon$, then plug into the CES gain formula. (And if you do *not* believe CES globally, you should treat this as local-to-the-observed price change.)

### Estimation flowchart (what to do with your data)

```{mermaid}
flowchart TD
  A["Goal: estimate productivity lift<br/>v(p_new)/v(p_old)<br/>for fixed time endowment"] --> B{"Are tasks mostly divisible<br/>at the relevant margin?"}

  B -->|Yes: continuous| C{"Are speedups small<br/>or broad-based?"}
  C -->|Yes| D["Use share-weighted log changes:<br/>d ln v ~= sum_i s_i d ln beta_i<br/>(Tornqvist/Divisia)"]
  C -->|No| E{"Do you have enough data to pin down substitution?<br/>e.g. CES elasticity, demand system"}
  E -->|Yes| F["Compute an exact/parametric index:<br/>P(p)=e(p,1), v=1/P<br/>CES closed form when applicable"]
  E -->|No| G["Trace a demand curve:<br/>vary effective AI cost/strength;<br/>estimate compensated shares; integrate"]

  B -->|No: setup or unit-demand| H["Model extensive margin:<br/>which tasks get activated?"]
  H --> I["Unit-demand tasks: bound the lift<br/>Laspeyres lower bound: pre-AI basket at post-AI time prices<br/>Paasche upper bound: post-AI basket at post-AI time prices"]
```

### What you can estimate with what data

| What you can measure | Recommended move | What you can credibly report |
|---|---|---|
| One big conditional speedup $\beta$ + a baseline share $s_0$ | Report complements/substitutes bounds; add CES sensitivity | A wide interval for $v(p')/v(p)$ unless you assume $\varepsilon$ |
| Pre and post shares $(s_0,s_1)$ for an AI bundle + a speedup $\beta$ | Use the logit formula to estimate $\varepsilon$; plug into CES gain | A model-based point estimate (local to the observed change) |
| Multiple randomized “AI price/quality” arms + observed shares | Estimate share response vs $\ln\beta$; integrate shares over the change | An “area under the (compensated) demand curve” estimate for large changes |
| Choice/activation data (tasks attempted) under multiple AI prices | Model activation thresholds / setup costs explicitly | Decomposition into intensive and extensive margins |

## Continuous (intensive-margin) model

Homotheticity makes the lift the inverse of a time price index
: The primal problem is
: $$
: v(p)\;=\;\max_{x\ge 0} u(x) \quad\text{s.t.}\quad \sum_i p_i x_i\le 1.
: $$
: Define the expenditure function
: $$
: e(p,\bar u)=\min_{x\ge 0} \Big\{\sum_i p_i x_i: u(x)\ge \bar u\Big\}.
: $$
: If $u(\cdot)$ is homothetic and degree-1 homogeneous, then $e(p,\bar u)=\bar u\,e(p,1)$. Define the unit time price index
: $$
: P(p)\equiv e(p,1)\quad\Rightarrow\quad v(p)=\frac{1}{P(p)}.
: $$
: This is the classic index-number framing applied to time prices (Caves, Christensen, and Diewert, @caves1982indexnumbers).

EV and CV translate time-price changes into welfare changes in time units
: Let $p^0\to p^1$ and $u^k=v(p^k)$. Equivalent and compensating variation (measured in time) are
: $$
: EV=e(p^0,u^1)-1,\qquad CV=e(p^1,u^0)-1.
: $$
: Under homotheticity,
: $$
: EV=\frac{P(p^0)}{P(p^1)}-1,\qquad CV=\frac{P(p^1)}{P(p^0)}-1.
: $$
: This is a clean way to translate LLM time savings into a welfare measure (Hausman, @hausman1981exact).

Small changes admit a share-weighted approximation
: Let $t_i(p)\equiv p_i x_i^*(p)$ be optimal time shares. For small changes in time prices,
: $$
: d\ln v\;=\;-d\ln P\;\approx\;\sum_i t_i\,d\ln \beta_i.
: $$
: This is the time-allocation analog of share-weighted Hulten-style logic (Hulten, @hulten1978growth).

Large changes require integrating compensated shares
: When LLM gains are large, constant-elasticity approximations are dangerous. Using Hicksian (compensated) shares $s_i^H(p)$,
: $$
: d\ln P(p)=\sum_i s_i^H(p)\,d\ln p_i.
: $$
: For a single changing price $p_2$,
: $$
: \ln\frac{P(p^1)}{P(p^0)}=\int s_2^H(p_2)\,d\ln p_2,
: $$
: i.e., exact welfare is the area under the compensated demand curve (Willig, @willig1976consumerssurplus).

CES yields closed-form formulas that are useful for back-of-envelope work
: For a CES aggregator
  $$
  u(x)=\left(\sum_i \alpha_i x_i^{\frac{\sigma-1}{\sigma}}\right)^{\frac{\sigma}{\sigma-1}},\quad\sigma>0,
  $$
  the price index and time shares are
  $$
  P(p)=\left(\sum_i \alpha_i^{\sigma}p_i^{1-\sigma}\right)^{\frac{1}{1-\sigma}},\qquad
   t_i(p)=\frac{\alpha_i^{\sigma}p_i^{1-\sigma}}{\sum_j \alpha_j^{\sigma}p_j^{1-\sigma}}.
  $$
  In the two-task case, if task 2 speeds up by $\beta$ and its ex-ante share is $s_0$, then
  $$
  \frac{y'}{y}=\left((1-s_0)+s_0\,\beta^{\varepsilon-1}\right)^{\frac{1}{\varepsilon-1}},\qquad \varepsilon\equiv\sigma.
  $$
  This is the clean continuous benchmark (Caves, Christensen, and Diewert, @caves1982indexnumbers).

#### Two-good CES yields closed-form gain and share shift

The output gain has a closed form in terms of $s_0,\beta,\varepsilon$
: In a two-good CES benchmark where only good 2 speeds up by $\beta$ (so $p_2'=p_2/\beta$),
  $$
  \frac{v(p')}{v(p)}=\left((1-s_0)+s_0\,\beta^{\varepsilon-1}\right)^{\frac{1}{\varepsilon-1}}.
  $$

The post-AI share shift pins down $\varepsilon$ when $s_1$ is observed
: The post-AI time share is
  $$
  s_1=\frac{s_0\,\beta^{\varepsilon-1}}{(1-s_0)+s_0\,\beta^{\varepsilon-1}}
  \quad\Leftrightarrow\quad
  \operatorname{logit}(s_1)-\operatorname{logit}(s_0)=(\varepsilon-1)\ln\beta.
  $$

::: {.callout-note collapse="true"}
## Proof (structured)

1. *Given* CES unit-expenditure (time price) index
   $$
   P(p)=\left(\alpha_1^{\varepsilon}p_1^{1-\varepsilon}+\alpha_2^{\varepsilon}p_2^{1-\varepsilon}\right)^{\frac{1}{1-\varepsilon}},
   \qquad v(p)=\frac{1}{P(p)}.
   $$
2. *Let* $p_2' = p_2/\beta$ with $p_1$ fixed. Then
   $$
   \frac{v(p')}{v(p)}=\frac{P(p)}{P(p')}=
   \left(
     \frac{\alpha_1^{\varepsilon}p_1^{1-\varepsilon}+\alpha_2^{\varepsilon}p_2^{1-\varepsilon}}
          {\alpha_1^{\varepsilon}p_1^{1-\varepsilon}+\alpha_2^{\varepsilon}(p_2/\beta)^{1-\varepsilon}}
   \right)^{\frac{1}{\varepsilon-1}}.
   $$
3. *Define* the ex-ante share
   $$
   s_0\equiv\frac{\alpha_2^{\varepsilon}p_2^{1-\varepsilon}}
                  {\alpha_1^{\varepsilon}p_1^{1-\varepsilon}+\alpha_2^{\varepsilon}p_2^{1-\varepsilon}}.
   $$
   Substituting into Step 2 yields the claimed gain formula.
4. *For shares,* note CES time shares satisfy
   $$
   \frac{s}{1-s}=\frac{\alpha_2^{\varepsilon}p_2^{1-\varepsilon}}{\alpha_1^{\varepsilon}p_1^{1-\varepsilon}}.
   $$
   Therefore,
   $$
   \frac{s_1/(1-s_1)}{s_0/(1-s_0)}
   =
   \frac{(p_2')^{1-\varepsilon}}{p_2^{1-\varepsilon}}
   =
   \left(\frac{p_2/\beta}{p_2}\right)^{1-\varepsilon}
   =
   \beta^{\varepsilon-1},
   $$
   which rearranges to the stated closed form for $s_1$ and the logit identity.
5. QED.
:::

## Discrete (extensive-margin) model

Task activation makes selection central
: The continuous model assumes you always do some of each task. That is wrong when tasks are lumpy, have setup costs, or are unit-demand. In those cases, LLMs can create newly affordable tasks, meaning the major effect is selection (which tasks get done at all), not intensive reallocation.

### Unit-demand formulation

Let each task have payoff $u_i$ and required time $w_i(p)$, with decision $q_i\in\{0,1\}$. Then
$$
\max_{q\in\{0,1\}^n}\sum_i u_i q_i\quad\text{s.t.}\quad \sum_i w_i(p) q_i\le 1.
$$

Speedups can turn tasks on at extensive-margin thresholds
: Speedups change $w_i$ by $\beta_i$, which can turn tasks on once a threshold is crossed. This is the basic lumpy-choice phenomenon: tasks that were too time-expensive become attractive after the LLM. A CES elasticity is not a good summary statistic in this regime.

Pre and post task baskets give Laspeyres and Paasche bounds on the lift
: Suppose you observe (i) the pre-AI chosen task vector $q^0$ and the post-AI chosen task vector $q^1$, and (ii) the per-task time requirements before and after AI, denoted $w_i^0$ and $w_i^1$. Define the time cost of a basket as $T^k(q)\equiv \sum_i w_i^k q_i$.
:
: Consider the fixed-basket lifts
: $$
: G_L \equiv \frac{T^0(q^0)}{T^1(q^0)} \qquad\text{and}\qquad
: G_P \equiv \frac{T^0(q^1)}{T^1(q^1)}.
: $$
: Under standard revealed-preference logic for a cost-of-living index (with time as the numeraire), the true time-price index lies between Paasche and Laspeyres, so the productivity lift lies between their inverses:
: $$
: G_L \;\le\; \frac{v(p^1)}{v(p^0)} \;\le\; G_P.
: $$
: Intuitively, $G_L$ is a conservative "hold the old basket fixed" estimate (lower bound), while $G_P$ allows both the basket and prices to shift (upper bound).

One useful empirical way to think about the “tasks you previously avoided” condition is to decompose time cost into a baseline component plus an “overhead” component (search, reading, drafting, refactoring). For example, write
$$
w_i=a_i+b_i,
$$
and suppose AI primarily reduces overhead, $b_i' = b_i/\beta^{O}$ while $a_i$ is unchanged. Then
$$
w_i' = a_i + \frac{b_i}{\beta^{O}},\qquad
\beta_i \equiv \frac{w_i}{w_i'} = \frac{a_i+b_i}{a_i+b_i/\beta^{O}},
$$
so $\beta_i$ is increasing in the overhead share $b_i/(a_i+b_i)$. Tasks you previously avoided plausibly have high overhead shares, so they get larger effective speedups and are more likely to cross activation thresholds.

### Setup-cost variant (bridging discrete and continuous)

Add a fixed setup time $\phi_i$ and a continuous intensity $x_i$:
$$
\max_{q,x}\;u(x)\quad\text{s.t.}\quad \sum_i \phi_i q_i + \sum_i p_i x_i \le 1,\; x_i=0\;\text{if }q_i=0.
$$

If $\phi_i=0$, we recover the continuous model. If $\phi_i>0$, large LLM speedups mostly expand the active set $\{i:q_i=1\}$, not the intensive shares.

### Worked example (discrete, not continuous)

Discrete choice does not identify a single time-savings without more structure
: Suppose you can pick one task (unit demand). Task A yields value $u_A=10$ and takes 1 hour. Task B yields $u_B=12$ and takes 2 hours. Without LLMs you choose A. Now an LLM speeds up task B so it takes 1 hour, and you switch to B. The observed switch is consistent with a wide range of time-equivalent gains:
  - upper bound: 1 hour (if the extra value $u_B-u_A$ is worth a full hour),
  - lower bound: 0 hours (if the extra value is only a small quality bump).

So the *observed* reallocation does not identify a precise time-savings without modeling discrete choice. This is why constant-elasticity summaries can be weak in the activation regime.

### Newly activated tasks

Definition of newly activated tasks
: Call a task newly activated if you would not do it at baseline time prices but you do once its time cost drops.

- literature reviews you previously would not attempt,
- custom data visualizations,
- long-form proofreading or refactoring.

These show up as extensive-margin choices, not smooth intensities
: In a unit-demand or setup-cost model, these tasks show up as newly activated $q_i=1$ choices, not as marginal increases in $x_i$. This is why AI share of time can jump even if underlying preferences are stable: the feasible set changed.

## Applications

### Application 1: from query-level time savings to an aggregate lift (Anthropic)

Anthropic (@anthropic2025estimatingproductivitygains) estimates time savings from Claude conversations by comparing time required with vs without AI for a sample of tasks drawn from usage logs.

Suppose the headline inputs (illustrative, in the spirit of the writeup) are:

- Claude is used for $s_0=10\%$ of baseline work (measured pre-AI / from logs),
- conditional time reduction is 80%, i.e. speedup $\beta=5$ when AI is used.

Then even in a two-good continuous approximation, the implied aggregate lift depends sharply on substitution.

| Assumption about substitution | What it means in this note | Assumption on $\varepsilon$ | Implied lift when $s_0=10\%$ and $\beta=5$ |
|---|---|---|---|
| Strong complements (Amdahl/Leontief-style) | Time shares do not move much when a subset of tasks speeds up | effectively $\varepsilon\to 0$ | about $+8.7\%$ |
| Cobb-Douglas benchmark | Unit elasticity, moderate reallocation | $\varepsilon=1$ | about $+17.4\%$ |
| CES example with stronger substitution | Time shifts strongly toward the sped-up bundle | $\varepsilon=2$ | about $+40\%$ |

The empirical question is therefore not only how big $\beta$ is when AI is used, but how time shares respond to the relative time-price change. In the CES benchmark that response is summarized by $\varepsilon$; outside CES, it is a demand-curve object.

Two concrete complications show up immediately in a log-based setting:

1. Endogenous task mix (extensive margin). If AI reduces time costs more for tasks you previously avoided (high search/reading/writing overhead), then the pre-AI share $s_0$ understates the mass of tasks that become attractive post-AI.
2. Quality-adjusted output. Some measured time savings are quality improvements (or vice versa). If your output index treats quality as part of output, you need a quality measure to map time changes into $u(x)$.

### Application 2: interpreting “uplift” RCTs (METR / open-source dev)

Becker et al. (@becker2025uplift) is an RCT where tasks are assigned to allow vs disallow AI tools, and the outcome is completion time (with additional self-reports and robustness checks). The headline is that AI access increases completion time on average in their setting.

From the demand-theory perspective:

- A simple back-of-envelope translation: if allowing AI increases completion time by 19%, then the implied “speedup” is $\beta \approx 1/1.19 \approx 0.84$ (a slowdown). Holding output constant, that’s about a 16% productivity hit on that task population.
- With a fixed task set (as in many RCTs), the main object you learn is a task-level time-price change $p_i\to p_i'$ (a $\beta_i$ distribution), holding task composition fixed.
- Translating that into an aggregate productivity index still requires an output/quality index: are we holding output constant (pure time saved), holding time constant (more output), or letting both adjust? In practice you often want quality-adjusted output per unit time.
- If, in future settings, AI access changes *which* tasks are attempted, then the design has to either (i) fix tasks (to isolate intensive effects) or (ii) explicitly allow selection and measure it (to capture the extensive margin).

## Experimental design

Designing studies that map $\{\beta_i\}$ into an aggregate lift $v(p')/v(p)$ is mostly about (a) measuring substitution, and (b) separating intensive vs extensive margins.

1. Decide the estimand up front. Same tasks, faster (hold $x$ fixed) is different from best use of a fixed time budget (maximize $u(x)$). The latter matches the substitution estimand in this note.
2. Run two complementary protocols.
   - Fixed-task protocol (intensive margin): randomize AI availability within a pre-specified task list; measure time and quality on each task.
   - Time-budget protocol (substitution/selection): give participants a fixed time budget and a menu (or open-ended objective); randomize AI availability; measure the output index achieved under each condition.
3. Measure quality explicitly. Use blinded human evaluation, unit tests, or objective metrics; pre-register how quality enters $u(x)$ (e.g. threshold vs continuous scoring).
4. Trace a demand curve by varying effective AI price. Randomize not only AI allowed, but the effective cost of using AI (token budgets, latency, model quality tiers, usage quotas). This gives variation to estimate how shares move with relative time prices (the object behind $\varepsilon$ or, more generally, Hicksian shares).
5. Handle learning and spillovers. Use cross-over designs, washout periods, or between-subject randomization where appropriate; measure experience and allow for dynamic treatment effects.

## Related literature (more explicit)

This note touches several literatures that are often cited separately. Here is a reading map that matches the objects used above (price indices, time allocation, discrete activation, and empirical measurement).

### Index numbers and exact welfare from price changes (continuous case)

Konus-style "true" cost-of-living indices are expenditure-function objects
: Konus (@konus1939trueindex) defines a cost-of-living index as the ratio of minimum expenditures needed to reach a fixed utility level at two price vectors. This is exactly the object $P(p)=e(p,1)$ (with time prices instead of money prices) that makes $v(p)=1/P(p)$ under homotheticity.

Exact/superlative indices justify share-weighted log changes as local approximations
: Diewert (@diewert1976exact) formalizes when index numbers (including Tornqvist/Divisia-type log-share formulas) are exact for flexible functional forms and motivates using share-weighted log changes as a local approximation.

Productivity measurement via index numbers is a duality result
: Caves, Christensen, and Diewert (@caves1982indexnumbers) is a central reference for exact index-number measurement of input, output, and productivity, and for the duality framing used in the continuous model here.

Large changes motivate integrating compensated demand
: Willig (@willig1976consumerssurplus) and Hausman (@hausman1981exact) connect equivalent/compensating variation and consumer surplus to integrals of compensated demand, clarifying why large AI shocks call for area under Hicksian demand rather than constant-elasticity shortcuts.

Demand systems are a practical route to Hicksian shares and welfare
: Deaton and Muellbauer (@deaton1980aids) introduce AIDS, a workhorse integrable demand system used to estimate demand and welfare effects of price changes in practice.

### Time allocation, information costs, and "overhead" time

Time allocation treats time as a fundamental constraint
: Becker (@becker1965allocation) frames time allocation as a core economic problem, and is a natural ancestor of treating task durations as time prices.

Time requirements can be modeled as shadow-price wedges
: DeSerpa (@deserpa1971time) emphasizes that goods require time to consume/produce and develops shadow-price implications close to the time-price interpretation here.

Many LLM speedups look like information/search cost reductions
: Stigler (@stigler1961information) treats costly search as central; empirically, many LLM speedups look like reductions in search/reading/writing overhead rather than reductions in core task time.

Tasks can be interpreted as bundles of characteristics with shifted implicit prices
: Lancaster (@lancaster1966consumer) gives an alternative lens: tasks bundle characteristics (information, drafting, formatting, reasoning), and AI shifts the implicit prices of those characteristics.

### Discrete activation, selection, and welfare with lumpy choices

Discrete choice has its own welfare tools
: Small and Rosen (@smallrosen1981welfare) is a classic reference for welfare analysis with discrete choice, directly relevant for unit-demand tasks and activation thresholds.

Applied discrete-choice estimation provides empirical tools for substitution
: Train (@train2003discretechoice) is a standard applied reference for estimating random-utility/discrete-choice models when you want to quantify substitution and welfare.

### Task-based technological change and aggregation

Task-based technological change aligns with a task-as-goods framing
: Autor, Levy, and Murnane (@autor2003skill) and Acemoglu and Autor (@acemoglu2011handbook) popularize task-based views of technology, conceptually aligned with treating tasks as goods with relative time prices.

Small shocks justify share-weighted approximations in many environments
: Hulten (@hulten1978growth) is the backbone for why $\Delta\ln v\approx \sum_i s_i\,\Delta\ln\beta_i$ can be reasonable for small or broad shocks.

Large shocks can break share-weighted logic
: Baqaee and Farhi (@baqaee2019macro) is a canonical reference on nonlinearities and large shocks and connects directly to why large AI shocks motivate demand-curve integration and careful treatment of selection.

Micro-to-macro aggregation depends on details of substitution
: Oberfield and Raval (@oberfield2021micro) is a useful reference on linking micro evidence on technology to macro implications.

CES is a workhorse benchmark for closed-form substitution
: Dixit and Stiglitz (@dixit1977monopolistic) sits behind many closed-form substitution formulas, including the two-good CES benchmark used above.

### Empirical AI productivity and usage measurement

Log-based time-savings evidence is informative but incomplete for an aggregate lift
: Anthropic (@anthropic2025estimatingproductivitygains) illustrates the appeal (and limits) of measuring conditional time savings and baseline shares in observational usage data.

RCTs with fixed tasks identify intensive effects but not substitution
: Becker et al. (@becker2025uplift) exemplifies randomized access to AI tools with completion-time outcomes, highlighting the importance of quality measurement and task selection.

Early gen-AI productivity studies vary in settings and mechanisms
: Noy and Zhang (@noy2023generative) and Brynjolfsson, Li, and Raymond (@brynjolfsson2023generative) are widely cited early studies with evidence on productivity and heterogeneity.

Usage composition matters for mapping micro speedups to aggregate lifts
: Chatterji et al. (@chatterji2025chatgpt) document usage patterns and task composition for ChatGPT, relevant when the task mix is endogenous.

Valuing time savings connects to the broader digital-goods valuation literature
: Varian (@varian2011economic) and Collis and Brynjolfsson (@collis2025welfare) are representative of work valuing digital services and time savings, complementary to the demand-theory framing here.
