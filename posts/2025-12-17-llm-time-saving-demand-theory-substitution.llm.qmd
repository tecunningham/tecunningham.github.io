---
title: "LLM Time-Saving, Demand Theory, and Task Activation"
author: "Tom Cunningham (METR)"
date: today
draft: true
citation: true
reference-location: document
bibliography: ai.bib
format:
  html:
    toc: true
    toc-depth: 3
execute:
  echo: false
  warning: false
  error: false
  cache: true
---

<!-- https://tecunningham.github.io/posts/2025-12-17-llm-time-saving-demand-theory-substitution.llm.html -->
<!-- Author note: review tools/test_llm_time_saving_qmd.py for tests and run it after edits. -->

```{python}
#| echo: false
#| results: asis
from __future__ import annotations

import subprocess
import sys
from pathlib import Path

from IPython.display import Markdown, display

# Quarto executes notebooks with a CWD that can differ from the repo root.
# Find the repo root by walking upward until we see both `tools/` and `posts/`.
repo_root: Path | None = None
for candidate in [Path.cwd(), *Path.cwd().parents]:
    if (candidate / "tools").is_dir() and (candidate / "posts").is_dir():
        repo_root = candidate
        break
if repo_root is None:
    repo_root = Path.cwd()

script_path = repo_root / "tools/test_llm_time_saving_qmd.py"

proc = subprocess.run(
    [sys.executable, str(script_path), "--no-llm"],
    capture_output=True,
    text=True,
    cwd=str(repo_root),
)
out = (proc.stdout or "").rstrip()
err = (proc.stderr or "").rstrip()
if not out and err:
    out = f"(stderr)\n{err}"
if proc.returncode != 0:
    out = f"[exit {proc.returncode}]\n{out}"

hide_lines = [
    "Programmatic: required sections present",
    "Programmatic: estimation flowchart present",
    "Programmatic: applications present",
    "Programmatic: experimental design present",
    "Programmatic: Mermaid renders via mermaid.ink (optional)",
]
filtered_lines: list[str] = []
for line in out.splitlines():
    if any(needle in line for needle in hide_lines):
        continue
    filtered_lines.append(line)
out = "\n".join(filtered_lines).strip()

md = (
    '<details class="validation-checklist">\n'
    "<summary>Validation Checks</summary>\n\n"
    "```text\n"
    f"{out}\n"
    "```\n"
    "</details>\n"
)
display(Markdown(md))
```

<style>
  dl {display: grid;}
  dt {grid-column-start: 1; width: 18em;}
  dd {grid-column-start: 2; margin-left: 1.25em;}
  details.validation-checklist {
    background: #f5f5f5;
    border: 1px solid #777;
    border-radius: 6px;
    padding: 0.5em 0.75em;
    margin-bottom: 1em;
  }
  details.validation-checklist > summary {
    cursor: pointer;
  }
</style>

<!--
Editing note: definition lists + multiline math

In Quarto, a definition list item starts with exactly one leading colon:

Term (a short claim)
: First paragraph of the definition.
  Continuation lines must be indented (e.g. two spaces).
  $$
  Multiline display math goes here.
  $$

Do NOT prefix continuation lines with additional ':' characters; that often breaks math rendering.
-->

#        Summary

We want to know the effect of AI-speedups on output.
: 

Many good:

- General Hicksian demand
- General CES
- For fixed allocation
- Lower and upper bounds from observing $A$ or $A'$.

Two-good case.



#        Setup

You allocate time between tasks.
: Let $t=(t_1,\dots,t_n)$ be time allocated across tasks with $\sum_i t_i=1$. Let $A=(A_1,\dots,A_n)$ be task-specific productivity (effective output per unit time on each task). Let the output index be
   $$y(A_1 t_1,\dots,A_n t_n),$$
   where $y(\cdot)$ is increasing in each argument.
  
    We assume people allocate time across tasks optimally, so the substitution-adjusted productivity level is:
    $$V(A)\equiv \max_{t\ge 0}\; y(A_1 t_1,\dots,A_n t_n)\quad\text{s.t.}\quad \sum_i t_i\le 1.$$
   An LLM changes productivity from $A$ to $A'$, and so the lift is $V(A')/V(A)$.


This is identical to a consumption problem.
: Write effective task output as $z_i\equiv A_i t_i$. Then
  $$
  V(A)=\max_{z\ge 0}\; y(z)\quad\text{s.t.}\quad \sum_i \frac{z_i}{A_i}\le 1.
  $$
  Define time prices $p_i\equiv 1/A_i$. This turns the constraint into $\sum_i p_i z_i\le 1$, so standard expenditure-function and index-number results apply. Define the unit-expenditure function $P(p)\equiv e(p,1)$. Under homotheticity, the productivity level is
  $$
  V(A)=\frac{1}{P(1/A)}.
  $$
  This is the classic index-number framing applied to time prices (see @caves1982indexnumbers).

EV and CV translate productivity changes into welfare changes in time units.
: Let $A\to A'$ and define $p\equiv 1/A$ and $p'\equiv 1/A'$. Equivalent and compensating variation (measured in time) are
  $$
  EV=e(p, V(A'))-1,\qquad CV=e(p', V(A))-1.
  $$
  Under homotheticity,
  $$
  EV=\frac{P(p)}{P(p')}-1,\qquad CV=\frac{P(p')}{P(p)}-1,
  $$
  which is a clean way to translate time-saving into a welfare measure (see @hausman1981exact).

Small changes admit a share-weighted approximation.
: Let $t_i(A)$ denote optimal time shares. For small changes,
  $$
  d\ln V \;\approx\;\sum_i t_i(A)\,d\ln A_i.
  $$
  This is the time-allocation analog of share-weighted Hulten-style logic (see @hulten1978growth).

Large changes require integrating compensated shares.
: When gains are large, constant-elasticity shortcuts are dangerous. Using Hicksian (compensated) shares $s_i^H(p)$ at fixed output,
  $$
  d\ln P(p)=\sum_i s_i^H(p)\,d\ln p_i.
  $$
  For a single changing price (equivalently, a single changing productivity component),
  $$
  \ln\frac{P(p')}{P(p)}=\int s_2^H(p_2)\,d\ln p_2,
  $$
  i.e., exact welfare is an area-under-the-compensated-demand-curve object (see @willig1976consumerssurplus).

CES yields closed-form formulas useful for back-of-the-envelope work.
: For a CES aggregator in $z$,
  $$
  y(z)=\left(\sum_i \alpha_i z_i^{\frac{\sigma-1}{\sigma}}\right)^{\frac{\sigma}{\sigma-1}},\quad\sigma>0,
  $$
  the associated price index and (compensated) shares have closed form. In a two-good reduction where only good 2 gets a productivity multiplier $A_2$ and its ex-ante time share is $t_2$,
  $$
  \frac{V(A')}{V(A)}=\left((1-t_2)+t_2\,A_2^{\varepsilon-1}\right)^{\frac{1}{\varepsilon-1}},\qquad \varepsilon\equiv\sigma.
  $$
  This is the clean continuous benchmark (see @caves1982indexnumbers).

In the multi-task case, a Tornqvist/Divisia log-index is a convenient approximation.
: A common approximation (using optimal time shares) is
  $$
  \Delta\ln V \;\approx\; \sum_i \bar t_i\,\ln\left(\frac{A_i'}{A_i}\right),\qquad \bar t_i \equiv \tfrac12(t_i+t_i'),
  $$
  which uses average (pre/post) time shares (see @caves1982indexnumbers).


#     Two Good Case

Collapse tasks into an AI-affected bundle and the rest.
: Define two bundles: good 2 is the set of tasks whose productivity rises by a factor $A_2$ when AI is allowed, and good 1 is everything else (normalized). Let $t_2$ be the pre-AI time share on good 2, and suppose only good 2 gets a productivity multiplier $A'_2>1$, with $A_1=1$.

The output gain has a closed form in terms of $t_2,A_2,\varepsilon$.
: In a two-good CES benchmark where only good 2 gets a multiplier $A_2$ (so $p_2'=p_2/A_2$),
  $$
  \frac{V(A')}{V(A)}=\left((1-t_2)+t_2\,A_2^{\varepsilon-1}\right)^{\frac{1}{\varepsilon-1}}.
  $$

The post-AI share shift pins down $\varepsilon$ when $t_2'$ is observed.
: The post-AI time share is
  $$
  t_2'=\frac{t_2\,A_2^{\varepsilon-1}}{(1-t_2)+t_2\,A_2^{\varepsilon-1}}
  \quad\Leftrightarrow\quad
  \operatorname{logit}(t_2')-\operatorname{logit}(t_2)=(\varepsilon-1)\ln A_2.
  $$

::: {.callout-note collapse="true"}
## Proof (structured)

1. *Given* CES unit-expenditure (time price) index
   $$
   P(p)=\left(\alpha_1^{\varepsilon}p_1^{1-\varepsilon}+\alpha_2^{\varepsilon}p_2^{1-\varepsilon}\right)^{\frac{1}{1-\varepsilon}},
   \qquad V(A)=\frac{1}{P(1/A)}.
   $$
2. *Let* $p_2' = p_2/A_2$ with $p_1$ fixed. Then
   $$
   \frac{V(A')}{V(A)}=\frac{P(p)}{P(p')}=
   \left(
     \frac{\alpha_1^{\varepsilon}p_1^{1-\varepsilon}+\alpha_2^{\varepsilon}p_2^{1-\varepsilon}}
         {\alpha_1^{\varepsilon}p_1^{1-\varepsilon}+\alpha_2^{\varepsilon}(p_2/A_2)^{1-\varepsilon}}
   \right)^{\frac{1}{\varepsilon-1}}.
   $$
3. *Define* the ex-ante share
   $$
   t_2\equiv\frac{\alpha_2^{\varepsilon}p_2^{1-\varepsilon}}
                  {\alpha_1^{\varepsilon}p_1^{1-\varepsilon}+\alpha_2^{\varepsilon}p_2^{1-\varepsilon}}.
   $$
   Substituting into Step 2 yields the claimed gain formula.
4. *For shares,* note CES time shares satisfy
   $$
   \frac{t_2}{1-t_2}=\frac{\alpha_2^{\varepsilon}p_2^{1-\varepsilon}}{\alpha_1^{\varepsilon}p_1^{1-\varepsilon}}.
   $$
   Therefore,
   $$
   \frac{t_2'/(1-t_2')}{t_2/(1-t_2)}
   =
   \frac{(p_2')^{1-\varepsilon}}{p_2^{1-\varepsilon}}
   =
   \left(\frac{p_2/A_2}{p_2}\right)^{1-\varepsilon}
   =
   A_2^{\varepsilon-1},
   $$
   which rearranges to the stated closed form for $t_2'$ and the logit identity.
5. QED.
:::

## Two-good reduction (AI-affected bundle vs the rest)

Common benchmarks are:

| Assumption about substitution                          | Implied output gain $V(A')/V(A)$                                            | Notes                                                    |
| ------------------------------------------------------ | --------------------------------------------------------------------------- | -------------------------------------------------------- |
| CES elasticity $\varepsilon$                           | $\left((1-t_2)+t_2\,A_2^{\varepsilon-1}\right)^{\frac{1}{\varepsilon-1}}$   | Requires $\varepsilon$ (or enough data to infer it)      |
| $\varepsilon\rightarrow0$ (perfect complements/Amdahl) | $\dfrac{1}{(1-t_2)+t_2/A_2}$                                                | Fixed proportions; $t_2$ is the fixed-proportion share   |
| $\varepsilon=1$ (Cobb-Douglas/Hulten)                  | $A_2^{t_2}$                                                                 | Share is constant; log change is $t_2\ln A_2$            |
| $\varepsilon\rightarrow\infty$ (perfect substitutes)   | $A_2$                                                                       | Assumes the other bundle is not required for output      |


Anthropic-style numbers span a wide range under different substitution assumptions.
: If $t_2=0.10$ and $A_2=5$ (a 5x multiplier when AI is used), the implied gain depends heavily on the benchmark.

| Benchmark | Gain factor | Percent |
|---|---:|---:|
| Complements bound | $1/(0.9+0.02)\approx 1.087$ | $+8.7\%$ |
| Cobb-Douglas | $5^{0.1}\approx 1.174$ | $+17.4\%$ |
| CES, $\varepsilon=2$ | $0.9+0.1\cdot 5 = 1.4$ | $+40\%$ |
| Substitutes bound | $5$ | $+400\\%$ |

The spread is the point.
: Large conditional speedups do not translate to a unique aggregate lift without additional structure or data.

## If you observe pre and post time shares, you can infer an elasticity (in CES)

In a two-good CES benchmark, post-AI shares identify the elasticity.
: In the same two-good CES setting, let $t_2'$ be the post-AI time share on good 2. Then
  $$
  \operatorname{logit}(t_2')-\operatorname{logit}(t_2)=(\varepsilon-1)\ln A_2,
  $$
  so
  $$
  \varepsilon \;=\; 1 + \frac{\operatorname{logit}(t_2')-\operatorname{logit}(t_2)}{\ln A_2}.
  $$
  Here $\operatorname{logit}(x)\equiv \ln\!\left(\frac{x}{1-x}\right)$.
  This is often the cleanest way to use both pre and post shares: estimate $\varepsilon$, then plug into the CES gain formula. This uses Hicksian shares (or Marshallian shares under homotheticity and a fixed time endowment). If you do not believe CES globally, treat this as local to the observed price change.

Task activation makes selection central.
: The continuous model assumes you always do some of each task. That is wrong when tasks are lumpy, have setup costs, or are unit-demand. In those cases, LLMs can create newly affordable tasks, meaning the major effect is selection (which tasks get done at all), not intensive reallocation.



## Application 1: from query-level time savings to an aggregate lift (Anthropic)

Anthropic-style log studies estimate conditional time savings on observed AI conversations.
: @anthropic2025estimatingproductivitygains estimates time savings from Claude conversations by comparing time required with vs without AI for a sample of tasks drawn from usage logs.

Back-of-the-envelope inputs can be summarized as a baseline share and a productivity multiplier.
: Illustratively, the inputs are:

| Quantity | Symbol | Value |
|---|---:|---:|
| Baseline time share on the AI-affected bundle | $t_2$ | $10\%$ |
| Productivity multiplier when AI is used | $A_2$ | $5$ |

Even in a two-good continuous approximation, the implied lift depends sharply on substitution.
: The table below shows how different assumptions about substitutability map the same inputs into very different lifts.

| Assumption about substitution | What it means in this note | Assumption on $\varepsilon$ | Implied lift when $t_2=10\%$ and $A_2=5$ |
|---|---|---|---|
| Strong complements (Amdahl/Leontief-style) | Time shares do not move much when a subset of tasks speeds up | effectively $\varepsilon\to 0$ | about $+8.7\%$ |
| Cobb-Douglas benchmark | Unit elasticity, moderate reallocation | $\varepsilon=1$ | about $+17.4\%$ |
| CES example with stronger substitution | Time shifts strongly toward the sped-up bundle | $\varepsilon=2$ | about $+40\%$ |

The key empirical object is the share response to an effective price change.
: The identification problem is not only how big $A_2$ is when AI is used, but how time shares respond when relative time prices change. In the CES benchmark that response is summarized by $\varepsilon$; outside CES, it is a demand-curve object.

Endogenous task mix can make pre-AI shares a poor guide.
: If AI reduces time costs more for tasks you previously avoided (high search/reading/writing overhead), then the pre-AI share $t_2$ understates the mass of tasks that become attractive post-AI.

Quality adjustment matters whenever "time saved" is partly quality.
: Some measured time savings are quality improvements (or vice versa). If your output index treats quality as part of output, you need a quality measure to map time changes into the output index $y(\cdot)$.

## Application 2: interpreting “uplift” RCTs (METR / open-source dev)

Uplift RCTs often identify intensive effects on a fixed task set.
: @becker2025uplift is an RCT where tasks are assigned to allow vs disallow AI tools, and the outcome is completion time (with additional self-reports and robustness checks). The headline is that AI access increases completion time on average in their setting.

A simple back-of-the-envelope converts time changes into a productivity multiplier.
: If allowing AI increases completion time by 19%, then the implied productivity multiplier is $A \approx 1/1.19 \approx 0.84$ (a slowdown). Holding output constant, that is about a 16% productivity hit on that task population.

Fixed-task designs identify an $A_i$ distribution but not substitution.
: With a fixed task set (as in many RCTs), the main object you learn is a task-level time change (an $A_i$ distribution if you convert time changes into productivity multipliers), holding task composition fixed. Mapping that into $V(A')/V(A)$ requires additional assumptions or variation that moves the task mix.

Aggregate interpretations require an explicit output and quality index.
: Translating time changes into an aggregate productivity index still requires an output/quality index: are we holding output constant (pure time saved), holding time constant (more output), or letting both adjust? In practice you often want quality-adjusted output per unit time.

If AI changes which tasks are attempted, the design must measure selection.
: If AI access changes which tasks are attempted, then the design has to either fix tasks (to isolate intensive effects) or explicitly allow selection and measure it (to capture the extensive margin).

# Related Literature


## Index numbers and exact welfare from price changes

Expenditure-function cost-of-living indices.
: @konus1939trueindex defines a cost-of-living index as the ratio of minimum expenditures needed to reach a fixed utility level at two price vectors. This is exactly the object $P(p)=e(p,1)$ (with time prices instead of money prices) that corresponds here to $V(A)=1/P(1/A)$ under homotheticity.

Exact/superlative indices and share-weighted approximations.
: @diewert1976exact formalizes when index numbers (including Tornqvist/Divisia-type log-share formulas) are exact for flexible functional forms and motivates using share-weighted log changes as a local approximation.

Duality and productivity measurement via index numbers.
: @caves1982indexnumbers is a central reference for exact index-number measurement of input, output, and productivity, and for the duality framing used in the continuous model here.

Large changes and compensated-demand integrals.
: @willig1976consumerssurplus and @hausman1981exact connect equivalent/compensating variation and consumer surplus to integrals of compensated demand, clarifying why large AI shocks call for area under Hicksian demand rather than constant-elasticity shortcuts.

Demand systems for practical welfare calculations.
: @deaton1980aids introduces AIDS, a workhorse integrable demand system used to estimate demand and welfare effects of price changes in practice.

## Time allocation, information costs, and "overhead" time

Time allocation as a core economic problem.
: @becker1965allocation frames time allocation as a core economic problem, and is a natural ancestor of treating task durations as time prices.

Time requirements and shadow prices.
: @deserpa1971time emphasizes that goods require time to consume/produce and develops shadow-price implications close to the time-price interpretation here.

Information/search costs as a mechanism for time savings.
: @stigler1961information treats costly search as central; empirically, many LLM speedups look like reductions in search/reading/writing overhead rather than reductions in core task time.

Characteristics and implicit prices.
: @lancaster1966consumer gives an alternative lens: tasks bundle characteristics (information, drafting, formatting, reasoning), and AI shifts the implicit prices of those characteristics.

## Discrete activation, selection, and welfare with lumpy choices

Welfare tools for discrete choice.
: @smallrosen1981welfare is a classic reference for welfare analysis with discrete choice, directly relevant for unit-demand tasks and activation thresholds.

Applied discrete-choice estimation.
: @train2003discretechoice is a standard applied reference for estimating random-utility/discrete-choice models when you want to quantify substitution and welfare.

## Task-based technological change and aggregation

Task-based technological change.
: @autor2003skill and @acemoglu2011handbook popularize task-based views of technology, conceptually aligned with treating tasks as goods with relative time prices.

Small-shock share-weighted logic (Hulten-style).
: @hulten1978growth is the backbone for why $\Delta\ln V\approx \sum_i t_i\,\Delta\ln A_i$ can be reasonable for small or broad shocks.

Beyond small shocks: nonlinearities and large changes.
: @baqaee2019macro is a canonical reference on nonlinearities and large shocks and connects directly to why large AI shocks motivate demand-curve integration and careful treatment of selection.

Micro-to-macro aggregation from technology evidence.
: @oberfield2021micro is a useful reference on linking micro evidence on technology to macro implications.

CES as a workhorse benchmark.
: @dixit1977monopolistic sits behind many closed-form substitution formulas, including the two-good CES benchmark used above.

## Empirical AI productivity and usage measurement

Log-based time-savings evidence.
: @anthropic2025estimatingproductivitygains illustrates the appeal (and limits) of measuring conditional time savings and baseline shares in observational usage data.

Fixed-task RCT evidence.
: @becker2025uplift exemplifies randomized access to AI tools with completion-time outcomes, highlighting the importance of quality measurement and task selection.

Early gen-AI productivity evidence across settings.
: @noy2023generative and @brynjolfsson2023generative are widely cited early studies with evidence on productivity and heterogeneity.

Usage composition and task mix.
: @chatterji2025chatgpt documents usage patterns and task composition for ChatGPT, relevant when the task mix is endogenous.

Valuing time savings and digital services.
: @varian2011economic and @collis2025welfare are representative of work valuing digital services and time savings, complementary to the demand-theory framing here.
