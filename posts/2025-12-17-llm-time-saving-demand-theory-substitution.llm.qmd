---
title: "LLM Time-Saving, Demand Theory, and the Cadillac Tasks"
author: "Tom Cunningham (METR)"
date: today
draft: true
citation: true
reference-location: document
bibliography: ai.bib
format:
  html:
    toc: true
    toc-depth: 3
execute:
  echo: false
  warning: false
  error: false
  cache: true
---

<!-- https://tecunningham.github.io/posts/2025-12-17-llm-time-saving-demand-theory-substitution.llm.html -->

<!-- LLM note: review tools/test_llm_time_saving_qmd.py for tests and run it after edits. -->

```{python}
#| echo: false
#| results: asis
from __future__ import annotations

import importlib.util
import io
import contextlib
import html
import unittest
from pathlib import Path

from IPython.display import Markdown, display

qmd_path = Path("posts/2025-12-17-llm-time-saving-demand-theory-substitution.llm.qmd")
bib_path = Path("posts/ai.bib")
bib_tests_path = Path("tools/ai.bib.tests.py")
if not qmd_path.exists():
    qmd_path = Path("2025-12-17-llm-time-saving-demand-theory-substitution.llm.qmd")
if not bib_path.exists():
    bib_path = Path("ai.bib")
if not bib_tests_path.exists():
    bib_tests_path = Path("../tools/ai.bib.tests.py")

script_path = Path("tools/test_llm_time_saving_qmd.py")
if not script_path.exists():
    script_path = Path("../tools/test_llm_time_saving_qmd.py")

spec = importlib.util.spec_from_file_location("test_llm_time_saving_qmd", script_path)
assert spec and spec.loader
module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(module)  # type: ignore[assignment]

# Ensure tests point at the right paths regardless of CWD.
module.QMD_PATH = qmd_path
module.BIB_PATH = bib_path
module.BIB_TESTS_PATH = bib_tests_path

suite = unittest.defaultTestLoader.loadTestsFromModule(module)
class CountingResult(unittest.TestResult):
    def __init__(self) -> None:
        super().__init__()
        self.stats: dict[str, dict[str, int]] = {}
        self.has_subtests: set[str] = set()

    def _ensure(self, test_id: str) -> dict[str, int]:
        if test_id not in self.stats:
            self.stats[test_id] = {
                "total": 0,
                "passed": 0,
                "failed": 0,
                "errors": 0,
                "skipped": 0,
            }
        return self.stats[test_id]

    def addSuccess(self, test: unittest.TestCase) -> None:
        test_id = test.id()
        s = self._ensure(test_id)
        if test_id not in self.has_subtests:
            s["total"] += 1
            s["passed"] += 1
        super().addSuccess(test)

    def addFailure(self, test: unittest.TestCase, err) -> None:  # type: ignore[override]
        test_id = test.id()
        s = self._ensure(test_id)
        if test_id not in self.has_subtests:
            s["total"] += 1
            s["failed"] += 1
        super().addFailure(test, err)

    def addError(self, test: unittest.TestCase, err) -> None:  # type: ignore[override]
        test_id = test.id()
        s = self._ensure(test_id)
        if test_id not in self.has_subtests:
            s["total"] += 1
            s["errors"] += 1
        super().addError(test, err)

    def addSkip(self, test: unittest.TestCase, reason: str) -> None:
        test_id = test.id()
        s = self._ensure(test_id)
        if test_id not in self.has_subtests:
            s["total"] += 1
            s["skipped"] += 1
        super().addSkip(test, reason)

    def addSubTest(
        self,
        test: unittest.TestCase,
        subtest: unittest.TestCase,
        err,
    ) -> None:
        test_id = test.id()
        self.has_subtests.add(test_id)
        s = self._ensure(test_id)
        s["total"] += 1
        if err is None:
            s["passed"] += 1
        elif isinstance(err, unittest.SkipTest):
            s["skipped"] += 1
        else:
            s["failed"] += 1
        super().addSubTest(test, subtest, err)


result = CountingResult()
with contextlib.redirect_stdout(io.StringIO()), contextlib.redirect_stderr(io.StringIO()):
    suite.run(result)

def status_for(test_id: str) -> str:
    s = result.stats.get(test_id, {})
    if s.get("errors", 0) > 0 or s.get("failed", 0) > 0:
        return "❌"
    if s.get("skipped", 0) > 0:
        return "⏳"
    return "✅"


def ratio_for(test_id: str) -> str:
    s = result.stats.get(test_id, {})
    total = int(s.get("total", 0))
    passed = int(s.get("passed", 0))
    if total > 1:
        return f" ({passed}/{total})"
    return ""

NBSP = "\u00a0"  # non-breaking space so emoji/text aren't cramped

checks = [
    ("Programmatic: required sections present", "TestQmdStructure.test_required_sections_present"),
    ("Programmatic: Lamport-style proof structure", "TestQmdStructure.test_lamport_style_proof"),
    ("Programmatic: diagrams present", "TestQmdStructure.test_diagrams_present"),
    ("Programmatic: practical examples present", "TestQmdStructure.test_practical_examples_present"),
    ("Programmatic: bibliography tests", "TestBibliography.test_ai_bib_conventions"),
    ("Programmatic: citekeys resolve in `posts/ai.bib`", "TestQmdStructure.test_citations_resolve"),
    ("LLM-assisted: quality + citation plausibility gate (optional)", "TestLLMBasedChecks.test_llm_quality_gate"),
]

lines = [
    '<details class="validation-checklist">',
    "<summary>Validation Checks</summary>",
    "",
]
for label, short_id in checks:
    full_id = f"test_llm_time_saving_qmd.{short_id}"
    lines.append(f"- {status_for(full_id)}{NBSP}{label}{ratio_for(full_id)}")

    # Nested bibliography subtests (consistent style, no separate stdout block).
    if short_id == "TestBibliography.test_ai_bib_conventions":
        try:
            bib_results = module.run_ai_bib_tests_report()
            for r in bib_results:
                ok = bool(getattr(r, "ok", False))
                detail = getattr(r, "detail", "") or ""
                name = html.escape(str(getattr(r, "name", "")))
                detail_esc = html.escape(str(detail))
                lines.append(f"  - {'✅' if ok else '❌'}{NBSP}{name}{detail_esc}")
        except Exception as exc:
            lines.append(f"  - ❌{NBSP}Error running bibliography tests: {html.escape(str(exc))}")

lines.append("</details>")

display(Markdown("\n".join(lines)))
```

<style>
   details.validation-checklist {
      background: #f5f5f5;
      border: 1px solid #777;
      border-radius: 6px;
      padding: 0.5em 0.75em;
      margin-bottom: 1em;
   }
   details.validation-checklist > summary {
      cursor: pointer;
   }
   dl {display: grid;}
   dt {grid-column-start: 1; width: 10em;}
   dd {grid-column-start: 2; margin-left: 2em;}
</style>

## Results-first summary

This note reframes LLM time-saving as a *price index* problem in a time-allocation model, then adds a discrete extensive margin for “Cadillac tasks.” The continuous and discrete cases behave very differently, so I keep them separate.

**Continuous (intensive-margin) case:** treat each task’s time requirement as a price $p_i$, with LLM speedup $\beta_i$ implying $p_i' = p_i/\beta_i$. Under homothetic preferences, the output index is $v(p)=1/P(p)$ where $P(p)=e(p,1)$ is the unit time-expenditure function. Equivalent/compensating variation become time-index ratios, and small changes are share-weighted. Large changes require the *area under the compensated demand curve*. (Citations: @caves1982indexnumbers; @hausman1981exact; @willig1976consumerssurplus.)

**Discrete (extensive-margin) case:** tasks can require setup time or be unit-demand. Then LLM speedups change *which* tasks are done, not just *how much* of each task. This is the home of “Cadillac tasks” (tasks you’d never do absent the LLM) and the *worked example* below. Constant-elasticity formulas can fail here.

## Setup: time prices and speedups

**Objects.** Tasks $i=1,\dots,n$, outputs $x_i\ge 0$, time endowment normalized to $1$, time prices $p_i>0$ (time per unit of task output), and LLM speedups $\beta_i>0$ so $p_i' = p_i/\beta_i$.

We interpret $u(x)$ as an *output index* or “effective work accomplished,” with time prices defining the budget:
$$
\sum_i p_i x_i \le 1.
$$

The important split is:

1. **Continuous intensive margin:** choose continuous $x_i$ (smooth substitution).
2. **Discrete extensive margin:** choose which tasks to activate (unit demand or setup costs).

I treat these separately because the logic, formulas, and data requirements diverge.

## Continuous (intensive-margin) model

### Primal, dual, and the time price index

The primal problem is
$$
v(p)\;=\;\max_{x\ge 0} u(x) \quad\text{s.t.}\quad \sum_i p_i x_i\le 1.
$$

Define the expenditure function
$$
e(p,\bar u)=\min_{x\ge 0} \Big\{\sum_i p_i x_i: u(x)\ge \bar u\Big\}.
$$

If $u(\cdot)$ is homothetic and degree-1 homogeneous, then $e(p,\bar u)=\bar u\,e(p,1)$. Define the **unit time price index**
$$
P(p)\equiv e(p,1)\quad\Rightarrow\quad v(p)=\frac{1}{P(p)}.
$$

This is the classic index-number framing applied to time prices. @caves1982indexnumbers

### EV/CV in time units

Let $p^0\to p^1$ and $u^k=v(p^k)$. Equivalent and compensating variation (measured in *time*) are
$$
EV=e(p^0,u^1)-1,\qquad CV=e(p^1,u^0)-1.
$$

Under homotheticity,
$$
EV=\frac{P(p^0)}{P(p^1)}-1,\qquad CV=\frac{P(p^1)}{P(p^0)}-1.
$$

This is the cleanest way to translate LLM time savings into a welfare measure. @hausman1981exact

### Small changes (share-weighted)

Let $t_i(p)\equiv p_i x_i^*(p)$ be optimal time shares. For small changes in time prices,
$$
d\ln v\;=\;-d\ln P\;\approx\;\sum_i t_i\,d\ln \beta_i.
$$

This is the time-allocation analog of share-weighted Hulten-style approximations. @hulten1978growth

### Large changes (area under compensated demand)

When LLM gains are large, constant-elasticity approximations are dangerous. Using Hicksian (compensated) shares $s_i^H(p)$,
$$
d\ln P(p)=\sum_i s_i^H(p)\,d\ln p_i.
$$

For a single changing price $p_2$,
$$
\ln\frac{P(p^1)}{P(p^0)}=\int s_2^H(p_2)\,d\ln p_2,
$$

i.e., exact welfare is the **area under the compensated demand curve**. @willig1976consumerssurplus

### CES specialization (closed-form)

For a CES aggregator
$$
u(x)=\left(\sum_i \alpha_i x_i^{\frac{\sigma-1}{\sigma}}\right)^{\frac{\sigma}{\sigma-1}},\quad\sigma>0,
$$

the price index and time shares are
$$
P(p)=\left(\sum_i \alpha_i^{\sigma}p_i^{1-\sigma}\right)^{\frac{1}{1-\sigma}},\qquad
 t_i(p)=\frac{\alpha_i^{\sigma}p_i^{1-\sigma}}{\sum_j \alpha_j^{\sigma}p_j^{1-\sigma}}.
$$

In the two-task case, if task 2 speeds up by $\beta$ and its ex-ante share is $s_0$, then
$$
\frac{y'}{y}=\left((1-s_0)+s_0\,\beta^{\varepsilon-1}\right)^{\frac{1}{\varepsilon-1}},\qquad \varepsilon\equiv\sigma.
$$

This is the continuous benchmark; I will **not** use it for Cadillac tasks, which are discrete. @caves1982indexnumbers

#### Proposition (Lamport-style): CES output response

**Claim.** For two-task CES with ex-ante share $s_0$ on task 2 and speedup $\beta$, the optimized output ratio is
$$
\frac{y'}{y}=\left((1-s_0)+s_0\,\beta^{\varepsilon-1}\right)^{\frac{1}{\varepsilon-1}}.
$$

**Proof (Lamport style).**

1. *Given* CES price index $P(p)=\left(\alpha_1^{\varepsilon}p_1^{1-\varepsilon}+\alpha_2^{\varepsilon}p_2^{1-\varepsilon}\right)^{\frac{1}{1-\varepsilon}}$ and output $v(p)=1/P(p)$.
2. *Let* $p_2' = p_2/\beta$ while $p_1$ is fixed.
3. *Then* the output ratio is
   $$
   \frac{y'}{y}=\frac{P(p^0)}{P(p^1)}=
   \left(\frac{\alpha_1^{\varepsilon}p_1^{1-\varepsilon}+\alpha_2^{\varepsilon}p_2^{1-\varepsilon}}{\alpha_1^{\varepsilon}p_1^{1-\varepsilon}+\alpha_2^{\varepsilon}(p_2/\beta)^{1-\varepsilon}}\right)^{\frac{1}{\varepsilon-1}}.
   $$
4. *Define* the ex-ante share
   $$
   s_0\equiv\frac{\alpha_2^{\varepsilon}p_2^{1-\varepsilon}}{\alpha_1^{\varepsilon}p_1^{1-\varepsilon}+\alpha_2^{\varepsilon}p_2^{1-\varepsilon}}.
   $$
5. *Substitute* into Step 3 to obtain
   $$
   \frac{y'}{y}=\left((1-s_0)+s_0\,\beta^{\varepsilon-1}\right)^{\frac{1}{\varepsilon-1}}.
   $$
6. **QED.**

## Discrete (extensive-margin) model: Cadillac tasks live here

The continuous model assumes you always do *some* of each task. That is wrong when tasks are lumpy, have setup costs, or are unit-demand. In those cases, LLMs can create **newly affordable tasks**, meaning the major effect is *selection*, not *intensive* time reallocation.

### Unit-demand formulation

Let each task have payoff $u_i$ and required time $w_i(p)$, with decision $q_i\in\{0,1\}$. Then
$$
\max_{q\in\{0,1\}^n}\sum_i u_i q_i\quad\text{s.t.}\quad \sum_i w_i(p) q_i\le 1.
$$

Speedups change $w_i$ by $\beta_i$, which can **turn tasks on** once a threshold is crossed. This is exactly the “Cadillac tasks” phenomenon: tasks that were too time-expensive become attractive after the LLM. The usual CES elasticity is not a good summary in this regime.

### Setup-cost variant (bridging discrete and continuous)

Add a fixed setup time $\phi_i$ and a continuous intensity $x_i$:
$$
\max_{q,x}\;u(x)\quad\text{s.t.}\quad \sum_i \phi_i q_i + \sum_i p_i x_i \le 1,\; x_i=0\;\text{if }q_i=0.
$$

If $\phi_i=0$, we recover the continuous model. If $\phi_i>0$, large LLM speedups mostly expand the active set $\{i:q_i=1\}$, not the intensive shares.

### Worked example (discrete, not continuous)

**Example.** Suppose you can pick *one* task (unit-demand). Task A yields value $u_A=10$ and takes 1 hour. Task B yields $u_B=12$ and takes 2 hours. Without LLMs you choose A. Now an LLM speeds up task B so it takes 1 hour. You switch to B.

- **Upper bound on time-equivalent gain:** 1 hour (if the extra value $u_B-u_A$ is “worth” a full hour).
- **Lower bound:** 0 hours (if the extra value is just a small quality bump).

So the *observed* reallocation does not identify a precise time-savings without modeling discrete choice. This is why elasticity-of-substitution estimates are weak in the Cadillac regime.

### Cadillac tasks (discrete interpretation)

Cadillac tasks are those you would not do *at all* without the LLM, but you do once their time cost drops. Examples:

- literature reviews you previously would not attempt,
- custom data visualizations,
- long-form proofreading or refactoring.

In a unit-demand or setup-cost model, these tasks appear as **newly activated $q_i=1$ choices**, not as marginal increases in $x_i$. This is a discrete effect, so apply discrete logic—not the continuous CES approximation.

## Practical examples (grounding)

- **Query-level time savings:** If a chatbot is used for 10% of tasks and yields 5x speedups, naive share-weighted estimates imply large gains. But if those tasks are Cadillac tasks (discrete selection), the aggregate gain is much smaller. @anthropic2025estimatingproductivitygains
- **RCTs with task selection:** In uplift experiments, participants may choose different tasks once AI is available. That makes comparisons tricky unless you model the discrete choice margin. @becker2025uplift
- **Time allocation as a resource constraint:** Classic time-allocation models already interpret time as a shadow price. @deserpa1971time

## Diagrams

### Unified map (assumptions → objects → results)

```{mermaid}
graph TD
  A[Primitives: tasks i=1..n, time endowment=1] --> B[Technology: time prices p_i; AI => p_i_new = p_i / beta_i]
  B --> C[Choice: allocate time/output s.t. sum_i p_i x_i <= 1]

  C --> D{Preference/output aggregator u(x)}
  D --> D1[Homothetic (continuous)]
  D --> D2[CES]
  D --> D3[Discrete or setup-cost tasks]

  D1 --> E[Price index P(p)=e(p,1)]
  E --> F[Indirect output v(p)=1/P(p)]
  F --> G[EV/CV = ratios of P(p)]
  F --> H[Local: d ln v = sum_i t_i d ln beta_i]

  D2 --> I[Closed forms for P(p), shares]
  I --> J[Two-task formula]

  D3 --> K[Activation/threshold effects]
  K --> L[Cadillac tasks, discrete selection]
```

### Threshold diagram (discrete activation)

```{python}
#| fig-cap: "Discrete activation: speedups switch on tasks"
#| fig-align: center
import matplotlib.pyplot as plt

fig, ax = plt.subplots(figsize=(5.0, 3.6))

# Axes
ax.set_xlim(0, 4.2)
ax.set_ylim(0, 3.2)
ax.set_xlabel("time cost")
ax.set_ylabel("task value")

# Threshold
ax.axhline(1.5, linestyle="--", color="gray", linewidth=1)
ax.text(4.05, 1.5, "value threshold", ha="left", va="center", color="gray", fontsize=9)

# Points
ax.scatter([1.0], [2.2], color="black", s=20)
ax.scatter([3.2], [2.7], color="black", s=20)
ax.text(1.05, 2.28, "task A", ha="left", va="bottom", fontsize=9)
ax.text(3.25, 2.78, "task B", ha="left", va="bottom", fontsize=9)

# Speedup arrow (moves task B left)
ax.annotate(
    "",
    xy=(2.0, 2.7),
    xytext=(3.2, 2.7),
    arrowprops=dict(arrowstyle="->", color="blue", linewidth=2),
)
ax.text(2.2, 2.15, "activation", color="blue", fontsize=9)
ax.text(2.6, 2.82, "LLM speedup", color="blue", fontsize=9, ha="center")

# Clean look
ax.set_xticks([])
ax.set_yticks([])
for spine in ax.spines.values():
    spine.set_visible(True)

fig.tight_layout()
plt.show()
```

## Checklist for the desiderata

- **Bibliography validity:** citations are included and checked against `ai.bib`. See the validation checklist at the top.
- **Citation faithfulness:** the optional LLM-assisted check flags suspicious claim-to-citation mismatches (also in the checklist).
- **Lamport-style proofs:** proofs are structured with numbered steps and a QED marker.
- **Legible diagrams:** one Mermaid flowchart + one TikZ threshold figure.
- **Practical examples:** see the query-level and RCT examples above.

## Related literature (short pointers)

- Index-number theory for price changes and substitution. @caves1982indexnumbers
- Exact welfare measures and integrable demand systems. @hausman1981exact; @deaton1980aids
- Time allocation and shadow pricing. @deserpa1971time
- Task-based technological change. @autor2003skill; @acemoglu2011handbook
