---
title: Four Experimentation Problems
author: Tom Cunningham
execute:
  echo: false
  cache: true # caches chunk output
date: 2023-04-18
format:
   html:
      toc: true
      toc-depth: 2
      toc-location: left
      html-math-method:
         method: mathjax
         url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js"
         #     ^ this forces SVG instead of CHTML, otherwise xypic renders weird
      include-in-header:
         - text: |
            <script>window.MathJax = {
               loader: { load: ["https://cdn.jsdelivr.net/gh/sonoisa/XyJax-v3@3.0.1/build/xypic.js"]},
               tex: {packages: {'[+]': ['xypic','bm']},
                     macros: {  bm: ["\\boldsymbol{#1}", 1],
                                ut: ["\\underbrace{#1}_{\\text{#2}}", 2],
                                utt: ["\\underbrace{#1}_{\\substack{\\text{#2}\\\\\\text{#3}}}", 3] }
               }
            };
            </script>
engine: knitr
reference-location: margin
citation-location: margin
figure-location: margin
#fontsize: 10pt
draft: true
editor:
   render-on-save: true
---
<style>
    h1 {  border-bottom: 4px solid black;}
    h2 {  border-bottom: 1px solid #ccc;}
</style>

<!-- 
- http://tecunningham.github.io/posts/2023-04-18-netflix-cider-experimentation-note.html
- Good writeup at 2023-01-12 Giorgio / Inference on Winners
- **Leontief sandwich.**
- **Everyone is implicitly Bayesian.** The choice of which treatment to run an experiment on, and which metrics to look at, reflects your priors. We are doing mostly *empirical Bayes* here. Empirical Bayes is half-baked bread, frequentist is just raw dough, completely inedible.
-->

#              Introduction

<!-- **Tech firms have thousands of experiments running at a given time.** E.g. experiments changing user interface, changing weights in a ranking algorithm, changing price, changing the number of ads shown, or just canary experiments confirming that a new piece of code hasn't broken anything. -->

**In this note I give recommendation for four experiment-related issues using a common Bayesian framework:**^[The impetus for writing this up was Netflix's 2023 CIDER conference, many thanks to all participants especially Martin Tingley.]

1. **The inference problem.** What's our best estimate of the true effect given the observed effect? This comes up especially when we have a set of different observed effects, e.g. across experiments, across subgroups, or across time.

   Often people deal with these problems by adjusting confidence intervals (e.g. Bonferroni, always-valid). However I think a better approach is to let users make their own judgments but provide them with the most informative *benchmark* statistics so they can compare the results of any given experiment to the results from a reference group.

2. **The extrapolation problem.** Given an effect on metric A what's our best estimate of the effect on metric B? This describes the problem of observational inference, proximal goals, and extrapolation.

   There are three approaches to solving this: (1) priors; (2) correlation across units (surrogacy); (3) correlation across experiments (meta-analysis). I argue that approach (3) is generally the best option, but reasonable care needs to be taken in interpreting the results.

3. **The explore-exploit problem.** We would like to choose which experiments to run in an efficient and automated way. The technical solution is relatively clear but tech companies have struggled to implement it because good execution requires some discipline. I describe a simple algorithm that is not optimal but is very simple and robust.

4. **The culture problem.** Inside tech companies people keep misusing experiments and misinterpreting the results, especially running under-powered experiments, selectively choosing results, and looking at correlations without thinking about identification.

#                                  Setup

**Firms choose their policy to maximize user retention.** As a simplified model companies are choosing policies to maximize long-run retention (or revenue). A policy is, for example, a recommendation algorithm, or notification algorithm, or the text and images used in an advertisement or the UX on a signup page.

**Experiments are not the *primary* source of causal knowledge.** People already have substantial knowledge about the effects of their decisions without either randomized experiments or algorithmic estimators (IV, RDD, etc.). We built cathedrals, aeroplanes, welfare states, we doubled human life-expectancy, & WhatsApp grew to 1B users, all without experiments or regressions. Formal causal inference *augments* our already substatial causal knowledge. In a company the primary way people learn about causal relationships is raw data (e.g. dashboards) and common-sense reasoning about human behaviour.

**Experiments only solve the low-dimensional problem.** In most cases the dimensionality of the policy space is far higher than the dimensionality of experiment space, thus the responsibility for choosing policies is primarily human judgment, and then humans give a few variants to experiments to compare their performance.

**Most questions related to experiments can be expressed as conditional expectations.** A good workhorse model of experimentation is the following, suppose we have two metrics #1 and #2. Taking some set of experiments we can think of three joint distributions: the observed effects, the true effects, and the noise:^[For simplicitly assume the experiment doesn't have any effect on variances or covariances of outcomes, the effects are typically small enough that it doesn't matter.]

   $$\utt{\binom{\hat{t}_1}{\hat{t}_2}}{observed}{effects}
      =\utt{\binom{t_1}{t_2}}{true}{effects (ATE)}
         +\ut{\binom{e_1}{e_2}}{noise}
         $$

   For simplicity we'll assume everything is normally distributed and has mean zero, then we get two very simple expressions for conditional expectations, and I'll argue that these conditional expectations serve as answers to almost all interesting experimentation questions:

   $$\begin{aligned}
      E[t_1|\hat{t}_1] &= \utt{\frac{\sigma_{t1}^2}{\sigma_{t1}^2+\sigma_{e1}^2}}{signal-noise}{ratio}\hat{t}_1 
         && \text{(posterior estimate of treatment effect, AKA shrinkage)} \\
      E[t_2|\hat{t}_1] &= \utt{\rho_{t}\frac{\sigma_{t2}}{\sigma_{t1}}}{covariance}{of $t_1$ and $t_2$}
            \utt{\frac{\sigma_{t1}^2}{\sigma_{t1}^2+\sigma_{e1}^2}}{signal-noise}{ratio of $\hat{t}_1$}\hat{t}_1 
         && \text{(true effect on metric 2 given observed effect on metric 1)}
   \end{aligned}
   $$


   <!-- - Key point - conditional expectations ; observational inference condition can be empirically checked ; classical conditions are a waste of time ;  -->


#                                  The Inference Problem

**There are a number of experiment inference problems that we often find difficult.** For now we'll discuss these as pure inference problems without worrying about strategic behaviour (e.g. peeking, cherry-picking).
   
   1. Guess at the treatment effect given the observed treatment effect.
   2. Guess at the long-run treatment effect knowing the short-run observed effect.
   3. Guess at the treatment effect, knowing the observed effect, and additionally that the observed effect is the largest across some group of experiments.
   4. Guess at the treatment effect on a subgroup, knowing the observed effect, and knowing that it is the largest across all subgroups.
   
**The textbook approach uses p-values.** A common "textbook" understanding of testing is that you check the p-value: if it is below 0.05 then use the observed effect as your estimate, if the p-value is above 0.05 then use zero as your estimate. This approach leads to all sorts of well-known problems.


**Alternative recommendation: report benchmark statistics.** The ideal decision process lets humans make a judgment about estimated treatment effects given three ingredients:
   
   1. **Raw estimate.** The point estimate and standard error.^[Equivalently, the point-estimate and p-value, or the upper and lower confidence bounds.]
   2. **Benchmark statistic.** We should also report a statistic comparing this observed effect to observed effects of other similar treatments. There are many ways of benchmarking and I think they are all convey the same basic information, e.g. the empirical-bayes shrunk estimate (and there are various shrinkage estimators), the FDR-adjusted p-value, or the fraction of statistically significant experiments. We have to use judgment in defining what a "similar" experiment is, and it's important that we report to the end-user what class of similar experiments we're using and how many we have. For the remainder of the section I will assume we are reporting an empirical-bayes shrunk estimate.
   3. **Idiosyncratic details.** Any information about this treatment relative to the benchmark class, that could be relevant to its effect on this metric. E.g. if this experiment only affects iPhone users and the metric is an Android outcome this is informative, and we should probably ignore an effect unless it is highly significant.

**Benchmarking solves all the problems above.** An empirical-Bayes shrunk estimate represents our best guess at the true treatment effect conditional on the experiment being drawn from a given reference class.


**There are additionally some strategic problems.**

1. **Strategic stopping.** Engineers will wait until an experiment has a high estimated impact (or low p-value) and then try to launch it.
2. **Cherry-picking.** An engineer will cherry-pick results that support their preferred decision, e.g. Indian DAU went up.
3. **Alert thresholds.** We want to stop an experiment early if it has either a very good or very bad outcome, but it's difficult to know how to set the threshold in a principled way.


*[UNFINISHED: frequentist vs Bayesian approach to strategic problems]*

--------------------------------------------

**Useful shortcut: using the fraction of significant experiments to do shrinkage.** A convenient rule of thumb for doing empirical Bayes shrinkage is to use the fraction of experiments that are statistically significant in some class. If the fraction is zero then we should shrink all estimates to zero, if the fraction is 20% then we should shrink estimates by about 50%, and if the fraction is 1/2 then we should shrink estimates by about 20%. If everything's Gaussian and every experiment has the same $N$ then the optimal shrinkage factor is $1-(\frac{1}{1.96}\Phi^{-1}(\frac{q}{2}))^2$, where $q$ is the fraction of stat-sig experiments.

##       On Launch Criteria

**Choosing weights on metrics for a launch decisions involves many considerations:** network effects, noise, cross-metric proxy effects, and dynamic effects. To make clear decisions it's important to peel apart these layers, I recommend these steps:

   1. **Choose a set of final metrics.** These are the metrics we would care about *if we had perfect knowledge of the experimental effect.* We can define tradeoffs between them, it's convenient to express those tradeoffs in terms of percentage changes, e.g. we might be indifferent between 1% DAU, 2% time/DAU, and 5% prevalence of bad content.^[Arguably revenue or profit is a more truly final metric, and these are just proxies, but these are probably close enough to final for most purposes.]

   2. **Choose a set of proximal  metrics.** These are the metrics on which we are confident we can detect our experiment's effect, meaning the measured impact will be close to the true impact on these metrics (i.e. has a high signal-noise ratio). To determine whether a metric is moved we can use the fraction of a given class of experiments that have a statistically-significant effect on that metric: if the share is greater than 50% then we can be confident that the estimated effect is close to the true effect.

   3. **Identify *conversion factors* between proximal and final metrics.** These tell us the best-estimate impact on final metrics given the impact on proximal metrics. Conversion factors can be estimated either from (a) long-running tuning experiments; (b) a meta-analysis of prior experiments with similar designs.

   A final linear launch criteria can then be expressed as a set of conversion-factor weights applied to each of the proximal metrics.^[For derivation see Cunningham and Kim (2019).]



#                                  The Extrapolation Problem

**Many problems are predicting the effect one one metric (downstream) given the effect on another metric (upstream).** There are a variety of situations in which we cannot measure the effect on the downstream metric, either because it has high noise, or it is in the future:

| upstream               | downstream                                       |
| ---------------------- | ------------------------------------------------ |
| short-run revenue      | long-run revenue                                 |
| click                  | purchase                                         |
| engagement on content  | response to survey ("do you like this content?") |
| engagement on content  | retention                                        |
| exposure to content    | retention                                        |
| time on surface X      | time on all surfaces                             |
| purchase               | repeat purchase                                  |
| wait-time for delivery | retention                                        |
| price                  | quantity purchased                               |

<!-- 
**When does observational inference work well?** It will work well when the $e_1$ and $e_2$ vary in the same way.
   - Will work when the upstream variable is quasirandom.
   - Will work when 

   $$\xymatrix{
         *++[F:<15pt>]{unobserved}\ar[d]\ar@{.>}[dr]
         & *++[F:<15pt>]{unobserved}\ar[d]
         \\ *++[F]\txt{upstream}
         & *++[F]\txt{downstream}
      }
   $$    -->

<!-- **Observational inference has been disappointing.** Many projects in tech have tried to measure causal effects with observational inference but have been disappointing. [UNFINISHED]. -->

<!-- 
- Someone built a system at LinkedIn for observational inference (OCELOT), included various algorithms (matching, ), had a review committee. But eventually petered out and defunded.
-->

For concreteness we will treat the problem of predicting the long-run (LR) effect of an experiment on DAU from its short-run (SR) estimated effects on all metrics:

   $$E[\utt{\Delta\text{DAU}_{LR}}{true long-run}{effect on DAU} |
       \utt{\Delta \widehat{\text{DAU}}_{SR}, \ldots, \Delta\widehat{\text{engagement}}_{SR}}{estimated short-run effects}{}]$$

There are two obvious ways to calculate this:

1. **Meta-analysis.** We can run a regression across prior experiments:
   $$\Delta\widehat{\text{DAU}}_{LR} \sim 
       \Delta \widehat{\text{DAU}}_{SR} + \ldots + \Delta\widehat{\text{engagement}}_{SR}$$
   
   However the coefficients will be biased if we use on the LHS the *observed* long-run DAU, instead of the *true* long-run DAU. This bias is often large, and in fact if you run a bunch of AA tests (where the causal effect is zero) you'll find strong significant relationships between short-run and long-run impacts. I discuss below ways in which to adjust for this bias.

2. **Observational Inference.** We can run a regression across users:
   $$\text{DAU}_{LR} \sim 
       \text{DAU}_{SR} + \ldots + \text{engagement}_{SR}$$

   We can look at what is most predictive of long-run DAU across users. The problem here is obviously endogeneity, and so it's worth spending time drawing a DAG and running robustness tests to carefully think through the sources of variation we're using.

##             With Meta-Analysis

With $n$ metrics we can write the underlying model as:
   $$\utt{\pmatrix{\hat{t}_1\\\vdots\\\hat{t}_n}}{observed}{effects}
      = \utt{\pmatrix{t_1\\\vdots\\t_n}}{true}{effects}
         +\utt{\pmatrix{e_1\\\vdots\\e_n}}{noise}{(=user variation)}$$

Here we are treating $\Delta \text{DAU}_{SR}$  and $\Delta \text{DAU}_{LR}$ as two different metrics, but for some experiments we only observe the first. We thus want to estimate the effect on long-run retention (DAU$_{LR}$) given short-run metrics.
   $$E[\Delta\text{DAU}_{LR} |
       \Delta \widehat{\text{DAU}}_{SR}, \ldots, \Delta\widehat{\text{engagement}}_{SR}]$$

where
   $$\begin{aligned}
      \Delta\text{DAU}_{LR}   &= \textit{true}\text{ effect on long-run daily active users (AKA retention)}\\
      \Delta\widehat{\text{DAU}}_{SR} &= \textit{estimated}\text{ effect on short-run daily active users} \\
      \Delta\widehat{\text{engagement}}_{SR} &= \textit{estimated}\text{ effect on short-run engagement}
   \end{aligned}$$

**Running a Regression will be Biased.** The obvious thing to do is run a regression across experiments:
   $$\Delta\widehat{\text{DAU}}_{LR} \sim
      \Delta \widehat{\text{DAU}}_{SR} + \ldots + \Delta\widehat{\text{engagement}}_{SR}$$

However this will be biased. The simplest way to demonstrate the bias is to show that even with AA tests (where there is zero treatment effect on either metric) we will still get a strong predictive relationship between the observed treatment effects on each of the two metrics:

::: {.column-margin}
![A simulated scatter-plot showing 20 experiments, with N=1,000,000, $\sigma_{e1}^2=\sigma_{e2}^2=1$, with correlation 0.8. The experiments are all AA-tests, i.e. there are no true treatment effects, yet a regression of $\hat{t}_2$ on $\hat{t}_1$ will consistently yield statistically-significant coefficients of around 0.8.](images/2022-04-08-09-34-41.png)
:::

The bias is because in the regression our LHS variable is *estimated* retention ($\Delta\widehat{\text{DAU}}_{LR}$ instead of $\Delta\text{DAU}_{LR}$), and the noise in that estimate will be correlated with the noise in the estimates of short-run metrics. In the linear bivariate case (where we have just one RHS variable) then we can write:
   $$\begin{aligned}
      \ut{\frac{cov(\hat{t}_2,\hat{t}_1)}{var(\hat{t}_1)}}{regression}
      = \utt{\frac{cov(t_2,\hat{t}_1)}{var(\hat{t_1})}}{what we}{want to know}
         + \ut{\frac{cov(e_2,e_1)}{var(\hat{t}_1)}}{bias}
   \end{aligned}$$

The bias will be small if the short-run metrics have high signal-noise ratios (SNR), $\frac{var(t_1)}{var(e_1)}\gg 0$. A simple test for SNR ratio is the distribution of p-values: if most experiments are significant then the SNR is high. However in the typical case (1) $\Delta \widehat{\text{DAU}}_{SR}$ is the best predictor of $\Delta \widehat{\text{DAU}}_{LR}$; and (2) $\Delta \widehat{\text{DAU}}_{SR}$ has a low signal-noise ratio (i.e. few outcomes are stat-sig). This means the results are hard to interpret, the bias is large.


###          Adjusting for the Bias

Here are some alternatives:

1. **Run a regression just using the high-SNR metrics.** We could just drop $\Delta\widehat{\text{DAU}}_{SR}$ as a regressor because of the bias. But in practice we lose a predictive power ($R^2$), so it's hard to know when this will be a good idea without an explicit model.

2. **Adjust for bias in linear estimator.** If we want a linear estimator then we can estimate and adjust for the bias.
   $$\begin{aligned}
      \utt{\frac{cov(t_2,\hat{t}_1)}{var(\hat{t_1})}}{BLUE for}{$t_2$ given $\hat{t}_1$}
         &= \frac{cov(t_2,t_1)}{var(\hat{t}_1)}
         = \ut{\frac{cov(\hat{t}_2,\hat{t}_1)}{var(\hat{t}_1)}}{regression result}
            - \utt{\frac{cov(e_2,e_1)}{var(\hat{t}_1)}}{observable}{variables}
   \end{aligned}$$

   If everything is joint normal then the expectation is itself linear, and so this will be optimal. In practice the true distribution of effect-sizes is somewhat fat-tailed, which imply that the conditional expectation will be nonlinear in the observables. Nevertheless I think this is a good start. (One other complication is that the SNR is more complicated to calculate when experiments vary in their sample size).^[For derivation see Cunningham and Kim (2019).]

3. **Use experiment splitting.** You can randomly assign users in each experiment to one or other sub-experiments. You now effectively have a set of *pairs* of experiments, each of which has experiments with identical treatment effects ($\Delta \text{DAU}_{LR}$) but independent noise. Thus you can run a regression with LHS from one split, and RHS from other split, and you'll get an unbiased estimate. Additionally you can easily fit a nonlinear model.

4. **Run a regression just using the strongest experiments.** If the distribution of experiments is fat-tailed then the strongest experiments will have higher SNR, and so lower bias. A worry about this is that you're only estimating the relationship from outliers, so if there are nonlinearities you'll never know. At the same time the assumption of fat-tailed treatment-effects gives reason to believe the expectation will be nonlinear. (This is roughly how I interpret Dean Eckles & Alex Peysakhovich's experiments-as-instruments paper. They propose using L0 regularization and experiment-splitting cross-validations, which I think effectively just selects the strongest experiments.)


**Choosing a Reference Class.** It is important to think about the reference-class of experiments which we use to calibrate our estimates. The long-run DAU prediction can be though of as an empirical-bayes estimate, which is our best estimate conditional on the experiment being a random draw from this class of experiments.

In many cases a company's experiments will naturally fall into different classes: e.g. some have a very steep relationship between engagement and DAU, others have a very flat. It's important to both (1) visualize all the experiments, so that a reference-class can be chosen sensibly; (2) calculate the $R^2$ across experiments, so we can have some sense of confidence in our extrapolation.


##            Observational Inference

<!-- **Engagement and retention.** Returning to our application, we want to know the effect of an experiment on retention but we only measure the effect on short-run engagement. We can write this as:
      $$\xymatrix{
         *+[F]{\text{experiment}} \ar[r] \ar@{.>}@/_2pc/[rr]
         & *+[F]{\text{SR engagement}}\ar[r]
         & *+[F]{\text{LR retention}} 
      }$$

The identifying assumptions:

1. **Exclusion:** The effect of an experiment on LR retention is exclusively via its effects on SR engagement.
2. **Unconfoundedness.** The correlation betweeen engagement and retention is exclusively due to the causal effect of engagement on retention.

As stated these are slightly weird assumptions: we don't really believe that clicking "like" on a post *causes* retention, instead it's more plausible that high-quality contnet causes both engagement and retention:
      $$\xymatrix@R=.5cm{
         *+[F]\txt{experiment} \ar[r]
      &  *+[F-:<6pt>]\txt{timeline quality}\ar[d] \ar[r]
      &  *+[F]\txt{LR retention}
      \\
      &  *+[F]\txt{SR engagement}
      }$$

We can then think of factors which might violate this:

1. If someone's in a good mood that might increase both engagement and retention, but not via timeline quality.
2. If someone gets spooked about being tracked, that might lower engagement but not affect retention (and so cause attenuation).

To justify our identifying assumption we should be specific about what type of unobserved factors will affect propensity to engage and DAU. Some examples:

1. Things that make you use Twitter more: a holiday, a new phone, bad weather.
2. Random variation in tweets you see: some tweets you are more likely to engage, and may also cause you to return more often. -->

**What we want to know:** Given the short-run effect of a content experiment on engagement we want to predict the long-run effect on DAU. We can start with a simple regression along these lines:
   $$\utt{\text{DAU}_{u,t+1}}{long-run}{retention} \sim \utt{\text{engagement}_{u,t}}{short-run}{engagement}$$

**We could set up a DAG and discuss the surrogacy conditions.** The condition are that (1) all effects of an experiment on DAU are via short-run engagement; and (2) there is no unobserved factor which affects both SR engagement and LR DAU:

$$\xymatrix{
      &  *+[F-:<6pt>]\txt{unobserved}\ar@{.>}[d] \ar@{.>}[dr] \\
         *+[F]{\text{experiment}} \ar[r] \ar@{.>}@/_1pc/[rr]
         & *+[F]{\text{SR engagement}}\ar[r]
         & *+[F]{\text{LR DAU}} 
      }$$

In fact we know that engagement doesn't *literally* lie on the causal chain, instead we think engagement is a good proxy for *content* which might lie on the causal chain.

In any case I find the following setup an easier way to think about the assumptions necessary for identification:

**We can write it out a simple structural model as follows** (for compactness I leave out coefficients):

$$\begin{array}{rcccccccc}
   \text{engagement}_{u,t}
      &=& \utt{\text{temperament}_{u}}{user-specific}{propensity to engage}  
      &+& \utt{\text{mood}_{u,t}}{time-varying}{mood/holiday/etc.}
      &+& \utt{\text{content}_{u,t}}{content seen}{on platform}
      &+& \utt{\text{distractions}_{u,t}}{other platform effects}{e.g. messages, notifs}\\
   \text{DAU}_{u,t} 
      &=& \text{temperament}_{u} 
      &+&\text{mood}_{u,t}
      &+&\utt{\sum_{s=1}^\infty\beta^s\text{content}_{u,t-s}}{prior experience}{w content}
      &+&\text{distractions}_{u,t}\\
\end{array}$$


Some general observations:

1. **We would get a more credible estimate if we could directly measure content quality.** E.g. if we could use the quality of the content available to the user on the RHS, instead of just their engagement on that content. This wouldn't get perfect identification but it would help.
2. **The relative shares of variation in the RHS is important.** If most of the variation in engagement is due to variation in content (i.e. high $R^2$ from content), then we don't need to worry much about confounding from other effects. We can think of introducing control variables as a way of increasing the share of varation in engagement due to content.
3. **We should control for distractions.** If we have measures of app-related events that don't affect content-seen but do affect engagement, e.g. notifications, messages, then we should use those as controls. This will increase the relative share of variation in engagement due to content.
4. **Controlling for pre-treatment outcomes changes variation used.** If we control for `engagement`$_{t-1}$ this will change the relative contribution of each factor in the variation of engagement. Specifically it will reduce the share of the terms with higher autocorrelation. Thus by definition `temperament` will reduce its contribution. However it's unclear whether `mood` or `content` has higher autocorrelation, and so controlling for pre-treatment could either increase or decrease the relative contribution of `content`. It's probably worth doing some simple decomposition of variation in engagement into (1) user, (2) content, and (3) mood (the residual), both statically and over time.
5. **Univariate linear prediction is usually pretty good.** In my experience you can get a fairly good prediction of most user-level metrics with a linear function of the lagged values. If you use a multivariate or nonlinear function you'll get a better fit but only by a small amount (one exception: when predicting discrete variables like DAU it's useful to use a continuous lagged variable like time-spent). So I'm skeptical that adding more regressors or adding nonlinearity will significantly change the estimates or the credibility of the estimates.
6. **Estimand is not $\beta$ but $\frac{1}{1-\beta}$.** Suppose we see that 1 unit of engagement causes a certain increase in DAU over the following weeks. We then want to apply that estimate to an experiment which *permanently* increases engagement by 1 unit. We thus should take the integral over all the subsequent DAU effects. In the simple exponential case the effect of a shock at period $t$ on DAU at period $t+s$ will be $\beta^s$, and so the cumulative effect on all subsequent periods will be $1+\beta+\beta^2+\ldots=\frac{1}{1-\beta}$.
7. **Autocorrelation in content makes things messier.** If there is significant autocorrelation in content then the interpretation of `DAU~engagement` is more difficult. E.g. if we see that engagement on $t$ is correlated with DAU on $t+1$ this could be because either (1) content on $t$ content caused the DAU on $t+1$, or (2) good content on $t$ is correlated with good content on $t+1$, which in turn causes DAU on $t+1$. I don't think controlling for pre-treatment levels or trends solves this.


#                                  The Explore-Exploit Problem

**Companies know what they need to do but they keep messing it up.** For 10 years the big companies have known that they should be having some kind of bandit or adaptive way to tune parameters and to recommend content. Every year they fund a new project to work on it, every year the results are disappointing.

**Where explore-exploit is needed:**

- Tuning parameters on a recommendation algorithm to maximize retention.
- Tuning parameters on video or audio streaming to maximize satisfaction and retention.
- Tuning parameters on ad bidding to maximize net profit.
- Exploring different components of quality in recommendations:
   - Content quality
   - Producer quality
   - User-topic interest
   
   In each case showing some content that is *less* interesting to the user, but in return for learning more information.

**Why do these projects keep failing?** AB tests are easy to implement, explore-exploit projects are much harder to run. The typical scenario is that PhDs are brought in to consult on the design, the designs become too complex, everyone gets confused, and the project falls apart.

```{tikz}
#| column: margin
#| fig-cap: If $\beta_i^*$ is already close to the global optimum then there will be not much loss from perturbing some users because the loss function should be flat in that neighborhood.
\begin{tikzpicture}[scale=4]
   \draw[<->] (0,1) node[rotate=90,anchor=south east]{objective}
      --(0,0)-- (1,0) node[anchor=north east]{weight};
   \draw[domain=.1:.9, variable=\x, line width=1, red]
      plot ({\x}, {.8-3*(\x-.5)^(2)});
   \draw[dashed] (.3,0)--(.3,1) node[rotate=90,right]{$\beta_i^*-\varepsilon_i$};
   \draw[dashed] (.5,0)--(.5,1)  node[rotate=90,right]{$\beta_i^*$}; 
   \draw[dashed] (.7,0)--(.7,1)  node[rotate=90,right]{$\beta_i^*+\varepsilon_i$};
\end{tikzpicture}
```

**Recommendation: a simple tuning algorithm using weather stations.** Here is a crude but easy-to-execute method for dynamically optimizing parameters. It's less efficient than other algorithms but it's easy to describe, easy to implement (it uses the existing AB-test system), and easy to visualize and see that it's working as intended. In short: for each parameter we set up two permanent "weather stations" treatments: 1/3 of users get a slightly higher value, and 1/3 of users get a slightly lower value.

   Suppose we have $n$ parameters to tune $(\beta_1,\ldots,\beta_n)$: we run $n$ orthogonal experiments, each of which partitions the all users into 3 equal-sized buckets, with either (1) $\beta_n=\beta_n^*$ , (2) $\beta_n=\beta_n^*-\varepsilon_n$, (3) $\beta_n=\beta_n^*+\varepsilon_n$, where $\beta_n^*$ is the current production level of $\beta$. If $n=2$ then users would be assigned as such:

```{tikz}
#| column: margin
#| fig-cap: If we start at a point above the global optimum then the "low" group benefits and the "high" group suffers, but we can see that any short-term cost will be outweighed by long-term benefit.
\begin{tikzpicture}[scale=4]
   \draw[<->] (0,1) node[rotate=90,anchor=south east]{objective}
      --(0,0)-- (1,0) node[anchor=north east]{weight};
   \draw[domain=0.1:.75, variable=\x, line width=1, red]
      plot ({\x}, {.8-2*(\x-.2)^(2)});
   \draw[dashed] (.3,0)--(.3,1) node[rotate=90,right]{$\beta_i^*-\varepsilon_i$};
   \draw[dashed] (.5,0)--(.5,1)  node[rotate=90,right]{$\beta_i^*$}; 
   \draw[dashed] (.7,0)--(.7,1)  node[rotate=90,right]{$\beta_i^*+\varepsilon_i$};
\end{tikzpicture}
```

   |                         | $\beta_1-\varepsilon_1$ | $\beta_1$ | $\beta_1+\varepsilon_1$ |
   | ----------------------: | ----------------------- | --------- | ----------------------- |
   | $\beta_2-\varepsilon_2$ | 1/9                     | 1/9       | 1/9                     |
   |               $\beta_2$ | 1/9                     | 1/9       | 1/9                     |
   | $\beta_2+\varepsilon_2$ | 1/9                     | 1/9       | 1/9                     |

   The size of the perturbations $\varepsilon_i$ are easy to adjust dynamically as the data comes in: we can start small and keep increasing until we see a stat-sig difference in the outcome. We monitor the trajectory of each bucket continuously, and once/month make a formal decision about whether to adjust the production parameters, e.g. increasing $\beta_n$ to $\beta_n+\varepsilon_n$ or lowering it to $\beta_n-\varepsilon_n$. When interpreting these experiments it is important to monitor the full trajectory of outcomes over time, ideally a visualization will show a large matrix of trajectories, with one cell for each combination of experiment-bucket and metric. The shipping criteria can be a pre-specific weighted average of metrics or .
   
**We can use the data generated to explore other aspects:** (1) whether there are significant interaction effects between the different experiments (e.g. if the users who have both increasing $\beta_1$ and $\beta_2$ have a different effect), and (2) whether there are significant heterogeneities in outcomes across subgroups.

**This is the simplest general framework I know of for continuous optimization of a set of parameters.** I think that simplicity is by far the most important criterion: I have seen a long history of optimization projects get tangled in complexity and fail. Because of the past history of failures I think it's crucial to do the simplest and most transparent thing at each point until you have a steady rhythm and track record of making progress.

**The hard work is the choice of parameters to tune.** Once you have a small set of parameters to tune it's not too hard to find the global optimum. However in typical problems there are thousands or millions or billions of possible parameters, how should you choose which ones to tune?


<!-- 
- (Meta's AX is used for hyperparameter tuning, but not that much for product experiments)
-->



#                                  The Culture Problem [UNFINISHED]

**Tool-makers don't trust tool-users.** Some common themes:

   - If you give experimenters a lot of metrics they'll choose the subset which support their preferred decision.

   - If you give product leaders a goal on a metric that is a proxy for quality they'll increase the metric and meet the goal but in a way that doesn't increase quality.

   - If you let data scientists use observational-causal-inference tools they'll use them indiscriminately, hardly spending any time to think about whether the exogeneity assumption hold in their cases.

There are three broad approaches: 

1. Put restrictions on experimenters to prevent them from misinterpreting experiment results.
2. Change incentives for experimenters to prevent them from misusing experiment results.
3. Educate experiments so they use experiment resuts better.

[UNFINISHED]

<!-- **Taking culture as fixed.** People are trying to develop.

**Tech companies are full of quack doctors.** infested with dousing people ; witch hunters ; quack doctors ; medicine men.  -->



<!-- 
#                                   Bibliography


Tom Cunningham and Dominic Coey (2019, WWW) **[Improving Treatment Effect Estimators Through Experiment Splitting](https://research.fb.com/wp-content/uploads/2019/02/Improving-Treatment-Effect-Estimators-Through-Experiment-Splitting.pdf)**.

Tom Cunningham and Josh Kim (2020, CODE) **[Interpreting Experiments with Multiple Outcomes](https://t.co/cmP24ywRj5)** -->

