---
title: How Much has Social Media affected Polarization?
author: Tom Cunningham
date: 2023-08-04
execute:
  echo: false
  cache: true # caches chunk output
fig-align: center
bibliography: social-media.bib
#csl: nature.csl
reference-location: margin
#citation-location: margin
format:
   html:
      toc: true
      toc-depth: 2
      toc-location: left
      code-fold: true
      html-math-method:
         method: mathjax
         url: "https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js"
         #     ^ this forces SVG instead of CHTML, otherwise xypic renders weird
      include-in-header:
         - text: |
            <script>window.MathJax = {
                     loader: { load: ['[custom]/xypic.js'],
                                 paths: {custom: 'https://cdn.jsdelivr.net/gh/sonoisa/XyJax-v3@3.0.1/build/'}},
                  tex: {packages: {'[+]': ['xypic']},
                     macros: {
                        bm: ["\\boldsymbol{#1}", 1],
                        ut: ["\\underbrace{#1}_{\\text{#2}}", 2],
                        utt: ["\\underbrace{#1}_{\\substack{\\text{#2}\\\\\\text{#3}}}", 3]
                     }}};
            </script>

engine: knitr
draft: true
editor:
   render-on-save: true
---
<style>
    h1 {  border-bottom: 4px solid black;}
    h2 {  border-bottom: 1px solid gray; padding-bottom: 0px; font-size: 14px; color: black; }
</style>

<!-- <span style="background:yellow;">==Draft, don't circulate yet please.==</span> -->

#                    Summary

**Three experiments which changed Facebook's feed ranking found small effects on affective polarization:** the effects were less than 0.03 standard deviations, compared to a growth in polarization over the last 40 years of 1.1 standard deviations.[^thanks]

**However small effects in these experiments are consistent with large aggregate effects of social media.** The overall contribution of social media to polarization will differ from these experimental estimates in a number of ways, described below. Rough attempts to account for these considerations make me think the aggregate effect could easily be 10 or 20 times larger then the effect measured in these experiments, and so the small measured effects are consistent with large aggregate effects.

**Nevertheless, other lines of evidence imply that social media has not made a huge contribution to US polarization.** Specifically (1) social media is still a minority share of all partisan media, (2) polarization had been growing for 20 years prior to social media's introduction, and much of the growth since 2014 was in people without internet access.

[^thanks]: Thanks to Dean Eckles, Solomon Messing, Jeff Allen, & Brandon Silverman for discussion which led to this post. I put together the [spreadsheet summary of results](https://docs.google.com/spreadsheets/d/1_96kEzP9MFLcBFppVV0Bl7O3Cv9hQFxKArwS2zVCtXE/edit#gid=0) with Dean and Solomon.

<!-- methodology: filling out cells -->

#                    Introduction

**Last week's papers reported the results of three experiments on Facebook's News Feed.** The experiments were run between September and December 2020, and half-way through participants were asked about their feelings towards members of their own party and the opposing party, e.g. *"how warm do you feel about Republicans on a scale of 0-100?"* The question were aggregated to make an index of "affective polarization":
   $$\xymatrix@R=0em@C=6em{
      *+[F:<5pt>]\txt{rank items on News\\Feed chronologically}  \ar[dr] & \\
      *+[F:<5pt>]\txt{remove reshares\\on News Feed}  \ar[r] & 
         *+[F:<5pt>]\txt{affective\\polarization\\survey}\\
      *+[F:<5pt>]\txt{downrank likeminded\\items on News Feed}  \ar[ur]
      }
   $$

**The results.** The 95% confidence intervals on affective polarization are approximately $\pm$0.03 SDs, and the effect-sizes are all smaller than that (i.e. they do not estimate a significant effect). Dean Eckles, Solomon Messing, and myself put together a [spreadsheet summary](https://docs.google.com/spreadsheets/d/1_96kEzP9MFLcBFppVV0Bl7O3Cv9hQFxKArwS2zVCtXE/edit#gid=0) of the results from all the experiments reported so far, along with other results from the literature on political effects of media.

<!-- Over the last 20 years affective polarization in the US has increased by roughly 0.5 SD, so if we compare the raw effects then our CIs would say FB could not have contributed more than 1/25th of this increase. -->


**All three experiments found effects on polarization of less than 0.03 standard deviations (SDs).** The estimated effect sizes were all smaller than the width of the 95% confidence intervals, around 0.03 SDs. @guess2023chronological concludes:
 
   > "these findings suggest that social media algorithms may not be the root cause of phenomena such as increasing political polarization."

**What can we conclude about social media?** Suppose we want to estimate the total effect of social media on political polarization in 2020, compared to the effects estimated in these experiments. I can think of five basic ways in which the two quantities will differ:

   1. **Feature.** Whether changing one feature or disabling the app entirely.
   2. **Breadth.** Whether changing the experience for one user or for all users.
   3. **Duration.** Whether changing the experience for 1.5 months or for the whole history of social media.
   4. **Timing.** Whether changing the experience in Oct 2020, or the average effect over 2014-2020.
   5. **Category.** Whether changing the experience just for Facebook or for all social media.

Below I try to give quantitative estimates for each of these five differences, and it makes me think that having tight confidence intervals on the effects of the experiments (plus or minus 0.03 SDs) is still consistent with broad confidence intervals on the aggregate effect of social media (plus or minus 0.5 SDs).

   <!-- (@boxell2022PolarizationTrends estimate that US polarization has increased by around 0.3 SDs since 2014, when Facebook and other social media companies became widespread). -->

[^timing]: Although the treatments ran for 3 months (24 Sep--23 Dec 2020), the survey responses were collected during the experiment and the average survey measure was measured after around 1.5 months of treatment: see Figure S2 in the Supplementary Appendix.

#                 Comparing Experimental to Aggregate Effects

Here I compare the causal effect on polarization from these experiments, to the long-run aggregate effect of social media on political polarization. There are a lot of intermediate estimands that we might also be interested in, e.g. the effect of permanantly disabling Facebook for everybody, without disabling the other social networks; or the effect of temporarily disabling all social networks.

1. ***Duration*: the experiments only measure short-run effects.** These experiments measured the effect of a News Feed change on polarization after around 1.5-2 months, while most American adults have been using Facebook for perhaps 10 years.^[The polarization survey was run during the experiments, I believe the average outcome was mesaured around 1.5 months after the start of the experiment.] It is hard to judge how quickly we should expect polarization attitudes to respond to treatment, and I have not found useful academic literature. The national polarization trends documented in @allcott2019trends seem fairly stable despite a volatile news cycle suggesting attitudes change relatively slowly. If the half-life of adjustment was 1.5 months (which seems quite short to me) then the effects measured in these experiments would be half of the long-run effect. It seems likely that the effects of exposure do not decay at a constant rate: there is a short-run component that decays quickly (the effect of salience), and a long-run component that decays slowly.^[I could not find any discussion of dynamic effects of the experiments in any of the so-far published experiment analyses, despite each one having an online appendix with around 300 pages of supplementary material, and clear reason to expect dynamic effects.]

2. ***Feature*: the experiments have small effects on exposure.** Each of the experiments reported have effects on overall Facebook time-spent of much less than 50%, and on exposure to potentially polarizing material of likely less than 50%. Thus the aggregate effect of Facebook withdrawal seems likely to be at least 2X larger than measured by any of these experiments.

   | effects on metric[^fn]      | time-spent | political impression | cross-cutting impressions | untrustworthy impressions |
   | --------------------------- | ---------- | -------------------- | ------------------------- | ------------------------- |
   | baseline                    | ?          | 14pp                 | 21pp                      | 3pp                       |
   |                             |            |                      |                           |                           |
   | - rank chronologically      | -21%       | +12%                 | -10%                      | +60%                      |
   | - remove reshares           | -5%        | -14%                 | -3%                       | -32%                      |
   | - downrank likeminded posts | -0%        | -5%                  | +7%                       | ?                         |

   [^fn]: [source data](https://docs.google.com/spreadsheets/d/1_96kEzP9MFLcBFppVV0Bl7O3Cv9hQFxKArwS2zVCtXE/edit#gid=0

   <!-- (Note the deactivation experiment reported in @allcott2020welfare reduces their measure of "polarization" by 0.16 SD significantly, however this mainly driven by responses to a question about "exposure to congenial news", while the responses to the "affective polarization" do not move statistically significantly). -->

3. ***Breadth*: Experiments exclude network effects.** The effects of social media on polarization likely work not just through direct exposure but also downstream via peoples' interactions with friends and families, in both online and offline conversations. Thus it seems likely the aggregate effect could be 2X or larger than the individual effect.

4. ***Category*: The experiments affected only Facebook, but in 2020 Facebook probably accounted for around 1/4 of all partisan political content that people are exposed to on social media.** If we include YouTube, TikTok, Instagram, Twitter, Snapchat, Reddit, and the long tail of niche social networks.

5. ***Timing*: Experiments were run during the lead-up to the 2020 election.** The experiments ran between September and November 2020. If we compare this to the average experience on Facebook over the previous decade there are reasons to expect either larger or smaller effects on polarization:
   - Following the 2016 elections facebook invested very heavily in integrity systems following the 2016 election, reducing prevalence of many types of bad content by factors of between 2X and 10X. In May 2020 Guy Rosen [described](https://about.fb.com/news/2020/05/investments-to-fight-polarization/) "a number of important steps to reduce the amount of content that could drive polarization on our platform" over the prior years.
   - @allcott2019trends estimates that engagement with fake news domains on Facebook steadily grew over 2015 and 2016, roughly doubling, then steadily fell over 2017 and 2018, roughly halving.
      ![](images/2023-08-02-16-19-16.png)
   - Meta's Community Standards reports show a decline in prevalence of most types of harmful content by a factor of between 2 and 5 over roughly 2017 to 2022 (see chart [here](https://tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data.html#meta-facebook-instagram)). 
   - Prior to and during the 2020 election Facebook implemented a series of extra "break the glass" measures with the effect of suppressing extreme or fringe political content. 
   - During election seasons there tends to be significantly more political content circulating. However this is true both for social media and for other media so if the influence of social media on polarization depends on the *share* of exposure then the effect would be the same in election season as outside election season.
   - Since the 2020 election Facebook has applied additional demotions to politics. The WSJ reported that in late 2021 *"Mr. Zuckerberg and the board chose the most drastic [option], instructing the company to demote posts on “sensitive” [(politics and health)] topics as much as possible in the newsfeed that greets users when they open the app"*, and that in 2022 *"politics accounts for less than 3% of total content views in users’ newsfeed, down from 6% around the time of the 2020 election."* The article reports that these experiments reduced daily visitation (daily active users) by 0.2%.

If factors 1-4 each contributed a 2X amplification then the aggregate effect of social media would be 16X larger than the experimentally-measured effect, i.e. effective confidence intervals would be 0.48 SDs instead of 0.03 SDs, i.e. sufficient to explain the entire growth in polarization since 2004.

#                 Appendix: Data on Media and Polarization

I have made very rough estimates for affective polarization and for time-spent on partisan content:

|                              | 2004 | 2016 | 2020 |
| ---------------------------- | :--: | :--: | :--: |
| affective polarization score |  40  |  45  |  50  |
|                              |      |      |      |
| minutes/day/adult            |      |      |      |
| - media                      | 300  | 300  | 300  |
| - media civic                |  15  |  15  |  15  |
| - media civic partisan       |  5   |  8   |  6   |
| - Facebook                   |  0   |  60  |  60  |
| - Facebook civic             |  0   |  6   |  3   |
| - Facebook civic partisan    |  0   |  3   |  1   |
|                              |      |      |      |

**Content on Meta platforms.** @guess2023chronological has data from the control group in their 2020 experiments:

   | Share of Impressions               | Facebook | Instagram |
   | ---------------------------------- | -------- | --------- |
   | Political content                  | 14%      | 5%        |
   | Political news content             | 6%       | -         |
   | Content from untrustworthy sources | 3%       | 1%        |
   | Uncivil content                    | 3%       | 2%        |

[Pew 2022](https://www.pewresearch.org/journalism/fact-sheet/news-platform-fact-sheet/?tabId=tab-4ef8dece-845a-4b25-8637-ceb3114503c5) has data on where people get their news from:

   |               | pct adults regularly get news from |
   | ------------- | ---------------------------------- |
   | television    | 65%                                |
   | news websites | 63%                                |
   | search        | 60%                                |
   | social media  | 50%                                |
   | radio         | 47%                                |
   | print         | 33%                                |
   | podcasts      | 23%                               |

**Radio show popularity.** Around half of the top 20 most-listened radio shows in the US are conservative talk, with around 90M weekly listeners (this is double-counting overlapping users). [Data from 2021](https://en.wikipedia.org/wiki/List_of_most-listened-to_radio_programs).

**Television.** Fox News is Cable TV's most-watched network with around 5M regular viewers. ([source from 2016](http://www.adweek.com/tvnewser/2016-ratings-fox-news-channel-is-cable-tvs-most-watched-network/315009)).


**Trends in affective polarization.** @boxell2022PolarizationTrends document affective polarization across a dozen countries, 1978-2020:

   ![](images/2023-07-27-15-03-32.png)

   1. In the US affective polarization index increased from around 25 to 50, *"an increase of 1.08 standard deviations as measured in the 1978 distribution."*  (I'm not sure if the SD increased).

   2. Across the world there's no clear trend: some countries increased, other countries decreased. This weakens the simple argument that polarization has increased at the same time as social media use.

   3. In the US the trend seems to be entirely due to increasing negative feelings about the opposing party:
      ![](images/2023-08-03-12-51-42.png)

   The US timeseries can be seen [online](https://electionstudies.org/data-tools/anes-guide/anes-guide.html?chart=affective_polarization_parties) from the [ANES](https://electionstudies.org).

**Time spent on social media.** [Statista](https://www.statista.com/statistics/433871/daily-social-media-usage-worldwide/): Average time-spent 150 minutes/day/person on social networks

**Observational data finds that much of the growth in polarization in the US was among people who were not online.** @boxell2022PolarizationTrends say 

   > "the growth in polarization in recent years [1996-2012] is largest for the demographic groups least likely to use the internet and social media"

**The academic literature has identified other possible causes of polarization.** Some potential causes: southern realignment, 1968 changes to the primary system, the Obama presidency, the tea party movement (though each of these could be in part proximal causes). Martin & Yurcoglu (2017) argue that a large part of recent growth is due to cable news: 
   > "the cable news channels can explain an increase in political polarization of similar size to that observed in the US population over [2000-2008]. ... In absolute terms, however, this increase is fairly small."

See also Haidt and Bail's long document [Social Media and Political Dysfunction: A Collaborative Review](https://docs.google.com/document/d/1vVAtMCQnz8WVxtSNQev_e1cGmY9rnY96ecYuAj6C548/edit#heading=h.96bogdklzo1j)

**Notes on the effect of Facebook deactivation on polarization in @allcott2020welfare:**

- Unlike the questions used in typical population surveys, the questions were explicitly about their feelings during the period of the experiment, e.g. *"Thinking back over the last 4 weeks, how warm or cold did you feel towards the parties and the president on the feeling thermometer?"* 

- By far the largest effect was on the "congenial news exposure" question: *"over the last 4 weeks how often did you see news that made you better understand the point of view of the Democrat (Republican) party?"* The score was the difference between the answer for their own party vs the other-side party. It seems to me both that (1) deactivating Facebook would quite mechanically affect one's exposure to such news, and (2) this wouldn't normally be called a measure of "polarization" in the literature. The paper mentions in a footnote that *"the effect on the political polarization index is robust to excluding each of the seven individual component variables,"* but it turns out that removing "congenial news exposure" halves the effect-size and shifts the p-value from 0.00 to 0.09 (i.e. from very significant to non-significant).

#                 Appendix: How the Results were Described

**Is this effect small or large?** A lot of the discussion has described the effects as "small" or "undetectable." Describing the effect-sizes this way forces us to guess at what's the definition of "small", or to guess at the width of the confidence intervals.

**Description of effect sizes from the abstracts:**

Guess et al. (2023) ["Reshares on social media amplify political news but do not detectably affect beliefs or opinions"](https://www.science.org/doi/full/10.1126/science.add8424)

> "Contrary to expectations, the treatment does not significantly affect political polarization or any measure of individual-level political attitudes."

Guess et al. (2023) ["How do social media feed algorithms affect attitudes and behavior in an election campaign?"](https://www.science.org/doi/10.1126/science.abp9364?adobe_mc=MCMID%3D34187371435160557723817445106394877226%7CMCORGID%3D242B6472541199F70A4C98A6%2540AdobeOrg%7CTS%3D1690484833)

> "Despite these substantial changes in users’ on-platform experience, the chronological feed did not significantly alter levels of issue polarization, affective polarization, political knowledge, or other key attitudes during the 3-month study period.

Nyhan et al. (2023) ["Like-minded sources on Facebook are prevalent but not polarizing"](https://www.nature.com/articles/s41586-023-06297-w)

> "[treatment] had no measurable effects on eight preregistered attitudinal measures such as affective polarization, ideological extremity, candidate evaluations and belief in false claims. These precisely estimated results suggest that although exposure to content from like-minded sources on social media is common, reducing its prevalence during the 2020 US presidential election did not correspondingly reduce polarization in beliefs or attitudes."

**Discussion of Power:**

@guess2023chronological:

> "The large samples ... allowed for adequate statistical power to detect small effects (for example, for affective polarization, we were powered to detect population average treatment effects with Cohen’s d = 0.032 or larger for both Facebook and Instagram)."

> "In all cases, we could rule out effect sizes smaller than those found in previous research [citation to Allcott 2020]

> "did not cause detectable changes in downstream political attitudes, knowledge, or offline behavior, including survey-based measures of polarization and political participation.

> "these findings suggest that social media algorithms may not be the root cause of phenomena such as increasing political polarization"

**Description of effect sizes from authors' subsequent commentary:**

[Andy Guess](https://twitter.com/andyguess/status/1684632256403628035):

> "Despite these changes in users' on-platform experience, the chronological feed did not significantly alter levels of issue or affective polarization, political knowledge, political behavior or other key attitudes during the study. ... It’s interesting to note the disconnect between the relatively powerful effects we document in terms of platform experiences and behavior, and the downstream outcomes focused on knowledge, attitudes, and off-platform behavior.""

> "As with the chrono-feed experiment, removing reshares did not significantly affect political polarization or other individual-level political attitudes."

[Jennifer Pan](https://twitter.com/jenjpan/status/1684628685645832195):

> "Contrary to common belief, chronological feed did not significantly alter polarization, politics knowledge, or other survey-based outcomes, even though chronological feed led users to spend much less time on FB and Insta + changed what content they saw."

[Brendan Nyhan](https://twitter.com/BrendanNyhan/status/1684631397443489793)

> "Median FB user gets 50.4% of content from like-minded sources. But reducing exposure by ~1/3 for 3 months had no measurable effect on attitudes ... Decreasing exposure to content from like-minded sources had no measurable effect on a series of measures of political attitudes including affective polarization and ideological extremity."

**Other description of effect sizes:**

[Matthew Yglesias:](https://twitter.com/mattyglesias/status/1684654101626302464)

> "Eliminating the filter bubble doesn't seem to change political attitudes... why is the stuff we write/share so unpersuasive?

[Nick Clegg](https://about.fb.com/news/2023/07/research-social-media-impact-elections/):

> "show ... there is little evidence that key features of Meta’s platforms alone cause harmful ‘affective’ polarization, or have meaningful effects on key political attitudes, beliefs or behaviors."


#                 Appendix: Discussions of Power in the 2020 papers

@guess2023chronological:

> The large samples (Facebook: n = 23,391; Instagram: n = 21,373), comprising participants who completed the first two surveys and at least one of the subsequent three waves, allowed for adequate statistical power to detect small effects (for example, for affective polarization, we were powered to detect population average treatment effects with Cohen’s d = 0.032 or larger for both Facebook and Instagram). 

(the supplement and pre-analysis plan do not mention "power" or seem to discuss effect-sizes)

@guess2023reshares

> "allow for adequate statistical power to detect small effects (e.g., for affective polarization, we are powered to detect population average treatment effects with Cohen’s d = 0.032 or larger and sample average treatment effects d = 0.023 or larger)."


#                 Appendix: Model of Media Effects


The simplest model:

   $$\xymatrix@=3em{
   *+[F:<5pt>]\txt{attitude in\\media} \ar[r]|{\gamma}
      & *+[F:<5pt>]\txt{consumer\\attitude}
   }$$

We want to estimate $\gamma$, how much does media attitude influence consumer attitude? We can write this with an equation:

   $$\utt{y}{consumer}{attitude}
      = \utt{(1-\gamma)\cdot \tilde{y}}{pre-existing}{attitude}
         +\utt{\gamma \cdot m}{attitude}{in media}.$$

We can extend the model (1) to be dynamic, (2) where media attitude is the weighted average of all the media you're exposed to:

$$\begin{aligned}
   \utt{y_t}{attitude at}{time $t$}
      &= \utt{(1-\gamma)\tilde{y}}{exogenous}{attitude} + 
         \ut{\gamma \sum_{s=0}^\infty \beta^s \utt{\bar{m}_{t-s}}{attitude in}{media at $t-s$}}{media effect}
      && \text{(attitude a weighted avg of exogenous and exposure)} \\
   \bar{m}_t &= \frac{\sum_{i=1}^n x_{i,t}m_{i,t}}{\sum_{i=1}^nx_{i,t}}
      && \text{(avg attitude in media exposed to)} \\
   x_{i,t} &= \text{exposure to source $i$ at time $t$} \\
   m_{i,t} &= \text{attitude of source $i$ at time $t$}
\end{aligned}$$

This still leaves out (1) differently persuasive media; (2) non-exponential decay in influence; (3) interactions in influence over time (non-separabilities); (4) persuasion depending on total exposure rather than average exposure.

```{tikz}
#| column: margin
#| fig-cap: "**Persuasion depends on total exposure:** The marginal effect of exposure is independent of the share of total exposure."
\usetikzlibrary{positioning}
\begin{tikzpicture}[scale=4]
   \draw (0,1)-- node[midway,above,rotate=90]{partisan media exposure} (0,0)
      -- node[midway,below]{nonpartisan media exposure} (1,0);
   \draw[dashed] (0,0)--(1,1);
   \draw[dashed] (.3,0)--(1,.7);
   \draw[dashed] (.6,0)--(1,.4);
   \draw[dashed] (0,.3)--(.7,1);
   \draw[dashed] (0,.6)--(.4,1);
\end{tikzpicture}
```

```{tikz}
#| column: margin
#| fig-cap: "**Persuasion depends on average exposure:** The marginal effect of exposure is proportional to the share of total exposure."
\usetikzlibrary{positioning}
\begin{tikzpicture}[scale=4]
   \draw (0,1)-- node[midway,above,rotate=90]{partisan media exposure} (0,0)
      -- node[midway,below]{nonpartisan media exposure} (1,0);
   \draw[dashed] (0,0)--(1,1);
   \draw[dashed] (0,0)--(1,.66);
   \draw[dashed] (0,0)--(1,.33);
   \draw[dashed] (0,0)--(.66,1);
   \draw[dashed] (0,0)--(.33,1);
\end{tikzpicture}
```

**Note: two types of aggregation.** We could write the aggregator in two ways:

   $$\begin{aligned}
      y  &= (1-\gamma)\tilde{y} + \gamma\frac{\sum_{i=1}^n x_{i,t}m_{i,t}}{\sum_{i=1}^n x_{i,t}}
         && \text{(depends on avg exposure)}\\
      y  &= (1-\gamma)\tilde{y} + \gamma \sum_{i=1}^n x_{i,t}m_{i,t}
         && \text{(depends on total exposure)}
   \end{aligned}$$



#                 Appendix: Qualifications

There are a lot of sanity checks that I would ordinarily want to check if these were my own experiments, but I haven't had time to do. I expect these questions are addressed somewhere in the 300-page supplements but I thought worth briefly flagging here.

- **Differential response.** When using surveys as outcomes in experiments on social media you often have a problem with treatment affecting the propensity to respond to the survey. In these cases the surveys were administered off-platform so there's less danger, but I would still expect some effect.


**Quality of the papers.** The papers are very impressive pieces of work, and I thank the authors and everyone involved in getting them published. I have a couple of usual complaints about economics research: First, it's very confusing to describe an effect as "undetectable," the reader can only learn has no idea how to interpret that without knowing the confidence intervals, every effect is non-zero, we need to focus purely on how large they are. Second, I wish the papers and supplementary materials had far more rich descriptive visualizations, I think it'd give the reader a much better sense of what's going on.



#                    Offcuts

Here is I think the simplest model of what the experiments are doing, and what they tell us about the effect of social media on polarization:^[The "affective polarization" measure is a combination of a few survey questions: how "favorable" or "warm" do you feel about Democrat supporters and candidates (or Republicans), and how "smart" do you think are people who support Democrats? This metric is only calculated for people who either self-identify as Democrat or as Republican.]

$$\xymatrix@=1em{
*+[F:<5pt>]\txt{chronological ranking\\on News Feed}  \ar[drr] & \\
*+[F:<5pt>]\txt{remove reshares\\on News Feed}  \ar[rr] & 
   &
   *+[F:<5pt>]\txt{partisan content\\on FB News Feed}\ar[r] &
   *+[F:<5pt>]\txt{partisan content\\overall}\ar[r] &
   *+[F:<5pt>]\txt{affective\\polarization}\\
   *+[F:<5pt>]\txt{downrank likeminded\\on News Feed}  \ar[urr] \\
   *+[F:<5pt>]\txt{deactivate\\Facebook}  \ar[uurr] \\
}
$$

**We now know that the first three experiments showed no effect on affective polarization.** More precisely: after being exposed to treatment for around 1.5 months the treated group had affective polarization less than 0.03 standard deviations different from ordinary Facebook users (0.03 is the detectable effect-size).[^timing]

**Does this exonerate social media?** @guess2023chronological says *"these findings suggest that social media algorithms may not be the root cause of phenomena such as increasing political polarization."* However I think this is not clear. Descriptive literature says that affective polarization in the US has increased by around 0.2 standard deviations since 2004 (when Facebook was launched)^[@boxell2022PolarizationTrends]. However we should expect the experiments to under-state the impact of FB on aggregate polarization for a number of reasons: (1) the first 3 experiments do not seem to radically change exposure to partisan content on News Feed, e.g. removing reshares reduces political impressions by 14%; (2) the partisan posts seen on News Feed are only a share of all partisan content on Facebook, which in turn is only a share of all partisan content on social media; (3) the experiments only ran for 1.5 months before the outcome variable was measured, a fraction of the 20 years that Facebook has been running; (4) the experiments do not measure peer effects outside of Facebook, such as through family and friends. Thus it seems to me possible to believe both (1) these experiments have effects on polarization less than 0.03 SD; (2) Facebook has had a substantial impact on polarization in the US.

**Three new experiments appear to show Facebook has a small impact on political polarization:** The experiments rule out effects on affective polarization of more than 0.02 standard deviations (SD), a much smaller effect than the aggregate increase of 0.5 SD over the last 20 years.[^thanks]

**However we can be confident these experiments under-estimate the aggregate effect of Facebook use on polarization.** This is because they are (a) short-run, (b) user-level, (c) manipulate individual features, (d) were run during a period of conservative ranking. If each reason contributes a factor of 2X to effect-sizes then the long-run aggregate effect could be 16X larger than what is measured in these experiments, i.e. meaning we could only rule out an aggregate effect of 0.32 SDs.

**In any case there are other non-experimental reasons to think Facebook hasn't made a large contribution to US polarization.** Specifically (1) Facebook is only a small share of all political exposure, (2) most of the aggregate growth in polarization in the last 40 years was among people without internet access, (3) there are a number of other big potential culprits for polarization growth.