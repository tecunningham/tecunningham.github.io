---
title: The Curve
citation: true
bibliography: ai.bib
reference-location: section
citation-location: document
date: 2025-10-06
author: Tom Cunningham
draft: true
engine: knitr
format:
  html:
    toc: true
    other-formats: false
    lightbox: auto    # ‚Üê enables click-to-zoom for figures/images
---

<!-- https://tecunningham.github.io/posts/2025-10-06-the-curve.html -->



<style>
   dl {display: grid;}
   dt {grid-column-start: 1; width: 10em;}
   dd {grid-column-start: 2; margin-left: 2em;}
</style>

::: {.column-margin}
   Thanks to XXX
:::


Some topics that kept coming up at the Curve:

1. We need more forecasts of economic impacts.
2. We need more theory of capabilities.
3. We need more metrics of capabilities.
4. We need more theory of offense-defense balance.
5. There are two ways of modelling future capabilities.

I feel bad saying "we need," & scolding others for the work they're not doing, so I've also added my own very tentative best guesses about each.

![](images/2025-10-06-11-35-49.png){.column-margin}

##       We need more forecasts of economic impacts

We only have a few explicit forecasts of strong AI.
: Only a few people have written down explicit forecasts of the economic effect of strong AI, i.e. the effects on GDP, employment, wages, asset prices, welfare. The closest I'm aware of is Epoch's GATE model.

I think explicit forecasts would be incredibly useful.
: Many informal conversations and arguments often use vague terms, & it's not clear whether we're really disagreeing. If we are going to consider a policy then we should have a clear prediction. It's like early 2020: if we're going to make a decision about a lock-down it should be based on a clear idea about the counterfactual, which includes a lot of equilibrium effects (FWIW New Zealand managed to navigate COVID exceptionally well because they saw what was happening to other countries first).

Existing forecasts:
:     1. Most academic forecasts assume that AI capabilities plateau, i.e. it's a one-time labor-saving innovation, and the effects are spread out over time due to diffusion (Acemoglu, Aghion & Bunel, Wharton).
      1. Brynjolfsson and Korinek give a range of forecasts on GDP growth, but they're relatively lightly modelled.
      2. @korinek2024scenarios: over 15 years GDP triples; wages increase a little at first, then collapse when everything is automated. GDP continues to increase.
      3. Epoch's GATE model: they forecast full automation in 2034, by which point GWP has grown 10X. They forecast wages will increase dramatically then collapse somewhat after full automation is achieved.
      4. Tom Davidson's [2021 report on Explosive Growth](https://www.openphilanthropy.org/research/could-advanced-ai-drive-explosive-economic-growth/) (see also Davidson's later model of [takeoff speeds](https://takeoffspeeds.com)).

      We also have forecasting markets and individual forecats but these are hard to interpret: they are a combination of capabilities progress and the effects of those capabilities.

      What am I missing?

What are my forecasts?
: (...)

How can we get more people to forecast?
: One idea that Anna Yelikazova and I discussed: sponsor a couple of dozen econ grad students to make forecasts. Get them to write 5-page justifications, and give prizes to the most compelling ones.


##       We need more theory of capabilities.

There are many projects to collect data on the whole waterfall

: 1. Data on AI capabilities -- e.g. benchmarks that have representative tasks across the economy (e.g. GDP-val, APEX).
      1. Data on AI uplift -- how much more efficient does it make you?
      2. Data on AI adoption -- adoption by occupations, by industry, by demographic.
      3. Data on AI usage -- what types of economic tasks are LLMs used for.
      4. Data on AI impacts -- e.g. changes in hiring and wages by occupation.

Collecting data is hard without theory.

: I don't have that many opinionated *theories* on how each of these should move. I feel theories are super important because we're expecting things to change rapidly, both due to capability growth and adoption growth. If we don't have an explicit theory then we're using an implicit theory. Think of spending a lot of time & resources collecting samples of COVID, but not, at the same time, working on a theory of how epidemics evolve and who's more susceptible. 

Here are some existing theories of AI impacts:

:     1. Informal observations about what types of tasks LLMs do well on: verifiable, short-horizon, low-context.
      1. Eloundou et al.'s ranking of ONET task "exposure"
      2. METR's ranking of benchmark task "time horizon."

      A related argument: my feeling is we're more constrained on theory than data. 

Nathan Lambert seems to feel the same way.
: Nathan Lambert's post-curve [post](https://www.interconnects.ai/p/thoughts-on-the-curve) says *"many AI obsessors are more interested in where the technology is going rather than how or what exactly it is going to be."*


##       We need more metrics of capabilities

We don't have a standard way of defining AI capabilities.
: We say "strong AI", "transformative AI", "AGI", or "ASI".

      The best concrete metric is probably METR's time horizon index: we can say "what happens when AI can do a one month task?"

      The Forecasting Research Institute is working on a set of well-defined capability scenarios.

A metric I think would be useful: frontier cost-efficiency growth.
: There are hundreds of cost-efficiency metrics that have been regularly increasing over decades: transistor density, corn yield, compression efficiency. When AI becomes useful then we expect these metrics to start improving more quickly, & that rate of improvement is a useful metric because it's (1) unambiguous; (2) clearly economically relevant; (3) upstream of other economic impacts like employment.

![](images/2025-10-06-11-33-10.png){.column-margin}

##       We need more theory of offense-defense balance

Many discussions were about how AI will change the offense-defense balance

: Examples that came up in the Curve:

      - ransomware
      - spearfishing
      - media manipulation
      - drone assassinations
      - drone warfare. 
   
    There are dozens of others.

We should have some common theory.
: It seems wasteful to treat each of these problems independently, there ought to be some general principles we can apply on how AI will affect offense-defense balance. The closest I know is @garfinkel2019offensedefense, they hypothesise that in many cases, when investments are sufficiently large, adding more resources will tend to favor defense.

My best guess.
: The 


![](images/2025-10-06-11-30-58.png){.column-margin}

##       There are two ways of modelling AI's impact

There's a nice distinction between two approaches.

: From a lot of conversations about recursive self-improvement I realized it's useful to distinguish between two qualitatively different ways of thinking about AI's ability to do work:

      1. _Top down:_ AI replaces each of the human subtasks. Hits human-level ability.
      2. _Bottom-up:_ AI just does the entire procedure from first principles.
   
   I think 80% of discussion of economic impacts talks about top-down.

What would bottom-up modelling look like?
: Instead of.

What are the implications for recursive self-improvement?
: AI treats it as a pure optimization problem, already it's better at chip design, algorithm design.


Some related discussion:
: - Nathan Lambert recapitulates some discussions about recursive self-improvement [here](https://www.interconnects.ai/p/thoughts-on-the-curve).



##       Other things

What metrics should the labs report.
: .

My highest compliment to your work is that I didn't read it.
: If I start reading your essay and I realize it's really good then I put it aside until I can organize my own thoughts on this question. My thoughts can take a long time to organize. My favorite book, after 20 years, I still haven't made it farther than half-way through. If I tell you I've read your essay it means I liked it but I didn't love it.
