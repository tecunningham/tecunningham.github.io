---
title: Will LLMs Write Super-Persuasive Propaganda?
author: Tom Cunningham, [Integrity Institute](https://integrityinstitute.org/)
date: today
execute:
  echo: false
  cache: true # caches chunk output
fig-align: center
reference-location: margin
format:
  html:
    toc: true
    toc-depth: 2
    toc-location: left
engine: knitr
draft: true
bibliography: ai.bib
# editor:
#    render-on-save: true
---
<style>
    h1 {  border-bottom: 4px solid black; }
    h2 {  border-bottom: 1px solid gray; padding-bottom: 0px; color: black; }
    /* font-size: 14px;  */
</style>
<!-- https://tecunningham.github.io/posts/2023-08-22-ceiling-on-ai-performance-persuasion.html
TO ADD:
  - Monty Python's lethally funny joke
-->

#        Summary

::: {.column-margin}
   ![](20230825131919.png)
   This has benefited greatly from comments from members of the Integrity Institute, especially Grady Ward.
:::
<span style="background:yellow;">==*Still a draft, don't circulate!*==</span>


**Are LLMs likely to create superhumanly persuasive text?** Will LLMs be able to write messages more persuasive than those written by professional copywriters: messages which compel you to click a link, buy a product, or vote for a candidate?

**The same question applies to many other areas.** Should we expect AI models to overtake humans and create super-evocative prose, super-accurate medical diagnoses, super-creative proofs, super-groovy music, super-beautiful paintings?

**The current architecture of LLMs is unlikely to create super-persuasive text.** Speaking loosely LLMs respond to a prompt by imitating how a human would respond to that prompt. Thus if you ask an LLM to write persuasive text it will write *averagely* persuasive text: even if it could write super-persuasive text it would not want to because that is not what a human would write. There are a few recent papers which all report that the persuasiveness of LLM-generated text is in the same ballpark as that of human-generated text, none show meaningful signs of computer superiority. Simple increases in training data or model size do not seem likely to lead to super-human performance.

**However changes to architecture could conceivably lead to super-persuasive text.** Specifically:

   1. **Iteration.** Generating multiple variants and iteratively testing them for persuasiveness could help isolate the most persuasive variants. I am skeptical this would lead to a substantial improvement at least for text.
   2. **Hybrid human-computer creation.** Humans may struggle at creating persuasive text while still being able to *recognize* whether a given text is persuasive. If true then a human-computer hybrid could iterate very quickly over candidates to discover the most persuasive texts. This depends on humans having insight into persuadability not accessible to the computers.
   3. **Self-play.** Using the LLM weights in a variety of indirect ways might help discover which texts are most persuasive. It is hard to judge how effective this will be.
   4. **Supervised learning.** Datasets of real-world persuasiveness of text could be used to train a model directly. It seems plausible that this approach could uncover pockets of persuasiveness not yet discovered by humans.

**Data available today could help us make forecasts.** Below I discuss a number of empirical facts which we could measure today and would help us estimate the future trajectory of LLM-generated text. Facts like (1) the variance in persuasiveness between texts, (2) the diminishing returns of persuasiveness as more texts are created, (3) the correlation between true persuasiveness, human-estimated persuasiveness, and computer-estimated persuasiveness of a given text.

**It is unclear whether super-persuasive messages exist.** Finally there is a bigger question on how much more persuasive we can get. Over the last 100 years social scientists have continually invented supposed technologies of hyper-persuasion, they all turned out to be duds. We know that some messages are more persuasive than others but it is hard to know how much higher we can go: it's possible that the most persuasive writers today are already close to the ceiling.


#        Setup

::: {.column-margin}
   ![](20230825135614.png)
:::

**We are interested in the persuasive effect of a single short piece of text,** e.g.:

   - Effectiveness of an email in making people open an attached file.
   - Effectiveness of a tweet in making people buy a crypto coin.
   - Effectiveness of a blog post in making people vote for Ron DeSantis.

**The persuasiveness of text is relative to an audience and a provenance.** The same message will have a different effect on your decisions depending on whether you think it comes from your sister or from an old acquaintance. In the following discussion I'll assume the audience is the average American adult and they receive the message unsolicited from a stranger.^[Peoples' response to new messages is determined by their experience with past messages, in turn determined by the spam filters and ranking algorithms which they have been subject to.]

**Formal definition.** We can think of a function $p(t)$, where $t$ represents a piece of text drawn from the space of all possible texts, and $p(.)$ is its persuasive power: precisely, the average person's probability of performing a certain action after seeing that text. The function $p(t)$ will be basically flat for almost all values of $t$ because in the space of all possible sentences the overwhelming majority are meaningless or irrelevant and so likely to have infinitesimal effects on whether you take a specific action.

**I'm setting aside many related questions.** I will not consider (1) AI maintaining a conversation (chatbots, catfishing, pig butchering); (2) AI sending customized messages based on the respondent's demographics; (3) AI choosing who to send messages to, based on predicted persuadability; (4) AI sending messages repeatedly to increase persuasion. For each of these cases it seems unlikely that AIs could be super-humanly persuasive without also being super-humanly persuasive in the base case of creating a single message. However some additional considerations apply to the case of AI synthesizing recorded media: photos, audio, and video. Computers have now far superpassed human ability to synthesizing or altering recorded media to fit a set of criteria, and so the situation is qualitatively different from text. Discussion of the persuasiveness of synthetic media also involves the role of a recording as *prima facie* evidence that an event ocurred: as synthesized media becomes more prevalent that evidence-value is likely to decline.

**We want to compare persuasiveness of text created by different types of authors:**

   - Human subjects in a psychology experiment (e.g. undergraduates, mechanical turk participants)
   - Human employees of influence operation 	(e.g. Internet Research Agency, pig butcher scammers)
   - Human copywriters (e.g. marketing professionals)

   - LLMs in 2023 (e.g. GPT-4)
   - LLMs in the future: 2024, 2025, and 2050.

**There is substantial variation in human ability to write persuasive copy.** Copywriters are hired for their ability to come up with text that makes people perform an action, i.e. they are hired for their knowledge of the function $p(.)$, and it seems there must be substantial variation in individual ability: some people are better than others at writing persuasive slogans.[^claytonMakepeace]

   [^claytonMakepeace]: From the [obituary of a famous direct-marketing copywriter](https://www.awai.com/about/celebrating-clayton-makepeace/): "When you read some of the greatest and most iconic headlines in copywriting history — such as, “7 HORSEMEN of the Coming STOCK MARKET APOCALYPSE” … “Shameless Two-Faced S.O.B.s!” … and, “Health Breakthrough News — Cholesterol’s EVIL TWIN”— smile and think to yourself, ‘Clayton Makepeace wrote that.’"

   <!-- TO add: famous ad men -->
   <!-- Consider some famous slogans "america runs on dunkin", "snap crackle pop", "just do it", "the happiest place on earth". -->

**Social scientists have warned about hyper-persuasion many times in the last century.** In the past 100 years there have been a dozen apparent discoveries of hyper-effective means of persuasion. In retrospect I think it's fair to say they were mostly or all exaggerated: whether based on subconscious associations (Bernays), conditioning (Skinner), subliminal messages (Packard), brainwashing (Sargant), the power of conformity (Adorno, Asch, Zimbardo), priming and nudges (Bargh, Thaler), or statistical profiling (Cambridge Analytica).

#        Plain LLMs Will Not Exhibit Superhuman Performance

**In short: LLMs don’t perform tasks, they imitate humans performing tasks.** Loosely speaking LLMs are trained to predict the next word based on a corpus of text. If we ask an LLM to create a persuasive message it will predict a human's answer to that prompt, thus we should expect it to create a super-humanly persuasive message only by accident.[^Lichtenberg]

   [^Lichtenberg]: Reminiscent of Lichtenberg on writers: *"The critics instruct authors to stay close to nature, and they read this advice; but they always think it safer to stay close to authors who have stayed close to nature."*

**In what cases do LLMs outperform humans?** There are some cases where LLMs seem to outperform humans, I discuss those in the rest of this section, though the discussion is not yet complete.

<!-- **Consider an LLM trained solely to predict the next word on a text corpus.** For now assume no other tuning.^[This was the basic model used by OpenAI's GPT series of models until chat-GPT.] -->

<!-- ```{tikz}
#| column: margin
\begin{tikzpicture}[scale=2,line width=1]
   \draw (-1,-1)--(1,-1)--(1,1)--(-1,1)--(-1,-1);
   \draw[] (-.2,0) circle (.5) node[yshift=1.2cm,left] {Tom};
   \draw[] (.2,0) circle (.5) node[yshift=1.2cm,right]{Dick};
   \draw[blue] (0,-.2) circle (.5) node[yshift=-1.2cm]{LLM};
\end{tikzpicture}
``` -->

**Consider the set of all possible questions.** We can draw a Venn diagram (at right) showing the relationship between the questions that the LLM can answer accurately, and the sets that individual humans can answer. I will talk about "questions" but we can equally talk about "tasks". For simplicity I will just consider two humans, Tom and Dick.

**The LLM-answerable questions will not be a subset of any individual human.** We know that LLMs can answer questions with an encylopedic range and across different languages, i.e. beyond the range of any single person.

```{tikz}
#| column: margin
\begin{tikzpicture}[scale=2,line width=1]
   \draw (-1,-1)--(1,-1)--(1,1)--(-1,1)--(-1,-1);
   \draw[] (-.2,0) circle (.5) node[yshift=1.2cm,left] {Tom};
   \draw[] (.2,0) circle (.5) node[yshift=1.2cm,right]{Dick};
   \draw[blue] (0,0) ellipse (.5 and .3) node{LLM};
\end{tikzpicture}
```

**For factual questions the LLM will be inside the union of the humans.** Suppose we consider a set of factual questions, where the answer to each question is independent of the others, e.g. the number of chromosones a specific animal has, or the number of moons belonging to a given planet. An LLM can only answer the questions which it has read.


**There are some questions which an LLM can answer that no human can answer.** Some examples:

   - An LLM can translate between any pair of languages. Presumably there are many pairs of languages for which there is no human who speaks both, so the LLM can perform a task that no human has ever been able to perform.
   - An LLM can combine distinct pieces of information.

*[UNFINISHED: in short LLMs are discovering latent representations from the language output of multiple humans, which they can then combine, producing output that no human was capable of.]*

#        Paths to Superhuman Performance


::: {.column-margin}
   ![](20230825133035.png)
:::

##       (1) Iterative Testing

**In practice people are likely to generate multiple candidates, test each in the field, and choose the highest-performing variant.** However this would only change our conclusion about super-human persuasiveness if the distribution of persuasiveness is different between human-generated and computer-generated text.

   Suppose we can generate arbitrarily many passages of text $t_1,\ldots,t_n$, and the persuasiveness of each text is represented by $p_1,\ldots,p_n$. It's safe to assume that there will be diminishing returns in persuasiveness ($E[p_{n+1}]<E[p_n]$), both for human-generated and computer-generated text, however as long as there is some variance in persuasiveness then iterative testing will be worthwhile. Suppose that the first or most-promising generation of both a human and a computer are equally persuasive on average ($E[p_1^H]=E[p_1^C]$). 

```{tikz}
#| column: margin
#| fig-cap: "The diminishing returns to persuasiveness: here I illustrate a case where humans and computers have the same variance, but the the quality declines more quickly for computers."
\begin{tikzpicture}[scale=1,line width=1]
   \draw[<->] (0,5)  -- node[above,align=center,rotate=90]{persuasiveness ($p_n$)} (0,0)
      --node[below]{iteration ($n$)} (4,0) ;
   \draw[domain=0:3.5] plot ({\x}, {3*(\x+1)^(-.4)}) node[right,align=center]{human};
   \draw[domain=0:3.5,dashed,line width=.5] plot ({\x}, {3.5*(\x+1)^(-.4)});
   \draw[domain=0:3.5,dashed,line width=.5] plot ({\x}, {2.5*(\x+1)^(-.4)});
   \draw[color=blue,domain=0:3.5]plot ({\x}, {3*(\x+1)^(-.8)}) node[right,align=center]{computer};
   \draw[blue,dashed,domain=0:3.5,line width=.5] plot ({\x}, {3.5*(\x+1)^(-.8)});
   \draw[blue,dashed,domain=0:3.5,line width=.5] plot ({\x}, {2.5*(\x+1)^(-.8)});
\end{tikzpicture}
```

**The persuasiveness of computer-generated copy could be higher after the iterative process for one of two reasons.** These are empirical facts that should be relatively straight-forward to test, unfortunately I'm not aware of any literature that has useful results on these two questions.

**(1) If computer-generated text has higher variance.** $V[p_n^H]<V[p_n^C]$. The returns to exploration will increase in the variance of the outputs (holding fixed the mean), and likewise the expected persuasiveness of the ultimately selected alternative will be higher.

   I am skeptical that computer-generated text will have higher variance of persuasiveness than human-generated text. A common observation about LLMs is that they are less creative than humans at the same task. Variance can be increased by adjusting parameters of the model, e.g. generating lower-probability tokens by setting a higher temperature, but this will very likely decrease the average persuasiveness.
   
   A more subtle point is the covariance of persuasiveness between each element in the sequence. If an LLM generated minor variations on the same basic pattern then we would expect high covariance between texts and consequently relatively lower returns to iterative selection.

**(2) If computer-generated text has less-diminishing returns.** $\frac{E[p_{n+1}^H]}{E[p_{n}^H]}<\frac{E[p_{n+1}^C]}{E[p_{n}^C]}$.

   It is unclear to me whether we should expect returns to diminish faster or slower for computer-generated or human-generated text. 
   <!-- For a human generating additional candidate texts requires significant effort, but for a computer it has almost zero cost in either time or money. Some things are expensive to create additional candidates: e.g. paint a painting, film a television commercial, record a song. 
    -->


##       (2) Human-Computer Hybrid

**Estimating the persuasiveness of text with an experiment requires enormous experiments.** Most text has fairly low rate of persuasion, e.g. typically advertisements have click-through rates of 0.1 percentage points or less. In such a case it would take 400,000 observations to measure the click-through rate to within 10% of its true value.[^derivation] For some actions it also takes a long time to run an experiment, e.g. if persuading people to vote in an election or to keep using a product.

**Instead of running an experiment we could ask people to estimate persuasiveness.** We could simply ask *"how persuasive is this text on a scale of 1-100?"* We could either ask professionals or a representative sample of the population. This is not the ground truth but it requires vastly smaller sampler sizes to get an precise estimate.

**We can summarize the effectiveness of the hybrid with the correlation of three metrics.** Consider three different measures of persuasiveness:
   
   1. *True persuasiveness.* I.e. the true causal effect on click-through rate, which can only be estimated with a large and time-consuming experiment.
   2. *Human-estimated persuasiveness.* The response to a survey question about persuasiveness.
   3. *Computer-estimated persuasiveness.* This can be defined as the total probability of the sequence of tokens produced by the LLM ($P(w_1,\ldots,w_n)$).[^probability]

**The effectiveness of the hybrid strategy will depend the *incremental* value of human judgment.** It is unclear how strong the conditional correlation is bewteen true persuasiveness and human-estimated persuasiveness. I.e. if the LLM generates two texts with equal probability, will humans have a high likelihood of telling which is more truly persuasive?

[^derivation]: Derivation:
   $$\begin{aligned}
      p  &= 0.001\\
      SD &= \sqrt{p(1-p)} = 0.03 \\
      CI &= p * 0.1 = 0.0001\\
         & \text{(95\% CI width)}\\
      SE & = CI / 1.96 = 0.00005\\
         & \text{(95\% CI needs 1.96SEs)}\\
      N & = (SD/SE)^2 = 360,000
   \end{aligned}$$

[^probability]: A technical note on generating high-likelihood text: if you are given a prompt and wish to generate a *representative* completion (given a distribution $P(w_{t_1}|w_t,w_{t-1},\ldots)$) then you can successively generate individual words in proportion to their probability at each step because the full distribution is separable. However if you are trying to generate the *most likely* completion it is not sufficient to generate the most likely word at each step. My impression is that most popular LLMs do in fact generate tokens incrementally because the loss in most cases is not too large (the alternative is to explore multiple steps ahead, a "beam search").


##       (3) Self-Play  [UNFINISHED]

Suppose that LLMs can create only averagely competent persuasive material: still there’s the possibility of improving it by iterative improvement: e.g. find a cluster of questions along the lines of “is this persuasive?”^[Thanks to Grady Ward for this suggestion.]

Precedents from self-play in chess and Go. Analogy: palindrome, or playing a rubik’s cube. There is good reason to expect that LLMs will expand the frontier of mathematical reasoning through this ability. (Chain of thought, tree of thought, graph of thought).

It’s very difficult to create in one-shot, but could imagine an iterative solution.

<!-- exploring space of persuasiveness is unlike exploring space of external world: it's the internal world -->

<!-- For logical reasoning: seems plausible they will be able to exceed human ability, but limited evidence of this so far (e.g. mathematicians on GPT-4). -->

##       (4) Supervised Learning [UNFINISHED]

**Finally we could use a fundamentally different architecture:** instead of predicting what a human would say if they were asked to produce persuasive text, we could predict how persuasive a given piece of text is.

**This requires an enormous datasets.** To predict the causal propensity to be persuaded requires a very large dataset.

   ^[E.g. ImageNet contains 14 million hand-labelled images. DALL-E was trained on 150M images with captions scraped from the internet.]

   E.g. Facebook probably serves around 40 billion ad impressions/day.^[Assuming 4 billion active users/day, and 20 ad impressions/user/day].

In some cases we have large datasets with the ground truth of how persuasive a message is. E.g. Facebook databases have records of every ad impression and whether the user clicked (around 1 trillion/month). This is of sufficient scale that we could directly synthesize ads which the user is highly likely to click and there’s no reason for there to be a ceiling.


#        Empirical Studies on Persuasive Content

Since 2022 there has been a small literature comparing the persuasiveness of human-generated and computer-generated text, where persuasion is measured by a human survey response before and after reading an article. The studies have all found that LLMs can generate somewhat-persuasive content, none show strong evidence that LLMs can create more-persuasive content than moderately competent humans.

- @bai2023persuade
- @goldstein2023persuasive
- @hackenburg2023persuasive
- @matz2023personalized
- @palmer2023large
- @qin2023large

See more details [here](https://docs.google.com/document/d/1X0F-YOco4EWqk4BMTuqi_DpFAOCXGEXpfmaaGGV9F0s/).

#          Appendix: Additional Material

**Increases in persuasive power should cause increases in expenditure on advertising.** If computer-generated ads are more persuasive than human-generated ads then firms should be willing to spend more on each ad, and so the equilibrium price of advertising impressions will rise.^[Although the price of ads should increase it is unclear whether the quantity of ads shown would increase or decrease. In a simple model of ad-supported media the equilibrium quantity of ads is determined by (1) the curvature of demand from advertisers, (2) the curvature of disutility from consumers. An increase in the persuasive power of advertising will shift demand up but it's not clear to me whether the curvature would change in one direction or the other. However we would expect more investment in media to attract user attention.]

Aggreagte advertising expenditure has remained at around 1% of GDP since 1930, implying relatively little change in the ability to persuade people.^[[Ben Evans](https://www.ben-evans.com/presentations)]
   ![](images/2023-07-12-05-36-29.png)


We can make some very loose generalizations about human persuadability:

1. _Human attitudes are highly sensitive to influences._ A person's adult attitudes, beliefs, and preferences, are highly sensitive to their upbringing and context: if your parents are Catholic then you are highly likely to grow up Catholic. The same is true for eating fish, enjoying country music, being a Democrat, taking your shoes off inside, and your attitude to sex before marriage. The correlations can only be partly explained by direct genetic or economic effects, implying that attitudes are highly sensitive to experience.

2. _The effective means of persuasion are only crudely known._ In the past 100 years there have been a dozen apparent discoveries of hyper-effective means of persuasion, as discussed above.

3. _Carefully measured evidence of persuasive effects tend to be low._

4. _Nevertheless large resources are devoted to persuasion,_[^McCloskey].

A recent [review of the effectiveness of influence operations](https://carnegieendowment.org/2021/06/28/measuring-effects-of-influence-operations-key-findings-and-gaps-from-empirical-research-pub-84824) finds that influence through traditional broadcast media (e.g. Russia Today) probably have a bigger impact than covert campaigns through social media.

<!-- Expenditure on advertising in the US is around 1% of GDP, and expenditure on political advertising is 0.04% (using $250B on advertising, $10B on political advertising, and $23T GDP). [Estimate US marketing spend of $1.4T/year](https://martech.org/worldwide-spend-on-marketing-to-hit-4-7-trillion-by-2025/), 8% of company revenue. -->

[^McCloskey]: McCloskey and Klamer have a 1995 paper titled "One Quarter of GDP is Persuasion".
