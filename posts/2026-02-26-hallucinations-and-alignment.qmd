---
title: Knowledge of Success
citation: true
draft: true
engine: knitr
bibliography: ai.bib
---
<!-- - Note early on different terminology: "abstain", "refuse", "reject", "IDK/i don't know", "forfeit", "concede", "fold" (others?) -->

LLMs are much more useful if they tell you their confidence.
: 
    When asking an LLM to do a task it's reassuring to know that it's successful in some fraction of cases, but it's far more useful to know *which* cases it's successful in:

    - If you're answering a question, I want to know your confidence in the answer.
    - If you're supplying a proof for a theorem, I want to know whether you think the proof is valid.
    - If you're fixing a bug, I want to know if you think the fix is going to work.
    - If you're writing a poem for me, I want to know if the poem is good.

    Unfortunately until recently LLMs were trained just to maximize their success rates, and for that reason they often wouldn't report useful signals of confidence, which made them much less useful. This I think is a good explanation of why LLMs hallucinate (argued in @kalai2025why), but the same logic illuminates some other cases.
    
    The discussion below mostly follows @kalai2025why, but adds some arguments and visualization I did when working with those authors in 2024 at OpenAI. I think these points are fairly well-known within the industry but ought to be better known outside it.

This follows from a very simple model.
: 
    Suppose I have to make a choice among $N$ options, and I have no priors about which is most likely to be right. Then it's fine if the LLM just tells me which is the most-likely option, without telling me its probability.
    
    However if we add a touch of realism, then it suddenly becomes much more useful if the LLM tells me its probability (or it admits when it doesn't know). This will happen if any of the following are true: (1) I have some private information about the different options; (2) I can choose to spend some time verifying the proposed option, or searching for solutions; (3) I have the option of abstaining and not making a choice.
    

Some implications.
: 
    1. **Models seem overconfident because they are trained only on accuracy, not on calibration.** When models aren't allowed to fold they learn to bluff.
    2. **The value of a model will be a convex in its accuracy.** Going from 90% to 100% accuracy is more than twice as valuable as going from 80% to 90%, because it lowers the cost of verification, and lowers the likelihood of abstention. This is only true when models don't report their confidence.
    3. **Benchmarks should report both accuracy and reliability.** If you're choosing betwen two models it's useful to know not just the share of correct responses, but also whether the model will report when it fails (i.e. accuracy).
    4. **Models are good self-critics.** Somewhat surprisingly, a model can often identify its own mistakes. This makes sense for models that are trained only on accuracy, not on calibration, because they systematically exaggerate their success.

In this post.
: 
    I state the model very briefly and give a nice visual aid, to show the optimal threshold for abstaining.
    
    I also show how we can use a simplex diagram to illustrate tradeoffs between accuracy and confidence, showing both the frontier (plotting results from benchmarks) and .

<!-- 
- Related: implicit goals vs explicit goals.
- Related: soft verification vs hard verification.
- Related: models audit themselves, because they *know* whether they succeeded, so can easily see whether it's a success. 
- If you don't let the model fold then it will bluff.
 -->


# Model

Basic model: you just care about accuracy.
: 
    Suppose you have to choose which of $N$ options is correct, you get $u=1$ if you succeed and $u=0$ if you fail. You have no idea which is right (uniform priors), but you know the LLM has information, and chooses the right answer with probability $p$. In this case it's sufficent for the LLM to report the option with the highest probability, and the user's expected payoff is linear in the LLM's average accuracy, $p$.

    ```{tikz}
    #| fig-width: 4
    \begin{tikzpicture}[scale=6]
    \def\ySucc{2.0}
    \def\yAbst{0.0}
    \def\yFail{-2.0}
    \draw[<->] (0,1.1) -- node[midway,rotate=90,above] {utilty}
        (0,0) node[below]{0} -- node[midway,below]{probability, $p$} (1,0) node[below]{1} -- (1,1.1);

    \draw[blue,dashed] (0,0)--(1,0) node[right]{0 - fail};
    \draw[blue,dashed] (0,1)--(1,1) node[right]{1 - succeed};
    \draw[blue,thick] (0,0)-- node[midway,anchor=south east]{$E[u|p]$} (1,1);
    \end{tikzpicture}
    ```


If you can abstain, payoff is convex in accuracy.
: 
    Now suppose the user can choose to abstain, i.e. they refuse to make a choice and get $u=\pi_i$, with $0<\pi_a<1$. Then they will only consult the model if $p>\pi_a$, and so the value of the model will be convex in $p$. The threshold here is the same as that derived in @chow1970optimum.

    ```{tikz}
    #| fig-width: 4
    \begin{tikzpicture}[scale=6]
    \def\ySucc{2.0}
    \def\yAbst{0.0}
    \def\yFail{-2.0}
    \draw[<->] (0,1.1) -- node[midway,rotate=90,above] {utilty}
        (0,0) node[below]{0} -- node[midway,below]{probability, $p$} (1,0) node[below]{1} -- (1,1.1);

    \draw[blue,dashed] (0,0)--(1,0) node[right]{0 - fail};
    \draw[blue,dashed] (0,1)--(1,1) node[right]{1 - succeed};
    \draw[blue,dashed] (0,.6)--(1,.6) node[right]{$\pi_a$ - abstain};
    \draw[blue,dashed] (0,0)--(1,1);
    \draw[blue,thick] (0,.6) -- (.6,.6) node[anchor=south east]{$E[u|p]$} -- (1,1);
    \end{tikzpicture}
    ```

If there is costly verification then payoff will be three-part.
: 
    Finally, now assume you can pay a cost $c$ to verify whether an answer is correct, and if it's wrong then you abstain. Then there will be three regions:
    
    - If $p$ is low, you don't ask the LLM, and abstain.
    - If $p$ is intermediate you ask but verify (and abstain if it's wrong).
    - If $p$ is high then you ask and trust without verification.

    ```{tikz}
    #| fig-width: 4
    \begin{tikzpicture}[scale=6]
    \def\ySucc{2.0}
    \def\yAbst{0.0}
    \def\yFail{-2.0}
    \draw[<->] (0,1.1) -- node[midway,rotate=90,above] {utilty}
        (0,0) node[below]{0} -- node[midway,below]{probability, $p$} (1,0) node[below]{1} -- (1,1.1);

    \draw[blue,dashed] (0,0)--(1,0) node[right]{0 - fail};
    \draw[blue,dashed] (0,1)--(1,1) node[right]{1 - succeed};
    \draw[blue,dashed] (0,.6)--(1,.6) node[right]{$\pi_a$ - abstain};
    \draw[blue,dashed] (0,.5)--(1,.9) node[right]{verify};
    \draw[blue,dashed] (0,0)--(1,1);
    \draw[blue,thick] (0,.6) -- (.25,.6) --  node[midway,anchor=south east]{$E[u|p]$} ({5/6},{5/6}) -- (1,1);
    \end{tikzpicture}
    ```

<!-- TO ADD: (1)  -->


# Simplex Representation

It's very useful to draw the likelihood of three outcomes (succeed, fail, abstain) on a diagram.^[Economists know this as a Marschack-Machina diagram.] We can then draw indifference curves represnting different objective functions:

```{tikz}
\begin{tikzpicture}[>=stealth, scale=2.2]
  \input{_tikz/marschak_machina_shared.tex}

  % Reward accuracy: utility = p_s
  \begin{scope}[xshift=0cm]
    \MMBaseSimplex{Accuracy-only}
    \node[font=\scriptsize, fill=white, inner sep=1pt] at (0.50,0.86) {$U = p_s$};
    \begin{scope} 
      \clip (0,0) -- (1,0) -- (0,1) -- cycle;
      \foreach \xx in {0.08,0.16,0.24,0.32,0.40,0.48,0.56,0.64,0.72,0.80,0.88,0.96} {
        \draw[blue!65, dotted, line width=0.25pt] (\xx,-0.1) -- (\xx,1.1);
      }    
    \end{scope}
  \end{scope}

  % Punish failure: expected utility with strong negative pi_fail
  \begin{scope}[xshift=1.5cm]
    \def\piSucc{1.0}
    \def\piFail{-1.0}
    \def\piAbst{0.0}
    \MMBaseSimplex{Can abstain}
    \node[font=\scriptsize, fill=white, inner sep=1pt] at (0.50,0.86) {$U = p_s - p_f$};
    \MMIndifferenceGrid
  \end{scope}

  % F1 contours (single-question form, C=I=N=0):
  % F1 = 2 p_s / (1 + p_s + p_f), so p_f = ((2/u)-1)p_s - 1 for level u
  \begin{scope}[xshift=3cm]
    \MMBaseSimplex{F1 objective} 
    \node[font=\scriptsize, fill=white, inner sep=1pt] at (0.50,0.86) {$U = \frac{2p_s}{1+p_s+p_f}$};
    \begin{scope}
      \clip (0,0) -- (1,0) -- (0,1) -- cycle;
      \foreach \ubar in {0.05,0.1,0.15,0.2,0.25,0.3,0.35,0.40,0.45,0.50,0.55,0.60,0.65,0.70,0.75,0.80,0.85,0.90,0.95} {
        \draw[blue!65, dotted, line width=0.25pt, domain=-0.1:1.1, samples=2]
          plot (\x, {((2/\ubar)-1)*\x - 1}); 
      }
    \end{scope}
  \end{scope}
\end{tikzpicture}
```

Each point in the simplex is a lottery over outcomes: a model might succeed with probability $p_s$, fail with probability $p_f$, and abstain with probability $p_a$. The panels show three different objective families:

- **Accuracy-only** ($U=p_s$): success is rewarded, but failure and abstention are treated the same. This creates pressure to guess rather than abstain.
- **Penalize failure** (linear expected utility): failure is explicitly penalized relative to abstention, expanding the region where abstaining is optimal.
- **F1** (a non-linear metric): indifference curves bend, reflecting that the metric itself builds in a particular tradeoff between attempting and being correct.


### SimpleQA

```{tikz}
\begin{tikzpicture}[>=stealth, scale=3.0]
  \input{_tikz/marschak_machina_shared.tex}
  \MMBaseSimplex{SimpleQA}

  % All models (faint points).
  \foreach \x/\y in {
    0.051/0.196,
    0.057/0.193,
    0.235/0.369,
    0.289/0.361,
    0.086/0.905,
    0.382/0.608,
    0.081/0.634,
    0.427/0.481
  }{
    \fill[black, opacity=0.55] (\x,\y) circle (0.010);
  }

  % Callouts for a few labeled models.
  \begin{scope}[shift={(0.382,0.608)}]
    \fill[red!70!black] (0,0) circle (0.013);
    \draw[red!70!black, line width=0.35pt] (0,0) -- (0.08,0.02);
    \node[font=\scriptsize, anchor=west, fill=white, inner sep=1pt] at (0.08,0.02) {GPT-4o};
  \end{scope}
  \begin{scope}[shift={(0.427,0.481)}]
    \fill[red!70!black] (0,0) circle (0.013);
    \draw[red!70!black, line width=0.35pt] (0,0) -- (0.08,-0.02);
    \node[font=\scriptsize, anchor=west, fill=white, inner sep=1pt] at (0.08,-0.02) {o1-preview};
  \end{scope}
  \begin{scope}[shift={(0.086,0.905)}]
    \fill[red!70!black] (0,0) circle (0.013);
    \draw[red!70!black, line width=0.35pt] (0,0) -- (0.08,-0.10);
    \node[font=\scriptsize, anchor=west, fill=white, inner sep=1pt] at (0.08,-0.10) {GPT-4o-mini};
  \end{scope}
\end{tikzpicture}
```

### Abstain-QA

```{tikz}
\begin{tikzpicture}[>=stealth, scale=3.0]
  \input{_tikz/marschak_machina_shared.tex}
  \MMBaseSimplex{Abstain-QA}

  % All models (faint points).
  \foreach \x/\y in {
    0.661/0.197,
    0.720/0.191,
    0.611/0.374,
    0.541/0.370,
    0.590/0.291
  }{
    \fill[black, opacity=0.55] (\x,\y) circle (0.010);
  }

  % Callouts for a few labeled models.
  \begin{scope}[shift={(0.720,0.191)}]
    \fill[red!70!black] (0,0) circle (0.013);
    \draw[red!70!black, line width=0.35pt] (0,0) -- (0.06,0.02);
    \node[font=\scriptsize, anchor=west, fill=white, inner sep=1pt] at (0.06,0.02) {GPT-4 32K};
  \end{scope}
  \begin{scope}[shift={(0.611,0.374)}]
    \fill[red!70!black] (0,0) circle (0.013);
    \draw[red!70!black, line width=0.35pt] (0,0) -- (0.06,0.06);
    \node[font=\scriptsize, anchor=west, fill=white, inner sep=1pt] at (0.06,0.06) {GPT-3.5};
  \end{scope}
\end{tikzpicture}
```

### Reading the simplex plots

1. The horizontal axis is $p_s$ (succeed share) and the vertical axis is $p_f$ (fail share). The remaining probability is $p_a=1-p_s-p_f$ (abstain share), so points closer to the diagonal edge have lower abstention.

2. Moving down (lower $p_f$) corresponds to reducing failures; whether that is best depends on how much worse failure is than abstention ($\pi_f$ vs $\pi_a$).

3. The labeled points illustrate that a model can look good under an "answer-everything" regime by pushing $p_a$ toward zero, but that is exactly the regime that a payoff function with a harsh $\pi_f$ would discourage.

4. Don't over-interpret cross-benchmark comparisons: each source defines abstention differently, so these plots are best read as a geometric visualization of tradeoffs, not as a single unified leaderboard.



#           Related Literature

## Chronological sketch

- @chow1970optimum: introduces the Bayes-optimal reject option ("indecision") and derives the posterior-threshold rule.
- @herbei2006reject: formalizes "classification with a reject option" in modern statistical learning terms; there are many followups on learning and using reject policies (e.g. [@bartlett2008reject; @elyaniv2010selective; @geifman2019selectivenet]).
- @kadavath2022mostly: finds that language models can often estimate whether their own answers are correct, which is exactly the signal needed to implement a threshold rule in practice.
- Recent LLM work tries to *implement* the missing abstain/verify channel via refusal-aware tuning and explicit "IDK" tokens [@zhang-etal-2024-r; @cohen2024idontknowexplicit], verification loops [@dhuliawala2023chainofverificationreduceshallucinationlarge; @altinisik2026doireallyknow], and black-box uncertainty proxies like sampling-based checks or semantic uncertainty [@manakul-etal-2023-selfcheckgpt; @farquhar2024detectinghallucinationssemanticentropy].
- @kalai2025why: argues LLM hallucinations are a predictable consequence of binary grading that penalizes abstention; proposes a scoring rule that makes abstaining optimal below a stated confidence threshold.

### Chow (1970): Optimal reject rules

@chow1970optimum introduces the **reject option** (which he calls the "Indecision class" $I$) into pattern recognition and derives the optimal error-reject tradeoff. Chow's terminology maps directly onto ours:

| Chow's term | Our term |
|---|---|
| correct recognition | succeed |
| error (misclassification) | fail |
| rejection / indecision | abstain |

Chow's setup adds a third action (reject) to ordinary classification. In one common normalization, the costs are $0$ for a correct classification, $1$ for an error, and $t$ for a rejection.

The key result ("Chow's rule") is a posterior-threshold rule: accept and classify when confident enough, otherwise reject:
$$
\max_i P(G_i \mid x) \ge 1-t
\quad\Rightarrow\quad \text{accept;}
\qquad
\max_i P(G_i \mid x) < 1-t
\quad\Rightarrow\quad \text{reject.}
$$

It is optimal in the sense that, for a given rejection threshold (equivalently, a given reject rate), no other rule achieves a lower error rate.

The threshold $t$ is related to the costs of the three outcomes as $t = (C_r - C_c)/(C_e - C_c)$, where $C_e, C_r, C_c$ are the costs of error, rejection, and correct recognition. In our payoff notation $(\pi_s,\pi_f,\pi_a)$, Chow's rule becomes: predict iff

$$
\max_y P(y \mid x) \ge \frac{\pi_a - \pi_f}{\pi_s - \pi_f}.
$$

In the Marschak-Machina triangle, this threshold corresponds to one of the indifference lines: the boundary between the region where prediction is preferred and the region where abstention is preferred.

## Herbei and Wegkamp (2006) and followups: Learning a reject option

@herbei2006reject (and a large followup literature) reframes the reject option as a learning problem: you want a predictor that is accurate on the examples it attempts, while explicitly controlling how often it refuses.

For this post, the key takeaway is less about any one algorithm and more about the framing: "abstention is a first-class action" is standard in the statistical decision-theory literature, and the same expected-utility thresholds show up once you make the third outcome explicit.

## Kadavath et al. (2022): Models can sometimes score their own answers

If you want the simple $p^*$ rule to be usable, you need some per-question measure of correctness probability (like $p_{\max}(x)$ or a direct $P(\text{correct}\mid x)$ proxy).

@kadavath2022mostly study this in language models, finding that they can often estimate whether their own answers are correct. That makes "give the user a probability" a practical interface choice, not just a theoretical one.

## Kalai, Nachum, Vempala, and Zhang (2025): Why language models hallucinate

@kalai2025why argue that hallucinations are not a mysterious glitch but a predictable consequence of how models are trained and evaluated. Their central thesis is that the three-outcome structure (succeed, fail, abstain) is systematically distorted by binary evaluation:

The paper makes two distinct arguments:

**1. Pretraining origin.** Even with error-free training data, the statistical objective of pretraining produces hallucinations. The authors reduce the problem to binary classification ("Is-It-Valid"), showing that

$$
\text{generative error rate} \gtrsim 2 \cdot \text{IIV misclassification rate}.
$$

For arbitrary facts (like someone's birthday) where there is no learnable pattern, the hallucination rate after pretraining is at least the fraction of facts appearing exactly once in the training data.

**2. Post-training persistence.** Even after RLHF and other interventions, hallucinations persist because nearly all evaluation benchmarks use binary grading that penalizes abstention:

The fix they propose is exactly the payoff structure from our Marschak-Machina framework: penalize errors more than abstentions, with an explicit confidence threshold $t$ stated in the prompt. Their proposed scoring rule awards $+1$ for a correct answer, $-t/(1-t)$ for an incorrect answer, and $0$ for abstaining---so that answering is optimal iff confidence exceeds $t$. This is Chow's reject-option rule rediscovered in the LLM evaluation context.

## Recent mechanisms: refusal, uncertainty signals, and verification

Our model is deliberately abstract: it assumes the user has access to (i) a usable confidence signal $p$ and (ii) an abstain / verify channel. A lot of recent work can be read as ways of engineering those two ingredients:

- **Make abstention an explicit output.** Refusal-aware fine-tuning can teach a model to say "I don't know" on out-of-knowledge questions [@zhang-etal-2024-r]. Similarly, adding an explicit uncertainty token and training it to soak up probability mass on incorrect predictions effectively adds an abstain action to the model's output space [@cohen2024idontknowexplicit].

- **Pay a cost to verify.** Inference-time verification loops like Chain-of-Verification (CoVe) can be viewed as "spend extra tokens/compute to reduce $p_f$" [@dhuliawala2023chainofverificationreduceshallucinationlarge]. Very recent work trains this kind of behavior directly, with structured self-verification traces and an explicit final decision to answer vs abstain [@altinisik2026doireallyknow].

- **Construct a confidence signal without logits.** When output probabilities are unavailable (or untrustworthy), disagreement across samples can act as a proxy confidence signal. SelfCheckGPT does this with sampling-based consistency checks [@manakul-etal-2023-selfcheckgpt]; semantic-uncertainty methods like semantic entropy similarly use semantic variability across generations to predict and filter confabulations [@farquhar2024detectinghallucinationssemanticentropy], and followup work proposes cheaper "semantic entropy probes" in the same spirit [@kossen2024semanticentropyprobesrobust].

A cautionary note: neither uncertainty proxies nor "self-verification" are automatically reliable. Some hallucinations happen with high confidence (so uncertainty-based filters can miss them) [@simhi2025trustmeimwrong], and in logical reasoning settings models can struggle to identify their own errors (so internal self-checks can fail without external grounding) [@hong-etal-2024-closer].

Beyond factual QA, @mohamadi2025honestyaccuracytrustworthylanguage show on GSM8K/MedQA/GPQA that replacing binary RLVR rewards with a ternary scheme $(+1,0,-\lambda)$ produces controllable answer-vs-abstain tradeoffs and useful abstention-aware cascades. @jha2026rewardingintellectualhumilitylearning report on MedMCQA and Hendrycks Math that moderate abstention rewards reduce wrong answers without collapsing coverage, especially when paired with supervised abstention training. In code generation, @dai2025reducinghallucinationsllmgeneratedcode frame the task as "find a correct program or abstain" and use semantic triangulation to improve abstention decisions on LiveCodeBench/CodeElo. Complementarily, @oehri2025trusteduncertaintylargelanguage fuse multiple uncertainty signals into calibrated correctness probabilities and enforce user-specified risk budgets via refusal, including experiments on code generation with execution tests.


# Appendix: Verification option {#appendix-verification}

Suppose the user has a way to pay a cost $c$ to obtain the correct answer (e.g. look it up, run an expensive check, ask a human). In the simplest model, verification yields certain success with payoff $\pi_s-c$.

Since abstaining and verifying are both "outside options" (their payoff does not depend on the model's confidence), the only relevant outside-option payoff is
$$
\pi_{\text{outside}}=\max\{\pi_a,\;\pi_s-c\}.
$$

The attempt rule is the same threshold logic as before: attempt iff
$$
p \ge \frac{\pi_{\text{outside}}-\pi_f}{\pi_s-\pi_f}.
$$

In the example shown below, $(\pi_s,\pi_a,\pi_f)=(2,0,-2)$ and $\pi_s-c=1$, so
$$
p^*_{\mathrm{verify}}=\frac{1-(-2)}{2-(-2)}=\tfrac{3}{4}.
$$

Operationally, "verify" can mean many things: a web lookup, a separate fact-checking model, retrieval + citation, or even a structured self-checking loop that spends extra tokens before committing to an answer [@dhuliawala2023chainofverificationreduceshallucinationlarge; @altinisik2026doireallyknow].

```{tikz}
\begin{tikzpicture}[>=stealth, scale=1.0]
  % Example payoffs.
  \def\ySucc{2.0}
  \def\yAbst{0.0}
  \def\yFail{-2.0}
  \def\yVerify{1.0} % = \pi_s - c
  \def\yMin{-2.4}
  \def\yMax{2.4}
  \pgfmathsetmacro{\pVerify}{(\yVerify-\yFail)/(\ySucc-\yFail)}
  \pgfmathsetmacro{\xVerify}{6*\pVerify}

  % Axes
  \draw[->] (0,0) -- (6,0) node[right] {Confidence $p$};
  \draw[->] (0,\yMin) -- (0,\yMax) node[above] {Payoff};

  % Tick marks (x: 0 to 1)
  \foreach \x/\lab in {0/0, 1.5/0.25, 3/0.5, 4.5/0.75, 6/1}
    \draw (\x,0.08) -- (\x,-0.08) node[below] {\lab};

  % Right boundary at p=1
  \draw[thick] (6,\yMin) -- (6,\yMax);

  % Payoff labels for outcomes
  \draw[densely dotted] (0,\ySucc) -- (6,\ySucc);
  \draw[blue, dashed] (0,\yAbst) -- (6,\yAbst);
  \draw[densely dotted] (0,\yFail) -- (6,\yFail);
  \draw[red, dashed] (0,\yVerify) -- (6,\yVerify);
  \node[right] at (6,\ySucc) {$\pi_s$};
  \node[left] at (0,\yAbst) {$\pi_a$};
  \node[left] at (0,\yFail) {$\pi_f$};
  \node[right] at (6,\yVerify) {$\pi_s-c$};

  % Attempt line
  \draw[thick, blue] (0,\yFail) -- (6,\ySucc);

  % Threshold where verify equals expected payoff from attempt
  \draw[black, dashed] (\xVerify,\yMin) -- (\xVerify,\yMax);
  \node[above] at (\xVerify,\yMax)
    {$\pi_s-c = p\pi_s + (1-p)\pi_f$};
  \node[below] at (\xVerify,-0.45) {$p^*_{\mathrm{verify}}$};

  \node[font=\scriptsize] at (2.0,1.6) {verify};
  \node[font=\scriptsize] at (5.4,1.6) {attempt};
\end{tikzpicture}
```

## Appendix: Cross-Benchmark Outcome Table

To make cross-model plotting easier, the table below standardizes outputs from multiple benchmarks into a common schema.

| benchmark | model | p_s_pct | p_f_pct | p_a_pct |
|---|---|---:|---:|---:|
| SimpleQA | Claude-3-haiku (2024-03-07) | 5.1 | 19.6 | 75.3 |
| SimpleQA | Claude-3-sonnet (2024-02-29) | 5.7 | 19.3 | 75.0 |
| SimpleQA | Claude-3-opus (2024-02-29) | 23.5 | 36.9 | 39.6 |
| SimpleQA | Claude-3.5-sonnet (2024-06-20) | 28.9 | 36.1 | 35.0 |
| SimpleQA | GPT-4o-mini | 8.6 | 90.5 | 0.9 |
| SimpleQA | GPT-4o | 38.2 | 60.8 | 1.0 |
| SimpleQA | OpenAI o1-mini | 8.1 | 63.4 | 28.5 |
| SimpleQA | OpenAI o1-preview | 42.7 | 48.1 | 9.2 |
| Abstain-QA | GPT-4 Turbo | 66.1 | 19.7 | 14.2 |
| Abstain-QA | GPT-4 32K | 72.0 | 19.1 | 8.9 |
| Abstain-QA | GPT-3.5 Turbo | 61.1 | 37.4 | 1.5 |
| Abstain-QA | Mixtral 8x7b | 54.1 | 37.0 | 8.9 |
| Abstain-QA | Mixtral 8x22b | 59.0 | 29.1 | 11.9 |

Notes:
- Table columns are constructed to look like probabilities in the $(p_s,p_f,p_a)$ simplex: $p_s=\text{p\_s\_pct}/100$, $p_f=\text{p\_f\_pct}/100$, $p_a=\text{p\_a\_pct}/100$.
- For SimpleQA, these correspond directly to {Correct, Incorrect, Not attempted} shares.
- For Abstain-QA, these are constructed from the paper's summary metrics; interpret them as a mapping into a common coordinate system, not as identical underlying evaluation protocols.

### Data extraction details by source

**SimpleQA (Wei et al., 2024) [@wei2024measuringshortformfactualitylarge].**
The table uses all model rows shown in the main SimpleQA model-comparison table (8 models). Here, `p_s_pct` is **Correct**, `p_a_pct` is **Not attempted**, and `p_f_pct` is computed as $100-\text{Correct}-\text{Not attempted}$. I chose this slice because it is the paper's canonical cross-model summary and directly exposes explicit non-attempt behavior.

**Abstain-QA (Madhusudhan et al., 2024) [@madhusudhan2024llmsknowanswerinvestigating].**
This is a deliberate subset, not all values in the paper: I take the **MMLU / Standard clause / Base** rows (5 models) from the main result table. The paper reports **AAC** (answerable accuracy) and **AR** (abstention rate). I set $p_{a,\%}=\text{AR}$ and construct $p_{s,\%}$ and $p_{f,\%}$ by treating AAC as attempt-conditional accuracy:
$$
p_s = \text{AAC}\cdot(1-p_a),\qquad
p_f = (1-\text{AAC})\cdot(1-p_a),
$$
all expressed in percent.

**Important comparability caveats.**
Even after mapping all results into $(p_s,p_f,p_a)$ coordinates, the underlying tasks and abstention protocols differ: SimpleQA is short-form factual QA with optional non-attempts, while Abstain-QA is multiple-choice QA with an explicit IDK/NOTA option. So the combined table is useful for geometric intuition and directional comparisons, but not for strict leaderboard ranking across benchmarks.

Sources: @wei2024measuringshortformfactualitylarge; @madhusudhan2024llmsknowanswerinvestigating.

## Appendix: Benchmark Points in the Simplex
