<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.25">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tom Cunningham">
<meta name="dcterms.date" content="2026-02-10">
<meta name="description" content="Tom Cunningham blog">

<title>Recursive Self-Improvement | Tom Cunningham – Tom Cunningham</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-7b89279ff1a6dce999919e0e67d4d9ec.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-d5e7c60e6424aa6ccf163f01508596ce.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-12027453-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>
<script>window.MathJax = {
   loader: { load: ["https://cdn.jsdelivr.net/gh/sonoisa/XyJax-v3@3.0.1/build/xypic.js"]},
   tex: {packages: {'[+]': ['xypic','bm']},
         macros: {  bm: ["\\boldsymbol{#1}", 1],
                    ut: ["\\underbrace{#1}_{\\text{#2}}", 2],
                    utt: ["\\underbrace{#1}_{\\substack{\\text{#2}\\\\\\text{#3}}}", 3] }
   }
};
</script>
<style>
   h1 {  border-bottom: 8px solid #557;}
   h2 {  border-bottom: 1px solid #ccc;}
   .greyproof {
      background-color: #f5f5f5;
      padding: 1em;
      margin: 1em 0;
      border-radius: 4px;
   }
</style>
<meta name="quarto:status" content="draft">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="Recursive Self-Improvement | Tom Cunningham">
<meta name="twitter:description" content="Tom Cunningham blog">
<meta name="twitter:image" content="tecunningham.github.io/posts/2025-09-13-recursive-self-improvement-explosion_files/figure-html/unnamed-chunk-1-1.png">
<meta name="twitter:image-height" content="786">
<meta name="twitter:image-width" content="1178">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner"><div id="quarto-draft-alert" class="alert alert-warning"><i class="bi bi-pencil-square"></i>Draft</div>
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../index.html" class="navbar-brand navbar-brand-logo">
    </a>
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Tom Cunningham</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href=".././about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/testingham"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/tom-cunningham-a9433/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://tecunningham.github.io/index.xml"> <i class="bi bi-rss-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://scholar.google.com/citations?user=MDB_DgkAAAAJ"> 
<span class="menu-text">scholar</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Recursive Self-Improvement</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Tom Cunningham </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">February 10, 2026</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
        
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">





<style>
   h1 {  border-bottom: 4px solid black; }
   h2 {  border-bottom: 1px solid gray; padding-bottom: 0px; color: black; }
   dl {display: grid;}
   dt {grid-column-start: 1; width: 4cm;}
   dd {grid-column-start: 2; margin-left: 2em;}
</style>
<section id="summary" class="level1">
<h1>Summary</h1>
<dl>
<dt>TL;DR: an explosion is AI capabilities is <em>possible</em> but it’s intrinsically difficult to forecasat.</dt>
<dd>
<p>Define an explosion as a significant acceleration of the historical rates of progress in AI, e.g.&nbsp;measured by effective compute required for a given level of accuracy (e.g.&nbsp;perplexity).</p>
</dd>
<dt>Progress in AI is rapid but smooth.</dt>
<dd>
<p>We can measure progress in various ways, algorithmic progress has accelerated over the last 10-15 years. Epoch estimate that algorithmic efficiency has been growing at 3X/year between 2012-2024, measured by effective compute.</p>
</dd>
<dt>Forecasters expect an 8-20% chance of an explosion.</dt>
<dd>
They define an explosion as a 3X speedup in the historical rate of progress by some measure, from the METR/FRI survey.
</dd>
<dt>AI has already been accelerating AI research.</dt>
<dd>
<p>AI has been self-accelerating for decades: (1) automatic differentiation; (2) bayesian optimization of hyperparameters; (3) neural architecture search; (4) LLM coding autocomplete and chatbots; (5) LLM coding agents.</p>
</dd>
<dt>There’s good theoretical reason to expect an explosion.</dt>
<dd>
<p>(…)</p>
</dd>
<dt>AI usefulness for optimization depends on features of the setup.</dt>
<dd>
<p>Hassabis says AI will make progress wherever there’s (1) combinatorial search space; (2) clear feedback; (3) lots of data or an automatic validator.</p>
<p>However there’s clearly a fourth condition: that the data has some lower-dimensional latent structure. There are many problems that satisfy the first 3 but where we don’t expect substantial progress from autonomous NNs: (A) the telephone book; (B) mapping out stars in the sky; (C) documenting the genome.</p>
</dd>
<dt>AI speedups to discovery depends on the shape of the underlying landscape.</dt>
<dd>
<p>(abc)</p>
</dd>
</dl>
</section>
<section id="models" class="level1">
<h1>Models</h1>
<dl>
<dt>AK model of recursive growth.</dt>
<dd>
<p>Suppose we have <span class="math inline">\(Y=AK^\alpha\)</span>, and also we use some share <span class="math inline">\(s\)</span> of output on R&amp;D, which increases productivity <span class="math inline">\(A=sY^\beta\)</span>, then if <span class="math inline">\(\beta+\alpha&gt;1\)</span> we get a continuously increasing growth rate. Aghion, Jones and Jones (2017) elaborate on this model, where AI gradually is able to take over human tasks, &amp; they get a similar condition.</p>
</dd>
<dd>
<p>The critical question is whether the returns to R&amp;D effort are sufficiently steep. Some classic papers: Bloom et al.&nbsp;“are we running out of ideas”; and Erdil.</p>
</dd>
<dt>Landscape model.</dt>
<dd>
asdf
</dd>
</dl>
</section>
<section id="figure-evals" class="level1">
<h1>Figure / Evals</h1>
<p>Q: what evals would be useful? Suppose we can plot the following:</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="2025-09-13-recursive-self-improvement-explosion_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>We could plot data from a variety of different experiments on this plot, &amp; ask some interesting questions:</p>
<ol type="1">
<li>At what time horizon do we forecast models beating human-level ability at training models?</li>
<li>Do we see grokking, i.e.&nbsp;discontinuous jumps in model-training ability?</li>
<li>Are some models better than others at model-training?</li>
</ol>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="2025-09-13-recursive-self-improvement-explosion_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="references" class="level1">
<h1>References</h1>
<dl>
<dt>FRI/METR forecasts of AI progress.</dt>
<dd>
(…)
</dd>
<dt>Kortum (1997) <a href="https://egc.yale.edu/sites/default/files/Kortum_1997.pdf">“Research, Patenting, and Technological Change”</a></dt>
<dd>
<p>He assumes that technological progress comes from taking random draws from a distribution (undirected search). Then growth over time will be characterized by the extreme value distribution of the underlying distribution, &amp; returns to experience will depend on the thickeness of tails. Alternative assumptions:</p>
<ul>
<li>Bounded: growth slows as it approaches a ceiling.</li>
<li>Exponential (thin tails): <span class="math inline">\(A=\ln n\)</span></li>
<li>Pareto (thick tails): <span class="math inline">\(A = n^\gamma\)</span></li>
</ul>
</dd>
<dt>Erdil, Besiroglu, Ho (2024) <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4814445">“Estimating Idea Production: A Methodological Survey”</a></dt>
<dd>
<p>They discuss the difficulty of estimating the returns to R&amp;D on productivity across a few different domains. They have nice estimates for chess inputs &amp; outputs. They also estimate algorithmic progress:</p>
<ul>
<li>Computer vision: doubling time 9 months 2012-2022.</li>
<li>RL sample efficiency: doubling time 11 months, 2015-2019.</li>
<li>SAT solvers: doubling time 2 years, 1997-2018.</li>
<li>Linear programming: doubling time 6 years, 1998-2018.</li>
</ul>
<p>They include a nice derivation of the critical threshold for hyperbolic growth:</p>
<p><span class="math display">\[\begin{aligned}
     Y &amp;= AK^\alpha \\
     \dot{K} &amp;\propto Y &amp;&amp; \text{(constant savings rate)}\\
     \dot{A}/A &amp;\propto A^{-\beta}K^\lambda &amp;&amp; \text{(productivity growth)}
  \end{aligned}\]</span></p>
<p>You’ll get steady-state growth if and only if <span class="math inline">\(\lambda/\beta+\alpha=1\)</span>. You’ll get hyperbolic growth if <span class="math inline">\(\lambda/\beta+\alpha&gt;1\)</span>.</p>
</dd>
<dt>Aghion, Jones, and Jones (2019) “Artificial Intelligence and Economic Growth”</dt>
<dd>
They give conditions under which automating R&amp;D will cause an explosion in GDP growth (i.e.&nbsp;a steadily increasing rate of growth, AKA hyperbolic growth).
</dd>
</dl>
<p>David Owen (2024) <a href="https://epoch.ai/files/Interviewing_AI_researchers_on_automation_of_AI_R_D.pdf">“Automation of AI R&amp;D: Researcher Perspectives”</a></p>
<dl>
<dt>Eric Drexler (2025) <a href="https://aiprospects.substack.com/p/the-reality-of-recursive-improvement">“The Reality of Recursive Self-Improvement”</a></dt>
<dd>
<p>He argues that . Automating steps in the optimization loop: - Automating differentiation. - Neural architecture search. - Bayesian optimization.</p>
<blockquote class="blockquote">
<p>“The fundamental mechanism is systemic friction reduction — aggregate improvements expand possibilities by enabling faster progress and more ambitious goals</p>
</blockquote>
<blockquote class="blockquote">
<p>“In hyperparameter optimization, advances in Bayesian and multi-fidelity methods often achieve order-of-magnitude savings compared to naive grid search. What once required thousands of full model trainings can now be accomplished with fewer and more intelligent probes. As daunting costs of innovation fall, research becomes faster and more ambitious.</p>
</blockquote>
<blockquote class="blockquote">
<p>“The trajectory toward comprehensive AI capabilities makes these developments predictable, not in detail, but in outline.</p>
</blockquote>
</dd>
<dt>Eric Drexler (Aug 2025) <a href="https://aiprospects.substack.com/p/the-reality-of-recursive-improvement">“The Reality of Recursive Self-Improvement”</a></dt>
<dd>
<p>Automating steps in the optimization loop:</p>
<ul>
<li>Automating differentiation.</li>
<li>Neural architecture search.</li>
<li>Bayesian optimization.</li>
</ul>
</dd>
<dd>
<blockquote class="blockquote">
<p>“The fundamental mechanism is systemic friction reduction — aggregate improvements expand possibilities by enabling faster progress and more ambitious goals</p>
</blockquote>
<blockquote class="blockquote">
<p>“In hyperparameter optimization, advances in Bayesian and multi-fidelity methods often achieve order-of-magnitude savings compared to naive grid search. What once required thousands of full model trainings can now be accomplished with fewer and more intelligent probes. As daunting costs of innovation fall, research becomes faster and more ambitious.</p>
</blockquote>
<blockquote class="blockquote">
<p>“The trajectory toward comprehensive AI capabilities makes these developments predictable, not in detail, but in outline.</p>
</blockquote>
</dd>
</dl>
</section>
<section id="we-need-more-bottom-up-modelling-of-ais-economic-effect-unfinished" class="level1 page-columns page-full">
<h1>We need more bottom-up modelling of AI’s economic effect [UNFINISHED]</h1>
<dl class="page-columns page-full">
<dt>There are two approaches to modelling AI’s effect.</dt>
<dd>
<p>From a lot of conversations about recursive self-improvement I realized it’s useful to distinguish between two qualitatively different ways of thinking about AI’s ability to do work:</p>
<ol type="1">
<li><em>Top down:</em> AI replaces each of the human subtasks.</li>
<li><em>Bottom-up:</em> AI just does the entire procedure from first principles.</li>
</ol>
<p>I think 80% of discussion of economic impacts was of the top-down type.</p>
</dd>
<dt>What would bottom-up modelling look like?</dt>
<dd>
There are some papers with “model organisms” of recursive self-improvement: Grefenstette (1986) <a href="https://ui.adsabs.harvard.edu/abs/1986ITSMC..16..122G/abstract">genetic algorithm to learn parameters for genetic algorithsm</a>; <a href="https://arxiv.org/pdf/2003.03384">AutoML-Zero</a> (2020); Schmidhuber (2003) <a href="https://arxiv.org/abs/cs/0309048?utm_source=chatgpt.com">Godel Machines</a>.
</dd>
<dt>What are the implications for recursive self-improvement?</dt>
<dd>
AI treats it as a pure optimization problem, already it’s better at chip design, algorithm design.
</dd>
<dt>Some related discussion:</dt>
<dd class="page-columns page-full">
<p>Nathan Lambert recapitulates some discussions from the Curve about recursive self-improvement <a href="https://www.interconnects.ai/p/thoughts-on-the-curve">here</a>.</p>
<p>This is related to the Bresnahan/systems view of AI. He talks about the first wave of ML models: “The transition to ICT-based production has largely proceeded at the production system level, not the task level.”<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;“My empirical conclusion about these applications is that the lazy idea of AI – that is, of computer systems that are able to perform productive tasks previously done by humans– is irrelevant to understanding how these technologies create value. Here “irrelevant” does not mean that substitution of machine for human tasks is less important than other determinants of the value in use of AITs. It means irrelevant: task-level substitution of machine for human plays no role in these highly valuable systems.”</p></div></div></dd>
</dl>
</section>
<section id="overflow-offcuts" class="level1">
<h1>Overflow / Offcuts</h1>
<dl>
<dt>Some optimization landscapes that were already conquered.</dt>
<dd>
<ul>
<li><em>People used to compile tables by doing manual calculations</em> – digits of pi, values for pi, tolerance ranges of metals, random numbers.
<ul>
<li><em>Simplex optimization</em> – very many optimization problems can now be brute-forced. We already knew the algorithms, but it was slow to compute by hand. E.g. simplex.</li>
<li>(see a ChatGPT literature review I made on 2026-01-04)</li>
</ul></li>
</ul>
</dd>
</dl>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Domain / Problem</th>
<th>Pre-computer human method (what people did before; typical scale)</th>
<th>Computer-era breakthrough (year + system/organization)</th>
<th>What changed (order-of-magnitude scale/speed; new capability)</th>
<th>Key sources (2–3)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Census tabulation (large-scale counting &amp; cross-tabs)</td>
<td>Manual tally sheets + hand addition; limited ability to do many cross-tabulations; 1880 U.S. census tabulation famously took <strong>&gt;8 years</strong></td>
<td><strong>1890</strong>: U.S. Census Bureau adopts <strong>Hollerith punched‑card electric tabulators</strong></td>
<td>Mechanized “brute-force counting” (sort + count across huge card decks); basic totals moved from years to months; punched‑card tabulation became routine for decades (into the 1950s)</td>
<td>U.S. Census Bureau on Hollerith tabulation (<a href="https://www.census.gov/about/history/bureau-history/census-innovations/technology/hollerith-machine.html?utm_source=chatgpt.com" title="The Hollerith Machine">Census.gov</a>); IBM “Punch cards” history &amp; 1880 vs 1890 timing (<a href="https://en.wikipedia.org/wiki/Cryptanalysis_of_the_Lorenz_cipher?utm_source=chatgpt.com" title="Cryptanalysis of the Lorenz cipher">Wikipedia</a>); Census.gov history note on long use into 1950s (<a href="https://www.census.gov/about/history/stories/monthly/2016/january-2016.html?utm_source=chatgpt.com" title="January 2016: Herman Hollerith and Mechanical Tabulation">Census.gov</a>)</td>
</tr>
<tr class="even">
<td>Cryptanalysis: <strong>Enigma</strong> daily key recovery (systematic key search under constraints)</td>
<td>Human cryptanalysts built hypotheses (“cribs”), did hand checks / desk-calculator work; throughput limited and time‑sensitive</td>
<td><strong>1940</strong>: <strong>Bletchley Park</strong> deploys the (British) <strong>Bombe</strong> (Turing/Welchman concept; electro‑mechanical search)</td>
<td>Turned a largely manual, fragile workflow into a mechanized search over Enigma settings consistent with a crib—key recovery became operationally scalable (when cribs/traffic conditions allowed)</td>
<td>NSA history of the cryptanalytic bombe ; National Museum of Computing “Bombe Machine”</td>
</tr>
<tr class="odd">
<td>Cryptanalysis: <strong>Lorenz SZ40/42 (“Tunny”)</strong> wheel-setting search</td>
<td>Before Colossus, attacks relied on laborious statistical/correlation work with earlier aids + significant human effort</td>
<td><strong>1944</strong>: <strong>Colossus</strong> at Bletchley Park (Tommy Flowers / Post Office Research Station), programmable electronic machine for Tunny analysis</td>
<td>High-speed automated counting/correlation tests greatly accelerated wheel-setting searches and made the method operational at wartime scale (often described as the first programmable electronic computer)</td>
<td>National Museum of Computing “The Colossus Computer” (<a href="https://www.investors.com/news/management/leaders-and-success/herman-hollerith-pioneered-data-processing/?utm_source=chatgpt.com" title="Herman Hollerith Put A Punch Into Data Processing">Investors.com</a>); National Museum of Computing “Colossus: the world’s first programmable computer” (<a href="https://cs.stanford.edu/people/eroberts/courses/soco/projects/2008-09/colossus/history.html?utm_source=chatgpt.com" title="The History of the Lorenz Cipher and the Colossus Machine">Computer Science</a>)</td>
</tr>
<tr class="even">
<td>Operations research: <strong>Linear programming (LP)</strong> optimization (simplex)</td>
<td>Desk calculators + accounting machines; even “toy” LPs were expensive—e.g., the 77‑variable diet LP took ~<strong>120 person‑days</strong> on desk calculators</td>
<td><strong>1952</strong>: <strong>Project SCOOP</strong> (USAF/DoD context) uses <strong>UNIVAC</strong> in the Pentagon era to run LP; early practitioners report solving problems with <strong>hundreds</strong> of variables/constraints</td>
<td>LP becomes a “push-button” computational optimization workflow rather than a bespoke hand calculation; feasible problem sizes jump from tens to hundreds (and later far more)</td>
<td>Dantzig biography (desk calculators/IBM accounting equipment context) ; Gill note on 77‑variable diet LP taking ~120 man‑days by hand ; Saul Gass interview recounting UNIVAC’s 1952 arrival + early LP scale (e.g., 250 eqns/500 vars)</td>
</tr>
<tr class="odd">
<td>Operations research: <strong>Integer programming</strong> via cutting planes (exact discrete optimization)</td>
<td>Small integer decisions often handled by hand enumeration or ad hoc reasoning; scaling was rapidly prohibitive</td>
<td><strong>1958–1960</strong>: <strong>Ralph Gomory</strong> (RAND) introduces/implements <strong>cutting‑plane methods</strong>; early work implemented on <strong>IBM 704</strong></td>
<td>Created a systematic exact method: solve LP relaxations, generate valid cuts, iterate—turning discrete optimization into a repeatable computational procedure</td>
<td>Gomory’s 1958 “algorithm for integer solutions to linear programs” ; Gomory retrospective noting IBM 704 + FORTRAN implementation (summer 1958) ; RAND report on mixed‑integer algorithm (1960)</td>
</tr>
<tr class="even">
<td>Combinatorial optimization: <strong>TSP</strong> solved to optimality at modern scale (branch‑and‑cut)</td>
<td>Humans could do only small instances exactly; early exact “proof” instances were dozens of cities (often with special structure/hand work)</td>
<td><strong>2005–2006</strong> (computation reported): <strong>Concorde</strong> branch‑and‑cut solves <strong>pla85900 (85,900 cities)</strong> to optimality; formal certification described in later paper</td>
<td>Exact TSP moves from tens/hundreds → <strong>tens of thousands</strong> with machine‑checkable certificates; “exact + certified” becomes routine for benchmark sets (TSPLIB)</td>
<td>Concorde project page (largest TSPLIB solved = 85,900) ; pla85900 page (2005/06 computation context) ; Applegate et al.&nbsp;on certifying optimality for 85,900‑city TSP</td>
</tr>
<tr class="odd">
<td>Combinatorial search: <strong>SAT / automated theorem proving</strong> (DPLL/DP → modern CDCL)</td>
<td>Human proof search/case analysis; limited scale for complex logical formulas and circuit constraints</td>
<td><strong>1960</strong>: Davis–Putnam procedure framed for machine proof search; <strong>2001</strong>: <strong>Chaff</strong> delivers <strong>1–2 orders‑of‑magnitude</strong> speedups vs prior complete solvers</td>
<td>SAT becomes a practical “black-box” combinatorial search engine; real instances from verification can reach <strong>millions of variables / clauses</strong></td>
<td>Davis &amp; Putnam (1960) original procedure PDF ; Chaff paper reporting 1–2 orders‑of‑magnitude speedups ; Zhang et al.&nbsp;noting verification instances with millions of variables/clauses</td>
</tr>
<tr class="even">
<td>Mathematical case analysis: <strong>Four‑color theorem</strong> (finite unavoidable set + reducibility checking)</td>
<td>Purely human proofs stalled for a century; manual reducibility checking did not scale to the needed case set</td>
<td><strong>1976–1977</strong>: <strong>Appel &amp; Haken</strong> (Univ. of Illinois) complete the proof by reducing to a large finite set and checking reducibility by computer (papers published 1977)</td>
<td>Enabled exhaustive checking of <strong>thousands</strong> of configurations—often cited as the first major theorem proved with extensive computer assistance; counts reported vary (e.g., 1,936 vs later 1,482)</td>
<td>Illinois exhibit overview (timeline + “finite but large number of cases” + computer role) ; Appel–Haken Part I (1977) paper (primary) ; note on 1,482‑member unavoidable set (secondary)</td>
</tr>
<tr class="odd">
<td>Scientific least squares / estimation: <strong>orbit determination &amp; trajectory prediction</strong></td>
<td>“Human computers” using desk calculators (e.g., Friden-era workflows) integrated trajectories and iteratively corrected orbits from tracking data—slow and labor intensive</td>
<td><strong>Early 1960s</strong>: <strong>Project Mercury</strong> uses <strong>IBM 7090</strong> for trajectory computation and real‑time tracking/prediction; <strong>1963</strong> report documents orbit‑determination/prediction programs; JPL deep‑space navigation evolves orbit determination programs</td>
<td>Orbit estimation becomes fast enough for mission operations (frequent updates, higher data volume, more systematic differential correction/least‑squares style pipelines)</td>
<td>IBM history on Mercury using 7090s for trajectory parameters + real‑time tracking/prediction ; NASA/NTRS “Orbit Determination and Prediction, and Computer Programs” (1963) ; JPL navigation history (orbit determination program evolution)</td>
</tr>
<tr class="even">
<td>Cryptanalysis: <strong>DES</strong> exhaustive key search (demonstration attack)</td>
<td>By hand, exhaustive search is impossible; early “massive software” efforts showed feasibility but still took many days</td>
<td><strong>1998</strong>: EFF builds <strong>DES Cracker (“Deep Crack”)</strong> hardware; cracks RSA’s DES Challenge II in <strong>&lt;3 days</strong>; built for <strong>&lt; $250,000</strong></td>
<td>Made brute‑force key search demonstrably practical/affordable; shifted the security narrative for DES and reinforced the need for stronger standards</td>
<td>EFF project page (cost &lt; $250k; &lt;3 days; beat 39‑day prior record) ; <em>Cracking DES</em> book (technical design + purpose) ; NIST AES development history citing EFF’s “Cracking DES”</td>
</tr>
<tr class="odd">
<td>Game solving: <strong>Checkers</strong> (exhaustive retrograde analysis + proof search)</td>
<td>Human masters analyzed openings/endgames, but no proof of the game-theoretic value; endgame knowledge limited to manageable subsets</td>
<td><strong>2007</strong>: Schaeffer et al.&nbsp;(Chinook / Univ. of Alberta) announce <strong>checkers is solved</strong> (perfect play → draw) using retrograde analysis and related proof techniques</td>
<td>Replaced human analysis with a computer‑verified solution and large computed databases; established a definitive game-theoretic result</td>
<td><em>Science</em> “Checkers Is Solved” (primary) ; hosted PDF copy (same article)</td>
</tr>
<tr class="even">
<td>Game-tree search: <strong>Chess</strong> (Deep Blue)</td>
<td>Humans explored variations mentally; even strong humans can’t enumerate anywhere near machine scale; early engines were far weaker due to limited search</td>
<td><strong>1997</strong>: <strong>IBM Deep Blue</strong> defeats Kasparov; architecture combines parallelism + custom chess chips; reports include ~<strong>200 million positions/sec</strong> scale</td>
<td>Brute-force/alpha–beta style search at massive throughput became sufficient for world‑champion performance, making deep tactical exploration routine for machines</td>
<td>IBM history page (200M positions/sec; brute-force framing) ; Hsu (IEEE Micro, 1999) on Deep Blue chips &amp; system evolution ; Campbell et al.&nbsp;technical overview (Deep Blue system/search rates)</td>
</tr>
</tbody>
</table>
</section>
<section id="davidson-halperin-houlden-korinek-when-does-automating-research-produce-explosive-growth" class="level1">
<h1>Davidson, Halperin, Houlden, Korinek “When Does Automating Research Produce Explosive Growth?”</h1>
<section id="summary-1" class="level2">
<h2 class="anchored" data-anchor-id="summary-1">summary</h2>
<ul>
<li><p>Goal: forecast when we’re likely to get explosive intelligence growth.</p></li>
<li><p>Summary: once you automate a sufficient number of tasks (do them with capital) then it’ll kick off an intelligence explosion.</p></li>
<li><p>Basic model:</p>
<ul>
<li>If <span class="math inline">\(\dot{S}_t=S_t^{1-\beta}\)</span>, so you get explosion if <span class="math inline">\(\beta&lt;0\)</span>, steady-state growth if <span class="math inline">\(\beta=0\)</span>, and gradual slowing if <span class="math inline">\(\beta&gt;1\)</span>.</li>
<li>Now if you start automating inputs, the effective exponent gradually gets higher.</li>
</ul></li>
<li><p>Multi-sector version.</p>
<ul>
<li>You gradually automate some of the things, so they can be done with capital.</li>
<li>You have spillovers between different processes.</li>
</ul></li>
<li><p>Estimates of <span class="math inline">\(\beta\)</span>:</p>
<ul>
<li>Bloom overall <span class="math inline">\(\beta=3\)</span></li>
<li>For software R&amp;D they estimate <span class="math inline">\(\beta=3\)</span></li>
<li>they say in software <span class="math inline">\(\beta=0.1\)</span></li>
</ul></li>
<li><p>NOTE: <span class="math inline">\(\beta\)</span></p></li>
<li><p>Questions:</p>
<ul>
<li>Any historical domain where we’ve seen regimes of <span class="math inline">\(\beta&lt;0\)</span>?</li>
</ul></li>
</ul>
</section>
<section id="my-observations" class="level2">
<h2 class="anchored" data-anchor-id="my-observations">my observations</h2>
<ul>
<li><p>Q: how to think about the growth effect of uplift vs automation?</p></li>
<li><p>Automation makes things <em>free</em>, rather than being produced by capital.</p></li>
</ul>
</section>
<section id="tom-houlden-questions" class="level2">
<h2 class="anchored" data-anchor-id="tom-houlden-questions">tom houlden questions</h2>
<p>Q: is this graphical system novel?</p>
<p>Q: is there a nice metaphor/analogy to think about automation/explosion?</p>
</section>
</section>
<section id="chart" class="level1">
<h1>chart</h1>
<pre class="mermaid"><code>flowchart LR
  RD["R&amp;D"] --&gt; algorithms
  RD_compute["R&amp;D compute"] --&gt; algorithms
  algorithms --&gt; intelligence
  compute --&gt; intelligence
  data --&gt; intelligence
  intelligence --&gt;|??| economic_value["economic value"]
  intelligence --&gt;|??| RD</code></pre>
</section>
<section id="recursive-self-improvement" class="level1">
<h1>2026-01-13 | recursive self-improvement</h1>
<p>Three models:</p>
<p><span class="math display">\[\xymatrix{A \ar[r] &amp; B}\]</span></p>
<p><span class="math display">\[\xymatrix{
      &amp; *++[F:&lt;15pt&gt;]{unobserved}\ar[d]\ar@{.&gt;}[dr]
      &amp; *++[F:&lt;15pt&gt;]{unobserved}\ar[d]
      \\*++[F]\txt{experiment}\ar[r]\ar@{.&gt;}@/_1cm/[rr]
      &amp; *++[F]\txt{Favs}\ar[r]
      &amp; *++[F]\txt{mDAU}
   }\]</span></p>
<pre class="mermaid"><code>flowchart LR
  RD["R&amp;D"] --&gt; algorithms
  algorithms --&gt; algorithms</code></pre>
<dl>
<dt>Simplest model</dt>
<dd>
<ul>
<li>Simplest model: <span class="math display">\[\begin{aligned}
      \dot{A} &amp;= R^\gamma A^{1-\beta}
         &amp;&amp; \text{second }\\
   \end{aligned}\]</span></li>
</ul>
</dd>
<dt>Extrapolating.</dt>
<dd>
Dangerous to talk about “the” elasticity from one component to another component. A constant-elasticity relationship isn’t.
</dd>
<dt>Calibrating.</dt>
<dd>
How do you calibrate the effect of intelligence on R&amp;D? 1. Full automation -&gt; <span class="math display">\[
   2. Partial automation -&gt; \]</span> 3. Augmentation -&gt; $$
</dd>
<dt>Metaphors</dt>
<dd>
<ol type="1">
<li><strong>A tool factory makes better tools.</strong></li>
<li><strong>Drawing balls from an urn.</strong> Suppose we simply have</li>
<li><strong>Blacksmith.</strong> – you have a hammer and you can make horseshoes or hammers. A harder hammer can make horseshoes faster, or it can make a harder-again hammer.</li>
</ol>
</dd>
</dl>


</section>


</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp("tecunningham\.github\.io");
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
</div> <!-- /content -->




</body></html>