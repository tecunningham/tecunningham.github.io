<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tom Cunningham">
<meta name="description" content="Tom Cunningham blog">

<title>Knowledge of Success | Tom Cunningham – Tom Cunningham</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-12027453-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>
<script>window.MathJax = {
   loader: { load: ["https://cdn.jsdelivr.net/gh/sonoisa/XyJax-v3@3.0.1/build/xypic.js"]},
   tex: {packages: {'[+]': ['xypic','bm']},
         macros: {  bm: ["\\boldsymbol{#1}", 1],
                    ut: ["\\underbrace{#1}_{\\text{#2}}", 2],
                    utt: ["\\underbrace{#1}_{\\substack{\\text{#2}\\\\\\text{#3}}}", 3] }
   }
};
</script>
<style>
   h1 {  border-bottom: 8px solid #557;}
   h2 {  border-bottom: 1px solid #ccc;}
   .greyproof {
      background-color: #f5f5f5;
      padding: 1em;
      margin: 1em 0;
      border-radius: 4px;
   }
</style>
<meta name="quarto:status" content="draft">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="Knowledge of Success | Tom Cunningham">
<meta name="twitter:description" content="Tom Cunningham blog">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner"><div id="quarto-draft-alert" class="alert alert-warning"><i class="bi bi-pencil-square"></i>Draft</div>
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Tom Cunningham</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href=".././about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/testingham"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/tom-cunningham-a9433/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://tecunningham.github.io/index.xml"> <i class="bi bi-rss-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://scholar.google.com/citations?user=MDB_DgkAAAAJ"> 
<span class="menu-text">scholar</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Knowledge of Success</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
        
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">





<dl>
<dt>LLMs are much more useful if they tell you their confidence.</dt>
<dd>
<p>When asking an LLM to do a task it’s reassuring to know that it’s successful in some fraction of cases, but it’s far more useful to know <em>which</em> cases it’s successful in:</p>
<ul>
<li>If you’re answering a question, I want to know your confidence in the answer.</li>
<li>If you’re supplying a proof for a theorem, I want to know whether you think the proof is valid.</li>
<li>If you’re fixing a bug, I want to know if you think the fix is going to work.</li>
<li>If you’re writing a poem for me, I want to know if the poem is good.</li>
</ul>
<p>Unfortunately until recently LLMs were trained just to maximize their success rates, and for that reason they often wouldn’t report useful signals of confidence, which made them much less useful. This I think is a good explanation of why LLMs hallucinate (argued in <span class="citation" data-cites="kalai2025why">Kalai et al. (<a href="#ref-kalai2025why" role="doc-biblioref">2025</a>)</span>), but the same logic illuminates some other cases.</p>
<p>The discussion below mostly follows <span class="citation" data-cites="kalai2025why">Kalai et al. (<a href="#ref-kalai2025why" role="doc-biblioref">2025</a>)</span>, but adds some arguments and visualization I did when working with those authors in 2024 at OpenAI. I think these points are fairly well-known within the industry but ought to be better known outside it.</p>
</dd>
<dt>This follows from a very simple model.</dt>
<dd>
<p>Suppose I have to make a choice among <span class="math inline">\(N\)</span> options, and I have no priors about which is most likely to be right. Then it’s fine if the LLM just tells me which is the most-likely option, without telling me its probability.</p>
<p>However if we add a touch of realism, then it suddenly becomes much more useful if the LLM tells me its probability (or it admits when it doesn’t know). This will happen if any of the following are true: (1) I have some private information about the different options; (2) I can choose to spend some time verifying the proposed option, or searching for solutions; (3) I have the option of abstaining and not making a choice.</p>
</dd>
<dt>Some implications.</dt>
<dd>
<ol type="1">
<li><strong>Models seem overconfident because they are trained only on accuracy, not on calibration.</strong> When models aren’t allowed to fold they learn to bluff.</li>
<li><strong>The value of a model will be a convex in its accuracy.</strong> Going from 90% to 100% accuracy is more than twice as valuable as going from 80% to 90%, because it lowers the cost of verification, and lowers the likelihood of abstention. This is only true when models don’t report their confidence.</li>
<li><strong>Benchmarks should report both accuracy and reliability.</strong> If you’re choosing betwen two models it’s useful to know not just the share of correct responses, but also whether the model will report when it fails (i.e.&nbsp;accuracy).</li>
<li><strong>Models are good self-critics.</strong> Somewhat surprisingly, a model can often identify its own mistakes. This makes sense for models that are trained only on accuracy, not on calibration, because they systematically exaggerate their success.</li>
</ol>
</dd>
<dt>In this post.</dt>
<dd>
<p>I state the model very briefly and give a nice visual aid, to show the optimal threshold for abstaining.</p>
<p>I also show how we can use a simplex diagram to illustrate tradeoffs between accuracy and confidence, showing both the frontier (plotting results from benchmarks) and .</p>
</dd>
</dl>
<section id="model" class="level1">
<h1>Model</h1>
<dl>
<dt>Basic model: you just care about accuracy.</dt>
<dd>
<p>Suppose you have to choose which of <span class="math inline">\(N\)</span> options is correct, you get <span class="math inline">\(u=1\)</span> if you succeed and <span class="math inline">\(u=0\)</span> if you fail. You have no idea which is right (uniform priors), but you know the LLM has information, and chooses the right answer with probability <span class="math inline">\(p\)</span>. In this case it’s sufficent for the LLM to report the option with the highest probability, and the user’s expected payoff is linear in the LLM’s average accuracy, <span class="math inline">\(p\)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="2026-02-26-hallucinations-and-alignment_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="384"></p>
</figure>
</div>
</div>
</div>
</dd>
<dt>If you can abstain, payoff is convex in accuracy.</dt>
<dd>
<p>Now suppose the user can choose to abstain, i.e.&nbsp;they refuse to make a choice and get <span class="math inline">\(u=\pi_i\)</span>, with <span class="math inline">\(0&lt;\pi_a&lt;1\)</span>. Then they will only consult the model if <span class="math inline">\(p&gt;\pi_a\)</span>, and so the value of the model will be convex in <span class="math inline">\(p\)</span>. The threshold here is the same as that derived in <span class="citation" data-cites="chow1970optimum">Chow (<a href="#ref-chow1970optimum" role="doc-biblioref">1970</a>)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="2026-02-26-hallucinations-and-alignment_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="384"></p>
</figure>
</div>
</div>
</div>
</dd>
<dt>If there is costly verification then payoff will be three-part.</dt>
<dd>
<p>Finally, now assume you can pay a cost <span class="math inline">\(c\)</span> to verify whether an answer is correct, and if it’s wrong then you abstain. Then there will be three regions:</p>
<ul>
<li>If <span class="math inline">\(p\)</span> is low, you don’t ask the LLM, and abstain.</li>
<li>If <span class="math inline">\(p\)</span> is intermediate you ask but verify (and abstain if it’s wrong).</li>
<li>If <span class="math inline">\(p\)</span> is high then you ask and trust without verification.</li>
</ul>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="2026-02-26-hallucinations-and-alignment_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="384"></p>
</figure>
</div>
</div>
</div>
</dd>
</dl>
</section>
<section id="simplex-representation" class="level1 page-columns page-full">
<h1>Simplex Representation</h1>
<p>It’s very useful to draw the likelihood of three outcomes (succeed, fail, abstain) on a diagram.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> We can then draw indifference curves represnting different objective functions:</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Economists know this as a Marschack-Machina diagram.</p></div></div><div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="2026-02-26-hallucinations-and-alignment_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Each point in the simplex is a lottery over outcomes: a model might succeed with probability <span class="math inline">\(p_s\)</span>, fail with probability <span class="math inline">\(p_f\)</span>, and abstain with probability <span class="math inline">\(p_a\)</span>. The panels show three different objective families:</p>
<ul>
<li><strong>Accuracy-only</strong> (<span class="math inline">\(U=p_s\)</span>): success is rewarded, but failure and abstention are treated the same. This creates pressure to guess rather than abstain.</li>
<li><strong>Penalize failure</strong> (linear expected utility): failure is explicitly penalized relative to abstention, expanding the region where abstaining is optimal.</li>
<li><strong>F1</strong> (a non-linear metric): indifference curves bend, reflecting that the metric itself builds in a particular tradeoff between attempting and being correct.</li>
</ul>
<section id="simpleqa" class="level3">
<h3 class="anchored" data-anchor-id="simpleqa">SimpleQA</h3>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="2026-02-26-hallucinations-and-alignment_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="abstain-qa" class="level3">
<h3 class="anchored" data-anchor-id="abstain-qa">Abstain-QA</h3>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="2026-02-26-hallucinations-and-alignment_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="reading-the-simplex-plots" class="level3">
<h3 class="anchored" data-anchor-id="reading-the-simplex-plots">Reading the simplex plots</h3>
<ol type="1">
<li><p>The horizontal axis is <span class="math inline">\(p_s\)</span> (succeed share) and the vertical axis is <span class="math inline">\(p_f\)</span> (fail share). The remaining probability is <span class="math inline">\(p_a=1-p_s-p_f\)</span> (abstain share), so points closer to the diagonal edge have lower abstention.</p></li>
<li><p>Moving down (lower <span class="math inline">\(p_f\)</span>) corresponds to reducing failures; whether that is best depends on how much worse failure is than abstention (<span class="math inline">\(\pi_f\)</span> vs <span class="math inline">\(\pi_a\)</span>).</p></li>
<li><p>The labeled points illustrate that a model can look good under an “answer-everything” regime by pushing <span class="math inline">\(p_a\)</span> toward zero, but that is exactly the regime that a payoff function with a harsh <span class="math inline">\(\pi_f\)</span> would discourage.</p></li>
<li><p>Don’t over-interpret cross-benchmark comparisons: each source defines abstention differently, so these plots are best read as a geometric visualization of tradeoffs, not as a single unified leaderboard.</p></li>
</ol>
</section>
</section>
<section id="related-literature" class="level1">
<h1>Related Literature</h1>
<section id="chronological-sketch" class="level2">
<h2 class="anchored" data-anchor-id="chronological-sketch">Chronological sketch</h2>
<ul>
<li><span class="citation" data-cites="chow1970optimum">Chow (<a href="#ref-chow1970optimum" role="doc-biblioref">1970</a>)</span>: introduces the Bayes-optimal reject option (“indecision”) and derives the posterior-threshold rule.</li>
<li><span class="citation" data-cites="herbei2006reject">Herbei and Wegkamp (<a href="#ref-herbei2006reject" role="doc-biblioref">2006</a>)</span>: formalizes “classification with a reject option” in modern statistical learning terms; there are many followups on learning and using reject policies (e.g. <span class="citation" data-cites="bartlett2008reject elyaniv2010selective geifman2019selectivenet">(<a href="#ref-bartlett2008reject" role="doc-biblioref">Bartlett and Wegkamp 2008</a>; <a href="#ref-elyaniv2010selective" role="doc-biblioref">El-Yaniv and Wiener 2010</a>; <a href="#ref-geifman2019selectivenet" role="doc-biblioref">Geifman and El-Yaniv 2019</a>)</span>).</li>
<li><span class="citation" data-cites="kadavath2022mostly">Kadavath et al. (<a href="#ref-kadavath2022mostly" role="doc-biblioref">2022</a>)</span>: finds that language models can often estimate whether their own answers are correct, which is exactly the signal needed to implement a threshold rule in practice.</li>
<li>Recent LLM work tries to <em>implement</em> the missing abstain/verify channel via refusal-aware tuning and explicit “IDK” tokens <span class="citation" data-cites="zhang-etal-2024-r cohen2024idontknowexplicit">(<a href="#ref-zhang-etal-2024-r" role="doc-biblioref">Zhang et al. 2024</a>; <a href="#ref-cohen2024idontknowexplicit" role="doc-biblioref">Cohen et al. 2024</a>)</span>, verification loops <span class="citation" data-cites="dhuliawala2023chainofverificationreduceshallucinationlarge altinisik2026doireallyknow">(<a href="#ref-dhuliawala2023chainofverificationreduceshallucinationlarge" role="doc-biblioref">Dhuliawala et al. 2023</a>; <a href="#ref-altinisik2026doireallyknow" role="doc-biblioref">Altinisik et al. 2026</a>)</span>, and black-box uncertainty proxies like sampling-based checks or semantic uncertainty <span class="citation" data-cites="manakul-etal-2023-selfcheckgpt farquhar2024detectinghallucinationssemanticentropy">(<a href="#ref-manakul-etal-2023-selfcheckgpt" role="doc-biblioref">Manakul, Liusie, and Gales 2023</a>; <a href="#ref-farquhar2024detectinghallucinationssemanticentropy" role="doc-biblioref">Farquhar et al. 2024</a>)</span>.</li>
<li><span class="citation" data-cites="kalai2025why">Kalai et al. (<a href="#ref-kalai2025why" role="doc-biblioref">2025</a>)</span>: argues LLM hallucinations are a predictable consequence of binary grading that penalizes abstention; proposes a scoring rule that makes abstaining optimal below a stated confidence threshold.</li>
</ul>
<section id="chow-1970-optimal-reject-rules" class="level3">
<h3 class="anchored" data-anchor-id="chow-1970-optimal-reject-rules">Chow (1970): Optimal reject rules</h3>
<p><span class="citation" data-cites="chow1970optimum">Chow (<a href="#ref-chow1970optimum" role="doc-biblioref">1970</a>)</span> introduces the <strong>reject option</strong> (which he calls the “Indecision class” <span class="math inline">\(I\)</span>) into pattern recognition and derives the optimal error-reject tradeoff. Chow’s terminology maps directly onto ours:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Chow’s term</th>
<th>Our term</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>correct recognition</td>
<td>succeed</td>
</tr>
<tr class="even">
<td>error (misclassification)</td>
<td>fail</td>
</tr>
<tr class="odd">
<td>rejection / indecision</td>
<td>abstain</td>
</tr>
</tbody>
</table>
<p>Chow’s setup adds a third action (reject) to ordinary classification. In one common normalization, the costs are <span class="math inline">\(0\)</span> for a correct classification, <span class="math inline">\(1\)</span> for an error, and <span class="math inline">\(t\)</span> for a rejection.</p>
<p>The key result (“Chow’s rule”) is a posterior-threshold rule: accept and classify when confident enough, otherwise reject: <span class="math display">\[
\max_i P(G_i \mid x) \ge 1-t
\quad\Rightarrow\quad \text{accept;}
\qquad
\max_i P(G_i \mid x) &lt; 1-t
\quad\Rightarrow\quad \text{reject.}
\]</span></p>
<p>It is optimal in the sense that, for a given rejection threshold (equivalently, a given reject rate), no other rule achieves a lower error rate.</p>
<p>The threshold <span class="math inline">\(t\)</span> is related to the costs of the three outcomes as <span class="math inline">\(t = (C_r - C_c)/(C_e - C_c)\)</span>, where <span class="math inline">\(C_e, C_r, C_c\)</span> are the costs of error, rejection, and correct recognition. In our payoff notation <span class="math inline">\((\pi_s,\pi_f,\pi_a)\)</span>, Chow’s rule becomes: predict iff</p>
<p><span class="math display">\[
\max_y P(y \mid x) \ge \frac{\pi_a - \pi_f}{\pi_s - \pi_f}.
\]</span></p>
<p>In the Marschak-Machina triangle, this threshold corresponds to one of the indifference lines: the boundary between the region where prediction is preferred and the region where abstention is preferred.</p>
</section>
</section>
<section id="herbei-and-wegkamp-2006-and-followups-learning-a-reject-option" class="level2">
<h2 class="anchored" data-anchor-id="herbei-and-wegkamp-2006-and-followups-learning-a-reject-option">Herbei and Wegkamp (2006) and followups: Learning a reject option</h2>
<p><span class="citation" data-cites="herbei2006reject">Herbei and Wegkamp (<a href="#ref-herbei2006reject" role="doc-biblioref">2006</a>)</span> (and a large followup literature) reframes the reject option as a learning problem: you want a predictor that is accurate on the examples it attempts, while explicitly controlling how often it refuses.</p>
<p>For this post, the key takeaway is less about any one algorithm and more about the framing: “abstention is a first-class action” is standard in the statistical decision-theory literature, and the same expected-utility thresholds show up once you make the third outcome explicit.</p>
</section>
<section id="kadavath-et-al.-2022-models-can-sometimes-score-their-own-answers" class="level2">
<h2 class="anchored" data-anchor-id="kadavath-et-al.-2022-models-can-sometimes-score-their-own-answers">Kadavath et al.&nbsp;(2022): Models can sometimes score their own answers</h2>
<p>If you want the simple <span class="math inline">\(p^*\)</span> rule to be usable, you need some per-question measure of correctness probability (like <span class="math inline">\(p_{\max}(x)\)</span> or a direct <span class="math inline">\(P(\text{correct}\mid x)\)</span> proxy).</p>
<p><span class="citation" data-cites="kadavath2022mostly">Kadavath et al. (<a href="#ref-kadavath2022mostly" role="doc-biblioref">2022</a>)</span> study this in language models, finding that they can often estimate whether their own answers are correct. That makes “give the user a probability” a practical interface choice, not just a theoretical one.</p>
</section>
<section id="kalai-nachum-vempala-and-zhang-2025-why-language-models-hallucinate" class="level2">
<h2 class="anchored" data-anchor-id="kalai-nachum-vempala-and-zhang-2025-why-language-models-hallucinate">Kalai, Nachum, Vempala, and Zhang (2025): Why language models hallucinate</h2>
<p><span class="citation" data-cites="kalai2025why">Kalai et al. (<a href="#ref-kalai2025why" role="doc-biblioref">2025</a>)</span> argue that hallucinations are not a mysterious glitch but a predictable consequence of how models are trained and evaluated. Their central thesis is that the three-outcome structure (succeed, fail, abstain) is systematically distorted by binary evaluation:</p>
<p>The paper makes two distinct arguments:</p>
<p><strong>1. Pretraining origin.</strong> Even with error-free training data, the statistical objective of pretraining produces hallucinations. The authors reduce the problem to binary classification (“Is-It-Valid”), showing that</p>
<p><span class="math display">\[
\text{generative error rate} \gtrsim 2 \cdot \text{IIV misclassification rate}.
\]</span></p>
<p>For arbitrary facts (like someone’s birthday) where there is no learnable pattern, the hallucination rate after pretraining is at least the fraction of facts appearing exactly once in the training data.</p>
<p><strong>2. Post-training persistence.</strong> Even after RLHF and other interventions, hallucinations persist because nearly all evaluation benchmarks use binary grading that penalizes abstention:</p>
<p>The fix they propose is exactly the payoff structure from our Marschak-Machina framework: penalize errors more than abstentions, with an explicit confidence threshold <span class="math inline">\(t\)</span> stated in the prompt. Their proposed scoring rule awards <span class="math inline">\(+1\)</span> for a correct answer, <span class="math inline">\(-t/(1-t)\)</span> for an incorrect answer, and <span class="math inline">\(0\)</span> for abstaining—so that answering is optimal iff confidence exceeds <span class="math inline">\(t\)</span>. This is Chow’s reject-option rule rediscovered in the LLM evaluation context.</p>
</section>
<section id="recent-mechanisms-refusal-uncertainty-signals-and-verification" class="level2">
<h2 class="anchored" data-anchor-id="recent-mechanisms-refusal-uncertainty-signals-and-verification">Recent mechanisms: refusal, uncertainty signals, and verification</h2>
<p>Our model is deliberately abstract: it assumes the user has access to (i) a usable confidence signal <span class="math inline">\(p\)</span> and (ii) an abstain / verify channel. A lot of recent work can be read as ways of engineering those two ingredients:</p>
<ul>
<li><p><strong>Make abstention an explicit output.</strong> Refusal-aware fine-tuning can teach a model to say “I don’t know” on out-of-knowledge questions <span class="citation" data-cites="zhang-etal-2024-r">(<a href="#ref-zhang-etal-2024-r" role="doc-biblioref">Zhang et al. 2024</a>)</span>. Similarly, adding an explicit uncertainty token and training it to soak up probability mass on incorrect predictions effectively adds an abstain action to the model’s output space <span class="citation" data-cites="cohen2024idontknowexplicit">(<a href="#ref-cohen2024idontknowexplicit" role="doc-biblioref">Cohen et al. 2024</a>)</span>.</p></li>
<li><p><strong>Pay a cost to verify.</strong> Inference-time verification loops like Chain-of-Verification (CoVe) can be viewed as “spend extra tokens/compute to reduce <span class="math inline">\(p_f\)</span>” <span class="citation" data-cites="dhuliawala2023chainofverificationreduceshallucinationlarge">(<a href="#ref-dhuliawala2023chainofverificationreduceshallucinationlarge" role="doc-biblioref">Dhuliawala et al. 2023</a>)</span>. Very recent work trains this kind of behavior directly, with structured self-verification traces and an explicit final decision to answer vs abstain <span class="citation" data-cites="altinisik2026doireallyknow">(<a href="#ref-altinisik2026doireallyknow" role="doc-biblioref">Altinisik et al. 2026</a>)</span>.</p></li>
<li><p><strong>Construct a confidence signal without logits.</strong> When output probabilities are unavailable (or untrustworthy), disagreement across samples can act as a proxy confidence signal. SelfCheckGPT does this with sampling-based consistency checks <span class="citation" data-cites="manakul-etal-2023-selfcheckgpt">(<a href="#ref-manakul-etal-2023-selfcheckgpt" role="doc-biblioref">Manakul, Liusie, and Gales 2023</a>)</span>; semantic-uncertainty methods like semantic entropy similarly use semantic variability across generations to predict and filter confabulations <span class="citation" data-cites="farquhar2024detectinghallucinationssemanticentropy">(<a href="#ref-farquhar2024detectinghallucinationssemanticentropy" role="doc-biblioref">Farquhar et al. 2024</a>)</span>, and followup work proposes cheaper “semantic entropy probes” in the same spirit <span class="citation" data-cites="kossen2024semanticentropyprobesrobust">(<a href="#ref-kossen2024semanticentropyprobesrobust" role="doc-biblioref">Kossen et al. 2024</a>)</span>.</p></li>
</ul>
<p>A cautionary note: neither uncertainty proxies nor “self-verification” are automatically reliable. Some hallucinations happen with high confidence (so uncertainty-based filters can miss them) <span class="citation" data-cites="simhi2025trustmeimwrong">(<a href="#ref-simhi2025trustmeimwrong" role="doc-biblioref">Simhi et al. 2025</a>)</span>, and in logical reasoning settings models can struggle to identify their own errors (so internal self-checks can fail without external grounding) <span class="citation" data-cites="hong-etal-2024-closer">(<a href="#ref-hong-etal-2024-closer" role="doc-biblioref">Hong et al. 2024</a>)</span>.</p>
<p>Beyond factual QA, <span class="citation" data-cites="mohamadi2025honestyaccuracytrustworthylanguage">Mohamadi, Wang, and Li (<a href="#ref-mohamadi2025honestyaccuracytrustworthylanguage" role="doc-biblioref">2025</a>)</span> show on GSM8K/MedQA/GPQA that replacing binary RLVR rewards with a ternary scheme <span class="math inline">\((+1,0,-\lambda)\)</span> produces controllable answer-vs-abstain tradeoffs and useful abstention-aware cascades. <span class="citation" data-cites="jha2026rewardingintellectualhumilitylearning">Jha et al. (<a href="#ref-jha2026rewardingintellectualhumilitylearning" role="doc-biblioref">2026</a>)</span> report on MedMCQA and Hendrycks Math that moderate abstention rewards reduce wrong answers without collapsing coverage, especially when paired with supervised abstention training. In code generation, <span class="citation" data-cites="dai2025reducinghallucinationsllmgeneratedcode">Dai et al. (<a href="#ref-dai2025reducinghallucinationsllmgeneratedcode" role="doc-biblioref">2025</a>)</span> frame the task as “find a correct program or abstain” and use semantic triangulation to improve abstention decisions on LiveCodeBench/CodeElo. Complementarily, <span class="citation" data-cites="oehri2025trusteduncertaintylargelanguage">Oehri et al. (<a href="#ref-oehri2025trusteduncertaintylargelanguage" role="doc-biblioref">2025</a>)</span> fuse multiple uncertainty signals into calibrated correctness probabilities and enforce user-specified risk budgets via refusal, including experiments on code generation with execution tests.</p>
</section>
</section>
<section id="appendix-verification" class="level1">
<h1>Appendix: Verification option</h1>
<p>Suppose the user has a way to pay a cost <span class="math inline">\(c\)</span> to obtain the correct answer (e.g.&nbsp;look it up, run an expensive check, ask a human). In the simplest model, verification yields certain success with payoff <span class="math inline">\(\pi_s-c\)</span>.</p>
<p>Since abstaining and verifying are both “outside options” (their payoff does not depend on the model’s confidence), the only relevant outside-option payoff is <span class="math display">\[
\pi_{\text{outside}}=\max\{\pi_a,\;\pi_s-c\}.
\]</span></p>
<p>The attempt rule is the same threshold logic as before: attempt iff <span class="math display">\[
p \ge \frac{\pi_{\text{outside}}-\pi_f}{\pi_s-\pi_f}.
\]</span></p>
<p>In the example shown below, <span class="math inline">\((\pi_s,\pi_a,\pi_f)=(2,0,-2)\)</span> and <span class="math inline">\(\pi_s-c=1\)</span>, so <span class="math display">\[
p^*_{\mathrm{verify}}=\frac{1-(-2)}{2-(-2)}=\tfrac{3}{4}.
\]</span></p>
<p>Operationally, “verify” can mean many things: a web lookup, a separate fact-checking model, retrieval + citation, or even a structured self-checking loop that spends extra tokens before committing to an answer <span class="citation" data-cites="dhuliawala2023chainofverificationreduceshallucinationlarge altinisik2026doireallyknow">(<a href="#ref-dhuliawala2023chainofverificationreduceshallucinationlarge" role="doc-biblioref">Dhuliawala et al. 2023</a>; <a href="#ref-altinisik2026doireallyknow" role="doc-biblioref">Altinisik et al. 2026</a>)</span>.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="2026-02-26-hallucinations-and-alignment_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<section id="appendix-cross-benchmark-outcome-table" class="level2">
<h2 class="anchored" data-anchor-id="appendix-cross-benchmark-outcome-table">Appendix: Cross-Benchmark Outcome Table</h2>
<p>To make cross-model plotting easier, the table below standardizes outputs from multiple benchmarks into a common schema.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>benchmark</th>
<th>model</th>
<th style="text-align: right;">p_s_pct</th>
<th style="text-align: right;">p_f_pct</th>
<th style="text-align: right;">p_a_pct</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>SimpleQA</td>
<td>Claude-3-haiku (2024-03-07)</td>
<td style="text-align: right;">5.1</td>
<td style="text-align: right;">19.6</td>
<td style="text-align: right;">75.3</td>
</tr>
<tr class="even">
<td>SimpleQA</td>
<td>Claude-3-sonnet (2024-02-29)</td>
<td style="text-align: right;">5.7</td>
<td style="text-align: right;">19.3</td>
<td style="text-align: right;">75.0</td>
</tr>
<tr class="odd">
<td>SimpleQA</td>
<td>Claude-3-opus (2024-02-29)</td>
<td style="text-align: right;">23.5</td>
<td style="text-align: right;">36.9</td>
<td style="text-align: right;">39.6</td>
</tr>
<tr class="even">
<td>SimpleQA</td>
<td>Claude-3.5-sonnet (2024-06-20)</td>
<td style="text-align: right;">28.9</td>
<td style="text-align: right;">36.1</td>
<td style="text-align: right;">35.0</td>
</tr>
<tr class="odd">
<td>SimpleQA</td>
<td>GPT-4o-mini</td>
<td style="text-align: right;">8.6</td>
<td style="text-align: right;">90.5</td>
<td style="text-align: right;">0.9</td>
</tr>
<tr class="even">
<td>SimpleQA</td>
<td>GPT-4o</td>
<td style="text-align: right;">38.2</td>
<td style="text-align: right;">60.8</td>
<td style="text-align: right;">1.0</td>
</tr>
<tr class="odd">
<td>SimpleQA</td>
<td>OpenAI o1-mini</td>
<td style="text-align: right;">8.1</td>
<td style="text-align: right;">63.4</td>
<td style="text-align: right;">28.5</td>
</tr>
<tr class="even">
<td>SimpleQA</td>
<td>OpenAI o1-preview</td>
<td style="text-align: right;">42.7</td>
<td style="text-align: right;">48.1</td>
<td style="text-align: right;">9.2</td>
</tr>
<tr class="odd">
<td>Abstain-QA</td>
<td>GPT-4 Turbo</td>
<td style="text-align: right;">66.1</td>
<td style="text-align: right;">19.7</td>
<td style="text-align: right;">14.2</td>
</tr>
<tr class="even">
<td>Abstain-QA</td>
<td>GPT-4 32K</td>
<td style="text-align: right;">72.0</td>
<td style="text-align: right;">19.1</td>
<td style="text-align: right;">8.9</td>
</tr>
<tr class="odd">
<td>Abstain-QA</td>
<td>GPT-3.5 Turbo</td>
<td style="text-align: right;">61.1</td>
<td style="text-align: right;">37.4</td>
<td style="text-align: right;">1.5</td>
</tr>
<tr class="even">
<td>Abstain-QA</td>
<td>Mixtral 8x7b</td>
<td style="text-align: right;">54.1</td>
<td style="text-align: right;">37.0</td>
<td style="text-align: right;">8.9</td>
</tr>
<tr class="odd">
<td>Abstain-QA</td>
<td>Mixtral 8x22b</td>
<td style="text-align: right;">59.0</td>
<td style="text-align: right;">29.1</td>
<td style="text-align: right;">11.9</td>
</tr>
</tbody>
</table>
<p>Notes: - Table columns are constructed to look like probabilities in the <span class="math inline">\((p_s,p_f,p_a)\)</span> simplex: <span class="math inline">\(p_s=\text{p\_s\_pct}/100\)</span>, <span class="math inline">\(p_f=\text{p\_f\_pct}/100\)</span>, <span class="math inline">\(p_a=\text{p\_a\_pct}/100\)</span>. - For SimpleQA, these correspond directly to {Correct, Incorrect, Not attempted} shares. - For Abstain-QA, these are constructed from the paper’s summary metrics; interpret them as a mapping into a common coordinate system, not as identical underlying evaluation protocols.</p>
<section id="data-extraction-details-by-source" class="level3">
<h3 class="anchored" data-anchor-id="data-extraction-details-by-source">Data extraction details by source</h3>
<p><strong>SimpleQA (Wei et al., 2024) <span class="citation" data-cites="wei2024measuringshortformfactualitylarge">(<a href="#ref-wei2024measuringshortformfactualitylarge" role="doc-biblioref">Wei et al. 2024</a>)</span>.</strong> The table uses all model rows shown in the main SimpleQA model-comparison table (8 models). Here, <code>p_s_pct</code> is <strong>Correct</strong>, <code>p_a_pct</code> is <strong>Not attempted</strong>, and <code>p_f_pct</code> is computed as <span class="math inline">\(100-\text{Correct}-\text{Not attempted}\)</span>. I chose this slice because it is the paper’s canonical cross-model summary and directly exposes explicit non-attempt behavior.</p>
<p><strong>Abstain-QA (Madhusudhan et al., 2024) <span class="citation" data-cites="madhusudhan2024llmsknowanswerinvestigating">(<a href="#ref-madhusudhan2024llmsknowanswerinvestigating" role="doc-biblioref">Madhusudhan et al. 2024</a>)</span>.</strong> This is a deliberate subset, not all values in the paper: I take the <strong>MMLU / Standard clause / Base</strong> rows (5 models) from the main result table. The paper reports <strong>AAC</strong> (answerable accuracy) and <strong>AR</strong> (abstention rate). I set <span class="math inline">\(p_{a,\%}=\text{AR}\)</span> and construct <span class="math inline">\(p_{s,\%}\)</span> and <span class="math inline">\(p_{f,\%}\)</span> by treating AAC as attempt-conditional accuracy: <span class="math display">\[
p_s = \text{AAC}\cdot(1-p_a),\qquad
p_f = (1-\text{AAC})\cdot(1-p_a),
\]</span> all expressed in percent.</p>
<p><strong>Important comparability caveats.</strong> Even after mapping all results into <span class="math inline">\((p_s,p_f,p_a)\)</span> coordinates, the underlying tasks and abstention protocols differ: SimpleQA is short-form factual QA with optional non-attempts, while Abstain-QA is multiple-choice QA with an explicit IDK/NOTA option. So the combined table is useful for geometric intuition and directional comparisons, but not for strict leaderboard ranking across benchmarks.</p>
<p>Sources: <span class="citation" data-cites="wei2024measuringshortformfactualitylarge">Wei et al. (<a href="#ref-wei2024measuringshortformfactualitylarge" role="doc-biblioref">2024</a>)</span>; <span class="citation" data-cites="madhusudhan2024llmsknowanswerinvestigating">Madhusudhan et al. (<a href="#ref-madhusudhan2024llmsknowanswerinvestigating" role="doc-biblioref">2024</a>)</span>.</p>
</section>
</section>
<section id="appendix-benchmark-points-in-the-simplex" class="level2">




</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">Appendix: Benchmark Points in the Simplex</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-altinisik2026doireallyknow" class="csl-entry" role="listitem">
Altinisik, Enes, Masoomali Fatehkia, Fatih Deniz, Nadir Durrani, Majd Hawasly, Mohammad Raza, and Husrev Taha Sencar. 2026. <span>“Do i Really Know? Learning Factual Self-Verification for Hallucination Reduction.”</span> <a href="https://doi.org/10.48550/arXiv.2602.02018">https://doi.org/10.48550/arXiv.2602.02018</a>.
</div>
<div id="ref-bartlett2008reject" class="csl-entry" role="listitem">
Bartlett, Peter L., and Marten H. Wegkamp. 2008. <span>“Classification with a Reject Option Using a Hinge Loss.”</span> <em>Journal of Machine Learning Research</em> 9 (59): 1823–40. <a href="https://www.jmlr.org/papers/v9/bartlett08a.html">https://www.jmlr.org/papers/v9/bartlett08a.html</a>.
</div>
<div id="ref-chow1970optimum" class="csl-entry" role="listitem">
Chow, C. K. 1970. <span>“On Optimum Recognition Error and Reject Tradeoff.”</span> <em>IEEE Transactions on Information Theory</em>. <a href="https://doi.org/10.1109/TIT.1970.1054406">https://doi.org/10.1109/TIT.1970.1054406</a>.
</div>
<div id="ref-cohen2024idontknowexplicit" class="csl-entry" role="listitem">
Cohen, Roi, Konstantin Dobler, Eden Biran, and Gerard de Melo. 2024. <span>“I Don’t Know: Explicit Modeling of Uncertainty with an [IDK] Token.”</span> <a href="https://doi.org/10.48550/arXiv.2412.06676">https://doi.org/10.48550/arXiv.2412.06676</a>.
</div>
<div id="ref-dai2025reducinghallucinationsllmgeneratedcode" class="csl-entry" role="listitem">
Dai, Yihan, Sijie Liang, Haotian Xu, Peichu Xie, and Sergey Mechtaev. 2025. <span>“Reducing Hallucinations in <span>LLM</span>-Generated Code via Semantic Triangulation.”</span> <a href="https://doi.org/10.48550/arXiv.2511.12288">https://doi.org/10.48550/arXiv.2511.12288</a>.
</div>
<div id="ref-dhuliawala2023chainofverificationreduceshallucinationlarge" class="csl-entry" role="listitem">
Dhuliawala, Shehzaad, Mojtaba Komeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Celikyilmaz, and Jason Weston. 2023. <span>“Chain-of-Verification Reduces Hallucination in Large Language Models.”</span> <a href="https://doi.org/10.48550/arXiv.2309.11495">https://doi.org/10.48550/arXiv.2309.11495</a>.
</div>
<div id="ref-elyaniv2010selective" class="csl-entry" role="listitem">
El-Yaniv, Ran, and Yair Wiener. 2010. <span>“On the Foundations of Noise-Free Selective Classification.”</span> <em>Journal of Machine Learning Research</em> 11 (53): 1605–41. <a href="https://www.jmlr.org/papers/v11/el-yaniv10a.html">https://www.jmlr.org/papers/v11/el-yaniv10a.html</a>.
</div>
<div id="ref-farquhar2024detectinghallucinationssemanticentropy" class="csl-entry" role="listitem">
Farquhar, Sebastian, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. 2024. <span>“Detecting Hallucinations in Large Language Models Using Semantic Entropy.”</span> <em>Nature</em> 630 (8017): 625–30. <a href="https://doi.org/10.1038/s41586-024-07421-0">https://doi.org/10.1038/s41586-024-07421-0</a>.
</div>
<div id="ref-geifman2019selectivenet" class="csl-entry" role="listitem">
Geifman, Yonatan, and Ran El-Yaniv. 2019. <span>“SelectiveNet: A Deep Neural Network with an Integrated Reject Option.”</span> In <em>Proceedings of the 36th International Conference on Machine Learning</em>, 97:2151–59. Proceedings of Machine Learning Research. <a href="https://proceedings.mlr.press/v97/geifman19a.html">https://proceedings.mlr.press/v97/geifman19a.html</a>.
</div>
<div id="ref-herbei2006reject" class="csl-entry" role="listitem">
Herbei, Radu, and Marten H. Wegkamp. 2006. <span>“Classification with Reject Option.”</span> <em>The Canadian Journal of Statistics</em> 34 (4): 709–21. <a href="https://doi.org/10.1002/cjs.5550340410">https://doi.org/10.1002/cjs.5550340410</a>.
</div>
<div id="ref-hong-etal-2024-closer" class="csl-entry" role="listitem">
Hong, Ruixin, Hongming Zhang, Xinyu Pang, Dong Yu, and Changshui Zhang. 2024. <span>“A Closer Look at the Self-Verification Abilities of Large Language Models in Logical Reasoning.”</span> In <em>Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</em>, 900–925. Mexico City, Mexico: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2024.naacl-long.52">https://doi.org/10.18653/v1/2024.naacl-long.52</a>.
</div>
<div id="ref-jha2026rewardingintellectualhumilitylearning" class="csl-entry" role="listitem">
Jha, Abha, Akanksha Mahajan, Ashwath Vaithinathan Aravindan, Praveen Saravanan, Sai Sailaja Policharla, and Sonal Chaturbhuj Gehlot. 2026. <span>“Rewarding Intellectual Humility Learning When Not to Answer in Large Language Models.”</span> <a href="https://doi.org/10.48550/arXiv.2601.20126">https://doi.org/10.48550/arXiv.2601.20126</a>.
</div>
<div id="ref-kadavath2022mostly" class="csl-entry" role="listitem">
Kadavath, Saurav, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, et al. 2022. <span>“Language Models (Mostly) Know What They Know.”</span> <em>arXiv Preprint</em>. <a href="https://doi.org/10.48550/arXiv.2207.05221">https://doi.org/10.48550/arXiv.2207.05221</a>.
</div>
<div id="ref-kalai2025why" class="csl-entry" role="listitem">
Kalai, Adam Tauman, Ofir Nachum, Santosh S. Vempala, and Edwin Zhang. 2025. <span>“Why Language Models Hallucinate.”</span> <em>arXiv Preprint</em> arXiv:2509.04664 (September). <a href="https://doi.org/10.48550/arXiv.2509.04664">https://doi.org/10.48550/arXiv.2509.04664</a>.
</div>
<div id="ref-kossen2024semanticentropyprobesrobust" class="csl-entry" role="listitem">
Kossen, Jannik, Jiatong Han, Muhammed Razzak, Lisa Schut, Shreshth Malik, and Yarin Gal. 2024. <span>“Semantic Entropy Probes: Robust and Cheap Hallucination Detection in <span>LLM</span>s.”</span> <a href="https://doi.org/10.48550/arXiv.2406.15927">https://doi.org/10.48550/arXiv.2406.15927</a>.
</div>
<div id="ref-madhusudhan2024llmsknowanswerinvestigating" class="csl-entry" role="listitem">
Madhusudhan, Nishanth, Sathwik Tejaswi Madhusudhan, Vikas Yadav, and Masoud Hashemi. 2024. <span>“Do <span>LLM</span>s Know When to <span>NOT</span> Answer? Investigating Abstention Abilities of Large Language Models.”</span> <a href="https://doi.org/10.48550/arXiv.2407.16221">https://doi.org/10.48550/arXiv.2407.16221</a>.
</div>
<div id="ref-manakul-etal-2023-selfcheckgpt" class="csl-entry" role="listitem">
Manakul, Potsawee, Adian Liusie, and Mark Gales. 2023. <span>“SelfCheckGPT: Zero-Resource Black-Box Hallucination Detection for Generative Large Language Models.”</span> In <em>Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, 9004–17. Singapore: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2023.emnlp-main.557">https://doi.org/10.18653/v1/2023.emnlp-main.557</a>.
</div>
<div id="ref-mohamadi2025honestyaccuracytrustworthylanguage" class="csl-entry" role="listitem">
Mohamadi, Mohamad Amin, Tianhao Wang, and Zhiyuan Li. 2025. <span>“Honesty over Accuracy: Trustworthy Language Models Through Reinforced Hesitation.”</span> <a href="https://doi.org/10.48550/arXiv.2511.11500">https://doi.org/10.48550/arXiv.2511.11500</a>.
</div>
<div id="ref-oehri2025trusteduncertaintylargelanguage" class="csl-entry" role="listitem">
Oehri, Markus, Giulia Conti, Kaviraj Pather, Alexandre Rossi, Laia Serra, Adrian Parody, Rogvi Johannesen, Aviaja Petersen, and Arben Krasniqi. 2025. <span>“Trusted Uncertainty in Large Language Models: A Unified Framework for Confidence Calibration and Risk-Controlled Refusal.”</span> <a href="https://doi.org/10.48550/arXiv.2509.01455">https://doi.org/10.48550/arXiv.2509.01455</a>.
</div>
<div id="ref-simhi2025trustmeimwrong" class="csl-entry" role="listitem">
Simhi, Adi, Itay Itzhak, Fazl Barez, Gabriel Stanovsky, and Yonatan Belinkov. 2025. <span>“Trust Me, i’m Wrong: <span>LLM</span>s Hallucinate with Certainty Despite Knowing the Answer.”</span> <a href="https://doi.org/10.48550/arXiv.2502.12964">https://doi.org/10.48550/arXiv.2502.12964</a>.
</div>
<div id="ref-wei2024measuringshortformfactualitylarge" class="csl-entry" role="listitem">
Wei, Jason, Karina Nguyen, Hyung Won Chung, Yunxin Joy Jiao, Spencer Papay, Amelia Glaese, John Schulman, and William Fedus. 2024. <span>“Measuring Short-Form Factuality in Large Language Models.”</span> <a href="https://doi.org/10.48550/arXiv.2411.04368">https://doi.org/10.48550/arXiv.2411.04368</a>.
</div>
<div id="ref-zhang-etal-2024-r" class="csl-entry" role="listitem">
Zhang, Hanning, Shizhe Diao, Yong Lin, Yi Fung, Qing Lian, Xingyao Wang, Yangyi Chen, Heng Ji, and Tong Zhang. 2024. <span>“R-Tuning: Instructing Large Language Models to Say <span>“I</span> Don<span>’</span>t Know<span>”</span>.”</span> In <em>Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers)</em>, 7113–39. Mexico City, Mexico: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2024.naacl-long.394">https://doi.org/10.18653/v1/2024.naacl-long.394</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("tecunningham\.github\.io");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>