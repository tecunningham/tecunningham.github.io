<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tom Cunningham">
<meta name="dcterms.date" content="2025-10-15">
<meta name="description" content="Tom Cunningham blog">

<title>Implications of the Manifold Hypothesis | Tom Cunningham – Tom Cunningham</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-12027453-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>
<script>window.MathJax = {
   loader: { load: ["https://cdn.jsdelivr.net/gh/sonoisa/XyJax-v3@3.0.1/build/xypic.js"]},
   tex: {packages: {'[+]': ['xypic','bm']},
         macros: {  bm: ["\\boldsymbol{#1}", 1],
                    ut: ["\\underbrace{#1}_{\\text{#2}}", 2],
                    utt: ["\\underbrace{#1}_{\\substack{\\text{#2}\\\\\\text{#3}}}", 3] }
   }
};
</script>
<style>
   h1 {  border-bottom: 4px solid black;}
   h2 {  border-bottom: 1px solid #ccc;}
</style>
<meta name="quarto:status" content="draft">

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="Implications of the Manifold Hypothesis | Tom Cunningham">
<meta name="twitter:description" content="Tom Cunningham blog">
<meta name="twitter:card" content="summary">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner"><div id="quarto-draft-alert" class="alert alert-warning"><i class="bi bi-pencil-square"></i>Draft</div>
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Tom Cunningham</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href=".././about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/testingham"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/tom-cunningham-a9433/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://tecunningham.github.io/index.xml"> <i class="bi bi-rss-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://scholar.google.com/citations?user=MDB_DgkAAAAJ"> 
<span class="menu-text">scholar</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Implications of the Manifold Hypothesis</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Tom Cunningham </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 15, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
        
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">





<p>In short: recent developments in AI can be thought of as computers getting access to the latent space that underlies reality.</p>
<section id="summary" class="level1">
<h1>Summary</h1>
<dl>
<dt>In short.</dt>
<dd>
<p>Reality has a low-dimensional structure. The signals we send and receive are high-dimensional (images, audio, text), but they are <em>clustered</em> such that they can be represented almost without loss in a low-dimensional space.</p>
</dd>
<dd>
We have only recently taught computers to perform the mapping between low- and high-dimensional representations, and the economic effects of AI are a consequence of this.
</dd>
<dt>The manifold hypothesis.</dt>
<dd>
<p>Bengio et al.&nbsp;(2012) define the manifold hypothesis: <em>“real-world data presented in high dimensional spaces are expected to concentrate in the vicinity of a manifold of much lower dimensionality.”</em></p>
<p>This can be thought of as a nonlinear version of principal components analysis. Computer scientists say that neural nets work well because they’re a good fit for the nature of real-world manifolds (Bengio says neural nets encode a prior that the manifold is smooth, sparse, and hierarchical).</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">signal (high dimensional)</th>
<th style="text-align: center;">state (low dimensional)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">image</td>
<td style="text-align: center;">objects, angle, lighting</td>
</tr>
<tr class="even">
<td style="text-align: center;">audio</td>
<td style="text-align: center;">text, speaker, tone, volume</td>
</tr>
<tr class="odd">
<td style="text-align: center;">text</td>
<td style="text-align: center;">meaning, style, phrasing</td>
</tr>
</tbody>
</table>
</dd>
<dt>Statistical implications of the manifold hypothesis.</dt>
<dd>
<ol type="1">
<li><strong>Signals are redundant:</strong> if you remove a pixel from an image or a word from a text you can predict what the missing point was with high confidence.</li>
</ol>
</dd>
<dd>
<ol start="2" type="1">
<li><strong>Signals are compressible:</strong> you can reconstruct a signal with high accuracy using a low-dimensional representation.</li>
</ol>
</dd>
<dd>
<ol start="3" type="1">
<li><strong>Unsupervised learning is useful:</strong> Passive observation of the world helps you learn the manifold, and so makes you much better at subsequent supervised learning. Likewise learning to predict one label helps you predict other labels (transfer learning).</li>
</ol>
</dd>
<dd>
<ol start="4" type="1">
<li><strong>Shallow algorithms fail at high dimensional tasks:</strong> Non-hierarchical learning methods (regression, decision trees) work well if you feed them the low-dimensional state, but work badly if you feed them the raw high-dimensional signal.</li>
</ol>
</dd>
<dt>Application to human abilities.</dt>
<dd>
<p>It is useful to think of the human brain as extracting a low dimensional state from a high dimensional signal (perceptual input, words, etc.).</p>
<p>Most of our perceptual ability is clearly unconscious: we are able to make very subtle inferences from huge amounts of sense data but we have limited conscious introspection into that judgment, e.g.&nbsp;judging how far away a tree is, judging how old a person is from their face, judging someone’s identify from their voice. Psychologists will often say that the pre-conscious brain is doing some kind of Bayesian inference before presenting the results to the conscious brain.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<p>An important observation about human capabilities is that they are <em>asymmetric</em>: it’s trivial to recognize whether an object has some property (a joke is funny, a picture is beautiful) but it’s far harder to create an object that has that property. This type of asymmetry can arise from a computational asymmetry (P vs NP), but I think the cause is different, it’s because our konwledge is tacit.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
</dd>
<dt>Application to computer history.</dt>
<dd>
<p>Computers have been able to beat humans at all sorts of computational tasks since the 1940s, including low-dimensional statistical inference (e.g.&nbsp;linear regression).</p>
</dd>
<dd>
Only recently they’ve become able to match human ability in making inference about high-dimensional objects like text, audio, images. This is because it takes a lot of data to learn the mapping between low-dimensional state and high-dimensional signal. Notably computers can do the mapping both ways: they can recognize whether a picture contains a cat (like a human), and they can also create a picture that contains a cat (much harder for humans).
</dd>
</dl>
<dl>
<dt>Application to recommenders.</dt>
<dd>
<p>Historically recommenders which used the <em>content</em> of items were not very effective, e.g.&nbsp;recommending music to someone based on their preferences over tempo, or recommending web pages based on raw text matches. As a consequence the most successful recommender and classifier algorithms relied on engagement instead of content: e.g.&nbsp;pagerank, collaborative filtering.</p>
</dd>
<dd>
However neural nets are now powerful enough to extract the latent semantic content of an item, so we can directly model someone’s judgment of an item as a function of its content, instead of proxying their judgment with other peoples’ judgment of that item. Implications: (1) we can now recommend content even when we have no engagement data (solving the “cold start” problem); (2) we’re no longer constrained by what content already exists, we can now synthesize new content to maximize some function, e.g.&nbsp;synthesize advertisements to maximize click-through rate. (See a case study on the history of content classification <a href="https://tecunningham.github.io/posts/2023-11-18-history-automated-text-moderation.html">here</a>).
</dd>
<dt>Application to intellectual property.</dt>
<dd>
<p>Suppose that humans can recognize the properties of an object (whether a joke is funny, painting is pretty, etc.), but they can’t create an object with a given set of properties. Then society will be characterized by <em>copying</em>: people will repeat the same jokes, reproduce the same paintings. In this world it’s efficient to have protection of intellectual property to incentivize discovery of new objects.</p>
</dd>
<dd>
However now suppose we teach a computer to recognize the properties of an object (which means learning the manifold), but it can also do the reverse, i.e.&nbsp;it can easily create a funny joke, or paint a pretty painting. In equilibrium there will now be much less imitation, and the efficient intellectual property regime will change. We shouldn’t allow the first person (or algorithm) who is able to see the entire latent landscape to claim ownership of everything that they discover.
</dd>
<dt>Application to communication.</dt>
<dd>
<p>I wrote a <a href="https://tecunningham.github.io/posts/2023-06-06-effect-of-ai-on-communication.html">note last year</a> with some prediction about how LLMs will affect communications, which I think is consistent with this manifold hypothesis.</p>
</dd>
<dd>
<ol type="1">
<li>For properties where human judgment is the ground truth (e.g.&nbsp;whether something is grammatical, is hate speech, is pornographic), then AI classifiers will achieve perfect accuracy, &amp; this favors defense.</li>
</ol>
</dd>
<dd>
<ol start="2" type="1">
<li>For properties which refer to some outside fact (whether a statement is true, whether an image depicts a real event, whether a painting is a forgery), then AI synthesis will degrade the ordinary human ability to make inferences from the content of the item, &amp; so we will have to rely relatively more on signals of provenance.</li>
</ol>
</dd>
<dt>Application to LLMs.</dt>
<dd>
<p>We can think of pre-training an LLM as discovering the low-dimensional structure of text, and that low-dimensional structure includes all the knowledge expressed in the training text. Once you have learned how to transform a piece of high-dimensional text into a low-dimensional semantic representation then suddenly a lot of intractable problems become tractable. E.g. you can train a model on a few pairs of (question,answer) and get a pretty useful chatbot (AKA ChatGPT). This would be a wildly intractable problem without the low-dimensional representation.</p>
</dd>
<dd>
<p>There’s another interesting observation about chatbots: they’re mostly trained to give an answer that the user prefers. But why would you ask someone a question if you already know which answer is best? This is explicable either (1) if we train a chatbot to maximize the preferences of an expert, not the average person; (2) if it’s easier for humans to recognize a good answer than to create one, either because of tacit knowledge or a computational constraint.</p>
</dd>
<dt>Application to wages.</dt>
<dd>
<p>Here is a very stylized model which incorporates the manifold hypothesis. Suppose that comparative advantage across people is entirely due to their private knowledge: painters know how to paint, programmers know how to program, etc.. It’s hard to justify this assumption in a purely rational model because you could just write down the information and sell it, but it makes more sense if knowledge is tacit and there’s learning-by-doing.</p>
</dd>
<dd>
Suppose now that LLMs can observe everyone’s actions and extract their tacit knowledge. Intuitively, you now have an assistant in your pocket who can answer any question that you’d normally go to a domain expert for, and so the rents to expertise will collapse.
</dd>
<dd>
This can be formalized as a pure trade model, where each agent has a vector of productivities, and there’s some global equilibrium price vector. The LLM makes private knowledge public, and so raises everyone’s productivity at each task to the level of the world expert. This has the following implications:
</dd>
<dd>
<ol type="1">
<li>Wages of the highest-paid fall, but aggregate output increases (“leveling up”)</li>
</ol>
</dd>
<dd>
<ol start="2" type="1">
<li>Exchange will fall (and so GDP may fall) because you can do most things yourself – e.g.&nbsp;you wouldn’t call a doctor because you can diganose yourself.</li>
</ol>
</dd>
<dd>
<ol start="3" type="1">
<li>The incentives to acquire new knowledge decline (insofar as an LLM can extract your knowledge by watching you work).</li>
</ol>
</dd>
<dd>
However it’s notable that LLMs can beat most people on most knowledge-based questions, and yet they still have relatively minor productivity effects, so there’s something missing from their set of capabilities, &amp; that’s still somewhat of an open question.
</dd>
</dl>
<aside id="footnotes" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol>
<li id="fn1"><p>A classic reference is Pylyshyn (1999) on modularity of perception.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>I wrote a <a href="https://tecunningham.github.io/posts/2017-12-10-unconscious-influences.html">blog post</a> about evidence for tacit knowledge, &amp; a <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=MDB_DgkAAAAJ&amp;citation_for_view=MDB_DgkAAAAJ:WF5omc3nYNoC">paper</a> formalizing a tacit knowledge explanation of this asymmetry, &amp; trying to apply it to economic decision-making.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="types-of-problems" class="level1">
<h1>Types of Problems</h1>
<dl>
<dt>General setup:</dt>
<dd>
you’re given a question q∈Q, choose an answer a∈A, and get payoff y(q,a). My claim is the <em>shape</em> of the function y(.,.) will determine everything.
</dd>
<dt>There are two useful subtypes.</dt>
<dd>
<ol type="1">
<li><em>Low-dimensional question</em> (landscape navigation). Suppose we face the same problem over and over but there are many possible distinct answers, then so there’s an explore-exploit problem. More precisely, suppose <span class="math inline">\(y(q,a)|q\)</span> is a rugged function of <span class="math inline">\(a\)</span>.</li>
<li><em>Low-dimensional answer</em> (question-answering). Suppose we constantly get different questions but the answer is just a binary of scalar. This is like a classic supervised learning problem.</li>
</ol>
</dd>
</dl>
<table class="caption-top table">
<colgroup>
<col style="width: 7%">
<col style="width: 36%">
<col style="width: 7%">
<col style="width: 5%">
<col style="width: 43%">
</colgroup>
<thead>
<tr class="header">
<th>type</th>
<th>problem</th>
<th style="text-align: center;">question</th>
<th style="text-align: center;">answer</th>
<th>notes</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>questions</td>
<td>What digit is in this image (MNIST)?</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>What is the sum of X and Y?</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>What was last transaction by account XXX?</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td>Many real-world record-keeping problems like this</td>
</tr>
<tr class="even">
<td></td>
<td>What chess move to play here?</td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>Color pixel in Mandelbrot</td>
<td style="text-align: center;">low</td>
<td style="text-align: center;">low</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td></td>
</tr>
<tr class="odd">
<td>landscape</td>
<td></td>
<td style="text-align: center;"></td>
<td style="text-align: center;"></td>
<td></td>
</tr>
</tbody>
</table>
<dl>
<dt>Claims about capacities of human &amp; computer brains:</dt>
<dd>
<ul>
<li>Classic computers are bad when there’s high manifold curvature.</li>
<li>Classic computers are good when there’s</li>
</ul>
</dd>
</dl>
</section>
<section id="alternative-setup" class="level1">
<h1>Alternative Setup</h1>
<p>[TODO: add a diagram showing generating process, breadth and depth of generating process]</p>
<dl>
<dt>Two characteristics of prediction problems.</dt>
<dd>
<ol type="1">
<li><em>Manifold dimensionality.</em> – high dimensionality if it’s irreducibly complex, e.g.&nbsp;a telephone book full of numbers.</li>
</ol>
</dd>
<dd>
<ol start="2" type="1">
<li><em>Manifold curvature.</em> – high curvature if it’s complicated to map the surface dimensions to the output – low curvature if it’s all linear.</li>
</ol>
</dd>
</dl>
<table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 35%">
<col style="width: 36%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>low manifold dimension</th>
<th>high manifold dimension</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>low manifold curvature</strong></td>
<td>- add two numbers</td>
<td>- telephone number from name</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>- business database</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td>- directions from address</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td><strong>high manifold curvature</strong></td>
<td>- encrypt data</td>
<td>- classify an image</td>
</tr>
<tr class="odd">
<td></td>
<td>- play best chess move</td>
<td>- understand language</td>
</tr>
<tr class="even">
<td></td>
<td>- prove theorem true/false</td>
<td>- predict weather (chaotic system)</td>
</tr>
<tr class="odd">
<td></td>
<td>- find optimum point</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>- color a pixel in mandelbrot set</td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>- predict position of a star</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
<dl>
<dt>The learning rate depends on both dimensionality and curvature.</dt>
<dd>
<p>A nonparametric estimator will be slow if either (A) there’s high intrinsic dimensionality; or (B) the manifold is highly curved.</p>
<p>However if you <em>know</em> the shape of the manifold already, then you’re only limited by the intrinsic dimensionality.</p>
</dd>
<dt>Computers are good at problems with low curvature.</dt>
<dd>
<p>They can learn sets of facts and follow logical rules very well. They’re extremely good at problems with <em>low curvature</em>.</p>
</dd>
<dt>Humans have progressively found lower-dimensional representations of many problems.</dt>
<dd>
<p>E.g. Newton, Copernicus, Mendeleev, etc., all found much simpler latent representations of many problems.</p>
<p>They have discovered the curvature of the manifold, so apparently high-dimensional problems become low-dimensional.</p>
</dd>
<dt>The manifold hypothesis: many problems have low-dimensional representations.</dt>
<dd>
<p>Problems which <em>appear</em> to have high dimension actually have low dimension.</p>
</dd>
</dl>
<p>Implication: it’s valuable to do unsupervised or self-supervised pre-training, to figure out the manifold, and then you can do supervised learning on top of that.</p>
<dl>
<dt>Economics literature on returns to experience.</dt>
<dd>
<ul>
<li><a href="https://cowles.yale.edu/sites/default/files/2023-06/Deming_OJL_June2023.pdf">Deming (2024)</a>: more complex occupations have steeper returns to experience.</li>
<li><a href="https://www.innovationgrowth.com/fileadmin/innovationgrowth/publications/96021_Learning_by_Problem_Solving_2015_Peer_Ederer.pdf">Nedelkoska (2025)</a> “[we] find that employees receive a positive wage premium to the complexity of their job and that workers in highly complex occupations acquire twice as much skills throughout life compared to less complex occupations.”</li>
</ul>
</dd>
</dl>
</section>
<section id="formal-setup-sketch" class="level1 page-columns page-full">
<h1>Formal Setup [SKETCH]</h1>
<p>[THESE ARE ALL ROUGH WORKING NOTES – DON’T TAKE TOO SERIOUSLY]</p>
<p>See <code>2023-10-26-models-of-ai-and-the-world.qmd</code></p>

<div class="no-row-height column-margin column-container"><div class="">
<p><span class="math display">\[
   \xymatrix@R=1em@C=2em{
      \txt{state} &amp; \txt{signal} &amp; \txt{representation} \\
               &amp; \boxed{x_{1}}\ar[dddr]\ar[dr] \\
      v_1\ar@{-}[ur]\ar@{-}[r]\ar@{-}[dddr]\ar@{-}[ddr]
               &amp; \boxed{x_{2}}\ar[ddr]\ar[r]
                           &amp; \hat{v}_1\\
      {\tiny\vdots} &amp; {\tiny\vdots} &amp; {\tiny\vdots} \\
      v_q\ar@{-}[uuur]\ar@{-}[uur]\ar@{-}[r]\ar@{-}[dr]
               &amp; \boxed{x_{p-1}}\ar[r]\ar[uur]
                           &amp; \hat{v}_q\\
               &amp; \boxed{x_{p}}\ar[ur]\ar[uuur]}
\]</span></p>
</div><div class="">
<table class="caption-top table">
<thead>
<tr class="header">
<th style="text-align: center;">signal<br> (<span class="math inline">\(q\)</span>-dimensional)</th>
<th style="text-align: center;">state<br> (<span class="math inline">\(p\)</span>-dimensional)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;">image</td>
<td style="text-align: center;">objects, angle, lighting</td>
</tr>
<tr class="even">
<td style="text-align: center;">text</td>
<td style="text-align: center;">meaning, style, dialect, phrasing</td>
</tr>
<tr class="odd">
<td style="text-align: center;">audio</td>
<td style="text-align: center;">speaker, words, tone</td>
</tr>
</tbody>
</table>
</div></div>
<p><span class="math display">\[\begin{aligned}
   Q &amp;=\text{dimension of state}\\
   P &amp;=\text{dimension of signal}\\
   N &amp;=\text{number of observations}\\
   \bm{x}_n &amp;\in \mathbb{R}^P &amp;&amp;  \text{observation (image, text, sound)}\\
   \bm{v}_n &amp;\in \mathbb{R}^Q &amp;&amp; \text{state (objects, meaning, words)}\\
   P &amp;\gg Q &amp;&amp; \text{(signals have higher dimensionality than state)}\\
   f&amp;:R^Q\rightarrow \mathbb{R}^P &amp;&amp; \text{1:1 mapping between signal and state}
\end{aligned}
\]</span></p>
<p>Observations:</p>
<dl>
<dt>The space of signals will be sparse.</dt>
<dd>
Because <span class="math inline">\(P&gt;Q\)</span> and the mapping is 1:1 most realizations of <span class="math inline">\(x\in X\)</span> would never be observed. E.g. most configurations of pixels are static, most configurations of words are gibberish: they don’t have any interpretation at all (no <span class="math inline">\(\bm{v}\)</span>) and so you would almost never encounter them.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>
</dd>
<dt>Signals will be redundant.</dt>
<dd>
The state <span class="math inline">\(\bm{v}\)</span> over-determines the signal <span class="math inline">\(\bm{x}\)</span>, so if you remove some of the information from the signal then you can reconstruct them with very high confidence.
</dd>
<dt>Technically the state-space has high dimensionality, but effectively it has low dimensionality.</dt>
<dd>
<p>In some cases we think signals are <em>under-determined</em> by the state rather than over-determined, i.e.&nbsp;that <span class="math inline">\(q&gt;p\)</span>, e.g.&nbsp;(1) images are 2D projections of a 3D world, so the signal seems lower-dimensional than the state; (2) a given sentence is often ambiguous between many possible meanings, and the disambiguatin is done by context, implying the set of sentences is lower-dimensional than the set of meanings.</p>
</dd>
<dd>
In principle the state has very high dimensionality, higher than the signal, but in practice we have such strong priors about the state that its effective dimensionality is lower than the signal space. A full representation of the state requires a dimensionality <span class="math inline">\(\bar{q}\gg p\)</span>, but in practice the state has such strong regularities that the majority of the variation requires a much lower-dimensional representation <span class="math inline">\(q\)</span>, so we have:
</dd>
<dd>
<span class="math display">\[\utt{\bar{q}}{dimensionality}{of world}\gg
   \utt{p}{dimensionality}{of signal} \gg
   \utt{q}{dimensionality}{of state}\]</span>
</dd>
<dt>Signals are highly compressible.</dt>
<dd>
This model implies that you can compress signals from a <span class="math inline">\(P\)</span>-dimensional object down to a <span class="math inline">\(Q\)</span>-dimensional object.
</dd>
<dt>Computers can learn the state but it takes a lot of data.</dt>
<dd>
We can model computers as slowly learning how to transform between <span class="math inline">\(\bm{v}\)</span> and <span class="math inline">\(\bm{x}\)</span> but it requires an enormous amount of training data. We talk about the computer’s problem as a PCA problem below.
</dd>
<dt>Extension: humans find it easier to decode than encode.</dt>
<dd>
Humans have an asymmetry about some things: they tend to be better at decoding than encoding, e.g.&nbsp;we can recognize whether a picture looks like Rishi Sunak but we can’t draw a picture that looks like him. We can model this as humans being composed of two agents, conscious and pre-conscious: the pre-conscious brain has private information which it uses to calculate <span class="math inline">\(\hat{v}=E[v|x]\)</span>, and the conscious brain only observe the posteriors from the pre-conscious brain (nice analogy: a person with a sniffer dog).
</dd>
</dl>
<section id="problem-no-closed-form-solution" class="level2">
<h2 class="anchored" data-anchor-id="problem-no-closed-form-solution">Problem: No Closed-Form Solution</h2>
<p>We want a model where you observe a matrix of <span class="math inline">\(p\)</span> features for <span class="math inline">\(n\)</span> cases, which are generated from some lower-dimensional representation. There are two problems:</p>
<ol type="1">
<li>Doing the inversion – you infer the low-dimensional state for each case. This is straight-forward (I think).</li>
<li>Learning the inversion – finding an optimal low-dimensional decomposition. It’s not clear to me whether we can get analytic solutions. Udell says <em>“low rank approximation problems are not convex, and in general cannot be solved globally and efficiently.”</em></li>
</ol>
<aside id="footnotes-2" class="footnotes footnotes-end-of-section" role="doc-footnote">
<hr>
<ol start="3">
<li id="fn3"><p>More realistically, you assume they are generated by some other process without the usual latent state involved.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</aside>
</section>
<section id="model-1-pca" class="level2">
<h2 class="anchored" data-anchor-id="model-1-pca">Model 1: PCA</h2>
<p><span class="math display">\[\begin{aligned}
      \utt{\begin{bmatrix}h_1^1  \ldots h_p^1 \\ \ddots \\ h_1^n \ldots h_p^n\end{bmatrix}}{observed dataset}{$n$ cases and $p$ features}
         = \utt{\begin{bmatrix}w_1^1  \ldots w_q^1 \\ \ddots \\ w_1^n  \ldots w_p^n\end{bmatrix}}{$p$ loadings}{on $q$ factors}
          \utt{\begin{bmatrix}x_1^1  \ldots x_q^1 \\ \ddots \\ x_1^n  \ldots x_q^n\end{bmatrix}}{$n$ latent vectors}{on $q$ factors}
\end{aligned}
\]</span></p>
<p>Bengio et al.&nbsp;write it as: <span class="math display">\[h=W^Tx+b\]</span></p>
<p>and give a probabilistic interpration (“PCA can be given a natural probabilistic interpretation (Roweis, 1997; Tipping and Bishop, 1999) as factor analysis:”) <span class="math display">\[\begin{aligned} p(h)&amp;=N(h;0,\sigma_h^2\bm{I})\\
   p(x|h) &amp;= N(x;Wh+\mu_x,\sigma_x^2\bm{I})
\end{aligned}
\]</span></p>
<p>Tipping and Bishop (1999) write: <span class="math display">\[\bm{t}=W\bm{x}+\bm{u}\]</span></p>
<p>and they show that if you assume Normal distribution of <span class="math inline">\(\bm{x}\)</span> and <span class="math inline">\(\bm{u}\)</span> then you can find a weight matrix <span class="math inline">\(W\)</span> which maximizes likelihood. They do <em>not</em> assume a prior over the weights themselves (p614).</p>
</section>
<section id="pca-with-a-single-factor-gymnastics" class="level2">
<h2 class="anchored" data-anchor-id="pca-with-a-single-factor-gymnastics">PCA with a single factor (gymnastics)</h2>
<p>If there’s a single factor plus noise then we can write everything like this:</p>
<p><span class="math display">\[\begin{aligned}
      \utt{\begin{bmatrix}h_1^1  \ldots h_P^1 \\ \ddots \\ h_1^N \ldots h_P^N\end{bmatrix}}{observed dataset}{$N$ cases and $P$ features}
         = \utt{\begin{bmatrix}x^1  \\ \vdots \\ x^N \end{bmatrix}}{latent value}{for each case}
            \utt{\begin{bmatrix}w_1  &amp; \ldots &amp; w_P \end{bmatrix}}{weight for}{each feature}
   + \begin{bmatrix}\varepsilon_1^1  \ldots \varepsilon_P^1 \\ \ddots \\
       \varepsilon_1^N \ldots \varepsilon_P^N\end{bmatrix}
\end{aligned}
\]</span></p>
<p>We can also write it out like this:</p>
<p><span class="math display">\[\utt{h_{n,p}}{feature $p$ of}{case $n$} = \utt{x_n}{avg score}{of this case} \times \utt{w_p}{avg score}{of this feature} + \ut{e_{n,p}}{noise}\]</span></p>
<p><strong>Answer when <span class="math inline">\(P=1\)</span>.</strong> Suppose we have a single feature and there’s no noise. So there’s many contestants and just one judge. Then we have this: <span class="math display">\[\utt{h_n}{observed score}{of each case} = \utt{x_n}{true}{value} + \utt{w}{common}{noise}\]</span></p>
<p>Assume we have mean-zero Gaussian priors over all RHS variables, then we can write: <span class="math display">\[\begin{aligned}
   E[x_n|h_n] &amp;= \frac{\sigma_x^2}{\sigma_x^2+\sigma_w^2}h_n \\
   E[w|h_n] &amp;= \frac{\sigma_w^2}{\sigma_x^2+\sigma_w^2}h_n \\
   E[w|\bm{h}] &amp;= \frac{\sigma_w^2}{\sigma_w^2+\sigma_x^2/N}\frac{1}{N}\sum_{m=1}^Nh_m \\
   E[x_n|\bm{h}] &amp;= h_n-E[w|\bm{h}] \\
      &amp;= h_n-\frac{\sigma_w^2}{\sigma_w^2+\sigma_x^2/N}\bar{h} \\
\end{aligned}
\]</span></p>
<p>Implications:</p>
<ul>
<li>when <span class="math inline">\(N=1\)</span> then <span class="math inline">\(\hat{x}_n=\frac{\sigma_x^2}{\sigma_w^2+\sigma_x^2}h_n\)</span></li>
<li>when <span class="math inline">\(N=2\)</span> then <span class="math inline">\(\hat{x}_1=\frac{\sigma_w^2/2+\sigma_x^2/2}{\sigma_w^2+\sigma_x^2/2}h_1-\frac{\sigma_x^2/2}{\sigma_w^2+\sigma_x^2/2}h_2\)</span></li>
<li>when <span class="math inline">\(N\rightarrow\infty\)</span> then <span class="math inline">\(E[x_n|\bm{h}]\simeq h_n-\bar{h}\)</span>.</li>
</ul>
<p><strong>Old answer (but I think it’s wrong).</strong> Suppose we have priors over the RHS variables that are mean-zero with known variances. Then <em>I think</em> our estimate will be as follows, but I need to confirm:</p>
<p><span class="math display">\[\begin{aligned}
   \hat{x}_n= E[x_n | \bm{h}]
      &amp;= \frac{n \sigma_a^2}{n \sigma_a^2 + \sigma_e^2} \left( \frac{1}{P}\sum_{p=1}^P h_{n,p} - \frac{1}{NP}\sum_{m=1}^N\sum_{p=1}^Ph_{m,p} \right)   \\
      &amp;= \frac{n \sigma_a^2}{n \sigma_a^2 + \sigma_e^2} \left( \bar{h}_{n.} - \bar{h}_{..} \right)   \\
\end{aligned}
\]</span></p>
<ul>
<li>Suppose there’s no noise: then we learn relative row values and relative column values exactly, but we’re missing overall calibration. Our posterior is just the relative row value.</li>
</ul>
<p><strong>Variance of the posteriors [UNFINISHED].</strong> We now derive the variance of the posteriors. We’ll start by assuming no noise:</p>
<p><span class="math display">\[\begin{aligned}
   \hat{x}_n &amp;= \frac{1}{P}\sum_{p=1}^P h_{n,p} - \frac{1}{NP}\sum_{m=1}^N\sum_{p=1}^Ph_{m,p}\\
      &amp;= \\
   V[\hat{x}_n-x_n]
      &amp;= \sigma_N+\frac{1}{P^2}\sigma_P^2  \\
\end{aligned}
\]</span></p>
<p>(For derivation see <code>2024-07-20</code> note.)</p>
<table class="caption-top table">
<colgroup>
<col style="width: 62%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;">ChatGPT prompt.</td>
</tr>
<tr class="even">
<td style="text-align: left;">&gt; I want to find an analytically tractable expression for a model with dimensionality reduction. Suppose we have H=x’w+e, so we have <span class="math display">\[h_{n,p}= x_n + w_p + e_{n,p}\]</span></td>
</tr>
</tbody>
</table>
<p>Suppose we observe the cells of this 2x2 matrix, which is formed by adding rows and columns: <span class="math display">\[\begin{bmatrix}
   x_1 + y_1 &amp; x_2 + y_1 \\
   x_2 + y_1 &amp; x_2 + y_2
\end{bmatrix}\]</span></p>
<p>We have mean-zero Gaussian priors over x and y, can you write an expression for the posterior</p>
</section>
<section id="model-2-questions-and-answers" class="level2">
<h2 class="anchored" data-anchor-id="model-2-questions-and-answers">Model 2: Questions and Answers</h2>
<p>This is the model I used in my <em>imitation</em> note. We observe a set of <span class="math inline">\(n\)</span> questions and answers. Each question is a vector of <span class="math inline">\(p\)</span> attributes, and the answer is the weighted sum of those attributes, but we need to infer the weights.</p>
<p><span class="math display">\[\begin{aligned}
      \ut{\begin{bmatrix}a^1 \\ \vdots \\ a^n\end{bmatrix}}{answers}
         &amp;= \utt{\begin{bmatrix}q_1^1 + \ldots q_p^1 \\ \vdots \\ q_1^n + \ldots q_p^n\end{bmatrix}}{multi-attribute}{questions}
         \ut{\begin{bmatrix}w^1 \\ \vdots \\ w^p\end{bmatrix}}{weights}\\
      \utt{\bm{a}}{$n\times1$}{observed}        &amp;=
      \utt{Q}{$n\times p$}{observed}\cdot \utt{\bm{w}}{$p\times1$}{unobserved}
   \end{aligned}
\]</span></p>
<p>Then we have a simple expression for the posterior: <span class="math display">\[\begin{aligned}
   \hat{\bm{w}}=E[\bm{w}|Q,\bm{a}] &amp;= Q'(QQ')^{-1}\bm{a}
\end{aligned}
\]</span></p>
</section>
<section id="misc" class="level2">
<h2 class="anchored" data-anchor-id="misc">Misc</h2>
<ul>
<li><p>Another model: multidimensional signal, multidimensional state, and a mapping matrix: <span class="math display">\[\utt{\bm{x}_n}{$P\times 1$}{signal} = \utt{A}{$P\times Q$}{mapping} \cdot \utt{\bm{v}_n}{$Q\times1$}{state}+
    \utt{e_n}{$P\times1$}{noise}
\]</span></p></li>
<li><p>Related: solving a least-squares model when over-determined or under-determined - <a href="https://twitter.com/gabrielpeyre/status/1779736411366924549/photo/1">tweet</a>, <a href="https://en.wikipedia.org/wiki/Least_squares">Wikipedia</a></p></li>
</ul>
</section>
</section>
<section id="applications" class="level1 page-columns page-full">
<h1>Applications</h1>

<div class="no-row-height column-margin column-container"><div class="">
<p><span class="math display">\[\xymatrix@R=1em@C=.5em{
   \txt{high-\\dimensional\\state} &amp; \txt{low-\\dimensional\\signal} &amp; \txt{high-\\dimensional\\representation} \\
   v_1\ar[dr] &amp; &amp; \hat{v}_1 \\
         &amp; \boxed{x_1}\ar[ur]\ar[dr] &amp; \\
   v_2\ar[ur] &amp; &amp; \hat{v}_2}
\]</span></p>
</div></div><p><strong>Summary:</strong></p>
<p>In many fields we model problems as <span class="math inline">\(n&gt;p\)</span> and <span class="math inline">\(q&gt;p\)</span>.</p>
<ul>
<li>Statistics: <span class="math inline">\(n&gt;p\)</span></li>
<li>Economics: <span class="math inline">\(n=\infty\)</span>, <span class="math inline">\(q&gt;p\)</span>.</li>
<li>Signal extraction: <span class="math inline">\(n=\infty\)</span>, <span class="math inline">\(q&gt;p\)</span>.</li>
<li>Perception: (…).</li>
</ul>
<section id="application-to-communication" class="level2">
<h2 class="anchored" data-anchor-id="application-to-communication">Application to Communication</h2>
<p><strong>Application to internal properties.</strong> An internal property of a signal is one that depends entirely on the content of the signal itself, not on anything outside the signal: e.g.&nbsp;whether a text is hate speech, whether a photo contains nudity, whether a song is catchy.</p>
<ol type="1">
<li><strong>Baseline: human judgment.</strong> ordinarily humans can immediately judge <span class="math inline">\(\hat{\bm{v}}(\bm{x})\)</span> and using <span class="math inline">\(\hat{\bm{v}}\)</span> tell whether it has a given internal property.</li>
<li><strong>Computer with small <em>m</em>:</strong> When <span class="math inline">\(m\)</span> is small the computer learns only a very crude approximation of <span class="math inline">\(A\)</span>. In practice we give the computer labelled data, <span class="math inline">\(\hat{v}_1\)</span>, and we give the computer a very small set of features <span class="math inline">\(x_1,x_2\)</span>, and the computer runs a simple regression, <span class="math inline">\(\hat{v}_1\sim x_1+x_2\)</span>.</li>
<li><strong>Evasion of computer:</strong> If the human knows the computer’s algorithm, <span class="math inline">\(\hat{v}_1(x_1,x_2)\)</span>, then it’s trival to get around it: even if the human doesn’t know <span class="math inline">\(x\rightarrow v\)</span> perfectly, still they can just fiddle with <span class="math inline">\(x_1\)</span> and <span class="math inline">\(x_2\)</span>, e.g.&nbsp;mispelling the trigger words, or changing the colour of an image.</li>
<li><strong>Computer with large <em>m</em>:</strong> Now the computer learns <span class="math inline">\(A\)</span> perfectly, they have human-level performance, and it’s impossible to evade it.</li>
</ol>
<p><strong>Application to external properties.</strong> An external property depends on something outside the signal: e.g.&nbsp;whether a photo depicts something that actually happened, whether a poem was written by Shakespeare. The content of the signal can be informative but it’s not definitive.</p>
<ol type="1">
<li><strong>Baseline: signals all created by the world.</strong> Consider recorded media (photo, audio), and suppose that they are only created by events that actually happened. Thus encoding, <span class="math inline">\(\bm{v}\rightarrow\bm{x}\)</span>, is done by the laws of physics, and decoding is done by the human brain.</li>
<li><strong>Manipulation by humans.</strong> Suppose now a human wants to modify or synthesize a signal. It’s hard! Humans can automatically convert <span class="math inline">\(x\rightarrow\hat{v}\)</span>, but that is done by a pre-conscious part of the brain, so they don’t know how to tweak <span class="math inline">\(\bm{x}\)</span> to change <span class="math inline">\(\hat{\bm{v}}\)</span>. They could make random changes and see what happens to <span class="math inline">\(\hat{v}\)</span> but this is extremely slow. In addition, because the <span class="math inline">\(\bm{x}\)</span>-space is sparse, the receiver would recover a <span class="math inline">\(\hat{v}\)</span> which has extremely low probability, and they would infer that the signal has been tampered with (in practice: the photo would have weird inconsistencies, or the audio would be clipped).</li>
<li><strong>Computer with small <span class="math inline">\(m\)</span>.</strong> A computer with a small <span class="math inline">\(m\)</span> might be able to do crappy classification (<span class="math inline">\(x\rightarrow v\)</span>), but it wouldn’t be very robust: e.g.&nbsp;it would learn that images with yellow backgrounds are typically of camels. You could reverse the algorithm to produce an <span class="math inline">\(x\)</span> which maximizes <span class="math inline">\(\hat{v}\)</span> but it would be very ugly &amp; obviously fake.</li>
<li><strong>Computer with large <span class="math inline">\(m\)</span>.</strong> Now suppose the computer can perfect convert <span class="math inline">\(x\leftrightarrows v\)</span>: they can synthesize an arbitrary image, &amp; it’s impossible to discriminate from a real image. Suppose receivers are naive, they think that if they observe <span class="math inline">\(x\)</span>, then <span class="math inline">\(\hat{v}(x)\)</span> really happened. Then strategic senders can arbitrarily manipulate their beliefs, e.g.&nbsp;creating photos of politicians doing scandalous things.</li>
<li><strong>Equilibrium.</strong>
<ol type="1">
<li>If all senders are strategic then I think you get a babbling equilibrium: you now learn nothing from <span class="math inline">\(\bm{x}\)</span>.</li>
<li>If some senders are strategic then there will be some non-zero persuasive power of media. In the medium run you’d expect more entry by strategic senders until the returns to creating media go to zero: you could write equilibrium with the share of fakes pinned down by the intersection of two curves: (1) creation of fake media as a function of credence; (2) credence in media as a function of prevalence of fakes.</li>
<li>Platforms might pay some cost <span class="math inline">\(c\)</span> to check the veracity of some media, when the probability of being fake exceeds a threshold. This would put a ceiling on the influence of fake media.</li>
</ol></li>
</ol>
</section>
<section id="application-to-intellectual-property" class="level2">
<h2 class="anchored" data-anchor-id="application-to-intellectual-property">Application to Intellectual Property</h2>
<p>(see <code>AI and intellectual property</code>, I think it can be put in this framework)</p>
</section>
<section id="applications-to-economics" class="level2">
<h2 class="anchored" data-anchor-id="applications-to-economics">Applications to Economics</h2>
<p>Most models of inference in economics have the agent receives a signal that is not fully revealing of the state because it has lower cardinality than the state (e.g.&nbsp;a binary signal), or because there is noise (then you can think of the noise as part of the state, and so the state is higher-dimensional than the signal). E.g.:</p>
<ul>
<li>Estimating the productivity of an employee from their education.</li>
<li>Estimating demand conditions from sales.</li>
<li>Estimating the competence of a politician from economic conditions.</li>
<li>Estimating the quality of a product from peer usage of that product.</li>
</ul>
<p>Yet in practical situations information sets often <em>do not</em> seem to be discrete or univariate, instead they’re enormously rich: we have the newspaper, we have millions of datapoints about employees, we have the internet. Let’s try to reinterpret these situations:</p>
<ul>
<li>Estimating the productivity of an employee from meeting them and watching them work.</li>
<li>Estimating demand conditions from reading the newspaper and trying to infer the state of the world.</li>
<li>Estimating the competence of a politician from their speeches, their mannerisms, how journalists and other politicians talk about them.</li>
<li>Estimating the quality of a product from the label.</li>
</ul>
</section>
<section id="application-to-recommender-algorithms" class="level2">
<h2 class="anchored" data-anchor-id="application-to-recommender-algorithms">Application to Recommender Algorithms</h2>
<ol type="1">
<li><p><strong>We want to predict a person’s response to an item.</strong> The item could be a post, a song, a video, an advertisement, a product. The person’s response could be clicking, purchasing, upvoting, or labelling as toxic.</p></li>
<li><p><strong>Model: low-dimensional semantics.</strong> I will state a simple model of human judgment and then try to describe some of the facts we observe about recommendation. Assume that the content of each item (pixels, characters, etc.) can be represented in a low-dimensional space, the “semantics.” Each person’s response is fully determined by the semantics, though different people have different weights.</p></li>
<li><p><strong>World without computers.</strong> There are many practical decisions people make about others’ judgments: will people like this concerto? how many copies is this book likely to sell? is this painting obscene? Then we can use a mixture of our own judgments and simple statistics about others judgments – e.g.&nbsp;what are the best-selling books, what are the most-beloved paintings.</p></li>
<li><p><strong>Very small computers.</strong> Suppose our computers can digitize data about preference but not the full content of the items themselves. Then we can do collaborative filtering to predict preference, irrespective of the content.</p></li>
<li><p><strong>Small computers.</strong> Suppose we now have access to the full digital representation of each piece of content. Somewhat surprisingly this information is not very useful: the relationship between surface-level features and preference is pretty noisy. Shallow features are somewhat informative: e.g.&nbsp;the tempo of a song, the proportions of a painting, can be predictive of preference, but these features don’t have much predictive value relative to collaborative filtering.</p></li>
<li><p><strong>Large computers.</strong> Now suppose we have computers that are <em>large</em> such that they can extract the semantics of each item with high accuracy.</p>
<ul>
<li><em>Implication:</em> the content discovery problem (AKA cold start) is solved.</li>
<li><em>Implication:</em> we will start synthesizing content. (However a classifier that has high accuracy on its training set could perform badly off-distribution, and thus do badly in synthesizing content).</li>
</ul></li>
</ol>
<p>Suppose our raw data looks like this, where we have observations <span class="math inline">\(j\in 1\ldots J\)</span>: <span class="math display">\[\begin{aligned}
      \bm{x}^j    &amp;  &amp;&amp; \text{high dimensional input}\\
      i^j  &amp;\in N
         &amp;&amp; \text{identity of rater}\\
      R^j  &amp;\in R
         &amp;&amp; \text{rating}
   \end{aligned}
   \]</span></p>
<p><strong>Search engines are similar:</strong> History of search engines:</p>
<ol type="1">
<li>Text match between query and document.</li>
<li>Pagerank for the quality of a document.</li>
<li>Predict click-through rate and satisfaction rate.</li>
<li>Semantic match.</li>
</ol>
</section>
<section id="application-to-statistics" class="level2">
<h2 class="anchored" data-anchor-id="application-to-statistics">Application to Statistics</h2>
<p><strong>Applied to statistics.</strong> The canonical statistical inference problem is where the number of observations is larger than the number of features (<span class="math inline">\(n&gt;p\)</span>), e.g.&nbsp;linear regression is only well-defined when <span class="math inline">\(n&gt;p\)</span>.</p>
</section>
<section id="application-to-chatbots" class="level2">
<h2 class="anchored" data-anchor-id="application-to-chatbots">Application to Chatbots</h2>
<p>If you train on sufficiently many held-out words then you eventually learn the mapping from <span class="math inline">\(x\)</span> to <span class="math inline">\(v\)</span>.</p>
<p>Now you can post-train on question/answer pairs (SFT) or on human preferences (RLHF), and you’ll get very accurate very quickly.</p>
</section>
</section>
<section id="related-literature" class="level1 page-columns page-full">
<h1>Related Literature</h1>
<dl>
<dt><strong>Tipping and Bishop (1999) <a href="https://www.cs.columbia.edu/~blei/seminar/2020-representation/readings/TippingBishop1999.pdf">“Probabilistic Principal Component Analysis”</a></strong></dt>
<dd>
They show that “the maximum likelihood estimators for the isotropic error model … do correspond to principal component analysis.”
</dd>
<dd>
<p>Factor analysis differs from PCA in having a separate noise term: <span class="math display">\[\bm{t}=\bm{W}\bm{x}+\bm{\mu}+\bm{\epsilon}\]</span></p>
<blockquote class="blockquote">
<p>“The model parameters may be deremined by maximum likelihood, although … there is no closed form analytic solution for <span class="math inline">\(\bm{W}\)</span> and <span class="math inline">\(\bm{\Psi}\)</span></p>
</blockquote>
<blockquote class="blockquote">
<p>“This is where factor analysis fundamntally difers from standard PCA, which effectively treats covariance and variance identically.”</p>
</blockquote>
<blockquote class="blockquote">
<p>“in factor analysis neither of the factors found by a two-factor model is necessarily the same as that found by a single-factor model. In PPCA, we see above that the principal axes may be found incrementally.</p>
</blockquote>
<p>Section 2.2: if you assume that residual variances are equal (<span class="math inline">\(\psi_i=\sigma\)</span>) then “both <span class="math inline">\(W\)</span> and <span class="math inline">\(\sigma^2\)</span> may then be determined analytically though eigendecomposition of S, without resort to iteration”</p>
</dd>
<dt><span class="citation" data-cites="narayanan2010manifoldhypothesis">Narayanan and Mitter (<a href="#ref-narayanan2010manifoldhypothesis" role="doc-biblioref">2010</a>)</span> “Sample Complexity of Testing the Manifold Hypothesis”</dt>
<dd>
I believe this paper first coined the term “manifold hypothesis.”
</dd>
</dl>
<p>&nbsp;</p>
<dl>
<dt><span class="citation" data-cites="bengio2013representation">Bengio, Courville, and Vincent (<a href="#ref-bengio2013representation" role="doc-biblioref">2013</a>)</span> <a href="https://arxiv.org/abs/1206.5538">“Representation Learning: A Review and New Perspectives”</a></dt>
<dd>
<p>This is a highly cited paper, &amp; classic reference for the manifold hypothesis. They state it as follows:</p>
<blockquote class="blockquote">
<p>“real-world data presented in high dimensional spaces are expected to concentrate in the vicinity of a manifold M of much lower dimensionality.”</p>
</blockquote>
</dd>
<dd>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/2024-09-07-06-41-40.png" class="img-fluid figure-img"></p>
<figcaption>a manifold in 3D space, from <span class="citation" data-cites="bengio2013representation">Bengio, Courville, and Vincent (<a href="#ref-bengio2013representation" role="doc-biblioref">2013</a>)</span></figcaption>
</figure>
</div>
</dd>
<dd>
They list a bunch of priors about the data that have turned out to be useful: smoothness, multiple explanatory factors, hierarchical organization of factors, shared factors, manifolds, clustering, sparsity.
</dd>
<dd>
<strong>explaining away:</strong> “a priori independent causes of an event can become non-independent given the observation of the event.” Example: you get notified that your burglar alarm went off, then you hear that there was an earthquake. Your posteriors over being burgled and earthquake now become tightly correlated.
</dd>
<dt>Kevin Murphy (2024) <a href="https://probml.github.io/pml-book/book1.html">Probabilistic Machine Learning</a></dt>
<dd>
<p>Good recent textbook with treatment of manifold hypothesis &amp; various dimensionality-reduction &amp; manifold-learning algorithms.</p>
</dd>
<dt><span class="citation" data-cites="buchanan2025learning">Buchanan et al. (<a href="#ref-buchanan2025learning" role="doc-biblioref">2025</a>)</span></dt>
<dd>
<p>Good recent textbook which tries to unify many deep-learning methods around learning low-dimensional representations.</p>
</dd>
<dt><span class="citation" data-cites="udell2016generalized">Udell et al. (<a href="#ref-udell2016generalized" role="doc-biblioref">2016</a>)</span> “Generalized Low-Rank Models”</dt>
<dd>
An extension of PCA to (1) any type of tabular data, not just scalars; (2) penalization of the latent factors.
</dd>
<dd>
<ul>
<li>Can be used to impute missing values.</li>
</ul>
</dd>
<dd>
<ul>
<li><blockquote class="blockquote">
<p>“low rank approximation problems are not convex, and in general cannot be solved globally and efficiently.”</p>
</blockquote></li>
</ul>
</dd>
<dd>
<ul>
<li>PCA: equivalently (1) find another matrix that is similar but has lower rank; (2) find a common basis of rows and columns, so that entries are products of embeddings.</li>
</ul>
</dd>
<dd>
<ul>
<li>SVD: an exact decomposition into factors, then PCA is just truncation of the SVD factors. This is a way of analytically achieving a PCA solution, but for more complex cases need computable algorithms.</li>
</ul>
</dd>
<dd>
<ul>
<li>Can interpret PCA as denoising: “quadratically regularized PCA corresponds to a model in which features are observed with N(0,1) errors.”</li>
</ul>
</dd>
<dt>Markov blanket</dt>
<dd>
Given a set of random variables, a <a href="https://en.wikipedia.org/wiki/Markov_blanket">Markov blanket</a> with respect to a single variable <span class="math inline">\(Y\)</span> is a subset that is collectively sufficient to infer <span class="math inline">\(Y\)</span>.
</dd>
<dt>Inversion of embeddings</dt>
<dd>
<p><span class="citation" data-cites="morris2023textembeddingsrevealalmost">Morris et al. (<a href="#ref-morris2023textembeddingsrevealalmost" role="doc-biblioref">2023</a>)</span> show that you can recover a string from its embedding. They can perfectly recover 92% of 32-token strings from Wikipedia after they have been embedded into 1536 dimensions (using OpenAI’s production embedding).</p>
<p>Another paper by the same authors show that different embeddings all match each other, so you can map them to each other without any labels.</p>
</dd>
<dt>Information-Theoretic Framework</dt>
<dd>
<p>Jeon, Zhu &amp; van Roy (2022) <a href="https://arxiv.org/pdf/2203.00246">“An Information-Theoretic Framework for Supervised Learning”</a> – seems relevant.</p>
<blockquote class="blockquote">
<p>“This prior distribution gives rise to high-dimensional latent representations that, with high probability, admit reasonably accurate low-dimensional approximations.”</p>
</blockquote>
</dd>
<dd>
<p>Wilson (2025) <a href="https://arxiv.org/pdf/2503.02113">“Deep Learning is Not So Mysterious or Different”</a> – he argues that the ability for deep networks to generalize is consistent with prior literature, specifically that models have <em>soft inductive biases</em> meaning a bias towards simplicity.</p>
</dd>
</dl>
<section id="related-economics-literature" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="related-economics-literature">Related Economics Literature</h2>
<dl>
<dt>Rational inattention.</dt>
<dd>
(sims/caplin/woodford) is just about compressing a high-dimensional signal to fit it through a pipeline, not about ignorance of the distribution, or making inferences from it.
</dd>
<dt>Learning in games.</dt>
<dd>
Various people have written about Bayesian agents slowly learning the state of the world, e.g.&nbsp;Fudenberg and Levine on learning in games, though I think those models are typically low-dimensional.
</dd>
<dt>Misspecified models.</dt>
<dd>
(Alwyn Young, Matt Rabin) – but this is assuming it’s low-dimensional model. Maybe people have well-specified but imperfectly calibrated models.
</dd>
<dt>Agreeing to disagree.</dt>
<dd>
Aumann. Presumably the speed of convergence is proportional to complexity of the world models. If so then high-dimensional models could rationalize the substantial equilibrium variance in beliefs. Might be able to formalize this in Gaussian model.
</dd>
</dl>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/2023-11-02-10-44-49.png" class="img-fluid figure-img"></p>
<figcaption>from Callender</figcaption>
</figure>
</div>
</div></div><dl>
<dt>Rugged landscape.</dt>
<dd>
Steven Callender has some papers, e.g.&nbsp;“Innovation and Competition on a Rugged Technological Landscape”, e.g.&nbsp;see the graph in the margin.
</dd>
</dl>
</section>
</section>
<section id="misc-1" class="level1">
<h1>Misc</h1>
<section id="short-version" class="level2">
<h2 class="anchored" data-anchor-id="short-version">Short Version</h2>
<p>Here’s a big-picture take on econ modeling.</p>
<p>In short: we usually model agents making inferences given a low-dimensional signal (n&gt;p), but in fact maybe the more common decision problem is high dimensional (n &lt; p), i.e.&nbsp;people have plenty of signal but they don’t know how to interpret it.</p>
<p>Motivated by this observation: computers have been able to beat humans at statistical inference since the 1960s for low-dimensional problems, e.g.&nbsp;linear regression (n&gt;p), but it took another 50 years for computers to learn how to do high-dimensional problems (n &lt; p), e.g.&nbsp;to interpret text, images, and sound. An implication is that humans must have some powerful modules for interpreting high-dimensional data (AKA finding latent structures), &amp; perhaps modeling that process is important for understanding communication.</p>
<p>An additional observation: when we model decision-making under uncertainty we usually assume the problem is low-dimensional (n&gt;p). We assume people already know the joint distribution of everything (rational expectations), and their signal is coarse relative to the state of the world, i.e.&nbsp;they are constrained on p.&nbsp;</p>
<p>But in reality signals don’t seem so coarse: people read the newspaper, they observe rich details about each others’ behaviour, they have the ability to examine products carefully. Maybe instead the problem is that they have rich signals but they don’t know how to interpret them, i.e.&nbsp;maybe the more common economic inference problem is high-dimensional (p&lt;n).</p>
<p>Some implications:</p>
<ul>
<li><p>In this world you can’t reduce economic decision-making to a few variables: consumers make judgments using a <em>gestalt</em> from rich data, so predicting behavior just using price and quality will have limited explanatory power (similarly, businesses don’t just use interest rate &amp; unemployment to form expectations).</p></li>
<li><p>The value of more experience (doubling <span class="math inline">\(n\)</span>) is higher than the value of a richer signal (doubling <span class="math inline">\(p\)</span>).</p></li>
<li><p>Unsupervised learning is valuable, it helps you learn the latent structure of the world, thus agents with experience in one problem will be better at unrelated problems, e.g.&nbsp;judging quality or forecasting economic conditions, because they have a stronger grasp of the latent structure of the world.</p></li>
</ul>
<p>I know of a couple of things that are superficially similar but I think don’t quite capture this: rational inattention (sims/caplin/woodford) is just about compressing a high-dimensional signal, not about ignorance of the distribution; various people have written about Bayesian agents slowly learning the state of the world (e.g.&nbsp;Fudenberg and Levine on learning in games) but I think those models are low-dimensional.</p>
</section>
<section id="comments" class="level2">
<h2 class="anchored" data-anchor-id="comments">Comments</h2>
<p><strong>Rob Donelly comments:</strong></p>
<blockquote class="blockquote">
<p>“I agree with your claim that most of economic modeling has generally focused on restricting the agents uncertainty to a low dimensional set of signals, but I wonder how much of that is limitation of what is tractable in a theory model (and similar constraints for an empirical model).</p>
</blockquote>
<blockquote class="blockquote">
<p>“For example Frank Knight had his paper about unknown/unquantifiable risks back in 1921, but I don’t think there has been much progress on good approaches for modeling this. It’s reasonably challenging to have a model of uncertainty even with a single dimensional uncertainty (e.g.&nbsp;2001 Nobel prize for Akerlof, Spence, Stiglitz).</p>
</blockquote>
<blockquote class="blockquote">
<p>“In empirical models it’s especially difficult to try to learn what someone’s beliefs/expectations are, since we almost never have ground truth data on beliefs, so instead we have to try to infer those beliefs based on decisions. Usually that is only possible under very strong modeling assumptions about what people know and what they are uncertain about. At least in the consumer search models that I am most familiar with, that usually ends up with a model where consumers have accurate expectations/beliefs about nearly everything except for a very small number of things they are learning/updating about.</p>
</blockquote>
<blockquote class="blockquote">
<p>“It seems like the other extreme is a purely behavioral model e.g.&nbsp;that takes the entire lifetime history of everything a person has seen and done, and predicts what’ll they’ll do/choose next.</p>
</blockquote>
<blockquote class="blockquote">
<p>“A slightly different context, Athey has the paper that tries to predict for timber auctions what will happen when they move from a open bid to sealed bid auction. There she showed you can predict how the buyers will respond to the new auction format by assuming their prior behavior was generated under rational expectations + a single dimensional type</p>
</blockquote>
<blockquote class="blockquote">
<p>“It’s unclear to me under what circumstances you would trust a purely behaviorally grounded model to be able to extrapolate to new circumstances that are very different than the data you trained on</p>
</blockquote>
<p><strong>Sean Taylor:</strong></p>
<blockquote class="blockquote">
<p>“One concept I like for this is a”world model” which the intelligent agent uses to make predictions and choose actions. We are choosing models and parameters which are analytically convenient or which make it possible to identify the world model from data, but it’s a much more general concept … I believe it is reinforcement learning terminology.</p>
</blockquote>
</section>
<section id="other-notes" class="level2">
<h2 class="anchored" data-anchor-id="other-notes">Other Notes</h2>
<p><strong>Chemistry predicting properties of substances from atomic structure.</strong> It’s better to predict behaviour from behaviour of other macrophenomon, than to predict from microstructure.</p>
<p><strong>Application: forecasting.</strong> Given the same information (the same p) people have different experience (different n), and so substantially different ability to forecast. In addition, in many cases people are more constrained on having more history (base rates) than on knowing more about this particular situation.</p>
<p>Also note that good forecasters can way outperform low-dimensional statistical models, because the good forecasters have latent model of the world.</p>
<p><strong>The mind and the computer are just reflections of the world.</strong> We can study the world, the mind, and the computer. But we will find the same patterns reflected in all three.</p>
<p><strong>High-dimensional problems are the ones that used to be uniquely the domain of humans.</strong></p>
<p><em>Application to perception:</em></p>
<ul>
<li>Woodford will talk about how there’s too much information, but then sets it up as a problem of <em>compressing</em> the information to fit through a pipeline, rather than a problem of interpreting the information.</li>
</ul>
<ol type="1">
<li><p><strong>Humans have an uncanny ability to interpret high-dimensional data (n&lt;&lt;p).</strong> Computers have been able to beat humans at low-dimensional prediction problems for 50 years. But only in the last 10 years have computers figured out how to interpret images, text, sound.</p></li>
<li><p><strong>Typical econ approach: you know distribution, form expectation E[v|x].</strong> But</p></li>
</ol>
<p><em>A model of high-dimensional inference.</em></p>
<ul>
<li>You get signal x=Av, where A is unknown.</li>
<li>You have x_1,…,x_{p-1} and want to predict x_p.</li>
<li>You have n prior experiences of x.</li>
</ul>
<p><em>Applied to consumer choice:</em> purchase decision is <em>gestalt</em>, not just price and unidimensional quality.</p>
<p><strong>Data generating process for high dimensional</strong></p>
<p>We want a generate model such that:</p>
<ol type="1">
<li>Low dimensional state generates high-dimensional signal.</li>
<li>The state <em>cannot</em> be recovered with a simple algorithm (linear, nearest neighbor). You need something hierarchical like a neural net.</li>
</ol>
<p>Examples:</p>
<ul>
<li><em>2D view of 3D scene.</em> this has both (i) big interactions, e.g.&nbsp;green object looks cyan when the light is blue; (ii) complex environmental statistics – you need to gradually learn priors.</li>
</ul>
<p><strong>Perception wrongly treated as compression.</strong> It’s clear in many cases of perception that the signal is high-dimensional, yet there are some weird anomalies. A common model is that the signal is <em>compressed</em> and then reconstructed. E.g. Sim, Woodford, compressed sensing, etc. The <em>compression</em> problem has some subtle differences from the <em>inference</em> problem.</p>
<dl>
<dt>Could call the states “noumena” and the signals “phenomena”.</dt>
<dd>
asfd
</dd>
</dl>
</section>
</section>
<section id="causal-view-vs-manifold-view" class="level1">
<h1>2025-02-17 | causal view vs manifold view</h1>
<dl>
<dt>Causal inference people are skeptical about LLMs.</dt>
<dd>
Susan Athey seems to think that LLMs are fundamentally limited because they’re just next-word predictors, there’s no allowance for causal inference.[3]
</dd>
<dd>
<a href="https://www.youtube.com/live/W0QLq4qEmKg?t=3810s">Michael Jordan</a> says <em>“I don’t think there’s ever been an era in human history where a new field of technology has arisen where there was so much hype and hysteria around it”</em>
</dd>
<dt>LLM people <em>sound</em> like they’re making classic mistakes.</dt>
<dd>
The things that LLM people say are things that make causal-inference people roll their eyes. They say <em>“we just need more data”</em>, and <em>“we just need a better functional form”</em> which are exactly the things that people say when they are doing supervised learning which will yield confounded causal estimates. It’s also true that most LLM people have only a vague understanding of causal inference techniques.
</dd>
<dt>Despite this, LLMs are working anyway.</dt>
<dd>
Despite all this LLMs are clearly very useful, they can do a huge variety of tasks autonomously, and there’s a huge demand for people to use them to help solve problems. Their abilities are expanding rapidly.
</dd>
<dt>I think there’s a deep disagreement about knowledge.</dt>
<dd>
<p>Here are two simplified ways of characterizing knowledge about the world:</p>
<ol type="1">
<li><p><em>We are learning parameters.</em> E.g. we are slowly learning the causal effects of different medicines, learning the elasticities of demand, learning the returns to education. This is the causal inference worldview. There set of parameters are small. Progress comes from doing experiments and gradually mapping out the world better.</p></li>
<li><p><em>We are learning structures.</em> We are learning how to <em>factorize</em> a high dimensional world into low dimensions. E.g. (1) we learn to exlpain the motions of the stars with a heliocentric model, (2) we explain physical interactions with mass and momentum; (3) we categorize medical conditions with temperature, blood pressure, pulse; (4) we categorize economic conditions with GDP, inflation, employment.</p></li>
</ol>
</dd>
<dt>The causal inference worldview is narrow.</dt>
<dd>
<p>The textbook causal inference worldview is focussed on learning causal parameters. The classical case is when you have a low-dimensional dataset (<span class="math inline">\(n&gt;p\)</span>) and you are identifying some causal parameter of interest using an experiment or natural experiment. Then we can think of decision-makers as gradually updating their beliefs about causal effects using causal inference, e.g.&nbsp;updating price elasticities, or updating estimates of the returns to education. However this worldview has some problems:</p>
<ol type="1">
<li><p><em>In most circumstances there is no identifying variation.</em> Despite this people clearly have good causal knowledge, enough to cure diseases and build nation-states and aeroplanes and skyscrapers. Another way of making this point: our economic models usually assume that decision-makers know the true causal effects, yet econometricians say that they can’t figure the causal effects out themselves.</p></li>
<li><p><em>Almost all real-world datasets are high dimensional</em> (<span class="math inline">\(p&gt;n\)</span>), so there’s necessarily some dimensionality-reduction before we can estimate a causal effect.</p></li>
<li><p><em>There’s little reason to expect causal coefficients to be stable.</em> Economists talk about coefficients as “the” return to this or that, but often glide over how realistic it is that these be stable.</p></li>
<li><p>Most history of progress in science is more about discovering new regularities, than about better estimates of causal coefficients.</p></li>
</ol>
</dd>
<dt>There’s an alternative vision.</dt>
<dd>
(…)
</dd>
</dl>
<section id="background" class="level2">
<h2 class="anchored" data-anchor-id="background">Background</h2>
<p>I think their skepticism is because LLMs look like a mistake they’ve seen before.</p>
</section>
<section id="the-causal-worldview-and-its-problems" class="level2">
<h2 class="anchored" data-anchor-id="the-causal-worldview-and-its-problems">The Causal Worldview and its Problems</h2>
<dl>
<dt>This is a caricture of the worldview implicit in the “causal inference” approach.</dt>
<dd>

</dd>
<dt>We have a low-dimensional dataset (n&gt;p)</dt>
<dd>
E.g. a set of country-year pairs and macroeconomic variables, or state-year pairs and social variables, or person-year pairs and health variables.
</dd>
<dt>Bad science is correlations, good science is causation.</dt>
<dd>
Bad science is finding correlations between variables and interpreting them causally. E.g. bad public health,
</dd>
<dt>The goal is to find the few true causal effects.</dt>
<dd>
E.g. we want to estimate the fiscal multiplier, or the coefficient of relative risk aversion, or the returns to education.
</dd>
</dl>
</section>
<section id="problems-with-the-causal-worldview" class="level2">
<h2 class="anchored" data-anchor-id="problems-with-the-causal-worldview">Problems with the Causal Worldview</h2>
<p>2023: <a href="https://www.promarket.org/2023/07/08/a-conversation-with-susan-athey/">“A Conversation with Susan Athey”</a></p>
<ol type="1">
<li><em>View of the world:</em> there are a small number of causal effects we need to estimate (elasticities).</li>
<li><em>The hard part is the dimension-reduction.</em></li>
<li><em>Dimension reduction in physics</em> - temperature, center of mass, momentum, mass, pressure, charge, spin, angular momentum, rate of flow.</li>
</ol>
</section>
<section id="some-concrete-applications" class="level2">
<h2 class="anchored" data-anchor-id="some-concrete-applications">Some concrete applications</h2>
<ol type="1">
<li>We want to grow our crops better.</li>
<li>We want to .</li>
</ol>
</section>
<section id="summary-for-zoe-chris" class="level2">
<h2 class="anchored" data-anchor-id="summary-for-zoe-chris">Summary for Zoe &amp; Chris</h2>
<p>Chris and I were talking about Susan Athey’s skepticism of AI (Mike Jordan has a very similar attitude). Here’s my very big picture take.</p>
<p>First: I think Susan Athey’s bullshit detector goes off when she hears about LLMs, because she’s so used to seeing people confuse supervised learning and causal inference. So she feels comfortable dismissing a lot of the claims about LLMs because she thinks she knows why they’re confused.</p>
<p>In fact there are two polar views of scientific progress:</p>
<ol type="1">
<li><em>Learning parameters.</em> We’re slowly mapping out the world with experiments. We’re bottlenecked on having more causal data.</li>
<li><em>Learning structures.</em> We’re slowly <em>understanding</em> the world by finding latent structure, i.e.&nbsp;reducing high-dimensional world to low-dimensional. We’re not primarily bottlenecked on having more causal data. However it can be useful to have unsupervised data to learn the mapping from high-dimensional to low-dimensional (technically LLM pretraining is supervised but in a narrow sense).</li>
</ol>
<p>Economists have become very fixated on the “learning parameters” worldview, especially post-credibility-revolution.</p>
<p>However in fact almost all scientific and technological progress is <em>not</em> from learning parameters, but instead from learning structures. Most progress comes from factorizing the world into a low-dimensional structure. The reason NNs &amp; LLMs are revolutionary is that we’ve finally taught computers to do that low-dimensional factorization themselves, &amp; now they can reason about the world.</p>
</section>
</section>
<section id="everything-is-downstream-of-the-statistical-structure-of-the-world." class="level1">
<h1>2025-03-06 | Everything is downstream of the statistical structure of the world.</h1>
<dl>
<dt>Everything is downstream of the statistical structure of the world.</dt>
<dd>
Economists say that everything is determined by the production function. But the production function for knowledge work is determined by the statistical structure of the world, so that’s what everything’s truly downstream of.
</dd>
<dd>
<ul>
<li>If the world is highly repetitive, the returns to experience are small, and the returns to firm scale are small.</li>
</ul>
</dd>
<dd>
<ul>
<li>If the world is highly idiosyncratic, then returns to experience are small.</li>
</ul>
</dd>
<dd>
<ul>
<li>But if the world has deep latent structure, then the returns to experience are high.</li>
</ul>
</dd>
<dd>
Applications:
</dd>
<dd>
<ul>
<li><em>High returns to experience in medicine.</em></li>
</ul>
</dd>
<dd>
<ul>
<li><em>Google gets high margins because returns to information remain high at that scale</em></li>
</ul>
</dd>
<dd>
<ul>
<li><em>Margins in AI will depend on structure of the world.</em></li>
</ul>
</dd>
</dl>
</section>
<section id="section" class="level1">
<h1>2025-07-17 |</h1>
<p>Huh et al.&nbsp;(2025) <a href="https://arxiv.org/abs/2405.07987">The Platonic Representation Hypothessis</a></p>
<p>Jack Morris (2025) <a href="https://blog.jxmo.io/p/there-is-only-one-model">All Models Might be the Same</a></p>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-bengio2013representation" class="csl-entry" role="listitem">
Bengio, Yoshua, Aaron Courville, and Pascal Vincent. 2013. <span>“Representation Learning: A Review and New Perspectives.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 35 (8): 1798–828.
</div>
<div id="ref-buchanan2025learning" class="csl-entry" role="listitem">
Buchanan, Sam, Druv Pai, Peng Wang, and Yi Ma. 2025. <em>Learning Deep Representations of Data Distributions</em>. <a href="https://ma‐lab‐berkeley.github.io/deep-representation-learning-book/">https://ma‐lab‐berkeley.github.io/deep-representation-learning-book/</a>.
</div>
<div id="ref-morris2023textembeddingsrevealalmost" class="csl-entry" role="listitem">
Morris, John X., Volodymyr Kuleshov, Vitaly Shmatikov, and Alexander M. Rush. 2023. <span>“Text Embeddings Reveal (Almost) as Much as Text.”</span> <a href="https://arxiv.org/abs/2310.06816">https://arxiv.org/abs/2310.06816</a>.
</div>
<div id="ref-narayanan2010manifoldhypothesis" class="csl-entry" role="listitem">
Narayanan, Hariharan, and Sanjoy Mitter. 2010. <span>“Sample Complexity of Testing the Manifold Hypothesis.”</span> In <em>Advances in Neural Information Processing Systems (NIPS) 23</em>. Vol. 23. <a href="https://papers.neurips.cc/paper/3958-sample-complexity-of-testing-the-manifold-hypothesis.pdf">https://papers.neurips.cc/paper/3958-sample-complexity-of-testing-the-manifold-hypothesis.pdf</a>.
</div>
<div id="ref-udell2016generalized" class="csl-entry" role="listitem">
Udell, Madeleine, Corinne Horn, Reza Zadeh, Stephen Boyd, et al. 2016. <span>“Generalized Low Rank Models.”</span> <em>Foundations and Trends<span></span> in Machine Learning</em> 9 (1): 1–118.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("tecunningham\.github\.io");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>