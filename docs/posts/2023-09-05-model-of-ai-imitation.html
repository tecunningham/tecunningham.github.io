<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.57">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tom Cunningham">
<meta name="dcterms.date" content="2023-10-06">
<meta name="description" content="Tom Cunningham blog">

<title>An AI Which Imitates Humans Can Beat Humans | Tom Cunningham – Tom Cunningham</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-12027453-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>
<script>window.MathJax = {
   loader: { load: ["https://cdn.jsdelivr.net/gh/sonoisa/XyJax-v3@3.0.1/build/xypic.js"]},
   tex: {packages: {'[+]': ['xypic','bm']},
         macros: {  bm: ["\\boldsymbol{#1}", 1],
                    ut: ["\\underbrace{#1}_{\\text{#2}}", 2],
                    utt: ["\\underbrace{#1}_{\\substack{\\text{#2}\\\\\\text{#3}}}", 3] }
   }
};
</script>
<style>
   h1 {  border-bottom: 4px solid black;}
   h2 {  border-bottom: 1px solid #ccc;}
</style>
<script>window.MathJax = {
         loader: { load: ['[custom]/xypic.js'],
                     paths: {custom: 'https://cdn.jsdelivr.net/gh/sonoisa/XyJax-v3@3.0.1/build/'}},
      tex: {packages: {'[+]': ['xypic']},
         macros: {
            bm: ["\\boldsymbol{#1}", 1],
            bmatrix: ["\\begin{bmatrix}#1\\end{bmatrix}", 1],
            smallmatrix: ["\\begin{smallmatrix}#1\\end{smallmatrix}", 1],
            ut: ["\\underbrace{#1}_{\\text{#2}}", 2],
            utt: ["\\underbrace{#1}_{\\substack{\\text{#2}\\\\\\text{#3}}}", 3]
         }}};
</script>
<style>
   h1 {  border-bottom: 4px solid black; }
   h2 {  border-bottom: 1px solid gray; padding-bottom: 0px; color: black; }
   dl { margin-bottom: 0px; }
   dt strong { font-weight: bold; }
   dd { margin-left: 20px; }
   .cell-output-display p {padding: 0 0 0cm 0; margin: 0 0 0 0;}
</style>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="An AI Which Imitates Humans Can Beat Humans | Tom Cunningham">
<meta name="twitter:description" content="Tom Cunningham blog">
<meta name="twitter:card" content="summary">
</head>

<body class="floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Tom Cunningham</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href=".././about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/testingham"> <i class="bi bi-twitter" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/tom-cunningham-a9433/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://tecunningham.github.io/index.xml"> <i class="bi bi-rss-fill" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://scholar.google.com/citations?user=MDB_DgkAAAAJ"> 
<span class="menu-text">scholar</span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">An AI Which Imitates Humans Can Beat Humans</h1>
                      </div>
  </div>
    
  <div class="quarto-title-meta-author">
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-heading">Affiliation</div>
    
      <div class="quarto-title-meta-contents">
      <p class="author">Tom Cunningham </p>
    </div>
    <div class="quarto-title-meta-contents">
          <p class="affiliation">
              <a href="https://integrityinstitute.org/">
              Integrity Institute
              </a>
            </p>
        </div>
    </div>

  <div class="quarto-title-meta">

        
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">October 6, 2023</p>
      </div>
    </div>
    
      <div>
      <div class="quarto-title-meta-heading">Modified</div>
      <div class="quarto-title-meta-contents">
        <p class="date-modified">June 13, 2025</p>
      </div>
    </div>
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
  <li><a href="#setup-modelling-questions-and-answers" id="toc-setup-modelling-questions-and-answers" class="nav-link active" data-scroll-target="#setup-modelling-questions-and-answers">Setup: Modelling Questions and Answers</a></li>
  <li><a href="#graphical-argument" id="toc-graphical-argument" class="nav-link" data-scroll-target="#graphical-argument">Graphical Argument</a></li>
  <li><a href="#five-reasons-for-superhuman-performance" id="toc-five-reasons-for-superhuman-performance" class="nav-link" data-scroll-target="#five-reasons-for-superhuman-performance">Five Reasons for Superhuman Performance</a></li>
  <li><a href="#evidence-on-superhuman-performance" id="toc-evidence-on-superhuman-performance" class="nav-link" data-scroll-target="#evidence-on-superhuman-performance">Evidence on Superhuman Performance</a>
  <ul class="collapse">
  <li><a href="#timeline" id="toc-timeline" class="nav-link" data-scroll-target="#timeline">Timeline</a></li>
  <li><a href="#performance-by-task" id="toc-performance-by-task" class="nav-link" data-scroll-target="#performance-by-task">Performance by Task</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  <li><a href="#linear-model" id="toc-linear-model" class="nav-link" data-scroll-target="#linear-model">Linear Model</a>
  <ul class="collapse">
  <li><a href="#model-implications" id="toc-model-implications" class="nav-link" data-scroll-target="#model-implications">Model Implications</a></li>
  <li><a href="#derivation" id="toc-derivation" class="nav-link" data-scroll-target="#derivation">Derivation</a></li>
  <li><a href="#additional-observations" id="toc-additional-observations" class="nav-link" data-scroll-target="#additional-observations">Additional Observations</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">






<div class="no-row-height column-margin column-container"><div class="">
<p><img src="20230825131919.png" class="img-fluid"> Thanks to comments from many, especially <a href="http://www.giorgiomartini.com">Giorgio Martini</a>, Grady Ward, <a href="https://robdonnelly.me">Rob Donnelly</a>, <a href="https://sites.google.com/site/inesmorenodebarreda/">Inés Moreno de Barreda</a>, and Colin Fraser.</p>
</div></div><style>p { text-indent: -2em; margin-left: 2em; }</style>
<p><strong>If we train AIs to <em>imitate</em> humans, will they ever <em>beat</em> humans?</strong> AI has caught up to human performance on many benchmarks, largely by learning to predict what humans would do. It seems important to know whether this is a ceiling or we should expect them to shoot out ahead of us. Will LLMs be able to write superhumanly-persuasive prose? Will image models be able to see things in photos that we cannot? There is a lot of technical literature on imitation learning in AI but I haven’t found much discussion of this point (<span class="citation" data-cites="bowman2023eight">Bowman (<a href="#ref-bowman2023eight" role="doc-biblioref">2023</a>)</span> is a notable exception).</p>
<p></p>
<p></p>
<p><strong>In a formal model I derive five mechanisms by which imitative AI can beat humans.</strong></p>
<ol type="1">
<li><strong>Noise.</strong> Different humans give different answers to a question, and so if an LLM can consistently give the average answer it will do better than the average human (the error of the average being always smaller than the average of the error).</li>
<li><strong>Specialization.</strong> People tend to write about what they know, and so an LLM which learns to predict the typical answer to a given question will sound like a specialist in all areas: it will answer questions about water like a hydrologist and questions about bugs like an entomologist (although it will also answer questions about astrology like an astrologist).</li>
<li><strong>Interpolation.</strong> An LLM will interpolate responses from different humans, and this interpolation can be functionally equivalent to inference, meaning an LLM will sometimes be able to reliably answer questions that <em>no</em> human can answer.</li>
<li><strong>Priors.</strong> If an LLM has different priors than a human then they could uncover hidden structure that humans do not, e.g.&nbsp;an LLM trained on human observations of astronomical events could conceivably recover cycles in those events, and so give superior predictions to the human.</li>
<li><strong>Tacit knowledge.</strong> The majority of human knowledge is tacit, meaning it is used in forming judgments but we do not have conscious access to that knowledge. If AI models can accurately predict human judgments then the weights in those models effectively contain that tacit knowledge, and so the model can be re-engineered to use that knowledge in ways that humans cannot.</li>
</ol>
<p><strong>The evidence is unclear.</strong> There are many reasons why this could theoretically occur but I couldn’t find much evidence for superhuman performance: many benchmarks which we use to evaluate ML models have human labels as the ground truth, meaning we wouldn’t know when computers do pass us by.</p>
<p><strong>This blog post contains:</strong></p>
<ol type="1">
<li>A graphical argument illustrating the five mechanisms.</li>
<li>A deeper discussion of each of the five mechanisms.</li>
<li>A brief overview of the AI-human gap in various tasks.</li>
<li>Discussion of applications, related literature, and complications.</li>
<li>A simple formal model with a derivation of each of the five mechanisms.</li>
</ol>
<section id="setup-modelling-questions-and-answers" class="level1">
<h1>Setup: Modelling Questions and Answers</h1>
<p><strong>For concreteness I will describe humans and computers answering questions about the world.</strong> However I think the basic framework applies generally to performing tasks or following instructions. Some examples of questions and answers:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 93%">
<col style="width: 6%">
</colgroup>
<thead>
<tr class="header">
<th>question</th>
<th>answer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>What’s the capital of Switzerland?</td>
<td>Bern</td>
</tr>
<tr class="even">
<td>What’s the best response if white plays c4?</td>
<td>Nf6</td>
</tr>
<tr class="odd">
<td>Does this picture (🐄) depict a cow?</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>How much more likely is someone to buy a Coke after hearing the slogan “Coke refreshes”?</td>
<td>0.1%</td>
</tr>
</tbody>
</table>
</section>
<section id="graphical-argument" class="level1">
<h1>Graphical Argument</h1>
<p>Here I illustrate all the core points in a graphical framework. For simplicity I am representing a set of questions and answers which can be represented by a pair of numbers, e.g.&nbsp;asking what is elevation of a point along a given line of latitude? I treat the human and the computer as having smooth priors about the world which determines how they interpolate and extrapolate from their observations.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>(1) Let the curve above represent the truth about the world.</strong> Each question about the world is a point on the x-axis, and the answer to each question is represented by the curve.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>(2) A human forms beliefs about the world (red curve).</strong> The human asks two questions and gets two answers (red dots) and from these they form estimates of the answer to every other question (red line).</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>(3) A computer learns to predict the human’s answers (green curve).</strong> The human records some questions and their answers (green dots), and the computer learns to predict the human’s answers (green curve). Here I have illustrated a very favorable case, where the human has shared all her observations with the computer, and both the human and computer have the same priors. If the human gives inconsistent answers (imagine a thickening of the green line) then the computer will do better than the human by having less noise (the error of the average always being smaller than the average error).</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>(4) Add another human (blue curve).</strong> Suppose we have an additional human who asks some different questions (blue dots) and so forms different beliefs (blue line). Both humans’ beliefs are are accurate in the neighborhood of their own experience.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>(5) Let the computer predict answers from both humans (green curve).</strong> Now both humans record their experiences and the computer tries to predict human answers (green curve). Here we can see:</p>
<ol type="1">
<li><strong>Specialization.</strong> The computer’s predictions can match the humans’ responses in each of their domains of expertise</li>
<li><strong>Interpolation.</strong> The computer is better than <em>both</em> humans in the intermediate region, i.e.&nbsp;the computer effectively combines information from both humans.</li>
</ol>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>(6) Let the computer have superior priors.</strong> If the computer and human have different priors then they will make different extrapolations from the same dataset. Suppose the world has a strong cyclical structure, as shown in the black oscillating line. The human does not appreciate the regularity and fits their datapoints with a simple nearest-neighbor algorithm, but the computer, with different priors, could get a superior fit to the true model. A simple hypothetical: suppose we trained a language model to predict records of astronomical observations, it could conceivably discover cycles in these observations even if no human was aware of those cycles, such that computer predictions of out-of-sample human observations of the world would constitute super-human predictions about the world.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>(7) Let the human have tacit knowledge.</strong> Finally, suppose the human always knows correct the answer when they see the question (red), but their conscious understanding of the relationship (pink) is imperfect. When asked abstract questions about the world they cannot use their tacit knowledge, e.g.&nbsp;if asked what is the maximum of this function they would choose the maximum of the pink curve (conscious beliefs), not the red curve (tacit beliefs). However the computer could learn the tacit knowledge from observing sufficiently many answers, and then algorithmically find the maximum of this curve, substantially outperforming the human at these abstract questions.</p>
</section>
<section id="five-reasons-for-superhuman-performance" class="level1 page-columns page-full">
<h1>Five Reasons for Superhuman Performance</h1>
<p><strong>(1) Noise.</strong> For many tasks there is very high within-human and between-human variation, so any model which is deterministic will have a substantial advantage. Thus averaging multiple answers tends to do much better (the “wisdom of crowds”, and the “crowd within”). A computer with a deterministic outcome will thus have a substantial advantage. <span class="citation" data-cites="zhang2024transcendence">Zhang et al. (<a href="#ref-zhang2024transcendence" role="doc-biblioref">2024</a>)</span> shows that an LLM which is trained to predict chess moves of a group of players can outperform any of the players in that group - notably the effect is stronger when the model is trained on non-expert chess players, where the errors might be expected to be uncorrelated.</p>
<p><strong>(2) Specialization.</strong> We can see clear evidence of specialization in LLMs: they will answer questions about fish like an icthyologist, and questions about Ukraine like a Ukrainian. There is a nice discussion of this with many examples by <a href="https://ryxcommar.com/2023/03/28/chatgpt-as-a-query-engine-on-a-giant-corpus-of-text/">ryxcommar</a>:</p>
<blockquote class="blockquote">
<p>“When you ask ChatGPT a more intelligent question, you get a more intelligent answer. Just like how you ask ChatGPT a more Spanish question, you get a more Spanish answer.</p>
</blockquote>
<p>This works very well as long as the people who talk most about a topic tend to be the people who are most knowledgeable about that topic. If the reverse was true then AI would perform worse than the average person. In some cases it does seem to be true that the people who are most talkative are the least accurate, e.g.&nbsp;for conspiracy theories, politically partisan issues, or pseudosciences.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> Thus we can predict that asking an LLM about these issues will tend to give low-quality answers, and indeed if you ask GPT-4 “what are some characteristics of Virgoes?” it will give a quite factual answer listing the traits of Virgoes.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Analogically, in music or visual art, there might be some genres where the people who create artworks are uniquely bad at it, and so in these genres imitative AI would learn to make artworks worse than the average human would make.</p></div><div id="fn2"><p><sup>2</sup>&nbsp;As of Oct 2023.</p></div></div><p>The same point applies for general supervised learning: suppose we train an image model to recognize tumors, and the training set includes examples from different radiologists, each who is an expert in their area (e.g.&nbsp;a pediatric radiologist labels the scans from children, a vetenarian radiologist labels the scans from animals), then the trained model could outperform any single radiologist.</p>
<p><strong>(3) Interpolation.</strong> I have not yet come up with a crisp question which an LLM can accurately answer but no human can, however there are <em>tasks</em> which LLMs can perform which it likely no human can perform without help:</p>
<ul>
<li><p>Recent LLMs (e.g.&nbsp;GPT, Bard) can transpose styles very easily, e.g.&nbsp;writing a Shakespearean sonnet about a particular episode of a particular television show, which arguably cannot be done by any human being.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p></li>
<li><p><span class="citation" data-cites="armengolestape2021multilingual">Armengol-Estapé, Gibert Bonet, and Melero (<a href="#ref-armengolestape2021multilingual" role="doc-biblioref">2021</a>)</span> show that GPT-3 does fairly well at answering questions and producing text in Catalan, despite Catalan constituting only 35M words in the training set (0.02% of the total), implying that it can answer questions for which the answer is known by no Catalan speaker. In principle language models could translate between a pair of languages for which there exists no common speaker but I do not know of any explicit confirmation of this.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Thanks to Giorgio Martini for this point.</p></div><div id="fn4"><p><sup>4</sup>&nbsp;thanks to Rob Donnelly for first suggesting this point to me.</p></div></div><p><strong>(4) Priors.</strong> When making extrapolation from the same set of data humans and computers will given different answers because they have different priors, and in some cases computer priors might give a better fit.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p>
<p>In fact there is a literature from the 1950s showing that computers with linear regression models do better than humans in learning to predict from novel datasets, and indeed better than experts in making clinical judgments using a small number of cues (<span class="citation" data-cites="camerer1991processperformance">Camerer and Johnson (<a href="#ref-camerer1991processperformance" role="doc-biblioref">1991</a>)</span>). This is surprising because it took an additional 70 years for computers to catch up with human ability in making many other judgments. I think the key difference is that the 1950s results apply to low-dimensional cases (<span class="math inline">\(p&lt;n\)</span>), while only recently have we taught computers to deal with high-dimensional data (<span class="math inline">\(p\gg n\)</span>).</p>
<p>Unfortunately I don’t know of any clear-cut examples which show an LLM outperforming a human because of superior priors. Here is a hypothetical: suppose the training data of an LLM included a large set of scientific observations, and those observations contain some underlying pattern which was not recognized by any contemporary scientist. If you ask the LLM about the existence of patterns then it should answer like a scientist and say there is no known pattern. However if you ask the LLM to predict new observations then those predictions may obey the pattern, as a consequence the LLM could be used to systematically map out prediction, which may make it easier to identify the pattern. Thus you could imagine an LLM would correctly predict the position of the stars.</p>
<p>There is an interesting sublety in how the AI ought to be prompted to elicit superhuman knowledge. Consider these two prompts:</p>
<ol type="1">
<li>“Q: where will Venus be on June the 16th? ___”</li>
<li>“On June the 16th Venus was observed to be ___”</li>
</ol>
<p>If the AI learned superior priors to the human then we should expect it to answer these prompts differently: it would answer prompt #1 using the human model, and answer prompt #2 using the true model.</p>
<p></p>
<p><strong>(5) Tacit Knowledge.</strong></p>
<p>The relative success of machine learning over symbolic AI has often been connected to the importance of tacit human knowledge. I have written a lot about the importance of tacit knowledge in human decision-making, especially <span class="citation" data-cites="cunningham2015hierarchical">Cunningham (<a href="#ref-cunningham2015hierarchical" role="doc-biblioref">2015</a>)</span> and this <a href="https://tecunningham.github.io/posts/2017-12-10-unconscious-influences.html">post</a>, see also <span class="citation" data-cites="cunningham2022implicit">Cunningham and De Quidt (<a href="#ref-cunningham2022implicit" role="doc-biblioref">2022</a>)</span>. In memory a related phenomenon is the “recognition recall” gap: people are significantly better at recognizing whether they saw a word before than in recalling that word (<span class="citation" data-cites="macdougall1904recognition">MacDougall (<a href="#ref-macdougall1904recognition" role="doc-biblioref">1904</a>)</span>).</p>
<p><span class="citation" data-cites="stiennon2022learning">Stiennon et al. (<a href="#ref-stiennon2022learning" role="doc-biblioref">2022</a>)</span> train models to summarize text, based not on human summaries but on human evaluations of summaries. The model produces summaries that are preferred to human-produced summaries 70% of the time, i.e.&nbsp;superhuman production by training on human feedback. However this isn’t quite an apples-to-apples comparison because it’s unclear what the goals were of the humans who produced the baseline summaries: the human raters had explicit rubrics, but the human summarizers weren’t explicitly incentivized on those rubrics (as far as I can tell).</p>
<p>In addition I think that “inversion of tacit knowledge” is a reasonable description of image synthesis by neural nets: models are first trained to recognize images given captions, and then a new image can be synthesized to match a given caption, e.g.&nbsp;through a diffusion algorithm. Here there’s a striking asymmetry: algorithms can approximately match average human performance in recognition, but they far outperform human performance in construction of new artefacts.</p>
</section>
<section id="evidence-on-superhuman-performance" class="level1 page-columns page-full">
<h1>Evidence on Superhuman Performance</h1>
<section id="timeline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="timeline">Timeline</h2>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="images/2023-09-19-09-55-57.png" class="img-fluid"></p>
<p>The following table shows the year in which a computer (or mechanical device) could match performance with the best human:</p>
<table class="caption-top table">
<tbody>
<tr class="odd">
<td>arithmetic</td>
<td>1642</td>
</tr>
<tr class="even">
<td>chess</td>
<td>1997</td>
</tr>
<tr class="odd">
<td>Jeopardy</td>
<td>2005</td>
</tr>
<tr class="even">
<td>image recognition (ImageNet)</td>
<td>2015</td>
</tr>
<tr class="odd">
<td>handwriting recognition (MNIST)</td>
<td>2015</td>
</tr>
<tr class="even">
<td>question answering (SQuAD1.1)</td>
<td>2019</td>
</tr>
<tr class="odd">
<td>difficult math questions (MATH)</td>
<td>2023</td>
</tr>
<tr class="even">
<td>coding problems (MBPP)</td>
<td>(not yet)</td>
</tr>
</tbody>
</table>
</div><div id="fn5"><p><sup>5</sup>&nbsp;<span class="citation" data-cites="kiela2021dynabench">Kiela et al. (<a href="#ref-kiela2021dynabench" role="doc-biblioref">2021</a>)</span> also say that “models that achieve super-human performance on benchmark tasks (according to the narrow criteria used to define human performance) nonetheless fail on simple challenge examples and falter in real-world scenarios.”</p></div></div><p><strong>Computers have hit the ceiling on most benchmarks.</strong> <span class="citation" data-cites="kiela2023plottingprogress">Kiela et al. (<a href="#ref-kiela2023plottingprogress" role="doc-biblioref">2023</a>)</span> documents that most computer benchmarks have become “saturated,” i.e.&nbsp;computers get close-to-perfect performance, and that recently the speed of saturation has become quicker (see graph on right). They say identify only a single benchmark where performance is not close to the human baseline, and most of the models they discuss are imitation learning. As a consequence some work has moved to evaluating models against “adversarial” benchmarks where the problems are chosen specifically to fool computers (e.g.&nbsp;Dynabench, <span class="citation" data-cites="kiela2021dynabench">Kiela et al. (<a href="#ref-kiela2021dynabench" role="doc-biblioref">2021</a>)</span>).<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p>
<p><strong>On some tasks human performance <em>defines</em> success.</strong> On some tasks human performance effectively is the ground truth, and so by definition computers could never beat humans. This is roughly true for text comprehension: a sentence has a given meaning if and only if the average person believes it has that meaning. When we observe computer outperformance on this type of benchmark it is because either (1) there is human variation and the computer output is more consistent; or (2) computers outperform amateur humans but the ground truth is expert humans.</p>
</section>
<section id="performance-by-task" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="performance-by-task">Performance by Task</h2>
<p><strong><em>Arithmetic</em>: computers passed humans 300 years ago.</strong> Machines have been used to do calculations since the 17th century, e.g.&nbsp;<a href="https://en.wikipedia.org/wiki/Pascal's_calculator">Pascal’s calculator</a> from 1642.</p>

<div class="no-row-height column-margin column-container"><div class="">
<table class="caption-top table">
<tbody>
<tr class="odd">
<td>Backgammon</td>
<td>1979</td>
</tr>
<tr class="even">
<td>Chess</td>
<td>1997</td>
</tr>
<tr class="odd">
<td>Jeopardy</td>
<td>2005</td>
</tr>
<tr class="even">
<td>Atari games</td>
<td>2013</td>
</tr>
<tr class="odd">
<td>Go</td>
<td>2016</td>
</tr>
<tr class="even">
<td>Starcraft</td>
<td>2019</td>
</tr>
</tbody>
</table>
<p>(<a href="https://historyofyesterday.com/the-brutal-history-of-ai-defeating-every-human/">source</a>)</p>
</div></div><p><strong><em>Playing games</em>: computers passed humans over the last 45 years.</strong> See the table in the margin for games. I am not aware of any well-known games in which computers cannot reliably beat the best humans.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="images/2023-09-18-08-13-45.png" class="img-fluid"> (<a href="https://ourworldindata.org/brief-history-of-ai">source</a>)</p>
</div></div><p><strong><em>Image recognition</em>: computers surpassed humans in the 2010s.</strong> With the qualifications above about the limitations of benchmark tasks.</p>
<p><strong><em>Question answering</em>: computers surpassed humans in the 2010s.</strong> With the qualifications above about the limitations of benchmark tasks.</p>
<p><strong><em>Facial recognition</em>: computers seem to be equivalent to experts.</strong> <span class="citation" data-cites="towler2023facial">Towler et al. (<a href="#ref-towler2023facial" role="doc-biblioref">2023</a>)</span> say <em>“naturally skilled super-recognizers, trained forensic examiners and deep neural networks, … achiev[e] equivalent accuracy.”</em></p>
<p><strong><em>Coding</em>: computers still below expert.</strong> See the benchmarks on <a href="https://paperswithcode.com/task/code-generation">PapersWithCode</a>, also a graph on <a href="https://ourworldindata.org/grapher/ai-performance-coding-math-knowledge-tests">OurWorldInData</a>, specifically <a href="https://paperswithcode.com/dataset/apps">APPS</a> and <a href="https://paperswithcode.com/sota/code-generation-on-mbpp">MBPP</a>. The best-performing computers are still imperfect at solving these coding challengers (which presumably can be solved by an expert programmer), but progress is rapid.</p>
<p><strong><em>Writing persuasive text</em>: computer comparable to average human.</strong> A number of recent papers compare the persuasive power of LLM-generated text to human-generated text (<span class="citation" data-cites="bai2023persuade">Bai et al. (<a href="#ref-bai2023persuade" role="doc-biblioref">2023</a>)</span>, <span class="citation" data-cites="goldstein2023persuasive">Goldstein et al. (<a href="#ref-goldstein2023persuasive" role="doc-biblioref">2023</a>)</span>, <span class="citation" data-cites="hackenburg2023persuasive">Hackenburg and Margetts (<a href="#ref-hackenburg2023persuasive" role="doc-biblioref">2023</a>)</span>, <span class="citation" data-cites="matz2023personalized">Matz et al. (<a href="#ref-matz2023personalized" role="doc-biblioref">2023</a>)</span>, <span class="citation" data-cites="palmer2023large">Palmer and Spirling (<a href="#ref-palmer2023large" role="doc-biblioref">2023</a>)</span>, <span class="citation" data-cites="qin2023large">Qin et al. (<a href="#ref-qin2023large" role="doc-biblioref">2023</a>)</span>). They all find that LLMs do relatively well, but none show clear signs of computer superiority.</p>
<p><strong><em>Writing creative blurbs</em>: computer comparable to average human.</strong> <span class="citation" data-cites="koivisto2023creativity">Koivisto and Grassini (<a href="#ref-koivisto2023creativity" role="doc-biblioref">2023</a>)</span> compared GPT4 to online recruited humans (£2 for a 13 minute task) in giving “creative” uses for everyday items. The prompt was to “come up with original and creative uses for an object”, objects were “rope”, “box”, “pencil” and “candle.” The responses were rated by humans for their “creativity” or “originality.” GPT-4 responses were perhaps 1SD above the average human score, but the difference was smaller when choosing just the best response for each user.</p>
<p><strong><em>Summarizing text</em>: computer beats average human.</strong> Two recent papers found that LLM-generated summaries, trained with feedback, were preferred by humans to human-generated summaries (<span class="citation" data-cites="stiennon2022learning">Stiennon et al. (<a href="#ref-stiennon2022learning" role="doc-biblioref">2022</a>)</span> using RLHF and <span class="citation" data-cites="lee2023rlaif">Lee et al. (<a href="#ref-lee2023rlaif" role="doc-biblioref">2023</a>)</span> using RLHF). However in both cases it wasn’t clear to me exactly how the human summarizers were incentivized, and whether they were trying to perform the same task as the LLMs.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;<span class="citation" data-cites="lee2023rlaif">Lee et al. (<a href="#ref-lee2023rlaif" role="doc-biblioref">2023</a>)</span> say <em>“RLAIF summaries are preferred over the reference [human-written] summaries 79% of the time, and RLHF are preferred over the reference summaries 80% of the time.”</em></p></div><div id="fn7"><p><sup>7</sup>&nbsp;<span class="citation" data-cites="hendrycks2021measuring">Hendrycks et al. (<a href="#ref-hendrycks2021measuring" role="doc-biblioref">2021</a>)</span> says “We found that a computer science PhD student who does not especially like mathematics attained approximately 40% on MATH, while a three-time IMO gold medalist attained 90%”</p></div></div><p><strong><em>Doing math problems</em>: computer comparable to expert.</strong> The latest score on the <a href="https://paperswithcode.com/sota/math-word-problem-solving-on-math">MATH benchmark</a> is 84%, compared to 90% by a three-time IMO gold medalist. The scores have been rising very rapidly so it seems likely that computers will soon surpass humans.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p>
<p></p>
</section>
</section>
<section id="discussion" class="level1 page-columns page-full">
<h1>Discussion</h1>
<p><strong>I will use “superhuman” to mean the AI can answer some question better than any human can.</strong> We can formalize “superhuman” ability in a variety of ways. The notation is introduced more fully below, but briefly <span class="math inline">\(\bm{q}\)</span> represents a question, <span class="math inline">\(a(\bm{q})\)</span> represents the correct answer, <span class="math inline">\(\bar{a}(\bm{q})\)</span> represents the computer’s answer, and <span class="math inline">\(\hat{a}_i(\bm{q})\)</span> represents the answer of human <span class="math inline">\(i\)</span>. We assume squared error loss, and the expected error can be interpreted as either over the universe of all questions, or some subset of questions: <span class="math display">\[\begin{aligned}
      \text{weak:}&amp;&amp;    \ut{E[(a(q)-\bar{a}(q))^2]}{error of computer}
         &amp;\leq  \ut{\frac{1}{m}\sum_{i=1}^mE[(a(q)-\hat{a}_i(q))^2]}{avg error of human}\\
      \text{medium:}&amp;&amp;  \ut{E[(a(q)-\bar{a}(q))^2]}{error of computer}
         &amp;\leq  \ut{E\left[(a(q)-\frac{1}{m}\sum_{i=1}^m\hat{a}_i(q))^2\right]}{error of avg human}\\
      \text{strong:}&amp;&amp;  \ut{E[(a(q)-\bar{a}(q))^2]}{error of computer}
         &amp;\leq  \ut{\min_{i=1,\ldots,m}E[(a(q)-\hat{a}_i(q))^2]}{error of best human}\\
      \text{super-strong:}&amp;&amp; \ut{E[(a(q)-\bar{a}(q))^2]}{error of computer}
         &amp;\leq  \ut{E[\min_{i=1,\ldots,m}\{(a(q)-\hat{a}_i(q))^2\}]}{error of best human by question}\\
   \end{aligned}
   \]</span></p>
<p>Note that we cannot rank benchmark #2 and #3: the error of the best human could be either higher or lower than the error of the average human. The most interesting question is whether AI can exhibit “super strong” superhuman performance. It seems clear that imitative learning can easily lead to superhuman performance by all the other 3 definitions through (1) reducing noise, and (2) combining expertise. However super-strong superhuman performance would require either (1) interpolation, (2) superior priors, or (3) using tacit human knowledge.</p>
<p><strong>Training models with custom-written answers is still imitative learning.</strong> Recent LLMs don’t train just on predicting existing text (books, internet, twitter) they also use datasets of instructions and responses generated by paid raters (<span class="citation" data-cites="ouyang2022training">Ouyang et al. (<a href="#ref-ouyang2022training" role="doc-biblioref">2022</a>)</span>). We can still call this imitation but it’s putting relatively more weight on imitating the responses of specific set of people, the paid raters. This fine-tuning significantly improves performance on most benchmarks but I think it also has costs: the model is now predicting output of a specific set of people (i.e.&nbsp;non-expert paid raters), and so conceivably will do less well at incorporating niche information available to an expert.</p>
<p><strong>Training on human evaluations is using human tacit knowledge.</strong> Recent LLMs are not purely imitative, e.g.&nbsp;OpenAI’s GPT models are trained with human <em>evaluation</em> of their responses (called reinforcement learning with human feedback (RLHF)), and they find that it dramatically increases performance on instruction-following benchmarks.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> The key difference is that the goal now reflects how humans <em>rate</em> responses rather than how humans <em>generate</em> responses. In some domains the two functions might be identical but in others there’s a clear difference, I would argue that tacit knowledge is the core difference.</p>
<div class="no-row-height column-margin column-container"><div id="fn8"><p><sup>8</sup>&nbsp;E.g. see OpenAI’s 2022 InstructGPT (<span class="citation" data-cites="ouyang2022training">Ouyang et al. (<a href="#ref-ouyang2022training" role="doc-biblioref">2022</a>)</span>). In fact they run reinforcement learning against a model trained to predict the human evaluation of outputs, and other papers run reinforcement learning against LLM-produced evaluations, RLAIF (<span class="citation" data-cites="lee2023rlaif">Lee et al. (<a href="#ref-lee2023rlaif" role="doc-biblioref">2023</a>)</span>).</p></div></div><p><strong><span class="citation" data-cites="bowman2023eight">Bowman (<a href="#ref-bowman2023eight" role="doc-biblioref">2023</a>)</span> on super-human performance by LLMs.</strong> I have found surprisingly little online or academic discussion about whether LLMs will hit a ceiling defined by human performance. A good paper by <span class="citation" data-cites="bowman2023eight">Bowman (<a href="#ref-bowman2023eight" role="doc-biblioref">2023</a>)</span> has a section titled “human performance on a task isn’t an upper bound on LLM performance.” He says LLMs can outperform humans for two reasons: (1) “they are trained on far more data than any human sees,” and (2) “they are often given additional training using reinforcement learning … which trains them to produce responses that humans find helpful without requiring humans to demonstrate such helpful behavior.” I think these correspond to two of the five reasons I identified (specialization and tacit knowledge).</p>
<p><strong>For some tasks human-level performance is the ceiling by definition.</strong> The ground truth about language interpretation is humans interpretation, and so it is hard to see how a computer could exhibit superhuman performance (in the super-strong sense).<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> In benchmarks for natural language understanding the labels are typically written by the human authors of the benchmark, so it would be impossible to observe superhuman performance (<span class="citation" data-cites="tedeschi2023s">Tedeschi et al. (<a href="#ref-tedeschi2023s" role="doc-biblioref">2023</a>)</span>). A similar point applies to content moderation the definition of ground truth is typically either majority-vote among paid human raters, or the reflective judgment of a senior human employee, thus a computer could only outperform in the weak senses above.<a href="#fn10" class="footnote-ref" id="fnref10" role="doc-noteref"><sup>10</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;There are exceptions but I don’t think they are quantitatively important. Consider the a sentence like “Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo.” This sentence has at least one well-defined meaning according to the typical rules of English but arguably no human would correctly identify that meaning unless specifically prompted. A computer trained only on human comprehension, i.e.&nbsp;data which did not contain such outlier sentences, could plausibly identify its meaning.</p></div><div id="fn10"><p><sup>10</sup>&nbsp;Many recent language models do outperform the average human baseline on language understanding tasks, but <span class="citation" data-cites="tedeschi2023s">Tedeschi et al. (<a href="#ref-tedeschi2023s" role="doc-biblioref">2023</a>)</span> argue that for a variety of reasons the strength of these results is significantly exaggerated.</p></div></div><p></p>
<p><strong>Computers could outperform humans on recognition tasks, but we haven’t tested them yet.</strong> Most benchmarks for media recognition, e.g.&nbsp;testing for object detection in photos or speech recognition in audio, use human labels as the ground truth. However humans can be mistaken: they might think a photo has a dog in it when it does not or vice versa. Thus human judgment is not the ground truth. We could create a test set to measure superhuman performance either by (1) creating new media (e.g.&nbsp;taking new photos of dogs instead of using existing photos which humans identify as having a dog); (2) obfuscating existing media (e.g.&nbsp;blurring existing photos of dogs). By the argument in this note a classifiers trained only on human-provided labels could exhibit superhuman performance in such a test set, most plausibly through better priors: learning characteristic signs of dogs that humans do not.<a href="#fn11" class="footnote-ref" id="fnref11" role="doc-noteref"><sup>11</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn11"><p><sup>11</sup>&nbsp;Defining the ground truth in a recognition task is somewhat complicated because a given arrangement of pixels is consistent with an infinite variety of objects having caused that arrangement. We talk about an image representing an object in the world only because we have strong priors about the world which allow us to make that inference. So the ground truth in a recognition task must be something like “in ordinary circumstances, what is the probability that these pixels would be caused by a scene with a dog in them.”</p></div></div><p><strong>Self-play has helped performance in playing games.</strong> A common trick to teach computers to play games well is to have them play themselves (self-play), this has been used to get superhuman performance in Backgammon, Chess, Go, &amp; Minecraft. However this is not imitation learning: here the computer is trained against a non-human ground truth, the computer directly observes whether they have won the game. However there are analogues of self-play in imitative models: (1) training models to generate images, and to discriminate between computer-generated and real images (generative adversarial nets, GAN); (2) training an LLM to produce text based on LLM-generated feedback (RLAIF).</p>
<p><strong>The graphical model underplays the importance of model architecture.</strong> The graphical model shown above represents both questions and answers as unidimensional, and it makes it seem that a small sample is sufficient to get a reasonably good approximation of the true function. In reality the questions of interest are very high dimensional and most model architectures fail to generalize well at all. Neural nets, especially those with a transformer structure, have had remarkable success in fitting the data, leading to leaps in performance.</p>
<div class="cell page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div></div></div>
<p><strong>Venn diagram representation.</strong> This diagram shows an alternative way of representing some of the core claims: in general the questions answerable by an LLM will not be a subset of the questions answered in the training set, or even those answerable by the people who contributed to the training set. The Venn diagram’s disadvantage, relative to the visualizations above, is that it does not represent the mechanics of <em>why</em> the LLM can outperform humans, while the diagram above can be use to separately show five distinct reasons (averaging error, specialization, interpolation, different priors, and using tacit knowledge).</p>
<p><strong>Imitation learning has problems in dynamic situations.</strong> The discussion in this note has been about a purely static problem of supplying answers to questions, but text generation can also be considered as a dynamic problem of sequentially generating tokens. A common observation regarding dynamic imitation learning is that pure prediction of expert behaviour (“behavioural cloning”) is not very robust, because the algorithm does not know what to do in situations not observed before (out of distribution), and this has been used to explain weaknesses in the behaviour of autoregressive generative text models.<a href="#fn12" class="footnote-ref" id="fnref12" role="doc-noteref"><sup>12</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn12"><p><sup>12</sup>&nbsp;<span class="citation" data-cites="cundy2023sequencematch">Cundy and Ermon (<a href="#ref-cundy2023sequencematch" role="doc-biblioref">2023</a>)</span> say “[the] simple behaviour cloning approach results in a compounding error problem, where the further the trained model gets from the typical expert states, the worse the model performs, incurring increasing error.” I also found <a href="https://web.stanford.edu/class/cs237b/pdfs/lecture/cs237b_lecture_12.pdf">these notes</a> from Stanford’s CS273B useful.</p></div></div></section>
<section id="linear-model" class="level1 page-columns page-full">
<h1>Linear Model</h1>
<p><strong>Here I give a more formal model and derive some results.</strong> I wrote this model before coming up with the graphical argument above. There is a substantial overlap in implications, but I think there is some value in this linear model in the precision with which we define each quantity. The model has three steps:</p>
<p><span class="math display">\[\xymatrix@C=.5cm@R=0cm{
      \text{world}
      &amp;&amp; \text{human}
      &amp;&amp; \text{LLM}\\
      *+[F:&lt;5pt&gt;]{\bm{w}} \ar[rr]^{\bm{a}=Q\bm{w}}
      &amp;&amp; *+[F:&lt;5pt&gt;]{\hat{\bm{w}}} \ar[rr]^{\hat{\bm{a}}=\hat{Q}\hat{\bm{w}}}
      &amp;&amp; *+[F:&lt;5pt&gt;]{\bar{\bm{w}}} \ar[rr]^{\tilde{a}=\tilde{q}'\bar{\bm{w}}}
      &amp;&amp; \  \\
      \txt{unobserved\\truth\\about\\the\\world}
      &amp; \txt{answers\\to\\human\\questions}
      &amp; \txt{beliefs\\formed\\by\\human}
      &amp; \txt{text\\written\\by\\human}
      &amp; \txt{LLM\\model\\of\\human\\text}
      &amp; \txt{LLM's\\answers\\to\\new\\questions}
   }
   \]</span></p>
<p><strong>Questions and answers.</strong> A question is defined by a set of binary attributes (<span class="math inline">\(q_1,\ldots,q_p\in\{-1,1\}\)</span>), and the answer is a linear function of those attributes given some unobserved weights <span class="math inline">\(w_1,\ldots,w_p\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
      \ut{\bmatrix{a^1 \\ \vdots \\ a^n}}{answers}
         = \ut{\bmatrix{q_1^1 w_1 + \ldots q_p^1w_p \\ \vdots \\ q_1^n w_1 + \ldots q_p^nw_p}}{questions}
   \end{aligned}
   \]</span></p>
<p><strong>Human beliefs.</strong> After observing a set of question and their real-world answers the human will form beliefs about the weights <span class="math inline">\(w_1,\ldots,w_p\)</span>. We can explicitly write the human posteriors if we assume their priors are Gaussian and i.i.d. (<span class="math inline">\(\bm{w}\sim N(0,\sigma^2I)\)</span>):<a href="#fn13" class="footnote-ref" id="fnref13" role="doc-noteref"><sup>13</sup></a></p>
<div class="no-row-height column-margin column-container"><div id="fn13"><p><sup>13</sup>&nbsp;I am assuming <span class="math inline">\(\bm{w}\)</span> has zero-mean and is i.i.d. just to cut down on notation, the results all hold for the more general multivariate Normal case.</p></div></div><span class="math display">\[\begin{aligned}
      \bm{a}        &amp;= Q\bm{w}
         &amp;&amp; \text{(questions \&amp; answers given true weights $\bm{w}$)}\\
      \hat{\bm{w}} &amp;= Q'(QQ')^{-1}\bm{a}
         &amp;&amp; \text{(human estimate of weights $\bm{w}$ given $Q$ and $\bm{a}$)}
   \end{aligned}\]</span>
<p>I give a derivation of the human posteriors below.</p>
<p>I will assume that the number of unobserved weights is large relative to the human’s experience (<span class="math inline">\(p\gg n\)</span>), so the human will gradually learn more about reality as she observes the answer to more questions. She will be able to perfectly answer any question she’s seen before, but will never learn the full set of weights.</p>
<p><strong>Computer beliefs.</strong> Suppose that humans write down some set of questions, <span class="math inline">\(\hat{Q}\)</span>, and then record their best guesses at the answers. These could be questions that the humans already know the answer to (<span class="math inline">\(\hat{Q}\subseteq Q\)</span>), or they could be new questions that they are guessing the answer to. We then use those questions and answers to train a computer, and the computer likewise assumes a linear model with i.i.d. Gaussian weights. Note that the computer is being trained to predict human responses, not to predict properties of the world. We can write the computer-estimated weights as follows:</p>
<span class="math display">\[\begin{aligned}
      \hat{\bm{a}}        &amp;= \hat{Q}\hat{\bm{w}}
         &amp;&amp; \text{(human-generated questions \&amp; answers)}\\
      \bar{\bm{w}} &amp;= \hat{Q}'(\hat{Q}\hat{Q}')^{-1}\hat{\bm{a}}
         &amp;&amp; \text{(computer estimate of human weights $\hat{\bm{w}}$)}
   \end{aligned}\]</span>
<p><strong>Computer answers.</strong> Finally we can ask the computer a new question, <span class="math inline">\(\tilde{q}\)</span>, and observe its answer:</p>
<span class="math display">\[\begin{aligned}
      \tilde{a}        &amp;= \tilde{\bm{q}}'\bar{\bm{w}}
         &amp;&amp; \text{(computer answer to a novel question $\tilde{\bm{q}}$)}\\
   \end{aligned}\]</span>
<section id="model-implications" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="model-implications">Model Implications</h2>
<dl>
<dt><strong>If one human records all their observations then the computer will perfectly imitate them.</strong></dt>
<dd>
Suppose that there is one human and they write down all of their observations, <span class="math inline">\(\hat{Q}=Q\)</span>. Because the computer and human have the same priors, and observe the same data, then they will therefore end up with the same estimated weights (<span class="math inline">\(\hat{\bm{w}}=\bar{\bm{w}}\)</span>), and so the computer will answer every question exactly as the human does, though neither knows the truth (<span class="math inline">\(\bar{\bm{w}}\neq\bm{w}\)</span>).
</dd>
</dl>
<p><strong>If humans are noisy then the computer will outperform them.</strong> Suppose humans report their answers with some i.i.d. noise <span class="math inline">\(\epsilon\)</span>. If the computer observes sufficiently many answers for each question then the noise will be washed out and they will outperform.</p>
<dl>
<dt><strong>If humans record a subset of their observation then the computer will perform worse.</strong></dt>
<dd>
Suppose humans only write down some of their observations, i.e.&nbsp;<span class="math inline">\(\hat{Q}\)</span> is a row-wise subset of <span class="math inline">\(Q\)</span>. Then computers and humans will give the same answers for any question in the training set, but outside of that set computers will generally do worse than humans. And so for any question <span class="math inline">\(\bm{q}\not\in\hat{Q}\)</span> the computer will do worse in expectation: <span class="math display">\[E[\ut{(\bm{q}(\bm{w}-\bar{\bm{w}}))^2}{computer error}]\geq
     E[\ut{(\bm{q}(\bm{w}-\hat{\bm{w}}))^2}{human error}].\]</span> Note that we are fixing the question <span class="math inline">\(\bm{q}\)</span> and taking the expectation over all possible worlds, <span class="math inline">\(\bm{w}\)</span>. I think you could probably rewrite this such that, in the world we are in, we should observe worse average performance across a set of questions, but I think you’d need to add some conditions to make sure that the questions are sufficiently independent (e.g.&nbsp;if there was a single weight <span class="math inline">\(w_q\)</span> which dominated all the other weights then the computer might beat the human by accident).
</dd>
<dt><strong>If there are two humans then the computer will outperform them both.</strong></dt>
<dd>
Suppose there are two humans who each observe answers to different question, <span class="math inline">\(Q_A\)</span> and <span class="math inline">\(Q_B\)</span>, and they both write them all down, so <span class="math inline">\(\bar{Q}=(\smallmatrix{Q_A\\Q_B})\)</span> and <span class="math inline">\(\bar{\bm{a}}=(\smallmatrix{Q_A\bm{w}\\Q_B\bm{w}})\)</span>. Now the computer has a strictly larger set of observations than either human, and so if we let <span class="math inline">\(\hat{\bm{w}}(i)\)</span> represent the weights of human <span class="math inline">\(i\in\{A,B\}\)</span>, then for any question <span class="math inline">\(\bm{q}\)</span> we can write:<br>
<span class="math display">\[ E[\ut{(\bm{q}(\bm{w}-\bar{\bm{w}}))^2}{computer error}]\leq
  E[\ut{(\bm{q}(\bm{w}-\hat{\bm{w}}(i)))^2}{human error}].
   \]</span>
</dd>
</dl>
<p><strong>If there are multiple humans then the computer can answer question no human can answer.</strong> Suppose two humans observe the answers to the following questions:</p>
<span class="math display">\[\begin{aligned}
      Q_A &amp;= \bmatrix{1 &amp; 1 &amp; 1 \\ 1 &amp; -1 &amp; 1} \\
      Q_B &amp;= \bmatrix{1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; -1}
   \end{aligned}\]</span>
<p>The first human will learn the exact value of <span class="math inline">\(w_2\)</span> (<span class="math inline">\(\hat{w}_2=w_2\)</span>), and the second human will learn the exact value of <span class="math inline">\(w_3\)</span>, but neither will learn both values, and so neither could predict the answer to this question with perfect confidence:</p>
<span class="math display">\[\begin{aligned}
      \tilde{q} &amp;= \bmatrix{1 &amp; -1 &amp; -1} \\
   \end{aligned}\]</span>
<p>However if they both recorded their observations then the computer observes <span class="math inline">\(\bar{\bm{a}}=(\smallmatrix{Q_1\bm{w}\\Q_2\bm{w}})\)</span>, and so the computer will be able to infer both <span class="math inline">\(w_2\)</span> and <span class="math inline">\(w_3\)</span>, and thus will be able to perfectly answer the question <span class="math inline">\(\tilde{q}\)</span> above. We can see this behaviour in LLMs: they sometimes combine a pair of facts or a pair of abilities which no single human has access to, e.g.&nbsp;when an LLM translates between a pair of languages for which there exists no human speaker of both.</p>
<p><strong>If humans write outside their expertise then the computer will do worse.</strong> In the cases above we assumed that the two humans recorded only what they directly observed, <span class="math inline">\(\hat{Q}\subseteq Q\)</span>. This means the computer essentially had a window directly to the world. However the humans could instead have written down their estimated answers to other questions, for which they have never observed the ground truth. Suppose both humans wrote down answers to every possible question, <span class="math inline">\(\bm{q}\in\{-1,1\}^p\)</span>, then we could conjecture that the computer would learn the average of the two humans’ weights:<a href="#fn14" class="footnote-ref" id="fnref14" role="doc-noteref"><sup>14</sup></a> <span class="math display">\[\bar{\bm{w}}=\frac{1}{2}\hat{\bm{w}}_A+\frac{1}{2}\hat{\bm{w}}_B.\]</span> Here the computer will do worse than the two humans on the original questions, <span class="math inline">\(Q_A\)</span> and <span class="math inline">\(Q_B\)</span>. The implication is that LLMs work so well only because people tend to write about what they know. Put another way, when an LLM answers a question, it will not predict the answer given by the average person, but will predict the answer given by people who are likely to answer that question in the real world. Luckily there tends to be a positive correlation between having knowledge about a domain, and writing about that domain.</p>
<div class="no-row-height column-margin column-container"><div id="fn14"><p><sup>14</sup>&nbsp;We would have to augment the computer’s learning rule to allow for noise in answers - I need to confirm that the weighting will be exactly 1/2.</p></div><div id="fn15"><p><sup>15</sup>&nbsp;This is related to the “generator-discriminator” gap, but specific to knowledge rather than to logical implication.</p></div></div><p><strong>If humans have tacit knowledge, then computers can outperform in choosing a question to maximize the answer.</strong> We can model tacit knowledge with two separate sets of human weights:<a href="#fn15" class="footnote-ref" id="fnref15" role="doc-noteref"><sup>15</sup></a></p>
<span class="math display">\[\begin{aligned}
      \hat{\bm{w}}^T   &amp;= \text{tacit knowledge}\\
      \hat{\bm{w}}^E &amp;= \text{explicit knowledge}\\
   \end{aligned}\]</span>
<p>When the human encounters a new question <span class="math inline">\(\tilde{\bm{q}}\)</span> they will use their tacit knowledge to form an estimate of the answer, <span class="math inline">\(\hat{a}=\tilde{\bm{q}}'\hat{\bm{w}}^T\)</span>. But they have limited ability to introspect about that capacity, and so when asked how they make their judgments they can report only <span class="math inline">\(\hat{\bm{w}}^E\)</span>. For simplicity assume tacit knowledge is perfect (<span class="math inline">\(\hat{\bm{w}}^T=\bm{w}\)</span>), and explicit knowledge is imperfect (<span class="math inline">\(\hat{\bm{w}}^E\neq \bm{w}\)</span>).</p>
<p>The distinction becomes important when we want to create a new <span class="math inline">\(\bm{q}\)</span>. Here it’s useful to interpret <span class="math inline">\(\bm{q}\)</span> as an artefact, e.g.&nbsp;a text or image, and interpret <span class="math inline">\(a=\bm{q}'\bm{w}\)</span> as a property of that artefact, e.g.&nbsp;how persuasive is the text, or how attractive is the image. Suppose we want to choose <span class="math inline">\(\bm{q}\in\{-1,1\}^n\)</span> to maximize <span class="math inline">\(a\)</span>. If we had perfect access to our beliefs <span class="math inline">\(\bm{w}^T\)</span> this would be simple, however if we have access only to imperfect explicit knowledge <span class="math inline">\(\hat{\bm{w}}^E\)</span>, the artefact which maximizes that function will not generally be the one which maximizes <span class="math inline">\(a\)</span>. This represents an asymmetry in human cognition: we can recognize certain patterns (whether text is persuasive, whether a picture is pretty), without being able to produce those patterns.</p>
<p>Here the computer model is less constrained. Suppose the computer has observed sufficiently many questions such that they have perfectly learned human tacit knowledge, <span class="math inline">\(\bar{\bm{w}}=\hat{\bm{w}}^T\)</span>. If computation is costless we could query every single <span class="math inline">\(\bm{q}\in\{-1,1\}^p\)</span> to find the highest <span class="math inline">\(a\)</span>. In the real-world we use a diffusion algorithm, or reinforcement learning against human or computer evaluation, to find an artefact with a high <span class="math inline">\(a\)</span>.</p>
</section>
<section id="derivation" class="level2">
<h2 class="anchored" data-anchor-id="derivation">Derivation</h2>
<p><strong>Setup.</strong></p>
<span class="math display">\[\begin{aligned}
      Q &amp;= \bmatrix{q_1^1 &amp; \ldots &amp; q^1_p \\ &amp; \ddots \\ q^n_1 &amp; \ldots &amp; q^n_p}
         &amp;&amp; \text{(matrix of $n$ questions, each with $p$ parameters)} \\
      \bm{w}'  &amp;= \bmatrix{w_1 \ldots w_p}
         &amp;&amp; \text{(vector of $p$ unobserved weights)}\\
      \bm{a}    &amp;= \bmatrix{a^1 \\ \vdots \\ a^n}
         = \bmatrix{q_1^1 w_1 + \ldots q_p^1w_p \\ \vdots \\ q_1^n w_1 + \ldots q_p^n w_p}
         &amp;&amp; \text{(vector of $n$ observed answers)}\\
   \end{aligned}\]</span>
Written more compactly:
<span class="math display">\[\begin{aligned}
      Q      &amp;\in \{-1,1\}^{n\times p}
         &amp;&amp; \text{($n$ questions, each has $p$ binary parameters)}\\
      \bm{w} &amp;\sim N(0,\Sigma)
         &amp;&amp; (p\times 1\text{ vector of true parameters of the world)}\\
      \ut{\bm{a}}{$n\times1$}   &amp;= \ut{Q}{$n\times p$}\ut{\bm{w}}{$p\times1$}
         &amp;&amp; \text{(answers provided by the world)}\\
   \end{aligned}\]</span>
<p><strong>Human posteriors.</strong> Given you observe a subset of a set of multivariate normal variables there is a simple expression for your posteriors over the remaining unobserved variables (e.g.&nbsp;see <a href="https://cs.nyu.edu/~roweis/notes/gaussid.pdf">here</a>).</p>
<span class="math display">\[\begin{aligned}
      \hat{\bm{w}} &amp;= E[\bm{w}|Q,\bm{a}]
            &amp;&amp; \text{(human beliefs about the world)}\\
         &amp;= \ut{\Sigma Q'}{$Cov(\bm{w},\bm{a})$}
            (\ut{Q\Sigma Q'}{$Var(\bm{a})$})^{-1}
            \bm{a}
         &amp;&amp; \text{(from the Schur complement)}
   \end{aligned}\]</span>
<p>We can use the same formula to calculate computer beliefs.</p>
</section>
<section id="additional-observations" class="level2">
<h2 class="anchored" data-anchor-id="additional-observations">Additional Observations</h2>
<p>These are a few miscellaneous results additional results that helped me with intuition for the working of this model.</p>
<strong>With one observation and two weights.</strong> Suppose <span class="math inline">\(n=1, p=2\)</span>, then we have:
<span class="math display">\[\begin{aligned}
      Q  &amp;= \bmatrix{q_1 &amp; q_2} \\
      \bm{a}'  &amp;= \bmatrix{a} \\
      \bm{w}'  &amp;= \bmatrix{w_1 &amp; w_2 } \\
      \Sigma &amp;= \bmatrix{\sigma_1^2 &amp; \rho \\ \rho &amp; \sigma_2^2}\\
      \Sigma Q' &amp;= \bmatrix{ \sigma_1^2q_1 + \rho q_2 \\ \rho q_1 + \sigma_2^2 q_2 } \\
      Q\Sigma Q' &amp;= \bmatrix{ \sigma_1^2q_1^2 + 2\rho q_1q_2 + \sigma_2^2 q_2^2} \\
      \hat{\bm{w}}=\Sigma Q'(Q\Sigma Q')^{-1}\bm{a}
         &amp;= \bmatrix{ \frac{\sigma_1^2q_1 + \rho q_2}{\sigma_1^2q_1^2 + 2\rho q_1q_2 + \sigma_2^2 q_2^2} \\
                  \frac{\rho q_1 + \sigma_2^2 q_2}{\sigma_1^2q_1^2 + 2\rho q_1q_2 + \sigma_2^2 q_2^2}} a
   \end{aligned}\]</span>
<p>We can normalize <span class="math inline">\(q_1=q_2=1\)</span>, then we have: <span class="math display">\[\hat{w}_1 = \frac{\sigma_1^2+\rho}{\sigma_1^2+2\rho+\sigma_2^2}a,\]</span> Here we are dividing up responsibility for the answer (<span class="math inline">\(a\)</span>) into the contributions of each component, nice and simple.</p>
<strong>With two observations and one weight.</strong> Here we’re <em>over-identified</em>.
<span class="math display">\[\begin{aligned}
      Q  &amp;= \bmatrix{q^1 \\ q^2} \\
      \bm{a}  &amp;= \bmatrix{a^1 \\ a^2} \\
      \bm{w}  &amp;= \bmatrix{w } \\
      \Sigma &amp;= \bmatrix{\sigma^2 }\\
      \Sigma Q' &amp;= \bmatrix{ \sigma^2 q^1 &amp; \sigma^2 q^2 } \\
      Q\Sigma Q' &amp;= \bmatrix{ \sigma^2 q^1q^1 &amp; \sigma^2q^1q^2 \\ \sigma^2q^1q^2 &amp; \sigma^2q^2q^2}
         &amp;&amp; \text{(this matrix doesn't have an inverse)}
   \end{aligned}\]</span>
<strong>With noise.</strong> Suppose we only observe the answers with random noise, then we have
<span class="math display">\[\begin{aligned}
      \ut{\bm{a}}{$n\times1$}   &amp;= \ut{Q}{$n\times p$}\ut{\bm{w}}{$p\times1$}
         + \ut{\bm{e}}{$n\times 1$} \\
      \bm{e} &amp;\sim N(\bm{0},s^2I_n) &amp;&amp; \text{(i.i.d. noise with variance $s^2$)}\\
      Cov(\bm{w},\bm{a})   &amp;= \Sigma Q' \\
      Var(\bm{a}) &amp;= Q\Sigma Q' + s^2I_n \\
      E[\bm{w}|Q,\bm{a}]   &amp;= \Sigma Q'(Q\Sigma Q' + s^2I_n)^{-1}\bm{a}
   \end{aligned}\]</span>
<strong>Compare to Bayesian linear regression.</strong> We can compare this result to Bayesian linear regression (e.g.&nbsp;<a href="https://en.wikipedia.org/wiki/Bayesian_linear_regression">Wikipedia</a>):
<span class="math display">\[\begin{aligned}
      \bar{\beta}  &amp;= \Sigma Q'(Q\Sigma Q' + s^2I_n)^{-1}\bm{a}
         &amp;&amp; \text{(our result)} \\
      \tilde{\beta} &amp;= (Q'Q+s^{2}\Sigma^{-1})^{-1}Q'\bm{a}
         &amp;&amp; \text{(standard Bayesian linear regression)}\\
   \end{aligned}\]</span>
<p>I <em>believe</em> that these can be shown to be equivalent by the <a href="https://en.wikipedia.org/wiki/Woodbury_matrix_identity">matrix inversion lemma</a>, though I haven’t confirmed this. There is a concise proof in an online note from Utah State University.</p>
<p><strong>Extension: quadratic forms.</strong> Instead of answers being linear in question-features (<span class="math inline">\(a=q'w\)</span>) we could suppose they’re quadratic, <span class="math inline">\(a=q'Wq\)</span>, with <span class="math inline">\(W\)</span> a matrix having dimension <span class="math inline">\(p^2\)</span>. I am not sure whether we could still get an analytic solution for posteriors. One way to visualise <span class="math inline">\(W\)</span> is that each bit in <span class="math inline">\(q\)</span> adds an “L” (a row and a column) to the matrix, and <span class="math inline">\(a\)</span> is the sum of the cells where both the row and the column are activated.</p>
<p><strong>Extension: binary answers.</strong> In some cases it is natural to think of the answer, <span class="math inline">\(a\)</span>, as binary instead of continuous. We might be able to reinterpret the model with <span class="math inline">\(a\)</span> representing the log-odds ratio of a binary outcome. Alternatively there might be a way of having a beta-binomial conjugate prior over the probability of <span class="math inline">\(a\)</span>.</p>
<section id="whos-closer" class="level3">
<h3 class="anchored" data-anchor-id="whos-closer">Who’s closer</h3>
<p>Suppose two people, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>, have observed different training sets <span class="math inline">\(Q^A\)</span> and <span class="math inline">\(Q^B\)</span>, then we can characterize their expected error for the answer of a new question <span class="math inline">\(q\)</span> (where <span class="math inline">\(q\not\in Q^A\)</span>, <span class="math inline">\(q\not\in Q^B\)</span>):</p>
<p><strong>Setup.</strong> Let the true weights be <span class="math inline">\(\bm{w}\sim N(0,\Sigma)\)</span> and let the two individuals observe <span class="math inline">\(\bm a^i = Q^i\bm w\;(i\in\{A,B\})\)</span>. Their posterior mean is<br>
<span class="math display">\[\hat{\bm w}^{\,i}= \Sigma(Q^i)'\!\bigl(Q^i\Sigma(Q^i)'\bigr)^{-1}\bm a^i\]</span> and their posterior covariance is<br>
<span class="math display">\[\Sigma_{\mid i}= \Sigma-\Sigma(Q^i)'\!\bigl(Q^i\Sigma(Q^i)'\bigr)^{-1}Q^i\Sigma.\]</span></p>
<p><strong>Expected error for a new question <span class="math inline">\(q\)</span>.</strong> Both people answer the fresh question by <span class="math inline">\(\hat a_i = q'\hat{\bm w}^{\,i}\)</span>, while the truth is <span class="math inline">\(a=q'\bm w\)</span>. Conditioning on the training set we therefore have<br>
<span class="math display">\[
      \mathbb{E}\!\left[(a-\hat a_i)^2\;\middle|\;Q^i\right]
         =\mathbb{E}\!\left[(q'(\bm w-\hat{\bm w}^{\,i}))^2\;\middle|\;Q^i\right]
         = q'\Sigma_{\mid i}q.  \tag{1}
   \]</span></p>
<p><strong>Isotropic prior.</strong> If we specialise to <span class="math inline">\(\Sigma=\sigma^{2}I_p\)</span> then<br>
<span class="math inline">\(\Sigma_{\mid i}= \sigma^{2}\!\bigl(I-P^{i}\bigr)\)</span> with the orthogonal projector<br>
<span class="math inline">\(P^{i}= (Q^i)'\bigl(Q^i(Q^i)'\bigr)^{-1}Q^i\)</span> onto the row–span of <span class="math inline">\(Q^i\)</span>. Hence<br>
<span class="math display">\[
      \mathrm{err}_i(q)\equiv\mathbb{E}\!\left[(a-\hat a_i)^2\;\middle|\;Q^i\right]
      =\sigma^{2}\bigl\|(I-P^{i})\,q\bigr\|^{2}.
   \]</span></p>
<p>In words, the expected squared error is exactly the squared length of the component of <span class="math inline">\(q\)</span> that is <strong>orthogonal</strong> to the set of questions that person <span class="math inline">\(i\)</span> has already encountered. It is zero if <span class="math inline">\(q\)</span> is contained in the span of their past questions and grows with the distance of <span class="math inline">\(q\)</span> from that span.</p>
<p><strong>Who’s closer?</strong> Person <span class="math inline">\(A\)</span> is expected to be more accurate than <span class="math inline">\(B\)</span> precisely when<br>
<span class="math display">\[
        q'\Sigma_{\mid A}q \;&lt;\; q'\Sigma_{\mid B}q
        \qquad\bigl(\text{equivalently }
        \|(I-P^{A})q\| &lt; \|(I-P^{B})q\|\bigr).
   \]</span> Thus the tie-breaker between the two forecasters is which training set provides a better <strong>projection</strong> of the new question <span class="math inline">\(q\)</span>.</p>
</section>
</section>
</section>
<section id="references" class="level1">




</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-armengolestape2021multilingual" class="csl-entry" role="listitem">
Armengol-Estapé, Jordi, Ona de Gibert Bonet, and Maite Melero. 2021. <span>“On the Multilingual Capabilities of Very Large-Scale English Language Models.”</span> <a href="https://arxiv.org/abs/2108.13349">https://arxiv.org/abs/2108.13349</a>.
</div>
<div id="ref-bai2023persuade" class="csl-entry" role="listitem">
Bai, Hui, Jan G Voelkel, johannes C Eichstaedt, and Robb Willer. 2023. <span>“Artificial Intelligence Can Persuade Humans on Political Issues.”</span> OSF Preprints. <a href="https://doi.org/10.31219/osf.io/stakv">https://doi.org/10.31219/osf.io/stakv</a>.
</div>
<div id="ref-bowman2023eight" class="csl-entry" role="listitem">
Bowman, Samuel R. 2023. <span>“Eight Things to Know about Large Language Models.”</span> <em>arXiv Preprint arXiv:2304.00612</em>.
</div>
<div id="ref-camerer1991processperformance" class="csl-entry" role="listitem">
Camerer, Colin, and Eric J. Johnson. 1991. <span>“The Process-Performance Paradox in Expert Judgment - How Can Experts Know so Much and Predict so Badly?”</span> In. <a href="https://api.semanticscholar.org/CorpusID:67971809">https://api.semanticscholar.org/CorpusID:67971809</a>.
</div>
<div id="ref-cundy2023sequencematch" class="csl-entry" role="listitem">
Cundy, Chris, and Stefano Ermon. 2023. <span>“SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking.”</span> <em>arXiv Preprint arXiv:2306.05426</em>.
</div>
<div id="ref-cunningham2015hierarchical" class="csl-entry" role="listitem">
Cunningham, Tom. 2015. <span>“Hierarchical Aggregation of Information and Decision-Making.”</span> <em>Unpublished Manuscript, Columbia University</em>.
</div>
<div id="ref-cunningham2022implicit" class="csl-entry" role="listitem">
Cunningham, Tom, and Jonathan De Quidt. 2022. <span>“Implicit Preferences.”</span> <a href="http://jondequidt.com/pdfs/paper_implicit.pdf">http://jondequidt.com/pdfs/paper_implicit.pdf</a>.
</div>
<div id="ref-goldstein2023persuasive" class="csl-entry" role="listitem">
Goldstein, Josh A, Jason Chao, Shelby Grossman, Alex Stamos, and Michael Tomz. 2023. <span>“Can AI Write Persuasive Propaganda?”</span> SocArXiv. <a href="https://doi.org/10.31235/osf.io/fp87b">https://doi.org/10.31235/osf.io/fp87b</a>.
</div>
<div id="ref-hackenburg2023persuasive" class="csl-entry" role="listitem">
Hackenburg, Kobi, and Helen Margetts. 2023. <span>“Evaluating the Persuasive Influence of Political Microtargeting with Large Language Models.”</span> OSF Preprints. <a href="https://doi.org/10.31219/osf.io/wnt8b">https://doi.org/10.31219/osf.io/wnt8b</a>.
</div>
<div id="ref-hendrycks2021measuring" class="csl-entry" role="listitem">
Hendrycks, Dan, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. <span>“Measuring Mathematical Problem Solving with the Math Dataset.”</span> <em>arXiv Preprint arXiv:2103.03874</em>.
</div>
<div id="ref-kiela2021dynabench" class="csl-entry" role="listitem">
Kiela, Douwe, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, et al. 2021. <span>“Dynabench: Rethinking Benchmarking in NLP.”</span> <em>arXiv Preprint arXiv:2104.14337</em>.
</div>
<div id="ref-kiela2023plottingprogress" class="csl-entry" role="listitem">
Kiela, Douwe, Tristan Thrush, Kawin Ethayarajh, and Amanpreet Singh. 2023. <span>“Plotting Progress in AI.”</span> <em>Contextual AI Blog</em>.
</div>
<div id="ref-koivisto2023creativity" class="csl-entry" role="listitem">
Koivisto, Mika, and Simone Grassini. 2023. <span>“Best Humans Still Outperform Artificial Intelligence in a Creative Divergent Thinking Task.”</span> <em>Scientific Reports</em> 13 (1): 13601. <a href="https://doi.org/10.1038/s41598-023-40858-3">https://doi.org/10.1038/s41598-023-40858-3</a>.
</div>
<div id="ref-lee2023rlaif" class="csl-entry" role="listitem">
Lee, Harrison, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. 2023. <span>“RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback.”</span> <em>arXiv Preprint arXiv:2309.00267</em>.
</div>
<div id="ref-macdougall1904recognition" class="csl-entry" role="listitem">
MacDougall, Robert. 1904. <span>“Recognition and Recall.”</span> <em>The Journal of Philosophy, Psychology and Scientific Methods</em> 1 (9): 229–33.
</div>
<div id="ref-matz2023personalized" class="csl-entry" role="listitem">
Matz, Sandra, Jake Teeny, Sumer S Vaid, Gabriella M Harari, and Moran Cerf. 2023. <span>“The Potential of Generative AI for Personalized Persuasion at Scale.”</span> PsyArXiv. <a href="https://doi.org/10.31234/osf.io/rn97c">https://doi.org/10.31234/osf.io/rn97c</a>.
</div>
<div id="ref-ouyang2022training" class="csl-entry" role="listitem">
Ouyang, Long, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, et al. 2022. <span>“Training Language Models to Follow Instructions with Human Feedback.”</span> <em>Advances in Neural Information Processing Systems</em> 35: 27730–44.
</div>
<div id="ref-palmer2023large" class="csl-entry" role="listitem">
Palmer, Alexis, and Arthur Spirling. 2023. <span>“Large Language Models Can Argue in Convincing and Novel Ways about Politics: Evidence from Experiments and Human Judgement.”</span> Working paper), Technical report.
</div>
<div id="ref-qin2023large" class="csl-entry" role="listitem">
Qin, Zhen, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen, Tianqi Liu, et al. 2023. <span>“Large Language Models Are Effective Text Rankers with Pairwise Ranking Prompting.”</span> <a href="https://arxiv.org/abs/2306.17563">https://arxiv.org/abs/2306.17563</a>.
</div>
<div id="ref-stiennon2022learning" class="csl-entry" role="listitem">
Stiennon, Nisan, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2022. <span>“Learning to Summarize from Human Feedback.”</span> <a href="https://arxiv.org/abs/2009.01325">https://arxiv.org/abs/2009.01325</a>.
</div>
<div id="ref-tedeschi2023s" class="csl-entry" role="listitem">
Tedeschi, Simone, Johan Bos, Thierry Declerck, Jan Hajic, Daniel Hershcovich, Eduard H Hovy, Alexander Koller, et al. 2023. <span>“What’s the Meaning of Superhuman Performance in Today’s NLU?”</span> <em>arXiv Preprint arXiv:2305.08414</em>.
</div>
<div id="ref-towler2023facial" class="csl-entry" role="listitem">
Towler, Alice, James D. Dunn, Sergio Castro Martı́nez, Reuben Moreton, Fredrick Eklöf, Arnout Ruifrok, Richard I. Kemp, and David White. 2023. <span>“Diverse Types of Expertise in Facial Recognition.”</span> <em>Scientific Reports</em> 13 (1): 11396. <a href="https://doi.org/10.1038/s41598-023-28632-x">https://doi.org/10.1038/s41598-023-28632-x</a>.
</div>
<div id="ref-zhang2024transcendence" class="csl-entry" role="listitem">
Zhang, Edwin, Vincent Zhu, Naomi Saphra, Anat Kleiman, Benjamin L Edelman, Milind Tambe, Sham M Kakade, and Eran Malach. 2024. <span>“Transcendence: Generative Models Can Outperform the Experts That Train Them.”</span> <em>arXiv Preprint arXiv:2406.11741</em>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{cunningham2023,
  author = {Cunningham, Tom},
  title = {An {AI} {Which} {Imitates} {Humans} {Can} {Beat} {Humans}},
  date = {2023-10-06},
  url = {tecunningham.github.io/posts/2023-09-05-model-of-ai-imitation.html},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-cunningham2023" class="csl-entry quarto-appendix-citeas" role="listitem">
Cunningham, Tom. 2023. <span>“An AI Which Imitates Humans Can Beat
Humans.”</span> October 6, 2023. <a href="https://tecunningham.github.io/posts/2023-09-05-model-of-ai-imitation.html">tecunningham.github.io/posts/2023-09-05-model-of-ai-imitation.html</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("tecunningham\.github\.io");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>