<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.357">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tom Cunningham">
<meta name="dcterms.date" content="2023-09-18">
<meta name="description" content="Tom Cunningham blog">

<title>An AI Which Imitates Humans can Beat Humans | Tom Cunningham</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-12027453-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>
<script>window.MathJax = {
         loader: { load: ['[custom]/xypic.js'],
                     paths: {custom: 'https://cdn.jsdelivr.net/gh/sonoisa/XyJax-v3@3.0.1/build/'}},
      tex: {packages: {'[+]': ['xypic']},
         macros: {
            bm: ["\\boldsymbol{#1}", 1],
            bmatrix: ["\\begin{bmatrix}#1\\end{bmatrix}", 1],
            smallmatrix: ["\\begin{smallmatrix}#1\\end{smallmatrix}", 1],
            ut: ["\\underbrace{#1}_{\\text{#2}}", 2],
            utt: ["\\underbrace{#1}_{\\substack{\\text{#2}\\\\\\text{#3}}}", 3]
         }}};
</script>
<style>
   h1 {  border-bottom: 4px solid black; }
   h2 {  border-bottom: 1px solid gray; padding-bottom: 0px; color: black; }
   dl { margin-bottom: 0px; }
   dt strong { font-weight: bold; }
   dd { margin-left: 20px; }
   .cell-output-display p {padding: 0 0 1cm 0; margin: 0 0 0 0;}
</style>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="An AI Which Imitates Humans can Beat Humans | Tom Cunningham">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="tecunningham.github.io/posts/20230825131919.png">
<meta name="twitter:image-height" content="700">
<meta name="twitter:image-width" content="770">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Tom Cunningham</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href=".././about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/testingham" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/tom-cunningham-a9433/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://scholar.google.com/citations?user=MDB_DgkAAAAJ" rel="" target="">
 <span class="menu-text">scholar</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">An AI Which Imitates Humans can Beat Humans</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Tom Cunningham, <a href="https://integrityinstitute.org/">Integrity Institute</a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 18, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
  <li><a href="#setup-questions-and-answers" id="toc-setup-questions-and-answers" class="nav-link active" data-scroll-target="#setup-questions-and-answers">Setup: Questions and Answers</a></li>
  <li><a href="#graphical-argument" id="toc-graphical-argument" class="nav-link" data-scroll-target="#graphical-argument">Graphical Argument</a></li>
  <li><a href="#evidence-on-superhuman-performance" id="toc-evidence-on-superhuman-performance" class="nav-link" data-scroll-target="#evidence-on-superhuman-performance">Evidence on Superhuman Performance</a></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  <li><a href="#linear-model" id="toc-linear-model" class="nav-link" data-scroll-target="#linear-model">Linear Model</a>
  <ul class="collapse">
  <li><a href="#model-implications" id="toc-model-implications" class="nav-link" data-scroll-target="#model-implications">Model Implications</a></li>
  <li><a href="#derivation" id="toc-derivation" class="nav-link" data-scroll-target="#derivation">Derivation</a></li>
  </ul></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">





<div class="no-row-height column-margin column-container"><div class="">
<p><img src="20230825131919.png" class="img-fluid"> Thanks to comments from many, especially Grady Ward.</p>
</div></div><style>p { text-indent: -2em; margin-left: 2em; }</style>
<div class="cell page-columns page-full" data-hash="2023-09-05-model-of-ai-imitation_cache/html/unnamed-chunk-2_f379d0a95da803a0422b394c1bb6dd73">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">The ground truth (black line), human observations (red dots), and human performance (red line).</figcaption>
</figure>
</div>
</div></div></div>
<div class="page-columns page-full"><p><strong>If we train an AI to <em>imitate</em> humans, will it ever <em>beat</em> humans?</strong> Large language models (LLMs) are trained to predict what humans would say in a given situation (with some qualifications, see below). It would seem to follow that they would never be <em>better</em> at answering questions than a human.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;Many other ML models use human-generated labels as the ground truth: image recognition, language understanding, text classification, medical diagnosis.</p></li></div></div>
<p><strong>In a formal model, imitative AI can beat humans for three reasons:</strong></p>
<ol type="1">
<li><strong>Averaging out error.</strong> Different humans give different answers to a question, and so if an LLM can consistently give the average answer it will do better than the average human (the error of the average being always smaller than the average error).</li>
<li><strong>Specialization.</strong> People tend to write about what they know, and so an LLM which learns to predict the typical answer to a given question will sound like a specialist in all areas: it will answer questions about water like a hydrologist and questions about bugs like an entomologist (although it will also answer questions about astrology like astrologists).</li>
<li><strong>Interpolation.</strong> An LLM will interpolate responses from different humans, and this interpolation can be functionally equivalent to inference, meaning an LLM will sometimes be able to reliably answer questions that <em>no</em> human can answer.</li>
</ol>
<p><strong>Additionally AI can learn and use tacit human knowledge.</strong> The majority of human knowledge is tacit, meaning it is used in forming judgments but we do not have conscious access to that knowledge. If AI models can accurately predict human judgments then the weights in those models effectively contain that tacit knowledge, and so the model can be re-engineered to use that knowledge in ways that humans cannot. I will discuss tacit knowledge more in a followup post.</p>
<p><strong>This blog post contains:</strong> (1) a simple visual illustration of the argument; (2) a formal model and derivation of the claims above; (3) discussion of evidence that LLMs are able to perform super-human tasks; (4) discussion of applications, related literature, and complications. This is work in progress and I’m planning to keep adding material to this post.</p>
<section id="setup-questions-and-answers" class="level1">
<h1>Setup: Questions and Answers</h1>
<p><strong>For concreteness I will describe humans and computers answering questions about the world.</strong> However I think the basic framework applies generally to performing tasks or following instructions. Some examples of questions and answers:</p>
<table class="table">
<colgroup>
<col style="width: 93%">
<col style="width: 6%">
</colgroup>
<thead>
<tr class="header">
<th>question</th>
<th>answer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>What’s the capital of Switzerland?</td>
<td>Geneva</td>
</tr>
<tr class="even">
<td>What’s the best response if white plays c4?</td>
<td>c5</td>
</tr>
<tr class="odd">
<td>Does this picture contain a cow? 🐄</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>How much more likely are you to buy a Coke after hearing the slogan “Coke refreshes”?</td>
<td>0.1%</td>
</tr>
</tbody>
</table>
</section>
<section id="graphical-argument" class="level1">
<h1>Graphical Argument</h1>
<p><strong>(1) Let this curve represent the world.</strong> Each question about the world is a point on the x-axis, and the answer to each question is represented by the curve.</p>
<div class="cell" data-hash="2023-09-05-model-of-ai-imitation_cache/html/unnamed-chunk-4_0349531174ead0c98b69bfc1def595dd">
<div class="cell-output-display">
<p><img src="2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p><strong>(2) A human forms beliefs about the world (red curve).</strong> The human asks two questions and gets two answers (red dots) and from these they form estimates of the answer to every other question (red line).</p>
<div class="cell" data-hash="2023-09-05-model-of-ai-imitation_cache/html/unnamed-chunk-5_fc59d383f77526c17bc68e2bec0f1dc0">
<div class="cell-output-display">
<p><img src="2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p><strong>(3) A computer learns to predict the human’s answers (green curve).</strong> The human records some questions and their answers (green dots), and the computer learns to predict the human’s answers (green curve). Here I have drawn the best case, where the fit is perfect, but we would never expect the computer to do <em>better</em> than the human.</p>
<div class="cell" data-hash="2023-09-05-model-of-ai-imitation_cache/html/unnamed-chunk-6_3aaabec06efb97437e975bc9d1361abc">
<div class="cell-output-display">
<p><img src="2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p><strong>(4) Add another human (blue curve).</strong> Suppose we have an additional human who asks some different questions (blue dots) and so forms different beliefs. Each human’s beliefs are are accurate in the neighborhood of their experience.</p>
<div class="cell" data-hash="2023-09-05-model-of-ai-imitation_cache/html/unnamed-chunk-7_42dcaf0b196b8edad700cf6e15c70ff8">
<div class="cell-output-display">
<p><img src="2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p><strong>(5) Let the computer predict answers from both humans (green curve).</strong> Now both humans record their experiences and the computer tries to predict human answers (green curve). Here we can see:</p>
<ol type="1">
<li><strong>Specialization.</strong> The computer’s predictions can match the humans’ responses in each of their domains of expertise</li>
<li><strong>Interpolation.</strong> The computer is better than <em>both</em> humans in the intermediate region, i.e.&nbsp;the computer effectively combines information from both humans.</li>
</ol>
<div class="cell" data-hash="2023-09-05-model-of-ai-imitation_cache/html/unnamed-chunk-8_39ad5e6fdd16a8e57b7311f418f4e63d">
<div class="cell-output-display">
<p><img src="2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p><strong>(6) Let the human have tacit knowledge.</strong> Suppose the human always knows correct the answer when they see the question (red), but their conscious understanding of the relationship (pink) is imperfect. When asked which question would get the highest answer, i.e.&nbsp;what is the maximum of this function, then they will choose the maximum of the pink curve. However the computer could learn the red curve and algorithmically find the maximum, substantially outperforming the human.</p>
<p>In fact I think this is an accurate description of image synthesis by neural nets: models are first trained to recognize images given captions, and then a new image can be synthesized to match a given caption, e.g.&nbsp;through a diffusion algorithm. Here there’s a striking asymmetry: these algorithms can approximately match average human performance in recognition, but they far outperform human performance in construction of new artefacts.</p>
<div class="cell" data-hash="2023-09-05-model-of-ai-imitation_cache/html/unnamed-chunk-9_b4cc24ee0e5acacef3af5eac7a342101">
<div class="cell-output-display">
<p><img src="2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="evidence-on-superhuman-performance" class="level1">
<h1>Evidence on Superhuman Performance</h1>
<p><strong>Performance over time.</strong></p>
<p><a href="https://ourworldindata.org/brief-history-of-ai">Language and image recognition abilities.</a></p>
<p>However the paper that collected this data (<a href="https://arxiv.org/pdf/2104.14337.pdf">Kiela et al.</a>) argues that performance on benchmarks significantly exaggerates true performance: <em>“Models that achieve super-human performance on benchmark tasks (according to the narrow criteria used to define human performance) nonetheless fail on simple challenge examples and falter in real-world scenarios.”</em></p>
<p><img src="images/2023-09-18-08-13-45.png" class="img-fluid"></p>
<p><a href="https://ourworldindata.org/grapher/ai-performance-coding-math-knowledge-tests">Performance on coding, math &amp; knowledge tests.</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/2023-09-18-08-06-51.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">coding, math &amp; knowledge</figcaption>
</figure>
</div>
<p><a href="https://ourworldindata.org/grapher/computer-chess-ability">Chess ability</a></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="images/2023-09-18-08-10-14.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">chess ability</figcaption>
</figure>
</div>
<p><strong>Miscellaneous.</strong></p>
<ul>
<li><p><strong>Papers on persuasion.</strong> A number of papers compare the persuasive power of LLM-generated text to human-generated text (<span class="citation" data-cites="bai2023persuade">Bai et al. (<a href="#ref-bai2023persuade" role="doc-biblioref">2023</a>)</span>, <span class="citation" data-cites="goldstein2023persuasive">Goldstein et al. (<a href="#ref-goldstein2023persuasive" role="doc-biblioref">2023</a>)</span>, <span class="citation" data-cites="hackenburg2023persuasive">Hackenburg and Margetts (<a href="#ref-hackenburg2023persuasive" role="doc-biblioref">2023</a>)</span>, <span class="citation" data-cites="matz2023personalized">Matz et al. (<a href="#ref-matz2023personalized" role="doc-biblioref">2023</a>)</span>, <span class="citation" data-cites="palmer2023large">Palmer and Spirling (<a href="#ref-palmer2023large" role="doc-biblioref">2023</a>)</span>, <span class="citation" data-cites="qin2023large">Qin et al. (<a href="#ref-qin2023large" role="doc-biblioref">2023</a>)</span>). They all find that LLM does relatively well, but none show clear signs of computer superiority.</p></li>
<li><p><span class="citation" data-cites="koivisto2023creativity">Koivisto and Grassini (<a href="#ref-koivisto2023creativity" role="doc-biblioref">2023</a>)</span>: they compared GPT4 to online recruited humans (£2 for 13 minute task) in giving “creative” uses for everyday items. The prompt was to “come up with original and creative uses for an object”, objects were “rope”, “box”, “pencil” and “candle.” The responses were rated by humans for their “creativity/originality.” GPT-4 responses were perhaps 1SD above the average human score, but the difference was smaller for the best response for each user.</p></li>
</ul>
</section>
<section id="discussion" class="level1 page-columns page-full">
<h1>Discussion</h1>
<p><strong>Training on custom-written text.</strong> Recent LLMs don’t train just on predicting existing text (books, internet, twitter) they also use datasets of instructions and responses generated by paid raters (<span class="citation" data-cites="ouyang2022training">Ouyang et al. (<a href="#ref-ouyang2022training" role="doc-biblioref">2022</a>)</span>). We can still call this imitation but it’s putting relatively more weight on imitating the responses of specific set of people, the paid raters. This significantly improves performance on most benchmarks, but I think it also has costs: the model is now predicting output of a specific set of people (i.e.&nbsp;non-expert paid raters), and so conceivably will do less well at incorporating niche information available to an expert.</p>
<div class="page-columns page-full"><p><strong>Training on evaluation instead of production.</strong> Recent LLMs are not purely imitative, e.g.&nbsp;OpenAI’s GPT models are trained with human <em>evaluation</em> of their responses (called reinforcement learning with human feedback (RLHF).<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> The key difference is that the goal reflects how humans rate responses, rather than how humans generate responses. In some domains the two functions might be identical, but in others there’s a clear difference. This can have both costs and benefits: .</p><div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;E.g. see OpenAI’s 2022 InstructGPT (<span class="citation" data-cites="ouyang2022training">Ouyang et al. (<a href="#ref-ouyang2022training" role="doc-biblioref">2022</a>)</span>). Technically the reinforcement learning is against a model trained to predict the human evaluation of outputs.</p></li></div></div>
<p>This dramatically improves performance on instruction-following benchmarks. This is different from imitative learning insofar as humans have an asymmetry between their production and evaluation of responses: between how they follow an instruction and how they evaluate other people following that instruction.</p>
<p></p>
<p><strong>The importance of tacit knowledge.</strong> The relative success of machine learning over symbolic AI has often been connected to the importance of tacit human knowledge, e.g.&nbsp;by Geoff Hinton. I have written a lot about the importance of tacit knowledge in human decision-making (<a href="https://tecunningham.github.io/posts/2017-12-10-unconscious-influences.html">post</a>, <a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=MDB_DgkAAAAJ&amp;citation_for_view=MDB_DgkAAAAJ:WF5omc3nYNoC">paper</a>, <a href="http://jondequidt.com/pdfs/paper_implicit.pdf">paper</a>).</p>
<p><strong>Do current LLMs outperform humans?</strong> I will consider GPT-4 as a specific LLM.</p>
<ol type="1">
<li><strong>For any given human, GPT-4 clearly can beat them on some domains.</strong> GPT-4 has an encylopedic ability to answer questions, so for any individual human they can clearly outperform them on a large set of questions.</li>
<li><strong>Modern LLMs can translate between two languages with no common speaker.</strong> Modern language models can translate between arbitrary pairs of language. I haven’t seen explicit confirmation of this, but I assume this includes pairs for which there exists no common speaker and I assume that a purely imitative LLM would be able to do at least rudimentary translation.</li>
<li><strong>Modern LLMs can combine capabilities.</strong> GPT-4 is good at combining two different capabilities, e.g.&nbsp;writing a sonnet about quantum theory, or a ballad in Spanish about New Zealand. It seems likely there are many such combination tasks which no single human could perform unaided.</li>
</ol>
<p><strong>Other discussion of super-human performance by LLMs.</strong> I have found surprisingly little online or academic discussion about whether LLMs will hit a ceiling defined by human performance. A good paper by <span class="citation" data-cites="bowman2023eight">Bowman (<a href="#ref-bowman2023eight" role="doc-biblioref">2023</a>)</span> has a section titled “human performance on a task isn’t an upper bound on LLM performance.” He says LLMs can outperform humans for two reasons: (1) “they are trained on far more data than any human sees,” and (2) “they are often given additional training using reinforcement learning … which trains them to produce responses that humans find helpful without requiring humans to demonstrate such helpful behavior.” I think this discussion misses the more fine-grained reasons why a purely imitative LLM can outperform humans (averaging, specialization, interpolation, and using tacit knowledge).</p>
<div class="page-columns page-full"><p><strong>Computers can never outperform the best human on language understanding tasks.</strong> The ground truth about language interpretation is defined by how humans interpret that language, and so a computer can never outperform this.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> E.g. in benchmarks for natural language understanding the labels are typically written by the human authors of the benchmark (<span class="citation" data-cites="tedeschi2023s">Tedeschi et al. (<a href="#ref-tedeschi2023s" role="doc-biblioref">2023</a>)</span>). In content moderation the definition of ground truth is typically either majority-vote among paid human raters, or the reflective judgment of a senior human employee. It follows that a computer could outperform the average human only by reducing noise, or by better-approximating the output of a specific human.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;There are arguably exceptions but I don’t think they are quantitatively important. Consider the sentence “Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo”: it does have at least one well-defined meaning according to the typical rules of English but arguably no human would correctly identify that meaning unless specifically prompted. A computer trained only on human comprehension could plausibly identify its meaning.</p></li><li id="fn4"><p><sup>4</sup>&nbsp;Many recent language models do outperform the average human baseline on language understanding tasks, but <span class="citation" data-cites="tedeschi2023s">Tedeschi et al. (<a href="#ref-tedeschi2023s" role="doc-biblioref">2023</a>)</span> argue that for a variety of reasons the strength of these results is significantly exaggerated.</p></li></div></div>
<p>The same point applies to computer performance on exam problems: in almost all cases the person who wrote the exam can get a perfect score and so a computer could never outperform that person.</p>
<div class="page-columns page-full"><p><strong>Computers can outperform humans on recognition tasks.</strong> Most benchmarks for media recognition use human labels, e.g.&nbsp;for object detection in photos or speech recognition in audio. However humans can be mistaken: they might incorrectly think a photo has a dog in it when it does not or vice versa. Humans are very good at recognition, so human-labelled data has been sufficient, but computers are liable to exceed human performance on these tasks.<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn5"><p><sup>5</sup>&nbsp;In fact defining ground truth is somewhat complicated because an image is consistent with an infinite variety of objects causing that arrangement of pixels, we can only talk about an image representing an object because of priors about typical scenes. So the ground truth of interest must be something like “in ordinary circumstances, what is the probability that these pixels would be caused by a scene with a dog in them.”</p></li></div></div>
<p><strong>The graphical model underplays the importance of architecture.</strong> The graphical model shown above represents both questions and answers as unidimensional, and it makes it seem that a small sample is sufficient to get a reasonably good approximation of the true function. In reality the questions are very high dimensional and most architectures fail to get a good fit. Neural nets with transformer structure have had remarkable success in fitting the data.</p>
<p><strong>Computers can outperform humans by averaging out error (“wisdom of crowds”).</strong> An additional reason for superhuman performance, not illustrated in the graphical argument above, is that they can average out variance in how humans report their answers, whether that variance is due to noise in perception, belief formation, or in recording human answers. We could illustrate this with multiple red lines at step 2: it’s clear that if the computer tries to predict the average human response then it will end up closer to the ground truth than the average human. More generally: the average error will necessarily be larger than the error of the average, by Jensen’s inequality.</p>
<div class="page-columns page-full"><p><strong>Specalization depends on people writing what they know.</strong> The “specialization” result above depends on a correlation between evidence and error: for any given question the answer observed in the training data is more likely to come from a human who has direct experience (and so lower error). This seems a reasonable assumption, that people typically write about what they know. However there are a clear cases where the opposite occurs: where the people who write the most about a topic are the people who have the least accurate beliefs about it, e.g.&nbsp;conspiracy theories, politically partisan issues, or astrology. In these cases we would expect AI to answer question copying these people.<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn6"><p><sup>6</sup>&nbsp;Analogically, in music or visual art, there might be some genres where the people who create artworks are uniquely bad at it, and so in these genres imitative AI would learn to make artworks worse than the average human would make.</p></li></div></div>
<p><strong>Alternative definitions of superhuman abilities.</strong> We can formalize “superhuman” abilities in a variety of ways, e.g.:</p>
<p><span class="math display">\[\begin{aligned}
      \text{weak:}&amp;&amp; \ut{E[a(q)-\bar{a}(q)]^2}{error of computer}
         &amp;\leq  \ut{\frac{1}{n}\sum_{i=1}^nE[a(q)-\hat{a}_i(q)]^2}{avg error of human}\\
      \text{medium:}&amp;&amp; \ut{E[a(q)-\bar{a}(q)]^2}{error of computer}
         &amp;\leq  \ut{E\left[a(q)-\frac{1}{n}\sum_{i=1}^n\hat{a}_i(q)\right]^2}{error of avg human}\\
      \text{strong:}&amp;&amp; \ut{E[a(q)-\bar{a}(q)]^2}{error of computer}
         &amp;\leq  \ut{\min_{i=1,\ldots,n}E[a(q)-\hat{a}_i(q)]^2}{error of best human}\\
      \text{super-strong:}&amp;&amp; \ut{E[a(q)-\bar{a}(q)]^2}{error of computer}
         &amp;\leq  \ut{E[\min_{i=1,\ldots,n}\{a(q)-\hat{a}_i(q)\}]^2}{error of best human by question}\\
   \end{aligned}\]</span></p>
<p>Note that we cannot rank benchmark #2 and #3: the error of the best human could be either higher or lower than the error of the average human.</p>
<p><strong>AI could be used to create super-human artefacts.</strong> There has been a lot of discussion about whether AI be used to exceed human abilities to create certain artefacts, e.g.&nbsp;hyper-beautiful paintings, hyper-addictive clickbait, hyper-persuasive text. Using the existing architecture of LLMs, which predict human outputs, this is unlikely: they might be able to predict the objects that a very talented human would produce, but would not surpass their ability. However AI could be used to generate artfeacts indirectly: instead of predicting how a human would create such an arfect, the weights that the AI has learned could be used to find the artefact which maximizes the function of interest, e.g.&nbsp;hyper-persuasive, or hyper-beautiful. In this way the AI could create artefacts beyond the ability of any human. This is roughly how image synthesis algorithms work: after training a model to recognize images, new images can be created by maximizing the learned weights.</p>
<p><strong>Economic implication: imitative AI brings everybody to the knowledge frontier.</strong> An imitative LLM can effectively serve as a consultant: it will tell you how a domain-expert would answer your question. For some problems we’ve already been able to do that for a long time, e.g.&nbsp;a textbook will tell you scientific advice on crop management practices, but LLMs allow for much subtler contingencies and to incorporate tacit knowledge. We should therefore expect the productivity impact of LLMs to be mainly on those who are behind the knowledge frontier, and we might also expect a compression of incomes. </p>
<div class="cell page-columns page-full" data-hash="2023-09-05-model-of-ai-imitation_cache/html/unnamed-chunk-10_cb8c4fc2e93fe9ef20d44cff13e8f051">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<p><img src="2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid" width="672"></p>
</div></div></div>
<p><strong>Venn diagram representation.</strong> This diagram shows an alternative way of representing some of the core claims: in general the questions answerable by an LLM will not be a subset of the questions answered in the training set, or even those answerable by the people who contributed to the training set. The Venn diagram’s disadvantage, relative to the visualizations above, is that it does not represent the mechanics of <em>why</em> the LLM can outperform humans, while the diagram above can be use to separately show four distinct reasons (averaging error, specialization, interpolation, and using tacit knowledge).</p>
<div class="page-columns page-full"><p><strong>Imitation learning in dynamic problems.</strong> The discussion in this note has been about a purely static problem of supplying answers to questions, but text generation can also be considered as a dynamic problem of sequentially generating tokens. A common observation regarding dynamic policy is that pure prediction of expert behaviour (“behavioural cloning”) is not very robust, and this has been used to explain weaknesses in the behaviour of autoregressive generative text models.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn7"><p><sup>7</sup>&nbsp;<span class="citation" data-cites="cundy2023sequencematch">Cundy and Ermon (<a href="#ref-cundy2023sequencematch" role="doc-biblioref">2023</a>)</span> say “[the] simple behaviour cloning approach results in a compounding error problem, where the further the trained model gets from the typical expert states, the worse the model performs, incurring increasing error.” I also found <a href="https://web.stanford.edu/class/cs237b/pdfs/lecture/cs237b_lecture_12.pdf">these notes</a> from Stanford’s CS273B useful.</p></li></div></div>
</section>
<section id="linear-model" class="level1 page-columns page-full">
<h1>Linear Model</h1>
<p><strong>Here I give a more formal model and derive some results.</strong> I wrote this model before coming up with the graphical argument above. There is a substantial overlap in implications, but I think there is some value in this linear model in the precision with which we define each quantity. The model has three steps:</p>
<p><span class="math display">\[\xymatrix@C=.5cm@R=0cm{
      \text{world}
      &amp;&amp; \text{human}
      &amp;&amp; \text{LLM}\\
      *+[F:&lt;5pt&gt;]{\bm{w}} \ar[rr]^{\bm{a}=Q\bm{w}}
      &amp;&amp; *+[F:&lt;5pt&gt;]{\hat{\bm{w}}} \ar[rr]^{\hat{\bm{a}}=\hat{Q}\hat{\bm{w}}}
      &amp;&amp; *+[F:&lt;5pt&gt;]{\bar{\bm{w}}} \ar[rr]^{\tilde{a}=\tilde{q}'\bar{\bm{w}}}
      &amp;&amp; \  \\
      \txt{unobserved\\truth\\about\\the\\world}
      &amp; \txt{answers\\to\\human\\questions}
      &amp; \txt{beliefs\\formed\\by\\human}
      &amp; \txt{text\\written\\by\\human}
      &amp; \txt{LLM\\model\\of\\human\\text}
      &amp; \txt{LLM's\\answers\\to\\new\\questions}
   }\]</span></p>
<p><strong>Questions and answers.</strong> A question is defined by a set of binary attributes (<span class="math inline">\(q_1,\ldots,q_n\in\{-1,1\}\)</span>), and the answer is a linear function of those attributes given some unobserved weights <span class="math inline">\(w_1,\ldots,w_n\)</span> (see below for derivation):</p>
<p><span class="math display">\[\begin{aligned}
      \ut{\bmatrix{a^1 \\ \vdots \\ a^m}}{answers}
         = \ut{\bmatrix{q_1^1 w_1 + \ldots q_n^1w_n \\ \vdots \\ q_1^m w_1 + \ldots q_n^mw_n}}{questions}
   \end{aligned}\]</span></p>
<div class="page-columns page-full"><p><strong>Human beliefs.</strong> After observing a set of question and their real-world answers the human will form beliefs about the weights <span class="math inline">\(w_1,\ldots,w_n\)</span>. We can explicitly write the human posteriors if we assume their priors are Gaussian and i.i.d. (<span class="math inline">\(\bm{w}\sim N(0,\sigma^2I)\)</span>):<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> <span class="math display">\[\begin{aligned}
      \bm{a}        &amp;= Q\bm{w}
         &amp;&amp; \text{(questions \&amp; answers given true weights $\bm{w}$)}\\
      \hat{\bm{w}} &amp;= Q'(QQ')^{-1}\bm{a}
         &amp;&amp; \text{(human estimate of true weights $\bm{w}$)}
   \end{aligned}\]</span></p><div class="no-row-height column-margin column-container"><li id="fn8"><p><sup>8</sup>&nbsp;I am assuming <span class="math inline">\(\bm{w}\)</span> has zero-mean and is i.i.d. just to cut down on notation, the results all hold for the more general multivariate Normal case.</p></li></div></div>
<p>I will assume that the number of unobserved weights is large relative to the human’s experience (<span class="math inline">\(n\gg m\)</span>), so the human will gradually learn more about reality as she observes the answer to more questions, and will be able to perfectly answer any question she’s seen before, but will never learn the full set of weights.</p>
<p><strong>Computer beliefs.</strong> Suppose that humans write down some set of question and answer them given their beleifs. We use that text to train a computer to predict the answer given any question, and the computer likewise assumes a linear model with i.i.d. Gaussian weights. Then we can explicitly write the computer-estimated weights:</p>
<p><span class="math display">\[\begin{aligned}
      \hat{\bm{a}}        &amp;= \hat{Q}\hat{\bm{w}}
         &amp;&amp; \text{(human-generated questions \&amp; answers)}\\
      \bar{\bm{w}} &amp;= \hat{Q}'(\hat{Q}\hat{Q}')^{-1}\hat{\bm{a}}
         &amp;&amp; \text{(computer estimate of human weights $\hat{\bm{w}}$)}
   \end{aligned}\]</span></p>
<p><strong>Computer answers.</strong> Finally we can ask the computer a new question, <span class="math inline">\(\tilde{q}\)</span>, and observe its answer: <span class="math display">\[\begin{aligned}
      \tilde{a}        &amp;= \tilde{\bm{q}}'\bar{\bm{w}}
         &amp;&amp; \text{(computer answer to a novel question $\tilde{\bm{q}}$)}\\
   \end{aligned}\]</span></p>
<section id="model-implications" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="model-implications">Model Implications</h2>
<dl>
<dt><strong>If one human records all their observations then the computer will perfectly imitate them.</strong></dt>
<dd>
Suppose that there is one human and they write down all of their observations, <span class="math inline">\(\hat{Q}=Q\)</span>. Then the computer’s beliefs will be the same as the human’s beliefs (<span class="math inline">\(\hat{\bm{w}}=\bar{\bm{w}}\)</span>), and so the computer will answer every question exactly as the human does, though neither knows the truth (<span class="math inline">\(\bar{\bm{w}}\neq\bm{w}\)</span>).
</dd>
<dt><strong>If humans do not record all their observation then the computer will perform worse.</strong></dt>
<dd>
Suppose humans only write down some of their observations, i.e.&nbsp;<span class="math inline">\(\hat{Q}\)</span> is a row-wise subset of <span class="math inline">\(Q\)</span>. Then computers and humans will give the same answers for any question in the training set, but outside of that set computers will generally do worse than humans. And so for every question <span class="math inline">\(\bm{q}\)</span> the computer will do worse in expectation: <span class="math display">\[E[\ut{(\bm{q}\bm{a}-\bm{q}\bar{\bm{a}})^2}{computer error}]\geq
  E[\ut{\bm{q}(\bm{a}-\hat{\bm{a}})}{human error}].\]</span>
</dd>
</dl>
<dl class="page-columns page-full">
<dt><strong>If there are multiple humans then the computer will outperform them both.</strong></dt>
<dd>
Suppose there are two humans who each observe answers to different question, <span class="math inline">\(Q_A\)</span> and <span class="math inline">\(Q_B\)</span>, and they both write them all down, so <span class="math inline">\(\bar{Q}=(\smallmatrix{Q_A\\Q_B})\)</span> and <span class="math inline">\(\bar{\bm{a}}=(\smallmatrix{Q_A\bm{w}\\Q_B\bm{w}})\)</span>. Now the computer clearly has superior information to either human, and so for both <span class="math inline">\(i\in\{1,2\}\)</span> and every question <span class="math inline">\(\bm{q}\)</span> we can write: <span class="math display">\[E[\ut{(\bm{q}\bm{a}-\bm{q}\bar{\bm{a}})^2}{computer error}]\leq
  E[\ut{\bm{q}(\bm{a}-\hat{\bm{a}}_i)}{human error}].\]</span>
</dd>
<dt><strong>If there are multiple humans then the computer can answer question no human can answer.</strong></dt>
<dd>
<p>Suppose two humans observe the answers to the following questions: <span class="math display">\[\begin{aligned}
  Q_A &amp;= \bmatrix{1 &amp; 1 &amp; 1 \\ 1 &amp; -1 &amp; 1} \\
  Q_B &amp;= \bmatrix{1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; -1}
   \end{aligned}\]</span> The first human will learn the exact value of <span class="math inline">\(w_2\)</span>, and the second human will learn the exact value of <span class="math inline">\(w_3\)</span>, but neither will learn both, and so neither could perfectly predict the answer to this question: <span class="math display">\[\begin{aligned}
     \tilde{q} &amp;= \bmatrix{1 &amp; -1 &amp; -1} \\
  \end{aligned}\]</span></p>
<p>However if they both recorded their observations, so the computer observes <span class="math inline">\(\bar{\bm{a}}=(\smallmatrix{Q_1\bm{w}\\Q_2\bm{w}})\)</span>, the computer will be able to infer both <span class="math inline">\(w_2\)</span> and <span class="math inline">\(w_3\)</span>, and thus will be able to perfectly answer <span class="math inline">\(\tilde{q}\)</span>.</p>
<p>We can see this behaviour in LLMs: they sometimes combine a pair of facts or a pair of abilities which no single human has access to, e.g.&nbsp;when an LLM translates between two languages, for which there exists no human speaker of both.</p>
</dd>
<dt><strong>If humans write outside their expertise then the computer will do worse.</strong></dt>
<dd class="page-columns page-full">
<div class="page-columns page-full"><p>In the cases above we assumed that the two humans recorded only what they directly observed. This means the computer essentially had a window directly to the world. However the humans could instead have written down their estimated answers to other questions. Suppose they both wrote down answers to every possible question, then the computer would learn the <em>average</em> of the two human’s estimated weights:<a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a> <span class="math display">\[\bar{\bm{w}}=\frac{1}{2}\hat{\bm{w}}_A+\frac{1}{2}\hat{\bm{w}}_B.\]</span></p><div class="no-row-height column-margin column-container"><li id="fn9"><p><sup>9</sup>&nbsp;We would have to augment the computer’s learning rule to allow for noise in answers - I need to confirm that the weighting will be exactly 1/2.</p></li></div></div>
<p>Here the computer will do worse than the two humans on the original questions, <span class="math inline">\(Q_A\)</span> and <span class="math inline">\(Q_B\)</span>.</p>
<p>The implication is that LLMs work so well only because people tend to write about what they know. Put another way, when an LLM answers a question, it will not predict the answer given by the average person, but will predict the answer given by people who are likely to answer that question in the real world, and luckily those people tend to be people who are subject-matter experts.</p>
</dd>
</dl>
<dl>
<dt><strong>If humans have tacit knowledge, the computer model can outperform humans in creation of new artefacts.</strong></dt>
<dd>
<p>Suppose humans have tacit knowledge of the world, we can model this with two separate sets of beliefs: <span class="math display">\[\begin{aligned}
  \hat{\bm{w}}^T   &amp;= \text{tacit knowledge}\\
  \hat{\bm{w}}^E &amp;= \text{explicit knowledge}\\
   \end{aligned}\]</span></p>
<p>When the human encounters a new question <span class="math inline">\(\tilde{\bm{q}}\)</span> they will use their tacit knowledge to form an estimate of the answer, <span class="math inline">\(\hat{a}=\tilde{\bm{q}}'\hat{\bm{w}}^T\)</span>. For simplicity assume tacit knowledge is perfectly accurate <span class="math inline">\(\hat{\bm{w}}^T=\bm{w}\)</span>, and explicit knowledge is imperfect.</p>
<p>The distinction becomes important when we want to create a new question. Here it’s useful to interpret <span class="math inline">\(\bm{q}\)</span> not as a question but as an artefact, e.g.&nbsp;text or image, and <span class="math inline">\(\bm{a}=\bm{q}'\bm{w}\)</span> represents some abstract property, e.g.&nbsp;how beautiful or how rhythmic. Suppose we want to chosose <span class="math inline">\(\bm{q}\in\{-1,1\}^n\)</span> to maximize <span class="math inline">\(\bm{w}\bm{q}\)</span>. If we had perfect access to our beliefs <span class="math inline">\(\bm{w}^T\)</span> this would be simple, however if we have access only to imperfect explicit knowledge <span class="math inline">\(\hat{\bm{w}}^E\)</span>, the artefact which maximizes that function will not generally be the one which maximizes <span class="math inline">\(a\)</span>.</p>
<p>Here the computer model is less constrained. Suppose the computer has observed sufficiently many questions until they have perfectly learned the tacit knowledge, <span class="math inline">\(\bar{\bm{w}}=\hat{\bm{w}}^T\)</span>. Then if computation is free the computer could be used to query every single <span class="math inline">\(\bm{q}\in\{-1,1\}^n\)</span> to find the best possible artefact.</p>
</dd>
</dl>
</section>
<section id="derivation" class="level2">
<h2 class="anchored" data-anchor-id="derivation">Derivation</h2>
<p><span class="math display">\[\begin{aligned}
   Q &amp;= \bmatrix{q_1^1 &amp; \ldots &amp; q^1_n \\ &amp; \ddots \\ q^m_1 &amp; \ldots &amp; q^m_n}
      &amp;&amp; \text{(set of questions)} \\
   w'  &amp;= \bmatrix{w_1 \ldots w_n} \\
   a    &amp;= \bmatrix{a^1 \\ \vdots \\ a^m}
      = \bmatrix{q_1^1 w_1 + \ldots q_n^1w_n \\ \vdots \\ q_1^m w_1 + \ldots q_n^mw_n} \\
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
      \bm{w} &amp;\sim N(0,\Sigma)
         &amp;&amp; (n\times 1\text{ vector of parameters of the world)}\\
      Q      &amp;\in \{-1,1\}^{n\times m}
         &amp;&amp; \text{($m$ questions, each has $n$ binary parameters)}\\
      \ut{\bm{a}}{$m\times1$}   &amp;= \ut{Q}{$m\times n$}\ut{\bm{w}}{$n\times1$}
         &amp;&amp; \text{(answers provided by the world)}\\
      \hat{\bm{w}} &amp;= E[\bm{w}|Q,\bm{a}]
            &amp;&amp; \text{(human beliefs about the world)}\\
         &amp;= \ut{\Sigma Q'}{$Cov(\bm{w},\bm{a})$}
            (\ut{Q\Sigma Q'}{$Var(\bm{a})$})^{-1}
            \bm{a}
   \end{aligned}\]</span></p>
<p><strong>With one observation and two weights.</strong> Suppose <span class="math inline">\(m=1, n=1\)</span>, then we have: <span class="math display">\[\begin{aligned}
      Q  &amp;= \bmatrix{q_1 &amp; q_2} \\
      \bm{a}'  &amp;= \bmatrix{a} \\
      \bm{w}'  &amp;= \bmatrix{w_1 &amp; w_2 } \\
      \Sigma &amp;= \bmatrix{\sigma_1^2 &amp; \rho \\ \rho &amp; \sigma_2^2}\\
      \Sigma Q' &amp;= \bmatrix{ \sigma_1^2q_1 + \rho q_2 \\ \rho q_1 + \sigma_2^2 q_2 } \\
      Q\Sigma Q' &amp;= \bmatrix{ \sigma_1^2q_1^2 + 2\rho q_1q_2 + \sigma_2^2 q_2^2} \\
      \hat{\bm{w}}=\Sigma Q'(Q\Sigma Q')^{-1}\bm{a}
         &amp;= \bmatrix{ \frac{\sigma_1^2q_1 + \rho q_2}{\sigma_1^2q_1^2 + 2\rho q_1q_2 + \sigma_2^2 q_2^2} \\
                  \frac{\rho q_1 + \sigma_2^2 q_2}{\sigma_1^2q_1^2 + 2\rho q_1q_2 + \sigma_2^2 q_2^2}} a
   \end{aligned}
   \]</span></p>
<p>We can normalize <span class="math inline">\(q_1=q_2=1\)</span>, then we have <span class="math display">\[\hat{w}_1 = \frac{\sigma_1^2+\rho}{\sigma_1^2+2\rho+\sigma_2^2}a,\]</span></p>
<p>This means we are dividing up responsibility for the answer (<span class="math inline">\(a\)</span>) into the contributions of each component, nice and simple.</p>
<p><strong>With two observations and one weight.</strong> Here we’re <em>over-identified</em>. <span class="math display">\[\begin{aligned}
      Q  &amp;= \bmatrix{q^1 \\ q^2} \\
      \bm{a}  &amp;= \bmatrix{a^1 \\ a^2} \\
      \bm{w}  &amp;= \bmatrix{w } \\
      \Sigma &amp;= \bmatrix{\sigma^2 }\\
      \Sigma Q' &amp;= \bmatrix{ \sigma^2 q^1 &amp; \sigma^2 q^2 } \\
      Q\Sigma Q' &amp;= \bmatrix{ \sigma^2 q^1q^1 &amp; \sigma^2q^1q^2 \\ \sigma^2q^1q^2 &amp; \sigma^2q^2q^2}
         &amp;&amp; \text{(this matrix doesn't have an inverse)}
   \end{aligned}
   \]</span></p>
<p><strong>With noise.</strong> Suppose we only observe the answers with random noise, then we get this: <span class="math display">\[\begin{aligned}
   \ut{\bm{a}}{$m\times1$}   &amp;= \ut{Q}{$m\times n$}\ut{\bm{w}}{$n\times1$}
      + \ut{\bm{e}}{$n\times 1$} \\
   \bm{e} &amp;\sim N(\bm{0},s^2I_m) &amp;&amp; \text{(i.i.d. noise w variance $s^2$)}\\
   Cov(\bm{w},\bm{a})   &amp;= \Sigma Q' \\
   Var(\bm{a}) &amp;= Q\Sigma Q' + s^2I_m \\
   E[\bm{w}|Q,\bm{q}]   &amp;= \Sigma Q'(Q\Sigma Q' + s^2I_m)^{-1}\bm{a}
\end{aligned}\]</span></p>
<p><strong>Compare to Bayesian linear regression.</strong> We can compare this result to Bayesian linear regression (e.g.&nbsp;<a href="https://en.wikipedia.org/wiki/Bayesian_linear_regression">Wikipedia</a>): <span class="math display">\[\begin{aligned}
      \bar{\beta}  &amp;= \Sigma Q'(Q\Sigma Q' + s^2I_m)^{-1}\bm{a}
         &amp;&amp; \text{(our result)} \\
      \tilde{\beta} &amp;= (Q'Q+s^{2}\Sigma^{-1})^{-1}Q'\bm{a}
         &amp;&amp; \text{(Bayesian linear regression)}\\
   \end{aligned}\]</span></p>
<p>I <em>believe</em> that these can be shown to be equivalent by the <a href="https://en.wikipedia.org/wiki/Woodbury_matrix_identity">matrix inversion lemma</a>, though I haven’t confirmed this. There’s a <a href="https://digitalcommons.usu.edu/cgi/viewcontent.cgi?article=1212&amp;context=ece_facpub">note online</a> that appears to show equivalence.</p>
</section>
</section>
<section id="references" class="level1">




</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-bai2023persuade" class="csl-entry" role="listitem">
Bai, Hui, Jan G Voelkel, johannes C Eichstaedt, and Robb Willer. 2023. <span>“Artificial Intelligence Can Persuade Humans on Political Issues.”</span> OSF Preprints. <a href="https://doi.org/10.31219/osf.io/stakv">https://doi.org/10.31219/osf.io/stakv</a>.
</div>
<div id="ref-bowman2023eight" class="csl-entry" role="listitem">
Bowman, Samuel R. 2023. <span>“Eight Things to Know about Large Language Models.”</span> <em>arXiv Preprint arXiv:2304.00612</em>.
</div>
<div id="ref-cundy2023sequencematch" class="csl-entry" role="listitem">
Cundy, Chris, and Stefano Ermon. 2023. <span>“SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking.”</span> <em>arXiv Preprint arXiv:2306.05426</em>.
</div>
<div id="ref-goldstein2023persuasive" class="csl-entry" role="listitem">
Goldstein, Josh A, Jason Chao, Shelby Grossman, Alex Stamos, and Michael Tomz. 2023. <span>“Can AI Write Persuasive Propaganda?”</span> SocArXiv. <a href="https://doi.org/10.31235/osf.io/fp87b">https://doi.org/10.31235/osf.io/fp87b</a>.
</div>
<div id="ref-hackenburg2023persuasive" class="csl-entry" role="listitem">
Hackenburg, Kobi, and Helen Margetts. 2023. <span>“Evaluating the Persuasive Influence of Political Microtargeting with Large Language Models.”</span> OSF Preprints. <a href="https://doi.org/10.31219/osf.io/wnt8b">https://doi.org/10.31219/osf.io/wnt8b</a>.
</div>
<div id="ref-koivisto2023creativity" class="csl-entry" role="listitem">
Koivisto, Mika, and Simone Grassini. 2023. <span>“Best Humans Still Outperform Artificial Intelligence in a Creative Divergent Thinking Task.”</span> <em>Scientific Reports</em> 13 (1): 13601. <a href="https://doi.org/10.1038/s41598-023-40858-3">https://doi.org/10.1038/s41598-023-40858-3</a>.
</div>
<div id="ref-matz2023personalized" class="csl-entry" role="listitem">
Matz, Sandra, Jake Teeny, Sumer S Vaid, Gabriella M Harari, and Moran Cerf. 2023. <span>“The Potential of Generative AI for Personalized Persuasion at Scale.”</span> PsyArXiv. <a href="https://doi.org/10.31234/osf.io/rn97c">https://doi.org/10.31234/osf.io/rn97c</a>.
</div>
<div id="ref-ouyang2022training" class="csl-entry" role="listitem">
Ouyang, Long, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, et al. 2022. <span>“Training Language Models to Follow Instructions with Human Feedback.”</span> <em>Advances in Neural Information Processing Systems</em> 35: 27730–44.
</div>
<div id="ref-palmer2023large" class="csl-entry" role="listitem">
Palmer, Alexis, and Arthur Spirling. 2023. <span>“Large Language Models Can Argue in Convincing and Novel Ways about Politics: Evidence from Experiments and Human Judgement.”</span> Working paper), Technical report.
</div>
<div id="ref-qin2023large" class="csl-entry" role="listitem">
Qin, Zhen, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen, Tianqi Liu, et al. 2023. <span>“Large Language Models Are Effective Text Rankers with Pairwise Ranking Prompting.”</span> <a href="https://arxiv.org/abs/2306.17563">https://arxiv.org/abs/2306.17563</a>.
</div>
<div id="ref-tedeschi2023s" class="csl-entry" role="listitem">
Tedeschi, Simone, Johan Bos, Thierry Declerck, Jan Hajic, Daniel Hershcovich, Eduard H Hovy, Alexander Koller, et al. 2023. <span>“What’s the Meaning of Superhuman Performance in Today’s NLU?”</span> <em>arXiv Preprint arXiv:2305.08414</em>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>