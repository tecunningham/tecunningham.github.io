<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.357">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tom Cunningham">
<meta name="dcterms.date" content="2023-09-12">
<meta name="description" content="Tom Cunningham blog">

<title>How LLMs Learn About the World | Tom Cunningham</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-12027453-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>
<script>window.MathJax = {
         loader: { load: ['[custom]/xypic.js'],
                     paths: {custom: 'https://cdn.jsdelivr.net/gh/sonoisa/XyJax-v3@3.0.1/build/'}},
      tex: {packages: {'[+]': ['xypic']},
         macros: {
            bm: ["\\boldsymbol{#1}", 1],
            bmatrix: ["\\begin{bmatrix}#1\\end{bmatrix}", 1],
            smallmatrix: ["\\begin{smallmatrix}#1\\end{smallmatrix}", 1],
            ut: ["\\underbrace{#1}_{\\text{#2}}", 2],
            utt: ["\\underbrace{#1}_{\\substack{\\text{#2}\\\\\\text{#3}}}", 3]
         }}};
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="How LLMs Learn About the World | Tom Cunningham">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="tecunningham.github.io/posts/images/2023-09-05-19-11-49.png">
<meta name="twitter:image-height" content="2110">
<meta name="twitter:image-width" content="1383">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Tom Cunningham</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href=".././about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/testingham" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/tom-cunningham-a9433/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://scholar.google.com/citations?user=MDB_DgkAAAAJ" rel="" target="">
 <span class="menu-text">scholar</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">How LLMs Learn About the World</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Tom Cunningham, <a href="https://integrityinstitute.org/">Integrity Institute</a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 12, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
  <li><a href="#graphical-argument" id="toc-graphical-argument" class="nav-link active" data-scroll-target="#graphical-argument">Graphical Argument</a></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion">Discussion</a></li>
  <li><a href="#model-setup" id="toc-model-setup" class="nav-link" data-scroll-target="#model-setup">Model Setup</a></li>
  <li><a href="#model-implications" id="toc-model-implications" class="nav-link" data-scroll-target="#model-implications">Model Implications</a></li>
  <li><a href="#derivation" id="toc-derivation" class="nav-link" data-scroll-target="#derivation">Derivation</a></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">




<style>
    h1 {  border-bottom: 4px solid black; }
    h2 {  border-bottom: 1px solid gray; padding-bottom: 0px; color: black; }
    dl { margin-bottom: 0px; }
    dt strong { font-weight: bold; }
    dd { margin-left: 20px; }
    .cell-output-display p {padding: 0 0 0 0; margin: 0 0 0 0;}
</style>

<div class="no-row-height column-margin column-container"><div class="">
<p> <img src="images/2023-09-05-19-11-49.png" class="img-fluid"></p>
</div></div><p><span style="background:yellow;">==<em>Still a draft, don’t circulate!</em>==</span></p>
<p><strong>Summary.</strong></p>
<ol type="1">
<li><p><strong>I describe a simple model of the relationship between LLMs, humans, and the world.</strong> Large language models (LLMs) answer questions by predicting how a human would answer that question (in their training set), and humans answer questions based on their beliefs about the world.</p></li>
<li><p><strong>Implication: an LLM </strong></p>
<ol type="1">
<li><p><strong>An LLM trained on the output of a single human will generally do worse than that human.</strong> This is because (1) the text they are trained on represents only a subset of that human’s knowledge; (2) the model fit will always be imperfect. However if the human’s answers are inconsistent then the LLM can do better simply by being less noisy.</p></li>
<li><p><strong>An LLM trained on the output of multiple humans can outperform any one of them.</strong> People tend to answer only the questions that they know the answer to, so predicting the answer to a question will tend to predict the answer of an expert in that field. Thus the set of LLM-answerable questions can be thought of as including, at a minimum, the union of the questions answered by people in the training set.</p></li>
<li><p><strong>An LLM can answer questions no human can answer.</strong> LLMs are not designed to explicitly make inferences about the world but the act of interpolating between observed answers will, in some circumstances, functionally operate as inference, and as a consequence the set of LLM-answerable questions can exceed the union of human-answerable questions. We already see this in LLM outputs: they can answer questions that no single human knows the answer to.</p></li>
<li><p><strong>An LLM can be used to extract tacit knowledge.</strong> Much human knowledge is tacit, reflected in our lack of insight into the bases of our judgment. If an LLM can match human performance in answering questions then it will effectively make available the tacit knowledge used in those judgments, and so open up a range of capabilities much wider than any human can achieve.</p></li>
</ol></li>
</ol>
<p><strong>Real-World Implications.</strong></p>
<ol type="1">
<li><p><strong>LLMs bring everybody to the knowledge frontier.</strong> An LLM can effectively serve as a consultant: it will tell you how a domain-expert would answer your question. For some problems we’ve already been able to do that for a long time, e.g.&nbsp;a textbook will tell you scientific advice on crop management practices, but LLMs allow for much subtler contingencies and to incorporate tacit knowledge. We should therefore expect the productivity impact of LLMs to be mainly on those who are behind the knowledge frontier, and we might also expect a compression of incomes. </p></li>
<li><p><strong>AI could be used to create super-human artefacts.</strong> There has been a lot of discussion about whether AI be used to exceed human abilities to create certain artefacts, e.g.&nbsp;hyper-beautiful paintings, hyper-addictive clickbait, hyper-persuasive text. Using the existing architecture of LLMs, which predict human outputs, this is unlikely: they might be able to predict the objects that a very talented human would produce, but would not surpass their ability. However AI could be used to generate artfeacts indirectly: instead of predicting how a human would create such an arfect, the weights that the AI has learned could be used to find the artefact which maximizes the function of interest, e.g.&nbsp;hyper-persuasive, or hyper-beautiful. In this way the AI could create artefacts beyond the ability of any human.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p></li>
</ol>
<section id="graphical-argument" class="level1">
<h1>Graphical Argument</h1>
<p><strong>Puzzle:</strong> How can LLMs outperform humans if they’re trained to predict human responses?</p>
<p><strong>(1) Let this curve represent the world.</strong> Each question about the world is a point on the x-axis, and the curve represents the true answer (y-axis).</p>
<div class="cell" data-hash="2023-09-05-model-of-ai-imitation_cache/html/unnamed-chunk-2_b36eb930a2bdc1bcbd1b69e62af7ad13">
<div class="cell-output-display">
<p><img src="2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p><strong>(2) A human forms beliefs about the world (red curve).</strong> The human asks two questions and gets two answers (red dots) and from these they form a belief about the nature of the world (red line).</p>
<div class="cell" data-hash="2023-09-05-model-of-ai-imitation_cache/html/unnamed-chunk-3_31a99f00dfb7aea3650bd0b87a0e9245">
<div class="cell-output-display">
<p><img src="2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p><strong>(3) Train a computer to predict the human’s answers (green curve).</strong> The human records some questions and their answers (green dots), and the computer learns to predict the human’s answers (green curve). Here the fit is perfectly because (1) the human reported all and only their direct experiences; (2) the computer’s priors about humans’ answers are the same as the human’s priors about the world. In this setup we would never expect the computer to be <em>better</em> than the human at answering questions about the world.</p>
<div class="cell" data-hash="2023-09-05-model-of-ai-imitation_cache/html/unnamed-chunk-4_a62fd2155a2954d2d9bee17cd64a7079">
<div class="cell-output-display">
<p><img src="2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p><strong>(4) Add another human (blue curve).</strong> Now we have two humans, each which asks different question of the world (red and blue dots), and each forms beliefs about the world which are accurate in the neighborhood of their experience.</p>
<div class="cell" data-hash="2023-09-05-model-of-ai-imitation_cache/html/unnamed-chunk-5_38a4528f7ce7f031359262ecfb86a073">
<div class="cell-output-display">
<p><img src="2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid" width="672"></p>
</div>
</div>
<p><strong>(5) Let the computer predict answers from both humans (green curve).</strong> Now both humans record their experiences and the computer tries to predict human answers (green curve). Here we can see that (1) the computer’s predictions can match the humans’ responses in each of their domains of expertise; (2) the computer is better than <em>both</em> humans in the intermediate region, i.e.&nbsp;they effectively <em>combine</em> information from both humans.</p>
<div class="cell" data-hash="2023-09-05-model-of-ai-imitation_cache/html/unnamed-chunk-6_81a769e1e60c2fcfb8ff260563175bea">
<div class="cell-output-display">
<p><img src="2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid" width="672"></p>
</div>
</div>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<p><strong>When do LLMs outperform humans?</strong> I only have some</p>
<ul>
<li>Translating between</li>
</ul>
<ol type="1">
<li><strong>This depends on people talking about what they know.</strong> The performance of a computer.</li>
</ol>
</section>
<section id="model-setup" class="level1">
<h1>Model Setup</h1>
<p><strong>Consider questions and answers like the following.</strong></p>
<table class="table">
<colgroup>
<col style="width: 93%">
<col style="width: 6%">
</colgroup>
<thead>
<tr class="header">
<th>question</th>
<th>answer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>What’s the capital of Switzerland?</td>
<td>Geneva</td>
</tr>
<tr class="even">
<td>What’s the best response if white plays c4?</td>
<td>c5</td>
</tr>
<tr class="odd">
<td>Does this picture contain a cow? 🐄</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>How much more likely are you to buy a coke after hearing the slogan “coke refreshes”?</td>
<td>0.1%</td>
</tr>
</tbody>
</table>
<p><strong>The model has three steps.</strong></p>
<p><span class="math display">\[\xymatrix@C=.5cm@R=0cm{
      \text{world}
      &amp;&amp; \text{human}
      &amp;&amp; \text{LLM}\\
      *+[F:&lt;5pt&gt;]{\bm{w}} \ar[rr]^{\bm{a}=Q\bm{w}}
      &amp;&amp; *+[F:&lt;5pt&gt;]{\hat{\bm{w}}} \ar[rr]^{\hat{\bm{a}}=\hat{Q}\hat{\bm{w}}}
      &amp;&amp; *+[F:&lt;5pt&gt;]{\bar{\bm{w}}} \ar[rr]^{\tilde{a}=\tilde{q}'\bar{\bm{w}}}
      &amp;&amp; \  \\
      \txt{unobserved\\truth\\about\\the\\world}
      &amp; \txt{answers\\to\\human\\questions}
      &amp; \txt{beliefs\\formed\\by\\human}
      &amp; \txt{text\\written\\by\\human}
      &amp; \txt{LLM\\model\\of\\human\\text}
      &amp; \txt{LLM's\\answers\\to\\new\\questions}
   }\]</span></p>
<p><strong>Questions and answers.</strong> We will model everything as a set of questions and their answers. A question is defined by a set of binary attributes (<span class="math inline">\(q_1,\ldots,q_n\in\{-1,1\}\)</span>), and the answer is a linear function of those attributes given some unobserved weights <span class="math inline">\(w_1,\ldots,w_n\)</span>:</p>
<p><span class="math display">\[\begin{aligned}
      \ut{\bmatrix{a^1 \\ \vdots \\ a^m}}{answers}
         = \ut{\bmatrix{q_1^1 w_1 + \ldots q_n^1w_n \\ \vdots \\ q_1^m w_1 + \ldots q_n^mw_n}}{questions}
   \end{aligned}\]</span></p>
<p><strong>Human beliefs.</strong> After observing a set of question and their real-world answers the human will form beliefs about the weights <span class="math inline">\(w_1,\ldots,w_n\)</span>. We can explicitly write the human posteriors if we assume their priors are Gaussian and i.i.d. (<span class="math inline">\(\bm{w}\sim N(0,\sigma^2I)\)</span>):<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a> <span class="math display">\[\begin{aligned}
      \bm{a}        &amp;= Q\bm{w}
         &amp;&amp; \text{(questions \&amp; answers given true weights $\bm{w}$)}\\
      \hat{\bm{w}} &amp;= Q'(QQ')^{-1}\bm{a}
         &amp;&amp; \text{(human estimate of true weights $\bm{w}$)}
   \end{aligned}\]</span></p>
<p>I will assume that the number of unobserved weights is large relative to the human’s experience (<span class="math inline">\(n\gg m\)</span>), so the human will gradually learn more about reality as she observes the answer to more questions, and will be able to perfectly answer any question she’s seen before, but will never learn the full set of weights.</p>
<p><strong>Computer beliefs.</strong> Suppose that humans write down some set of question and answer them given their beleifs. We use that text to train a computer to predict the answer given any question, and the computer likewise assumes a linear model with i.i.d. Gaussian weights. Then we can explicitly write the computer-estimated weights:</p>
<p><span class="math display">\[\begin{aligned}
      \hat{\bm{a}}        &amp;= \hat{Q}\hat{\bm{w}}
         &amp;&amp; \text{(human-generated questions \&amp; answers)}\\
      \bar{\bm{w}} &amp;= \hat{Q}'(\hat{Q}\hat{Q}')^{-1}\hat{\bm{a}}
         &amp;&amp; \text{(computer estimate of human weights $\hat{\bm{w}}$)}
   \end{aligned}\]</span></p>
<p><strong>Computer answers.</strong> Finally we can ask the computer a new question, <span class="math inline">\(\tilde{q}\)</span>, and observe its answer: <span class="math display">\[\begin{aligned}
      \tilde{a}        &amp;= \tilde{\bm{q}}'\bar{\bm{w}}
         &amp;&amp; \text{(computer answer to a novel question $\tilde{\bm{q}}$)}\\
   \end{aligned}\]</span></p>
</section>
<section id="model-implications" class="level1">
<h1>Model Implications</h1>
<dl>
<dt><strong>If one human records all their observations then the computer will perfectly imitate them.</strong></dt>
<dd>
Suppose that there is one human and they write down all of their observations, <span class="math inline">\(\hat{Q}=Q\)</span>. Then the computer’s beliefs will be the same as the human’s beliefs (<span class="math inline">\(\hat{\bm{w}}=\bar{\bm{w}}\)</span>), and so the computer will answer every question exactly as the human does, though neither knows the truth (<span class="math inline">\(\bar{\bm{w}}\neq\bm{w}\)</span>).
</dd>
<dt><strong>If humans do not record all their observation then the computer will perform worse.</strong></dt>
<dd>
Suppose humans only write down some of their observations, i.e.&nbsp;<span class="math inline">\(\hat{Q}\)</span> is a row-wise subset of <span class="math inline">\(Q\)</span>. Then computers and humans will give the same answers for any question in the training set, but outside of that set computers will generally do worse than humans. And so for every question <span class="math inline">\(\bm{q}\)</span> the computer will do worse in expectation: <span class="math display">\[E[\ut{(\bm{q}\bm{a}-\bm{q}\bar{\bm{a}})^2}{computer error}]\geq
  E[\ut{\bm{q}(\bm{a}-\hat{\bm{a}})}{human error}].\]</span>
</dd>
</dl>
<dl>
<dt><strong>If there are multiple humans then the computer will outperform them both.</strong></dt>
<dd>
Suppose there are two humans who each observe answers to different question, <span class="math inline">\(Q_A\)</span> and <span class="math inline">\(Q_B\)</span>, and they both write them all down, so <span class="math inline">\(\bar{Q}=(\smallmatrix{Q_A\\Q_B})\)</span> and <span class="math inline">\(\bar{\bm{a}}=(\smallmatrix{Q_A\bm{w}\\Q_B\bm{w}})\)</span>. Now the computer clearly has superior information to either human, and so for both <span class="math inline">\(i\in\{1,2\}\)</span> and every question <span class="math inline">\(\bm{q}\)</span> we can write: <span class="math display">\[E[\ut{(\bm{q}\bm{a}-\bm{q}\bar{\bm{a}})^2}{computer error}]\leq
  E[\ut{\bm{q}(\bm{a}-\hat{\bm{a}}_i)}{human error}].\]</span>
</dd>
<dt><strong>If there are multiple humans then the computer can answer question no human can answer.</strong></dt>
<dd>
<p>Suppose two humans observe the answers to the following questions: <span class="math display">\[\begin{aligned}
  Q_A &amp;= \bmatrix{1 &amp; 1 &amp; 1 \\ 1 &amp; -1 &amp; 1} \\
  Q_B &amp;= \bmatrix{1 &amp; 1 &amp; 1 \\ 1 &amp; 1 &amp; -1}
   \end{aligned}\]</span> The first human will learn the exact value of <span class="math inline">\(w_2\)</span>, and the second human will learn the exact value of <span class="math inline">\(w_3\)</span>, but neither will learn both, and so neither could perfectly predict the answer to this question: <span class="math display">\[\begin{aligned}
     \tilde{q} &amp;= \bmatrix{1 &amp; -1 &amp; -1} \\
  \end{aligned}\]</span></p>
<p>However if they both recorded their observations, so the computer observes <span class="math inline">\(\bar{\bm{a}}=(\smallmatrix{Q_1\bm{w}\\Q_2\bm{w}})\)</span>, the computer will be able to infer both <span class="math inline">\(w_2\)</span> and <span class="math inline">\(w_3\)</span>, and thus will be able to perfectly answer <span class="math inline">\(\tilde{q}\)</span>.</p>
<p>We can see this behaviour in LLMs: they sometimes combine a pair of facts or a pair of abilities which no single human has access to, e.g.&nbsp;when an LLM translates between two languages, for which there exists no human speaker of both.</p>
</dd>
<dt><strong>If humans write outside their expertise then the computer will do worse.</strong></dt>
<dd>
<p>In the cases above we assumed that the two humans recorded only what they directly observed. This means the computer essentially had a window directly to the world. However the humans could instead have written down their estimated answers to other questions. Suppose they both wrote down answers to every possible question, then the computer would learn the <em>average</em> of the two human’s estimated weights:<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a> <span class="math display">\[\bar{\bm{w}}=\frac{1}{2}\hat{\bm{w}}_A+\frac{1}{2}\hat{\bm{w}}_B.\]</span></p>
<p>Here the computer will do worse than the two humans on the original questions, <span class="math inline">\(Q_A\)</span> and <span class="math inline">\(Q_B\)</span>.</p>
<p>The implication is that LLMs work so well only because people tend to write about what they know. Put another way, when an LLM answers a question, it will not predict the answer given by the average person, but will predict the answer given by people who are likely to answer that question in the real world, and luckily those people tend to be people who are subject-matter experts.</p>
</dd>
</dl>
<dl>
<dt><strong>If humans have tacit knowledge, the computer model can outperform humans in creation of new artefacts.</strong></dt>
<dd>
<p>Suppose humans have tacit knowledge of the world, we can model this with two separate sets of beliefs: <span class="math display">\[\begin{aligned}
  \hat{\bm{w}}^T   &amp;= \text{tacit knowledge}\\
  \hat{\bm{w}}^E &amp;= \text{explicit knowledge}\\
   \end{aligned}\]</span></p>
<p>When the human encounters a new question <span class="math inline">\(\tilde{\bm{q}}\)</span> they will use their tacit knowledge to form an estimate of the answer, <span class="math inline">\(\hat{a}=\tilde{\bm{q}}'\hat{\bm{w}}^T\)</span>. For simplicity assume tacit knowledge is perfectly accurate <span class="math inline">\(\hat{\bm{w}}^T=\bm{w}\)</span>, and explicit knowledge is imperfect.</p>
<p>The distinction becomes important when we want to create a new question. Here it’s useful to interpret <span class="math inline">\(\bm{q}\)</span> not as a question but as an artefact, e.g.&nbsp;text or image, and <span class="math inline">\(\bm{a}=\bm{q}'\bm{w}\)</span> represents some abstract property, e.g.&nbsp;how beautiful or how rhythmic. Suppose we want to chosose <span class="math inline">\(\bm{q}\in\{-1,1\}^n\)</span> to maximize <span class="math inline">\(\bm{w}\bm{q}\)</span>. If we had perfect access to our beliefs <span class="math inline">\(\bm{w}^T\)</span> this would be simple, however if we have access only to imperfect explicit knowledge <span class="math inline">\(\hat{\bm{w}}^E\)</span>, the artefact which maximizes that function will not generally be the one which maximizes <span class="math inline">\(a\)</span>.</p>
<p>Here the computer model is less constrained. Suppose the computer has observed sufficiently many questions until they have perfectly learned the tacit knowledge, <span class="math inline">\(\bar{\bm{w}}=\hat{\bm{w}}^T\)</span>. Then if computation is free the computer could be used to query every single <span class="math inline">\(\bm{q}\in\{-1,1\}^n\)</span> to find the best possible artefact.</p>
</dd>
</dl>
</section>
<section id="derivation" class="level1">
<h1>Derivation</h1>
<p><span class="math display">\[\begin{aligned}
   Q &amp;= \bmatrix{q_1^1 &amp; \ldots &amp; q^1_n \\ &amp; \ddots \\ q^m_1 &amp; \ldots &amp; q^m_n}
      &amp;&amp; \text{(set of questions)} \\
   w'  &amp;= \bmatrix{w_1 \ldots w_n} \\
   a    &amp;= \bmatrix{a^1 \\ \vdots \\ a^m}
      = \bmatrix{q_1^1 w_1 + \ldots q_n^1w_n \\ \vdots \\ q_1^m w_1 + \ldots q_n^mw_n} \\
\end{aligned}\]</span></p>
<p><span class="math display">\[\begin{aligned}
      \bm{w} &amp;\sim N(0,\Sigma)
         &amp;&amp; (n\times 1\text{ vector of parameters of the world)}\\
      Q      &amp;\in \{-1,1\}^{n\times m}
         &amp;&amp; \text{($m$ questions, each has $n$ binary parameters)}\\
      \ut{\bm{a}}{$m\times1$}   &amp;= \ut{Q}{$m\times n$}\ut{\bm{w}}{$n\times1$}
         &amp;&amp; \text{(answers provided by the world)}\\
      \hat{\bm{w}} &amp;= E[\bm{w}|Q,\bm{a}]
            &amp;&amp; \text{(human beliefs about the world)}\\
         &amp;= \ut{\Sigma Q'}{$Cov(\bm{w},\bm{a})$}
            (\ut{Q\Sigma Q'}{$Var(\bm{a})$})^{-1}
            \bm{a}
   \end{aligned}\]</span></p>
<p><strong>With one observation and two weights.</strong> Suppose <span class="math inline">\(m=1, n=1\)</span>, then we have: <span class="math display">\[\begin{aligned}
      Q  &amp;= \bmatrix{q_1 &amp; q_2} \\
      \bm{a}'  &amp;= \bmatrix{a} \\
      \bm{w}'  &amp;= \bmatrix{w_1 &amp; w_2 } \\
      \Sigma &amp;= \bmatrix{\sigma_1^2 &amp; \rho \\ \rho &amp; \sigma_2^2}\\
      \Sigma Q' &amp;= \bmatrix{ \sigma_1^2q_1 + \rho q_2 \\ \rho q_1 + \sigma_2^2 q_2 } \\
      Q\Sigma Q' &amp;= \bmatrix{ \sigma_1^2q_1^2 + 2\rho q_1q_2 + \sigma_2^2 q_2^2} \\
      \hat{\bm{w}}=\Sigma Q'(Q\Sigma Q')^{-1}\bm{a}
         &amp;= \bmatrix{ \frac{\sigma_1^2q_1 + \rho q_2}{\sigma_1^2q_1^2 + 2\rho q_1q_2 + \sigma_2^2 q_2^2} \\
                  \frac{\rho q_1 + \sigma_2^2 q_2}{\sigma_1^2q_1^2 + 2\rho q_1q_2 + \sigma_2^2 q_2^2}} a
   \end{aligned}
   \]</span></p>
<p>We can normalize <span class="math inline">\(q_1=q_2=1\)</span>, then we have <span class="math display">\[\hat{w}_1 = \frac{\sigma_1^2+\rho}{\sigma_1^2+2\rho+\sigma_2^2}a,\]</span></p>
<p>This means we are dividing up responsibility for the answer (<span class="math inline">\(a\)</span>) into the contributions of each component, nice and simple.</p>
<p><strong>With two observations and one weight.</strong> Here we’re <em>over-identified</em>. <span class="math display">\[\begin{aligned}
      Q  &amp;= \bmatrix{q^1 \\ q^2} \\
      \bm{a}  &amp;= \bmatrix{a^1 \\ a^2} \\
      \bm{w}  &amp;= \bmatrix{w } \\
      \Sigma &amp;= \bmatrix{\sigma^2 }\\
      \Sigma Q' &amp;= \bmatrix{ \sigma^2 q^1 &amp; \sigma^2 q^2 } \\
      Q\Sigma Q' &amp;= \bmatrix{ \sigma^2 q^1q^1 &amp; \sigma^2q^1q^2 \\ \sigma^2q^1q^2 &amp; \sigma^2q^2q^2}
         &amp;&amp; \text{(this matrix doesn't have an inverse)}
   \end{aligned}
   \]</span></p>
<p><strong>With noise.</strong> Suppose we only observe the answers with random noise, then we get this: <span class="math display">\[\begin{aligned}
   \ut{\bm{a}}{$m\times1$}   &amp;= \ut{Q}{$m\times n$}\ut{\bm{w}}{$n\times1$}
      + \ut{\bm{e}}{$n\times 1$} \\
   \bm{e} &amp;\sim N(\bm{0},s^2I_m) &amp;&amp; \text{(i.i.d. noise w variance $s^2$)}\\
   Cov(\bm{w},\bm{a})   &amp;= \Sigma Q' \\
   Var(\bm{a}) &amp;= Q\Sigma Q' + s^2I_m \\
   E[\bm{w}|Q,\bm{q}]   &amp;= \Sigma Q'(Q\Sigma Q' + s^2I_m)^{-1}\bm{a}
\end{aligned}\]</span></p>
<p><strong>Compare to Bayesian linear regression.</strong> We can compare this result to Bayesian linear regression (e.g.&nbsp;<a href="https://en.wikipedia.org/wiki/Bayesian_linear_regression">Wikipedia</a>): <span class="math display">\[\begin{aligned}
      \bar{\beta}  &amp;= \Sigma Q'(Q\Sigma Q' + s^2I_m)^{-1}\bm{a}
         &amp;&amp; \text{(our result)} \\
      \tilde{\beta} &amp;= (Q'Q+s^{2}\Sigma^{-1})^{-1}Q'\bm{a}
         &amp;&amp; \text{(Bayesian linear regression)}\\
   \end{aligned}\]</span></p>
<p>I <em>believe</em> that these can be shown to be equivalent by the <a href="https://en.wikipedia.org/wiki/Woodbury_matrix_identity">matrix inversion lemma</a>, though I haven’t confirmed this. There’s a <a href="https://digitalcommons.usu.edu/cgi/viewcontent.cgi?article=1212&amp;context=ece_facpub">note online</a> that appears to show equivalence.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>This is roughly how image synthesis algorithms work: after training a model to recognize images, new images can be created by maximizing the learned weights.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>I am assuming <span class="math inline">\(\bm{w}\)</span> has zero-mean and is i.i.d. just to cut down on notation, the results all hold for the more general multivariate Normal case.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>We would have to augment the computer’s learning rule to allow for noise in answers - I need to confirm that the weighting will be exactly 1/2.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>