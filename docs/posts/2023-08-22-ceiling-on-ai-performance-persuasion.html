<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.357">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tom Cunningham">
<meta name="dcterms.date" content="2024-04-28">
<meta name="description" content="Tom Cunningham blog">

<title>Will LLMs Write Super-Persuasive Propaganda? | Tom Cunningham</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-12027453-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>
<style>
   h1 {  border-bottom: 4px solid black;}
   h2 {  border-bottom: 1px solid #ccc;}
</style>
<script>window.MathJax = {
         loader: { load: ['[custom]/xypic.js'],
                     paths: {custom: 'https://cdn.jsdelivr.net/gh/sonoisa/XyJax-v3@3.0.1/build/'}},
      tex: {packages: {'[+]': ['xypic']},
         macros: {
            bm: ["\\boldsymbol{#1}", 1],
            ut: ["\\underbrace{#1}_{\\text{#2}}", 2],
            utt: ["\\underbrace{#1}_{\\substack{\\text{#2}\\\\\\text{#3}}}", 3]
         }}};
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="Will LLMs Write Super-Persuasive Propaganda? | Tom Cunningham">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="tecunningham.github.io/posts/20230825131919.png">
<meta name="twitter:image-height" content="700">
<meta name="twitter:image-width" content="770">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="floating nav-fixed slimcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Tom Cunningham</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href=".././about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/testingham" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/tom-cunningham-a9433/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://tecunningham.github.io/index.xml" rel="" target=""><i class="bi bi-rss-fill" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://scholar.google.com/citations?user=MDB_DgkAAAAJ" rel="" target="">
 <span class="menu-text">scholar</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Will LLMs Write Super-Persuasive Propaganda?</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Tom Cunningham, <a href="https://integrityinstitute.org/">Integrity Institute</a> </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 28, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal sidebar-navigation floating overflow-auto">
    <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul class="collapse">
  <li><a href="#summary" id="toc-summary" class="nav-link active" data-scroll-target="#summary">Summary</a></li>
  <li><a href="#setup" id="toc-setup" class="nav-link" data-scroll-target="#setup">Setup</a></li>
  <li><a href="#plain-llms-will-not-exhibit-superhuman-performance" id="toc-plain-llms-will-not-exhibit-superhuman-performance" class="nav-link" data-scroll-target="#plain-llms-will-not-exhibit-superhuman-performance">Plain LLMs Will Not Exhibit Superhuman Performance</a>
  <ul class="collapse">
  <li><a href="#an-llm-answering-a-question-no-human-can" id="toc-an-llm-answering-a-question-no-human-can" class="nav-link" data-scroll-target="#an-llm-answering-a-question-no-human-can">An LLM Answering a Question No Human Can</a></li>
  </ul></li>
  <li><a href="#superhuman-persuasion-from-iterative-testing" id="toc-superhuman-persuasion-from-iterative-testing" class="nav-link" data-scroll-target="#superhuman-persuasion-from-iterative-testing">Superhuman Persuasion from Iterative Testing</a></li>
  <li><a href="#superhuman-persuasion-from-search" id="toc-superhuman-persuasion-from-search" class="nav-link" data-scroll-target="#superhuman-persuasion-from-search">Superhuman Persuasion from Search</a></li>
  <li><a href="#superhuman-persuasion-from-supervised-learning" id="toc-superhuman-persuasion-from-supervised-learning" class="nav-link" data-scroll-target="#superhuman-persuasion-from-supervised-learning">Superhuman Persuasion from Supervised Learning</a></li>
  <li><a href="#appendix-studies-on-the-persuasiveness-of-computer-generated-text" id="toc-appendix-studies-on-the-persuasiveness-of-computer-generated-text" class="nav-link" data-scroll-target="#appendix-studies-on-the-persuasiveness-of-computer-generated-text">Appendix: Studies on the Persuasiveness of Computer-Generated Text</a></li>
  <li><a href="#appendix-on-advertising-and-persuasion" id="toc-appendix-on-advertising-and-persuasion" class="nav-link" data-scroll-target="#appendix-on-advertising-and-persuasion">Appendix: On Advertising and Persuasion</a></li>
  <li><a href="#appendix-on-synthesis" id="toc-appendix-on-synthesis" class="nav-link" data-scroll-target="#appendix-on-synthesis">Appendix: On Synthesis</a></li>
  <li><a href="#extra-material" id="toc-extra-material" class="nav-link" data-scroll-target="#extra-material">Extra Material</a>
  <ul class="collapse">
  <li><a href="#other-applications" id="toc-other-applications" class="nav-link" data-scroll-target="#other-applications">Other Applications</a></li>
  </ul></li>
  </ul>
</nav>
</nav>
<div id="quarto-sidebar-glass" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar,#quarto-sidebar-glass"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
    </div>
<!-- main -->
<main class="content quarto-banner-title-block page-columns page-full" id="quarto-document-content">




<style>
    h1 {  border-bottom: 4px solid black; }
    h2 {  border-bottom: 1px solid gray; padding-bottom: 0px; color: black; }
    dl { margin-bottom: 0px; }
    dt strong { font-weight: bold; }
    dd { margin-left: 20px; }
</style>
<section id="summary" class="level1 page-columns page-full">
<h1>Summary</h1>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="20230825131919.png" class="img-fluid"> This has benefited greatly from comments from members of the Integrity Institute, especially Grady Ward.</p>
</div></div><p><span style="background:yellow;">==<em>Still a draft, don’t circulate!</em>==</span></p>
<p><strong>Are LLMs likely to create superhumanly persuasive text?</strong> Will large language models (LLMs) be able to write messages more persuasive than those written by professional copywriters: messages which compel you to click a link, buy a product, or vote for a candidate?</p>
<p><strong>There is an argument that LLMs would <em>not</em> be able to outperform humans:</strong> LLMs are trained to predict text, and the text they are trained on is generated by humans. Thus an LLM knows about the world only at third hand: <span class="math display">\[\xymatrix{
      *+[F:&lt;5pt&gt;]{\text{world}} \ar[r]
      &amp; *+[F:&lt;5pt&gt;]{\text{human}} \ar[r]
      &amp; *+[F:&lt;5pt&gt;]{\text{text}} \ar[r]
      &amp; *+[F:&lt;5pt&gt;]{\text{LLM}}
   }\]</span></p>
<p>When you ask an LLM a question it generates an answer by predicting how it would be answered in its training data, which was generated by humans, and so we might expect an LLM to perform at best level with the average human.</p>
<p><strong>However there are three ways in which LLMs could outperform humans.</strong></p>
<p><strong>(1) LLMs can combine the output of multiple humans.</strong> LLMs are trained on hundreds of billion of words of text, written by many different people. As a consequence: (1) the answer of an LLM to any given question is better than the answer of the <em>average</em> person in their training data – this is because people tend to write about things they know; (2) when an LLM interpolates between text generated by different people it can effectively combine their knowledge.</p>
<p><span class="math display">\[\xymatrix{
      *+[F:&lt;5pt&gt;]{\text{world}} \ar[r]\ar[dr]
      &amp; *+[F:&lt;5pt&gt;]{\text{human}_1} \ar[r]
      &amp; *+[F:&lt;5pt&gt;]{\text{text}_1} \ar[r]
      &amp; *+[F:&lt;5pt&gt;]{\text{LLM}}\\
      &amp; *+[F:&lt;5pt&gt;]{\text{human}_2} \ar[r]
      &amp; *+[F:&lt;5pt&gt;]{\text{text}_2} \ar[ur]
   }\]</span></p>
<p><strong>(2) LLMs can access tacit knowledge.</strong> Suppose an LLM could accurately predict what a human would do in every conceivable circumstance. By combining that information they might learn more about the world than they would just from asking questions to the human. Humans are far better at recognizing whether an object belongs to a category than at synthesizing a new object that belongs to a category. E.g. you can recognize whether or not a painting looks like your cousin but it’s notoriously difficult to paint a new picture that looks like your cousin.</p>
<p><strong>(3) LLMs can be augmented with direct access to the world.</strong></p>
<p><strong>However changes to architecture could conceivably lead to super-persuasive text.</strong> There are three basic approaches:</p>
<ol type="1">
<li><p><strong>Testing and selection.</strong> We could generate multiple variants, test each for persuasiveness, and then choose the best-performing. However this is already possible with human-written text so the addition of computer-generated texts to the pool of candidates would make a difference only if they had markedly different statistical properties, and there is no clear reason why this would be true.</p></li>
<li><p><strong>Using tacit knowledge.</strong> Instead of directly asking an LLM to compose a persuasive text we could use a battery of prompts to search for the texts that are most persuasive. There is reason to expect this would be effective because human knowledge of persuasiveness is partly <em>tacit</em>, meaning that our ability to recognize whether a given text is persuasive outstrips our ability to create new persuasive texts. Because computers can evaluate the persuasiveness of texts at a far higher speed and lower cost this would allow us to systematically canvas the landscape of persuasion in a way that is not possible with humans. </p></li>
<li><p><strong>Supervised learning.</strong> Instead of simulating human judgment of persuasiveness we can train a model directly on the true persuasiveness of text. The effectiveness of this will depend on the statistical properties of the data, and how reliably we can learn from observational data, but it seems plausible that this approach could uncover pockets of persuasiveness not yet discovered by humans.</p></li>
</ol>
<p><strong>We can project the future abilities of AI with data available today.</strong> Below I discuss a number of empirical facts which we could measure today and would help us predict the future capabilities of LLM-generated text. Facts like (1) the variance in persuasiveness between texts, (2) the diminishing returns of persuasiveness as more texts are created, (3) the correlation between true persuasiveness, human-estimated persuasiveness, and computer-estimated persuasiveness of a given text.</p>
<p><strong>It is unclear whether super-persuasive messages exist.</strong> Finally there is a bigger question on how much more persuasive we can get. Over the last 100 years social scientists have continually invented supposed technologies of hyper-persuasion, they all turned out to be duds. We know that some messages are more persuasive than others but it is hard to know how much higher we can go: it’s possible that the most persuasive writers today are already close to the ceiling.</p>
</section>
<section id="setup" class="level1 page-columns page-full">
<h1>Setup</h1>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="20230825135614.png" class="img-fluid"></p>
</div></div><p><strong>We are interested in the persuasive effect of a single short piece of text,</strong> e.g.:</p>
<ul>
<li>Effectiveness of an email in making people open an attached file.</li>
<li>Effectiveness of a tweet in making people buy a crypto coin.</li>
<li>Effectiveness of a blog post in making people vote for Ron DeSantis.</li>
</ul>
<p><strong>The persuasiveness of text is relative to an audience and a provenance.</strong> The same message will have a different effect on different people, at different times, and will depend on who it comes from. For concreteness assume in the rest of this post that the audience is the average American adult in 2023 and they receive the message unsolicited from a stranger.</p>
<p></p>
<p><strong>Formal definition.</strong> We can think of a function <span class="math inline">\(p(t)\)</span>, where <span class="math inline">\(t\)</span> represents a piece of text drawn from the space of all possible texts, and <span class="math inline">\(p(.)\)</span> is its persuasive power: precisely, the average person’s probability of performing a certain action after seeing that text. The function <span class="math inline">\(p(t)\)</span> will be basically flat for almost all values of <span class="math inline">\(t\)</span> because in the space of all possible sentences the overwhelming majority are meaningless or irrelevant and so likely to have infinitesimal effects on whether you take a specific action.</p>
<p><strong>I’m setting aside many related questions.</strong> I will not consider (1) AI maintaining a conversation (chatbots, catfishing, pig butchering); (2) AI sending customized messages based on the respondent’s demographics; (3) AI choosing who to send messages to, based on predicted persuadability; (4) repeated exposure to the same message. For each of these cases it seems unlikely that AIs could be super-humanly persuasive without also being super-humanly persuasive in the base case of creating a single message. However some additional considerations apply to the case of AI synthesizing recorded media (photos, audio, and video), because it is very difficult for a human to create, for example, a photo from scratch, while computers can do it very reliably. In addition recorded media has historically served as <em>prima facie</em> evidence that some event ocurred: as synthesized media becomes more prevalent that evidence-value is likely to decline.</p>
<p><strong>We want to compare persuasiveness of text created by different types of authors:</strong></p>
<ul>
<li><p>Human subjects in a psychology experiment (e.g.&nbsp;undergraduates, mechanical turk participants)</p></li>
<li><p>Human employees of influence operation (e.g.&nbsp;Internet Research Agency, pig butcher scammers)</p></li>
<li><p>Human copywriters (e.g.&nbsp;marketing professionals)</p></li>
<li><p>LLMs in 2023 (e.g.&nbsp;GPT-4)</p></li>
<li><p>LLMs in the future: 2024, 2025, and 2050.</p></li>
</ul>
<div class="page-columns page-full"><p><strong>There is substantial variation in human ability to write persuasive copy.</strong> Copywriters are hired for their ability to come up with text that makes people perform an action, i.e.&nbsp;they are hired for their knowledge of the function <span class="math inline">\(p(.)\)</span>, and it seems there must be substantial variation in individual ability: some people are better than others at writing persuasive slogans.<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;From the <a href="https://www.awai.com/about/celebrating-clayton-makepeace/">obituary of a famous direct-marketing copywriter</a>: “When you read some of the greatest and most iconic headlines in copywriting history — such as, “7 HORSEMEN of the Coming STOCK MARKET APOCALYPSE” … “Shameless Two-Faced S.O.B.s!” … and, “Health Breakthrough News — Cholesterol’s EVIL TWIN”— smile and think to yourself, ‘Clayton Makepeace wrote that.’”</p></li></div></div>
<p> </p>
<p><strong>Social scientists have warned about hyper-persuasion many times in the last century.</strong> In the past 100 years there have been a dozen apparent discoveries of hyper-effective means of persuasion, whether based on subconscious associations (Bernays), conditioning (Skinner), subliminal messages (Packard), brainwashing (Sargant), the power of conformity (Adorno, Asch, Zimbardo), priming and nudges (Bargh, Thaler), or statistical profiling (Cambridge Analytica). In retrospect I think it’s fair to say that all these warnings were, at best, exaggerated.</p>
</section>
<section id="plain-llms-will-not-exhibit-superhuman-performance" class="level1 page-columns page-full">
<h1>Plain LLMs Will Not Exhibit Superhuman Performance</h1>
<blockquote class="blockquote">
<p><em>TO ADD:</em> Treat each person as a probability distribution over strings of text, and we have 1M draws from each person. Then we get the <em>average</em> distribution. But at each step it’ll be a weighted average, so if you start off a string in French it’ll continue like the French speaker.*</p>
</blockquote>
<div class="page-columns page-full"><p><strong>In short: LLMs don’t perform tasks, they imitate humans performing tasks.</strong> The basic architecture of contemporary large language models (LLMs) is to predict the next word given a prior string of words, trained on a large corpus of written text. Thus if we ask an LLM to create a persuasive message it will predict a human’s answer to that request, and so we should expect it to respond with a super-humanly persuasive message only by accident.<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;Lichtenberg said something similar about writers: <em>“The critics instruct authors to stay close to nature, and they read this advice; but they always think it safer to stay close to authors who have stayed close to nature.”</em></p></li></div></div>
<div class="cell page-columns page-full" data-hash="2023-08-22-ceiling-on-ai-performance-persuasion_cache/html/unnamed-chunk-1_d2aca5700b986675c72dd0a7cb94ce99">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2023-08-22-ceiling-on-ai-performance-persuasion_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">A representation of the questions that can be correctly answered by two humans (Tom and Dick) and a computer (LLM).</figcaption>
</figure>
</div>
</div></div></div>
<p><strong>There are some cases where LLMs outperform all humans.</strong> I discuss these cases in the rest of this section, and argue that generating persuasive text is not such a case. I will talk about an LLM’s ability to correctly answer questions but the same arguments equally apply to performing tasks. We can draw a Venn diagram (at right) showing the relationship between the questions that the LLM can answer accurately and the sets that individual humans can answer. I will suppose the LLM is trained on a corpus of text generated by two humans, Tom and Dick. </p>
<p><strong>The LLM can outperform any given human.</strong> Because the LLM is trained on the text produced by more than one human. We know that LLMs can answer questions with an encylopedic range and across different languages, i.e.&nbsp;beyond the range of any single person.</p>
<div class="cell page-columns page-full" data-hash="2023-08-22-ceiling-on-ai-performance-persuasion_cache/html/unnamed-chunk-2_70a84017c6be04092b0d835e04483cfb">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2023-08-22-ceiling-on-ai-performance-persuasion_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">abc</figcaption>
</figure>
</div>
</div></div></div>
<p><strong>For independent factual questions the LLM will be inside the union of the humans.</strong> Suppose we consider a set of factual questions where the answer to each question is independent of the others, e.g.&nbsp;the number of chromosones a specific animal has, or the number of moons belonging to a given planet. In this case any question that the LLM can answer will be answerable by at least one human, i.e.&nbsp;the LLM-answerable questions will fall inside the union of the human-answerable questions.</p>
<p><strong>There are some questions which an LLM can answer that no human can answer.</strong> Some examples:</p>
<ul>
<li>An LLM can translate between any pair of languages. Presumably there are many pairs of languages for which there is no human who speaks both, so the LLM can perform a task that no human has ever been able to perform.</li>
<li>An LLM can combine distinct pieces of information.</li>
</ul>
<p><em>[UNFINISHED: in short LLMs are discovering latent representations from the language output of multiple humans, which they can then combine, producing output that no human was capable of.]</em></p>
<section id="an-llm-answering-a-question-no-human-can" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="an-llm-answering-a-question-no-human-can">An LLM Answering a Question No Human Can</h2>
<p>Consider the following simplified model: an LLM is trained entirely on strings with the following form:</p>
<table class="table">
<thead>
<tr class="header">
<th>prompt</th>
<th>completion</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>cats are mammals</td>
<td>is true</td>
</tr>
<tr class="even">
<td>mammals are animals</td>
<td>is true</td>
</tr>
<tr class="odd">
<td>cats are animals</td>
<td>is true</td>
</tr>
</tbody>
</table>
<p>The three statements above satisfy a law of logical implication (<em>modus ponens</em>). If the training set contains sufficiently many similar cases then the LLM may learn to apply the pattern to new cases such that, if the following two datapoints are in its training set,</p>
<table class="table">
<thead>
<tr class="header">
<th>prompt</th>
<th>completion</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Asterix is a Gaul</td>
<td>is true</td>
</tr>
<tr class="even">
<td>Gauls live in France</td>
<td>is true</td>
</tr>
</tbody>
</table>
<p>then the model will answer “is true” to the prompt “Asterix lives in France.”</p>
<p></p>
<p>It is possible that no individual human knows the answer to both of the premises, and so no human can correctly answer whether Asterix lives in France, but the LLM can. Thus here the set of LLM-answerable questions extends beyond the union of human-answerable questions.</p>
<div class="page-columns page-full"><p>In this example we could say that the LLM has learned true statements and laws of logical implication, however of course this is only to the degree that the training data consists of true statements which satisfy the laws of logical implication. An LLM would equally happily learn false statements and invalid patterns of implication.<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;If a set of statements are true then they necessarily will satisfy the laws of implication, but the converse does not hold.</p></li></div></div>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="20230825133035.png" class="img-fluid"></p>
</div></div></section>
</section>
<section id="superhuman-persuasion-from-iterative-testing" class="level1 page-columns page-full">
<h1>Superhuman Persuasion from Iterative Testing</h1>
<p><strong>In practice people are likely to generate multiple candidates, test each in the field, and choose the highest-performing variant.</strong> However this would only change our conclusion about super-human persuasiveness if the distribution of persuasiveness is different between human-generated and computer-generated text.</p>
<p>Suppose we can generate arbitrarily many passages of text <span class="math inline">\(t_1,\ldots,t_n\)</span>, and the persuasiveness of each text is represented by <span class="math inline">\(p_1,\ldots,p_n\)</span>. It’s safe to assume that there will be diminishing returns in persuasiveness (<span class="math inline">\(E[p_{n+1}]&lt;E[p_n]\)</span>), both for human-generated and computer-generated text, however as long as there is some variance in persuasiveness then iterative testing will be worthwhile. Suppose that the first or most-promising generation of both a human and a computer are equally persuasive on average (<span class="math inline">\(E[p_1^H]=E[p_1^C]\)</span>).</p>
<div class="cell page-columns page-full" data-hash="2023-08-22-ceiling-on-ai-performance-persuasion_cache/html/unnamed-chunk-3_78adcda2d0f31d69fea424e53d9314fc">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="2023-08-22-ceiling-on-ai-performance-persuasion_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption class="figure-caption">The diminishing returns to persuasiveness: here I illustrate a case where humans and computers have the same variance, but the the quality declines more quickly for computers.</figcaption>
</figure>
</div>
</div></div></div>
<p><strong>The persuasiveness of computer-generated copy could be higher after the iterative process for one of two reasons.</strong> These are empirical facts that should be relatively straight-forward to test, unfortunately I’m not aware of any literature that has useful results on these two questions.</p>
<p><strong>(1) If computer-generated text has higher variance.</strong> <span class="math inline">\(V[p_n^H]&lt;V[p_n^C]\)</span>. The returns to exploration will increase in the variance of the outputs (holding fixed the mean), and likewise the expected persuasiveness of the ultimately selected alternative will be higher.</p>
<p>I am skeptical that computer-generated text will have higher variance of persuasiveness than human-generated text. A common observation about LLMs is that they are less creative than humans at the same task. Variance can be increased by adjusting parameters of the model, e.g.&nbsp;generating lower-probability tokens by setting a higher temperature, but this will very likely decrease the average persuasiveness.</p>
<p>A more subtle point is the covariance of persuasiveness between each element in the sequence. If an LLM generated minor variations on the same basic pattern then we would expect high covariance between texts and consequently relatively lower returns to iterative selection.</p>
<p><strong>(2) If computer-generated text has slower-diminishing returns.</strong> <span class="math inline">\(\frac{E[p_{n+1}^H]}{E[p_{n}^H]}&lt;\frac{E[p_{n+1}^C]}{E[p_{n}^C]}\)</span>.</p>
<p>It is unclear to me whether we should expect the average persuasiveness of texts to decline faster for humans or for computers.</p>
</section>
<section id="superhuman-persuasion-from-search" class="level1 page-columns page-full">
<h1>Superhuman Persuasion from Search</h1>
<div class="page-columns page-full"><p><strong>Estimating the persuasiveness of text with an experiment requires enormous experiments.</strong> Most text has fairly low rate of persuasion, e.g.&nbsp;typically advertisements have click-through rates of 0.1 percentage points or less. In such a case it would take 400,000 observations to measure the click-through rate to within 10% of its true value.<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a> For some actions it also takes a long time to run an experiment, e.g.&nbsp;if persuading people to vote in an election or to keep using a product.</p><div class="no-row-height column-margin column-container"><li id="fn4"><p><sup>4</sup>&nbsp;Derivation: <span class="math display">\[\begin{aligned}
     p  &amp;= 0.001\\
     SD &amp;= \sqrt{p(1-p)} = 0.03 \\
     CI &amp;= p * 0.1 = 0.0001\\
        &amp; \text{(95\% CI width)}\\
     SE &amp; = CI / 1.96 = 0.00005\\
        &amp; \text{(95\% CI needs 1.96SEs)}\\
     N &amp; = (SD/SE)^2 = 360,000
  \end{aligned}\]</span></p></li></div></div>
<p><strong>Instead of running an experiment we could ask people to estimate persuasiveness.</strong> We could simply ask <em>“how persuasive is this text on a scale of 1-100?”</em> We could either ask professionals or a representative sample of the population. This is not the ground truth but it requires vastly smaller sampler sizes to get an precise estimate.</p>
<p><strong>We can summarize the effectiveness of the hybrid with the correlation of three metrics.</strong> Consider three different measures of persuasiveness:</p>
<ol type="1">
<li><em>True persuasiveness.</em> I.e. the true causal effect on click-through rate, which can only be estimated with a large and time-consuming experiment.</li>
<li><em>Human-estimated persuasiveness.</em> The response to a survey question about persuasiveness.</li>
<li><em>Computer-estimated persuasiveness.</em> This can be defined as the total probability of the sequence of tokens produced by the LLM (<span class="math inline">\(P(w_1,\ldots,w_n)\)</span>).<a href="#fn5" class="footnote-ref" id="fnref5" role="doc-noteref"><sup>5</sup></a></li>
</ol>
<div class="no-row-height column-margin column-container"><li id="fn5"><p><sup>5</sup>&nbsp;A technical note on generating high-likelihood text: if you are given a prompt and wish to generate a <em>representative</em> completion (given a distribution <span class="math inline">\(P(w_{t_1}|w_t,w_{t-1},\ldots)\)</span>) then you can successively generate individual words in proportion to their probability at each step because the full distribution is separable. However if you are trying to generate the <em>most likely</em> completion it is not sufficient to generate the most likely word at each step. My impression is that most popular LLMs do in fact generate tokens incrementally because the loss in most cases is not too large (the alternative is to explore multiple steps ahead, a “beam search”).</p></li></div><p><strong>The effectiveness of the hybrid strategy will depend the <em>incremental</em> value of human judgment.</strong> It is unclear how strong the conditional correlation is bewteen true persuasiveness and human-estimated persuasiveness. I.e. if the LLM generates two texts with equal probability, will humans have a high likelihood of telling which is more truly persuasive?</p>
</section>
<section id="superhuman-persuasion-from-supervised-learning" class="level1 page-columns page-full">
<h1>Superhuman Persuasion from Supervised Learning</h1>
<p><strong>Finally we could train a model using historical data on real-world persuasion.</strong> Instead of predicting what a human would say if they were asked to produce persuasive text we could train a model directly on examples of text where we know the true persuasive effect, from experiments.</p>
<p><strong>Observational data may not be sufficient.</strong> Gordon et al.&nbsp;(2019) and Gordon et al.&nbsp;(2023) both attempt to predict the experimental effect of Facebook advertisments on conversions, using only observational (non-experimnental) data, and they fail: <em>“despite having access to large-scale experiments and rich user-level data, we are unable to reliably estimate an ad campaign’s causal effect.”</em> The latter paper say they have a dataset of 663 experiments.</p>
<div class="page-columns page-full"><p><strong>Many platforms have huge datasets on persuasion.</strong> Facebook probably serves around 40 billion ad impressions/day (Assuming 4 billion active users/day, and 20 ad impressions/user/day.)<a href="#fn6" class="footnote-ref" id="fnref6" role="doc-noteref"><sup>6</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn6"><p><sup>6</sup>&nbsp;For comparison <em>“today’s vision datasets typically contain a few hundred million high-resolution color images from the web (e.g.&nbsp;Google has JFT-300M, OpenAI CLIP was trained on a 400M), but grow to as large as a small few billion.”</em> – <a href="http://karpathy.github.io/2022/03/14/lecun1989/">Andrej Karpathy</a> in 2022.</p></li></div></div>
<p></p>
</section>
<section id="appendix-studies-on-the-persuasiveness-of-computer-generated-text" class="level1">
<h1>Appendix: Studies on the Persuasiveness of Computer-Generated Text</h1>
<p>Since 2022 there has been a small literature comparing the persuasiveness of human-generated and computer-generated text, where persuasion is measured by a human survey response before and after reading an article. The studies have all found that LLMs can generate somewhat-persuasive content, none show strong evidence that LLMs can create more-persuasive content than moderately competent humans.</p>
<ul>
<li><span class="citation" data-cites="bai2023persuade">Bai et al. (<a href="#ref-bai2023persuade" role="doc-biblioref">2023</a>)</span></li>
<li><span class="citation" data-cites="goldstein2023persuasive">Goldstein et al. (<a href="#ref-goldstein2023persuasive" role="doc-biblioref">2023</a>)</span></li>
<li><span class="citation" data-cites="hackenburg2023persuasive">Hackenburg and Margetts (<a href="#ref-hackenburg2023persuasive" role="doc-biblioref">2023</a>)</span></li>
<li><span class="citation" data-cites="matz2023personalized">Matz et al. (<a href="#ref-matz2023personalized" role="doc-biblioref">2023</a>)</span></li>
<li><span class="citation" data-cites="palmer2023large">Palmer and Spirling (<a href="#ref-palmer2023large" role="doc-biblioref">2023</a>)</span></li>
<li><span class="citation" data-cites="qin2023large">Qin et al. (<a href="#ref-qin2023large" role="doc-biblioref">2023</a>)</span></li>
</ul>
<p>See more details <a href="https://docs.google.com/document/d/1X0F-YOco4EWqk4BMTuqi_DpFAOCXGEXpfmaaGGV9F0s/">here</a>.</p>
</section>
<section id="appendix-on-advertising-and-persuasion" class="level1 page-columns page-full">
<h1>Appendix: On Advertising and Persuasion</h1>
<div class="page-columns page-full"><p><strong>Increases in persuasive power should cause increases in expenditure on advertising.</strong> If computer-generated ads are more persuasive than human-generated ads then firms should be willing to spend more on each ad, and so the equilibrium price of advertising impressions will rise.<a href="#fn7" class="footnote-ref" id="fnref7" role="doc-noteref"><sup>7</sup></a></p><div class="no-row-height column-margin column-container"><li id="fn7"><p><sup>7</sup>&nbsp;Although the price of ads should increase it is unclear whether the quantity of ads shown would increase or decrease. In a simple model of ad-supported media the equilibrium quantity of ads is determined by (1) the curvature of demand from advertisers, (2) the curvature of disutility from consumers. An increase in the persuasive power of advertising will shift demand up but it’s not clear to me whether the curvature would change in one direction or the other. However we would expect more investment in media to attract user attention.</p></li></div></div>
<div class="page-columns page-full"><p>Aggregate advertising expenditure has remained at around 1% of GDP since 1930, implying relatively little change in the ability to persuade people.<a href="#fn8" class="footnote-ref" id="fnref8" role="doc-noteref"><sup>8</sup></a> <img src="images/2023-07-12-05-36-29.png" class="img-fluid"></p><div class="no-row-height column-margin column-container"><li id="fn8"><p><sup>8</sup>&nbsp;<a href="https://www.ben-evans.com/presentations">Ben Evans</a></p></li></div></div>
<p><strong>Some very loose generalizations about human persuadability:</strong></p>
<ol type="1">
<li><p><em>Human attitudes are highly sensitive to influences.</em> A person’s adult attitudes, beliefs, and preferences, are highly sensitive to their upbringing and context: if your parents are Catholic then you are highly likely to grow up Catholic. The same is true for eating fish, enjoying country music, being a Democrat, taking your shoes off inside, and your attitude to sex before marriage. The correlations can only be partly explained by direct genetic or economic effects, implying that attitudes are highly sensitive to experience.</p></li>
<li><p><em>The effective means of persuasion are only crudely known.</em> In the past 100 years there have been a dozen apparent discoveries of hyper-effective means of persuasion, as discussed above.</p></li>
<li><p><em>Carefully measured estimates of persuasive effects tend to be small.</em></p></li>
<li><p><em>Nevertheless large resources are devoted to persuasion,</em><a href="#fn9" class="footnote-ref" id="fnref9" role="doc-noteref"><sup>9</sup></a>.</p></li>
</ol>
<div class="no-row-height column-margin column-container"><li id="fn9"><p><sup>9</sup>&nbsp;McCloskey and Klamer have a 1995 paper titled “One Quarter of GDP is Persuasion”.</p></li></div></section>
<section id="appendix-on-synthesis" class="level1">
<h1>Appendix: On Synthesis</h1>
<p><strong>In a nutshell:</strong></p>
<ol type="1">
<li><p><strong>Q: Should we expect LLMs to exhibit superhuman performance?</strong> E.g. will they be able to create hyper-persuasive advertising copy?</p></li>
<li><p><strong>As they’re currently used the answer is no.</strong> An LLM generates an answer to a question by predicting how a human would answer that question. So we could expect a flawless LLM to match human performance, but not exceed it.</p></li>
<li><p><strong>However if we can query the LLM arbitrarily many times then the answer is likely yes.</strong> This is because human judgment has an asymmetry between <em>recognizing</em> and <em>creating</em>: our ability to recognize persuasive text exceeds our ability to create persuasive text. Thus an LLM could be used to search the space of all possible strings of text and find the one which maximizes human-estimated persuasiveness.</p></li>
<li><p><strong>The same argument applies to many other cases.</strong> In many cases humans show a gap between (a) recognizing whether text has some property, and (b) creating text that has that property:</p>
<table class="table">
<thead>
<tr class="header">
<th>recognize (easy)</th>
<th>create (hard)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>___ is a persuasive slogan</td>
<td>A persuasive slogan is ___</td>
</tr>
<tr class="even">
<td>___ is a funny joke</td>
<td>A funny joke is ___</td>
</tr>
<tr class="odd">
<td>___ is the Prime Minister</td>
<td>The Prime Minister is ___</td>
</tr>
</tbody>
</table>
<p>An LLM can be used to close each of these gaps: they can find the text which satisfies these properties.</p>
<p>An outstanding example is for images: neural nets first learned to match human performance in recognizing the properties of an image (e.g.&nbsp;recognizing horses, recognizing astronauts, recognizing Van Gogh’s style). They are now are able to synthesize new images that satisfy an arbitrary set of properties, exceeding the ability of humans to create such images manually.</p></li>
</ol>
<p><strong>Related.</strong></p>
<p><em>Why do people have an asymmetry between recognition and production?</em></p>
<p><em>Factual vs logical asymmetry.</em> - Hash function.</p>
</section>
<section id="extra-material" class="level1">
<h1>Extra Material</h1>
<ul>
<li>Monty Python’s lethally funny joke.</li>
</ul>
<p><strong>Q: has there been progress over time?</strong> Have people got better at choosing values of <span class="math inline">\(t\)</span> which maximize <span class="math inline">\(p\)</span>?</p>
<ul>
<li><em>Aesthetic things:</em> jokes, music, plays, novels. The surface p(.) changes, but also we accumulate examples which are better.</li>
<li><em>Persuasion:</em> history of advertising would say we discovered certain things: pretty girls, wording of slogans, etc. But share of GDP which is persuasion has been stuck at 1%.</li>
<li><em>Inventions:</em> wheels on luggage; lime with corn; chemical compounds – drugs that have certain properties.</li>
</ul>
<section id="other-applications" class="level2">
<h2 class="anchored" data-anchor-id="other-applications">Other Applications</h2>
<dl>
<dt>super-accurate medical diagnoses</dt>
<dd>
here there’s a ground truth that you can train LLMs directly on, so perfectly reasonable that they’d shoot past.
</dd>
<dt>super-good chess</dt>
<dd>
there’s a ground truth, though training on human play is a good shortcut to the frontier
</dd>
<dt>super-evocative prose</dt>
<dd>
abc
</dd>
<dt>prove theorems beyond human abilities</dt>
<dd>
abc
</dd>
<dt>super-groovy music</dt>
<dd>
abc
</dd>
<dt>super-beautiful paintings</dt>
<dd>
abc
</dd>
</dl>



</section>
</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-bai2023persuade" class="csl-entry" role="listitem">
Bai, Hui, Jan G Voelkel, johannes C Eichstaedt, and Robb Willer. 2023. <span>“Artificial Intelligence Can Persuade Humans on Political Issues.”</span> OSF Preprints. <a href="https://doi.org/10.31219/osf.io/stakv">https://doi.org/10.31219/osf.io/stakv</a>.
</div>
<div id="ref-goldstein2023persuasive" class="csl-entry" role="listitem">
Goldstein, Josh A, Jason Chao, Shelby Grossman, Alex Stamos, and Michael Tomz. 2023. <span>“Can AI Write Persuasive Propaganda?”</span> SocArXiv. <a href="https://doi.org/10.31235/osf.io/fp87b">https://doi.org/10.31235/osf.io/fp87b</a>.
</div>
<div id="ref-hackenburg2023persuasive" class="csl-entry" role="listitem">
Hackenburg, Kobi, and Helen Margetts. 2023. <span>“Evaluating the Persuasive Influence of Political Microtargeting with Large Language Models.”</span> OSF Preprints. <a href="https://doi.org/10.31219/osf.io/wnt8b">https://doi.org/10.31219/osf.io/wnt8b</a>.
</div>
<div id="ref-matz2023personalized" class="csl-entry" role="listitem">
Matz, Sandra, Jake Teeny, Sumer S Vaid, Gabriella M Harari, and Moran Cerf. 2023. <span>“The Potential of Generative AI for Personalized Persuasion at Scale.”</span> PsyArXiv. <a href="https://doi.org/10.31234/osf.io/rn97c">https://doi.org/10.31234/osf.io/rn97c</a>.
</div>
<div id="ref-palmer2023large" class="csl-entry" role="listitem">
Palmer, Alexis, and Arthur Spirling. 2023. <span>“Large Language Models Can Argue in Convincing and Novel Ways about Politics: Evidence from Experiments and Human Judgement.”</span> Working paper), Technical report.
</div>
<div id="ref-qin2023large" class="csl-entry" role="listitem">
Qin, Zhen, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen, Tianqi Liu, et al. 2023. <span>“Large Language Models Are Effective Text Rankers with Pairwise Ranking Prompting.”</span> <a href="https://arxiv.org/abs/2306.17563">https://arxiv.org/abs/2306.17563</a>.
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>