<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.357">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Tom Cunningham">
<meta name="dcterms.date" content="2017-04-15">
<meta name="description" content="Tom Cunningham blog">

<title>The Repeated Failure of Laws of Behaviour | Tom Cunningham</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<script src="../site_libs/quarto-html/quarto.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

<script type="text/javascript">

(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-12027453-1', 'auto');

ga('send', {
  hitType: 'pageview',
  'anonymizeIp': true,
});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../styles.css">
<meta name="twitter:title" content="The Repeated Failure of Laws of Behaviour | Tom Cunningham">
<meta name="twitter:description" content="">
<meta name="twitter:image" content="https://www.dropbox.com/s/g5zrkpncc4mff29/krazy.jpg?raw=1">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../index.html">
    <span class="navbar-title">Tom Cunningham</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href=".././about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/testingham" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/tom-cunningham-a9433/" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="https://scholar.google.com/citations?user=MDB_DgkAAAAJ" rel="" target="">
 <span class="menu-text">scholar</span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">The Repeated Failure of Laws of Behaviour</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 15, 2017</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.dropbox.com/s/g5zrkpncc4mff29/krazy.jpg?raw=1" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">krazy kat</figcaption>
</figure>
</div>
<section id="nutshell" class="level1">
<h1>Nutshell</h1>
<ol type="1">
<li><p>In retrospect a lot of behaviour that was studied in the lab, which we thought was telling us about the wiring of animals, actually was telling us about the world outside the animal..</p></li>
<li><p>It has turned out, over and over again, that an animal’s response to a stimulus reflects the animal’s <em>beliefs</em> about what that stimulus represents in the world. So the “laws” of behaviour that we discovered are actually just describing, at a remove, regularities in the world.</p>
<table class="table">
<colgroup>
<col style="width: 47%">
<col style="width: 52%">
</colgroup>
<thead>
<tr class="header">
<th>“law” of behaviour</th>
<th>truth about the environment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Animals will tend to repeat whichever action is rewarded</td>
<td>Actions which have been rewarded in the past tend to be rewarded in the future</td>
</tr>
<tr class="even">
<td>Objects appear darker to people when neighboring objects are brighter</td>
<td>Objects <em>are</em> darker when neighboring objects are brighter</td>
</tr>
<tr class="odd">
<td>Blue objects appear more distant to people</td>
<td>Blue objects <em>are</em> more distant</td>
</tr>
<tr class="even">
<td>People brake when the rate of change of the angle of an approaching object (<span class="math inline">\(\frac{\dot{\theta}}{\theta}\)</span>) exceeds some threshold</td>
<td><span class="math display">\[\frac{\dot{\theta}}{\theta}\]</span> determines the time to impact of an approaching object in typical circumstances, and so the optimal time to brake</td>
</tr>
<tr class="odd">
<td>Consumers’ expenditure increases less than proportionally with changes in income</td>
<td>Changes in income are typically temporary, and so imply a less-than-proportional response to maintain stable long-run expenditure</td>
</tr>
</tbody>
</table></li>
<li><p>Here’s the point expressed formally. Some scientist observes response <span class="math inline">\(r\)</span> and stimulus <span class="math inline">\(s\)</span>, and proposes a law of behaviour, a simple function from <span class="math inline">\(s\)</span> to <span class="math inline">\(r\)</span>. But for any such law, <span class="math inline">\(r(s)\)</span> there exists at least one rationalization, under which the organism has beliefs about an unobserved variable <span class="math inline">\(x\)</span>, and they choose <span class="math inline">\(r\)</span> optimally given what they infer about <span class="math inline">\(x\)</span> from observing <span class="math inline">\(s\)</span>, i.e.,</p>
<p><span class="math display">\[r(s) = \arg \max_{r} \int u(s,r,x) f(x|s)\]</span></p>
<p>Given some pattern of behaviour <span class="math inline">\(r(s)\)</span>, we can back out the beliefs that would justify that behaviour, <span class="math inline">\(f(x\|s)\)</span>, and we’ve seen – many times repeated – that those beliefs turn out to be <em>accurate</em> – as in the cases above, even when the scientist wasn’t aware of that truth.</p></li>
<li><p>You could reply that, sure, it’s optimal in the typical situation, but animals keep applying the same behaviours in cases where it’s not optimal, and that’s why they are laws of behaviour. There are some cases like that, but it seems to me that there are many more cases which go in the other direction: when the situation is changed, the behaviour changes, and it turns out the animal does what’s optimal, not what the law implies.</p></li>
<li><p>The biggest example of the failure of behavioural laws is the theory of <strong>conditioning and associative learning</strong>. Psychologists started with proposing a simple function that governs behaviour – the more often you are rewarded for doing something, the more often you do it – but then were forced to add a long list of qualifications and special cases (context-dependence, blocking, intermittent reinforcement, extinction, matching). It gradually became clear that the complications were not arbitrary, but made sense from the animal’s point of view: they are sensible strategies based on what an animal should expect in a typical environment. So the complex rules that we had been mapping out were not telling us about the animal’s wiring, they were instead telling us about the world that the animal lives in. Animals tend to repeat actions that are rewarded (i.e.&nbsp;obey the laws of reinforcement learning) only when they have reason to believe that rewards will be positively correlated across time. When they are in a situation where they don’t expect that correlation, then they no longer obey the rules of reinforcement learning. Mitchell et al.&nbsp;(2009) cite a lot of evidence about human associative learning and say:</p>
<blockquote class="blockquote">
<p>“we reconsider (and reject) one of the oldest and most deeply entrenched dual-system theories in the behavioral sciences, namely the traditional view of associative learning as an unconscious, automatic process that is divorced from higher-order cognition.”</p>
</blockquote>
<p>Their rejection is based on evidence that, when people learn associations, they only follow them insofar as those associations are good guides to achieving their goals.</p></li>
<li><p>There’s a very similar case in <strong>perception</strong>, where psychologists have been trying to learn the function from sensation to perception. A famous observation was of lateral inhibition: a stimulus seems less bright when the neighbouring stimulus gets brighter. In the 1950s this was thought to be due to wiring of neurons in the eye, but gradually it became clear that the effect only occurs in certain cases, and in other cases the opposite effect is observed. And then people realized that the cases in which it works are exactly the cases where it would be a reasonable inference in a typical environment. Adelson (1993, <em>Science</em>):</p>
<blockquote class="blockquote">
<p>“All of the phenomena discussed above lead to the same conclusion: Brightness judgments cannot be simply explained with low-level mechanisms. Geometrical changes that should be inconsequential for low-level mechanisms can cause dramatic changes in the brightness report. It is as if the visual system automatically estimates the reflectances of surfaces in the world[.]”</p>
</blockquote></li>
<li><p>A similar thing has happened in the study of <strong>control laws</strong>, or invariants, simple principles which map stimulus to response. For example Lee’s (1976) tau-dot model of braking: you brake when <span class="math inline">\(\dot{\tau}\)</span> is above some threshold, where <span class="math inline">\(\tau=\frac{\theta}{\dot{\theta}}\)</span>, and <span class="math inline">\(\theta\)</span> is the angle of an approaching object. Many people spent a lot of time proposing control laws for different domains, and testing control laws against each other, but the field (I believe) has now mostly given up the hope of finding simple laws to model behaviour. Weber and Fajen (2014) say:</p>
<blockquote class="blockquote">
<p>“numerous studies have demonstrated that observers often rely on non-invariants and that the particular optical variables upon which they rely to guide action can change—as a consequence of practice, as a function of the range of conditions that are encountered, and as a function of the dynamics of the controlled system.”</p>
</blockquote></li>
<li><p>Slightly more of a stretch - there are some similar episodes in the study of <strong>economic decision-making</strong>. In the 1950s economists had established various laws about how expenditure related to a person’s income. Milton Friedman, in 1957, showed that many properties and puzzles of the expenditure function could be understood as byproducts of a person rationally planning to spread their income over time. Likewise Lucas (1976) argued that, in three different cases, where a statistical regularity in decisions was observed, you could explain <em>why</em> they occur if you model people as making tradeoffs given sensible beliefs about their economic prospects.</p></li>
<li><p>I’m not trying to argue that all behaviour is rational and that the brain optimally combines all available information. But looking at the track record of psychologists they have systematically underestimated the brain – they keep proposing simple behavioural rules – response as a function of stimulus – which later turn out to be only true insofar as they reflect some deeper organizing principle.</p></li>
<li><p>I think the same is true for a lot of <strong>behavioural biases.</strong> Economists sometimes treat “loss aversion”, “probability weighting”, etc., as if they are hard-wired, and scratch their heads when an experiment finds behaviour going in the opposite direction. But almost certainly these regularities are just local manifestations of some deeper - as yet unknown - principles. (An earlier <a href="../2016/04/30/relative-thinking/">post</a> makes this point about “relative thinking” effects).</p></li>
<li><p>The rest of this note gives some more detail about how this pattern played out in the history of reinforcement learning.</p></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.dropbox.com/s/u3pqkseoxfqvh1k/herriman-03.jpg?raw=1" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Krazy Kat</figcaption>
</figure>
</div>
</section>
<section id="reinforcement-learning" class="level1">
<h1>Reinforcement Learning</h1>
<p>For a long time I’d been confused about the status of reinforcement learning theory – the theory was massively popular in the first half of the 20th century, but these days psychologists seem to treat it as defunct, an ex-theory. And yet it’s still being used: people who train animals still talk a lot about conditioning, neuroscientists are crazy about reinforcement learning, as are people who work in artificial intelligence.</p>
<p>From having read a lot of this stuff my basic understanding is now this: The generalizations about how animals and humans learn are valid only in specific contexts, they are <em>not</em> deep and fundamental laws of behaviour, as had been once believed. However the theory remains a very useful simple model of decision-making because it’s a good model of the <em>environment</em> in which decision-makers typically work. I.e., the theory doesn’t tell us much about how the brain works, it tells us mostly about the environment which the brain was designed for.</p>
</section>
<section id="classic-examples-of-reinforcement-learning" class="level1">
<h1>Classic Examples of Reinforcement Learning</h1>
<p>In 1905 Thorndike proposed a basic principle of animal learning: “Of several responses … those which are accompanied or closely followed by satisfaction … will be more likely to recur.” The same basic idea was later described as “instrumental conditioning,” “operant conditioning”, or “reinforcement learning.” The term ‘behaviorism’ referred to a school of psychology, popular in the middle of the 20th century, based on this principle or variants.</p>
<p>Rats and pigeons in boxes will pretty quickly learn to tap a button, when tapping the button is followed by receiving a food pellet. They can also learn much more complex functions through reinforcement, like learning to tap on the left button when they see a square, and on the right button when they see a circle. (B F Skinner worked on using pigeons to guide missiles, by training them to tap on pictures of military targets.)</p>
<p>In the 1950s there were some influential studies in which human behaviour was manipulated through conditioning. In Greenspoon (1955) a subject would be asked to say aloud a sequence of words, whichever came to mind, and the experimenter would give subtle positive cues when he heard certain types of word - for example plural nouns. Gradually subjects would start saying plural nouns more often, without being aware of this. The implication drawn was that many of our everyday decisions, which we think of as conscious and deliberate, are actually just imprints of previous patterns of reinforcement.</p>
<p>I believe that lots of animal training still uses principles of conditioning: you give the animal a small reward whenever it does something you want it to, and you gradually build up more complicated behaviours. They also use other concepts from conditioning like secondary reinforcers and intermittent reinforcement.</p>
<p>In 1992 IBM built a backgammon-playing neural net that used a kind of reinforcement learning – in short the computer would be more likely to make the same move again if it had a good outcome in previous cases. The ultimate reinforcement was from winning the game, and expectations propagated back from that, to learn the value of positions in the middle of the game. The program was a great success – it was trained against itself, and quickly became good enough to beat most human players.</p>
<p>In the 1990s there was a lot of excitement when some neuroscientists discovered that levels of dopamine in the mid-brain responded to rewards in a way consistent with a reinforcement-learning model. In particular, dopamine didn’t correlate with the level of reward, but correlated with the level of <em>unexpected</em> reward: i.e., if you receive a reward in a situation where you wouldn’t normally. This is the kind of calculation which an algorithm would do if it was implementing reinforcement learning: it would update weights when a rewards is different from the expected level of reward.</p>
</section>
<section id="additional-laws" class="level1">
<h1>Additional Laws</h1>
<p>There are some interesting additional laws that were discovered about conditioning.</p>
<p><strong>blocking.</strong> The order in which associations are learned is important. Suppose a pigeon learns to press a lever whenever she hears a beep. Subsequently, the beep is always accompanied by a flash. When the flash appears by itself, the pigeon won’t have learned to peck. But if the beep and flash were paired right from the beginning, then both the beep or flash would, by themselves, be sufficient for the pigeon to peck. So learning one association can “block” another association from being learned. (“Reverse blocking” is sometimes, but not always, also observed: the pigeon learns to associate A and B with a reward, but then when she finds that B predicts the reward by itself, she subsequently ignores A.).</p>
<p><strong>intermittent reinforcement.</strong> Intermittent reinforcement is often found to create more robust associations than unvarying reinforcement. If you give a pigeon a pellet every time she pecks the button, then when you stop giving her pellets she’ll stop pecking the button. If you only give her a pellet occasionally, then the behaviour will take much longer to die out.</p>
<p><strong>matching.</strong> If you give a pigeon two different levers, each of which will release a pellet with a fixed probability, then the bird will learn to peck preferentially on the lever with the higher probability. However it will still occasionally peck on the lever with the lower probability, roughly in proportion to the ratio of probabilities. This seems to be a violation of rationality because if the pigeon had learned the probabilities, and was maximizing expected value, then it should peck constantly at the high-value lever.</p>
<p>(also: secondary reinforcement; overtraining &amp; extinction; superstition.)</p>
<p>At the height of the enthusiasm for conditioning many people thought these laws gave insight into all aspects of human behaviour - mental illness, adolescent delinquency, sexual behaviour, language.</p>
</section>
<section id="difficult-cases" class="level1">
<h1>Difficult Cases</h1>
<p>In the first few decades of reinforcement learning many confirmations of the basic theory were published but, as often happens, the published evidence became less coherent as time went on. Many of the laws of reinforcement learning turn out to apply only in a subset of situations, or the parameters varied widely, and in other situations the effects seem to reverse.</p>
<p><strong>reverse reinforcement.</strong> An old finding, regarding rats running mazes, is that when the rat finds a piece of cheese down one passage, then they were <em>less</em> likely to go down that passage the next time they were in the maze. According to reinforcement learning they should be more likely to go down that passage.</p>
<p><strong>context specificity.</strong> The speed of learning associations between stimuli and responses is very different depending on the stimulus and the response. Some associations can be learned firmly with just a single experience, for example a rat refusing to eat red pellets after getting nauseous after eating a red pellet. Others associations take far longer, e.g.&nbsp;a rat learning to associate a sound with getting nauseous.</p>
<p><strong>awareness.</strong> In humans, despite many attempts, very few cases have been found in which reinforcement can affect behaviour without people being consciously aware of it. Some of the classic findings have been reconsidered: in the study which manipulated peoples’ choice of words, it was found that the effect only occurred among subjects who were consciously aware of the association. Additionally, many learned responses can be turned on or off by simply telling the person. Colgan (1970) told subjects, after they learned an association, that the association is no longer valid (“from now on the bell will not signal an electric shock”) and he found that, although this didn’t entirely extinguish the flinching, it was much less pronounced after the instruction.</p>
</section>
<section id="putting-it-back-together" class="level1">
<h1>Putting it Back Together</h1>
<p>As I said, the laws of reinforcement learning turn out to apply only in a subset of situations.</p>
<p>One interpretation is that there do exist laws of learning, but that they are more complex. However the agreements and deviations from reinforcement-learning are not at random: in many cases they can be understood: the theory works in just those contexts where past associations tend to be a good guide to future associations; and it fails in contexts where that’s not true. E.g., a rat knows that the taste of food is a good predictor of whether it’ll make you sick, but doesn’t have reason to believe that the sound you hear when you eat food is a very good predictor.</p>
<p>Consider the rat who is less likely to run down a passage when they were previously rewarded for running down that passage. This makes sense if the rat remembers that he just ate the food down that passage, and so wants to look elsewhere. In this situation the rat expects the future payoff of an action to be <em>negatively</em> correlated with the past payoff, rather than positively correlated, and so we get the opposite effect than that predictedb by reinforcement learning.</p>
<p>The basic law of reinforcement learning can be recast in terms of beliefs: if you expect the future payoff of an action to be positively correlated with its past payoff, then it is rational to perform whichever act was rewarded in a similar situation in the past.</p>
<p>The other laws of reinforcement can also be recast in terms of beliefs. And the situations in which those laws are violated are often exactly the situations where such beliefs would not apply.</p>
<p><strong>intermittent reinforcement.</strong> Suppose an act was only occasionally reinforced. This means that on previous occasions when rewards stopped they resumed again later. So it’s not surprising that, having experienced rewards stop and resume once before, when they stop again you expect them to resume again.</p>
<p><strong>matching.</strong> There’s an obvious argument that probability matching is rational – in your usual environment the probability of reward changes over time, so it makes sense to continue monitoring each action, to see if the payoff has changed (see Estes, 1976).</p>
<p><strong>blocking.</strong> Blocking can be explained by a learning model, given a prior that <em>either</em> A or B is predictive of the reward, but not both. (See “Explaining away in Weight Space” by Dayan and Kakade, and good summary at http://www.cs.cmu.edu/~ggordon/conditioning-slides.pdf ).</p>
<p>Sometimes we observe that forward blocking occurs but not backward blocking. This can be explained by a mechnical prediction model (a Kalman filter), where you update weights only when unexpected things happen. So, you learn that A &amp; B are both associated with reward, then you are exposed to cases with just A and reward. Can this model be rationalized? It’s not clear to me. (AKA, what priors would justify a Kalman filter, given that it depends on the order of presentation?)</p>
<p>Mitchell (2009) also notes that there is <em>less</em> blocking when you introduce cognitive load, implying that it’s not an automatic or mechanical effect (subjects “showed blocking of skin conductance CRs only when blocking was a valid inference.”)</p>
<p><strong>transfer learning.</strong> There are some examples where organisms can transfer patterns they have learned across quite different stimuli, e.g.&nbsp;learning patterns of complementarity/substitutability (Mitchell (2009) p190). This would require an elaborate reinforcement learning model to rationalize, but is simple with a rational model.</p>
<p><strong>context specificity.</strong> De Houwer, Vandorpe and Beckers (2005) say (in “Why have associative models fared so well?”)<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a></p>
<blockquote class="blockquote">
<p>The two types of models can be differentiated … by manipulating variables that influence the likelihood that people will reason in a certain manner but that should have no impact on the operation of the associative model. We have seen that such variables (e.g., instructions, secondary tasks, ceiling effects, nature of the cues and outcomes) do indeed have a huge effect. Given these results, it is justified to entertain the belief that participants are using controlled processes such as reasoning and to look for new ways to model and understand these processes.</p>
</blockquote>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>I guess there are two natural followup questions to this argument:</p>
<p><strong>(1) If you looked in the history of psychology, could you find as many examples of making the opposite mistake?</strong></p>
<p>In other words, how often have we have over-estimated the rationality of human behaviour. Yeah, maybe you’re right. It’s not hard to find economists who will insist that, whatever people do, it’s in their best interest. I just think that psychologists tend to make the other mistake.</p>
<p><strong>(2) If you’re so down on it, then why is reinforcement learning useful to dog trainers and to computer programmers?</strong></p>
</section>
<section id="misc-notes" class="level1">
<h1>Misc Notes</h1>
<hr>
<p><strong>economic applications of association-based decision-making.</strong> Gilboa &amp; Schmeidler: case-based decision-making; Camerer: experience-weighted attraction learning. The NYU guy has a paper. Erev &amp; Roth (1998) say that reinforcement learning does a good job predicting behaviour in some games, better than equilibrium play. I wouldn’t defend completely rational behaviour, but on the other hand I wouldn’t expect RL behaviour to be <em>stable</em>: probably behaviour approximates RL in some contexts, and does the opposite in others. It’s not obvious that the RL model is a very good level of abstraction to describe behaviour at. Charness &amp; Levin (2005) run an experiment where reinforcement &amp; Bayesian updating give different predictions: you choose between urns, one more sensitive to state, one less sensitive. If you draw from the less-sensitive urn, and you receive a positive outcome, then you update about the state, and the Bayesian prediction is that you should switch urns, while reinforcement learning says you’ll stay with the same urn. They find that people largely stay with the same urn.</p>
<hr>
<p><strong>the gambler’s fallacy goes in the opposite direction to reinforcement learning.</strong> The gambler’s fallacy: winning a gamble at time <span class="math inline">\(t\)</span> makes you <em>decrease</em> the expectation of winning at <span class="math inline">\(t+1\)</span>, i.e.&nbsp;the <em>opposite</em> prediction of a simple reinforcement model. However there’s a heuristic rationalization similar to the rationalization of rats in a maze: caveman Ug is shaking trees to get coconuts out. If there’s no coconut at time t, then there’s an increased probability of a coconut at t+1.</p>
<p><strong>Poggio and visual perception.</strong> In 1983 Poggio found that he could reinterpret prior findings in perception as implementation of Bayesian inference: &gt; “All problems in vision and more general perception were inverse problems, going back from the image to 3-D properties of objects and scenes. They were also, as typical for inverse problems, ill-posed. We used regularization techniques to “solve” specific vision problems such as edge detection and motion computation. In the process, we found that some of the existing algorithms for shape-from-shading, optical flow, and surface interpolation were a form of regularization. Our main contribution was to recognize illposedness as the main characteristic of vision problems and regularization as the set of techniques to be used for solving them.”</p>
<p><strong>Shephard’s theory of generalization.</strong> Shepard (1987) and Tenenbaum and Griffiths (2001) give a persuasive argument that apparent laws governing generalization between stimuli are context-dependent, in a way that is consistent with Bayesian inference.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Also Seligman (1970) <em>On the Generality of Laws of Learning</em>, “That all events are equally associable and obey common laws is a central assumption of general process learning theory … A review of data from the traditional learning paradigms shows that the assumption of equivalent associability is false … it is speculated that the laws of learning themselves may vary with the preparedness of the organism for the associa- tion and that different physiological and cognitive mechanisms may covary with the dimension.”<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>