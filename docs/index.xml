<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Tom Cunningham</title>
<link>tecunningham.github.io/</link>
<atom:link href="tecunningham.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>{{&lt; meta description-meta &gt;}}</description>
<generator>quarto-1.5.57</generator>
<lastBuildDate>Mon, 22 Sep 2025 07:00:00 GMT</lastBuildDate>
<item>
  <title>Too Much Good News is Bad News</title>
  <dc:creator>Tom Cunningham</dc:creator>
  <link>tecunningham.github.io/posts/2024-12-26-heavy-tailed-noise.html</link>
  <description><![CDATA[ 





<p>Here are two nice pieces of Bayesian logic observations that help explain everyday intuitions:</p>
<ol type="1">
<li>When an outcome is the sum of two components then your belief about the contribution of the thinner-tailed component will be first increasing then decreasing in the realization of the outcome.</li>
<li>When you observe an outlier in some process, which is the sum of multiple components, then:
<ul>
<li>If the components are thin-tailed, then the outlier implies each of the components is elevated.</li>
<li>If the components are fat-tailed, then the outlier implies just one of the components is elevated and the others are at their ordinary levels.</li>
</ul></li>
</ol>
<section id="too-much-good-news-is-often-bad-news" class="level1">
<h1>Too Much Good News is Often Bad News</h1>
<p>In many cases a signal which is good news eventually starts to become bad news:</p>
<ol type="1">
<li>The longer you wait for a bus the likelier it is to be about to arrive, until at some point it’s more likely that you’ve missed it.</li>
<li>The lower the price the better the value until it becomes suspiciously cheap.</li>
<li>If a drug is associated with a 5% higher rate of birth defects it’s probably a selection effect, if it’s associated with a 500% higher rate of birth defects it’s probably causal.<sup>1</sup> </li>
<li>If an AB test shows an effect of +2% (<img src="https://latex.codecogs.com/png.latex?%5Cpm"> 1%) it’s very persuasive, but if it shows a an effect of +50% (<img src="https://latex.codecogs.com/png.latex?%5Cpm"> 1%) then the experiment was probably misconfigured, and it’s not at all persuasive.<sup>2</sup></li>
<li>When reading a biography each detail makes the the subject seem more impressive until you start to doubt the neutrality of the biographer.</li>
</ol>
<p>In each of these cases I think it’s because the noise has a fatter-tailed distribution than the signal. As a consequence when you see a very-high observation you conclude it’s mostly noise, which implies that after a certain point an increase in the observed outcome becomes bad news instead of good news.</p>
</section>
<section id="outliers-usually-have-one-cause-not-many" class="level1 page-columns page-full">
<h1>Outliers Usually Have One Cause, Not Many</h1>
<p>This is a related but slightly different point. Scott Sumner argued <a href="https://www.themoneyillusion.com/author/ssumner/page/183/">“extreme events generally have multiple causes”</a>, with a few examples:</p>
<ol type="1">
<li>Italy’s very high COVID death rates are probably due to a number of different factors, not one.</li>
<li>The great depression was due to “multiple policy errors, on both the supply side and the demand side.”</li>
<li>Bob Beamon’s record long-jump in 1968 was probably a coincidence of multiple causes.<sup>3</sup></li>
</ol>
<p>I think Sumner is wrong in his generalization: extreme events typically have a single cause, not multiple causes. Formally (following Nair, Weierman and Zwart below), extreme draws from a sum of thin-tailed influences tend to have many causes, but extreme draws from a sum of fat-tailed influences tend to have one cause.</p>
<p>We often cannot observe the distributions of the individual components but we can observe the distribution of the final aggregate outcome, and if the final outcome is fat-tailed then at least some of the components must be fat-tailed.</p>
<p>I’m not sure about Italy’s COVID and the great depression, but for Bob Beamon’s jump in 1968 it looks like a non-Normal outlier, not the sum of orthogonal influences (see the spike on the blue line at right). Thus from this evidence we should expect, all else equal, that Beamon’s 8.9 metres was due to one big cause not many small ones.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="tecunningham.github.io/posts/images/2024-12-26-07-59-52.png" class="img-fluid"></p>
</div></div></section>
<section id="formalizing-this" class="level1 page-columns page-full">
<h1>Formalizing This</h1>
<dl>
<dt>We can summarize the theory with three observations.</dt>
<dd>
Suppose you observe a variable that is a sum of multiple components. Then:
</dd>
<dd>
<ol type="1">
<li>If components are IID and thin-tailed: outliers are due to many small causes (“conspiracy”).</li>
</ol>
</dd>
<dd>
<ol start="2" type="1">
<li>If components are IID and fat-tailed: outliers are due to a few big causes (“catastrophe”).</li>
</ol>
</dd>
<dd>
<ol start="3" type="1">
<li>If components are a mixture of thin-tailed and fat-tailed: low observations are due to the thin-tailed components, high observations are due to the fat-tailed components (“rejection”).</li>
</ol>
</dd>
</dl>
<p><strong>Literature.</strong> De Finetti wrote a paper in 1961, “The Bayesian Approach to the Rejection of Outliers”, giving a simple example where, with fat-tailed noise, a Bayesian will discount outliers.</p>
<p>There seem to be two modern strands of this literature that use somewhat different terminology:</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="tecunningham.github.io/posts/images/2024-12-28-15-29-13.png" class="img-fluid"></p>
</div></div><ol type="1">
<li><p>Anthony O’Hagan and Luis Pericchi (2012) <a href="https://projecteuclid.org/download/pdfview_1/euclid.bjps/1341320249">Bayesian heavy-tailed models and conflict resolution: a review</a>. They give a number of conditions under which you get “conflict resolution” or “rejection of outliers”, broadly speaking when the noise has fatter tails than the signal. The critical condition is the relative speed of decline of the tails of the distributions of signal and noise. The illustration at right is very elegant, showing how the posterior will be bimodal when you have the sum of two heavy-tailed distributions.</p></li>
<li><p>Nair, Weierman and Zwart have a chapter “Catastrophes, conspiracies, and subexponential distributions” in their book <a href="https://adamwierman.com/wp-content/uploads/2021/05/book-05-11.pdf">“The Fundamentals of Heavy Tails”</a>.</p>
<ul>
<li>Suppose you observe the sum of a set of <img src="https://latex.codecogs.com/png.latex?N"> IID random variables. They discuss two polar ways in which your posteriors about the components will depend on the sum:</li>
<li>If the components are heavy tailed you get the “catastrophe principle”: the probability that the sum exceeds some value <img src="https://latex.codecogs.com/png.latex?t"> will be approximately equal to the probability that the maximum of <img src="https://latex.codecogs.com/png.latex?N"> components exceeds <img src="https://latex.codecogs.com/png.latex?t">, as <img src="https://latex.codecogs.com/png.latex?t%5Crightarrow%5Cinfty">.</li>
<li>If the components are light-tailed you get the “conspiracy principle”: the probability that the sum exceeds some value <img src="https://latex.codecogs.com/png.latex?t"> <img src="https://latex.codecogs.com/png.latex?t"> dominates the probability of the maximum exceeding <img src="https://latex.codecogs.com/png.latex?t">, as <img src="https://latex.codecogs.com/png.latex?t%5Crightarrow%5Cinfty">.</li>
<li>Applications of the catastrophe principle: If a certain year has many earthquake deaths then probably there was one large earthquake, not many small ones. If a random group of people has a high average number of Twitter followers, probably one member of the group is a big outlier, and the others have an ordinary number.</li>
<li>Applications of the conspiracy principle: if a random group of people has a high average height then probably each individual is tall.</li>
</ul></li>
</ol>
<dl>
<dt>Note: non-monotonicity if and only if the sum is log-convex.</dt>
<dd>
Suppose we observe <img src="https://latex.codecogs.com/png.latex?x"> which is the sum of <img src="https://latex.codecogs.com/png.latex?v"> and <img src="https://latex.codecogs.com/png.latex?u">. Tweedie’s formula will give us <img src="https://latex.codecogs.com/png.latex?E%5Bv%7Cx%5D"> from the empirical distribution of <img src="https://latex.codecogs.com/png.latex?x">, as long as the distribution of <img src="https://latex.codecogs.com/png.latex?u"> is from an exponential family. If <img src="https://latex.codecogs.com/png.latex?u"> is Normal with variance <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2_u"> then we have: <img src="https://latex.codecogs.com/png.latex?E%5Bv%7Cx%5D%20=%20x%20+%20%5Csigma%5E2_u%20%5Cfrac%7Bd%7D%7Bdx%7D%5Clog%20f(x).">
</dd>
<dd>
There are two interesting bad news cases (i.e.&nbsp;non-monotonicity): <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%5Cfrac%7BdE%5Bv%7Cx%5D%7D%7Bdx%7D%20&amp;%3C0%20&amp;&amp;%20%5Ctext%7B($x$%20is%20bad%20news%20about%20$v$)%7D%5C%5C%0A%20%20%5Cfrac%7BdE%5Bv%7Cx%5D%7D%7Bdx%7D%20&amp;%3E1%20&amp;&amp;%20%5Ctext%7B($x$%20is%20bad%20news%20about%20$u$)%7D%5C%5C%0A%20%20%20%5Cend%7Baligned%7D">
</dd>
<dd>
These correspond to: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%5Cfrac%7Bd%5E2%7D%7Bdx%5E2%7D%5Clog%20f(x)%20&amp;%3C%20-%5Cfrac%7B1%7D%7B%5Csigma%5E2_u%7D%20&amp;&amp;%20%5Ctext%7B(very%20log-concave)%7D%5C%5C%0A%20%20%5Cfrac%7Bd%5E2%7D%7Bdx%5E2%7D%5Clog%20f(x)%20&amp;%3E%200%20&amp;&amp;%20%5Ctext%7B(log-convex)%7D%5C%5C%0A%20%20%20%5Cend%7Baligned%7D">
</dd>
<dd>
A nice observation: if the empirical distribution is log-convex at some point <img src="https://latex.codecogs.com/png.latex?x">, then the expectation of the Normal component <img src="https://latex.codecogs.com/png.latex?u"> must be decreasing at that point. (Note that log-concavity is a common way of characterizing light-tailed distributions.)
</dd>
<dt>Note: bus arrival times.</dt>
<dd>
Suppose the bus arrival time is <img src="https://latex.codecogs.com/png.latex?a">, then at time <img src="https://latex.codecogs.com/png.latex?t"> the expected wait time is <img src="https://latex.codecogs.com/png.latex?E%5Ba-t%7Ca%3Et%5D">. We’ll assume <img src="https://latex.codecogs.com/png.latex?a%5Csim%20N(0,1)">, then there are two interesting quantities: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20P(a%3Et)%20&amp;=%20F(t)%20&amp;&amp;%20%5Ctext%7B(probability%20bus%20has%20arrived)%7D%20%5C%5C%0A%20%20%20E%5Ba%7Ca%3Et%5D-t%20&amp;=%20%5Cfrac%7Bf(t)%7D%7B1-F(t)%7D-t%20&amp;&amp;%20%5Ctext%7B(expected%20wait%20time)%7D%0A%5Cend%7Baligned%7D">
</dd>
<dd>
Here the expected wait time is uniformly decreasing in <img src="https://latex.codecogs.com/png.latex?t">. However suppose that if we’ve missed the bus then the next expected arrival time is <img src="https://latex.codecogs.com/png.latex?T"> (e.g.&nbsp;tomorrow). Then we have expected wait time: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20&amp;F(t)T+(1-F(t))%5Cfrac%7Bf(t)%7D%7B1-F(t)%7D%20-%20t%5C%5C%0A%20%20%20=&amp;%20F(t)T%20+%20f(t)%20-%20t%0A%5Cend%7Baligned%7D">
</dd>
</dl>
<p>which will be decreasing, then increasing, then decreasing in <img src="https://latex.codecogs.com/png.latex?t">.</p>
<p>You could supplement this model by saying you arrived at the bus-stop at time <img src="https://latex.codecogs.com/png.latex?t">, and it’s now time <img src="https://latex.codecogs.com/png.latex?t'">. then we’ll have terms like <img src="https://latex.codecogs.com/png.latex?P(a%3Et'%7C(a%3Ct)%5Cwedge(a%3Et'))">, but I think the basic non-monotonicity will still hold.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Bradford-Hill: “the mortality of chimney sweeps from scrotal cancer was some 200 times that of workers who were not specially exposed to tar or mineral oils.”↩︎</p></li>
<li id="fn2"><p>This case is somewhat subtle: we generally think that treatment effects are fat-tailed, and we can be confident that noise is Normal because it’s the sum of many IID variables. However there’s an additional source of noise from implementation error which has even fatter tails than the distribution of treatment effects.↩︎</p></li>
<li id="fn3"><p><em>“While Beamon received mostly accolades, there also were detractors. The critics harped on the conditions — a following wind of 2.0 meters per second (the maximum allowable velocity for a record), a lightning fast runway and, most important, the thin air of Mexico City. Beamon’s defenders point out that the other competitors, which included the world record co-holders, had the same factors going for them and they didn’t jump close to Beamon.”</em>↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>tecunningham.github.io/posts/2024-12-26-heavy-tailed-noise.html</guid>
  <pubDate>Mon, 22 Sep 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>A Model of ChatGPT</title>
  <dc:creator>Tom Cunningham</dc:creator>
  <link>tecunningham.github.io/posts/2025-06-14-model-of-chatgpt.html</link>
  <description><![CDATA[ 





<p>This paper develops a simple model of human and AI ability to answer questions. Each question <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D"> is a high-dimensional vector, with a true scalar answer <img src="https://latex.codecogs.com/png.latex?a">. An agent’s estimate of the answer is an interpolation based on previously-seen questions and answers <img src="https://latex.codecogs.com/png.latex?(%5Cbm%7Bq%7D%5Ei,a%5Ei)_%7Bi=1,%5Cldots,n%7D">. This framework extends an <a href="https://tecunningham.github.io/posts/2023-09-05-model-of-ai-imitation.html">earlier model</a> developed for a different purpose.</p>
<p>The model yields several implications:</p>
<ol type="1">
<li><p><strong>The quality of an answer to a new question depends on its distance from the training set.</strong> For a new question <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D">, the expected error is a function of the distance between <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D"> and the training set <img src="https://latex.codecogs.com/png.latex?%5Cbm%7BQ%7D">.</p></li>
<li><p><strong>The quality of answers increases with the size of the training set.</strong> The expected error decreases linearly with the number of linearly-independent examples in the training set.</p></li>
<li><p><strong>The value of advice from another agent depends on the distance between their training sets.</strong></p></li>
</ol>
<p>This framework can be interpreted as a model of an agent, “the user,” who must provide an estimate for the answer to a question <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D"> and can choose whether to consult an AI model like ChatGPT. The key components of the model are:</p>
<ol type="1">
<li><strong>The dimensionality of the question (<img src="https://latex.codecogs.com/png.latex?p">).</strong> A higher-dimensional problem may be more costly to enter into the AI, but it also increases the potential benefit.</li>
<li><strong>The public information set.</strong> These are the training questions that the AI has observed, which we can conceptualize as the corpus of public knowledge (e.g., the internet).</li>
<li><strong>The private information set.</strong> These are the questions that the user has personally encountered and for which they have observed the true answer.</li>
</ol>
<p>A user will consult the AI if and only if the expected improvement in their answer exceeds the associated cost. The model predicts that an AI will be most useful for questions with components that are novel to the user but contained within the AI’s public training data.</p>
<p>This leads to several corollaries:</p>
<ol type="1">
<li>An AI will not be used for questions the user has encountered before.</li>
<li>An AI is more likely to be used for domains with higher <em>latent</em> dimensionality (<img src="https://latex.codecogs.com/png.latex?p">).</li>
<li>An AI is more likely to be used for domains with lower <em>surface</em> dimensionality, as this reduces the cost of specifying the question.</li>
<li>An AI is more likely to be used by humans with less experience in a domain (i.e., smaller <img src="https://latex.codecogs.com/png.latex?n_%7B%5Ctext%7Bprivate%7D%7D">).</li>
</ol>
<dl>
<dt>We can make some conjectures about adoption by occupation:</dt>
<dd>

</dd>
</dl>
<table class="caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 15%">
<col style="width: 52%">
</colgroup>
<thead>
<tr class="header">
<th>occupation</th>
<th>predicted ChatGPT use</th>
<th>reason</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>software engineer</td>
<td>high</td>
<td>many novel discrete problems, similar to those on public internet</td>
</tr>
<tr class="even">
<td>software engineer - idiosyncratic language</td>
<td>low</td>
<td>many novel discrete problems, not similar to those on public internet</td>
</tr>
<tr class="odd">
<td>physician</td>
<td>high</td>
<td>many novel discrete problems, similar to those on public internet</td>
</tr>
<tr class="even">
<td>contact center worker</td>
<td>low</td>
<td>novel problems, but not similar to those on the internet</td>
</tr>
<tr class="odd">
<td>architect</td>
<td>low</td>
<td>novel problems, not discrete, not text-based</td>
</tr>
<tr class="even">
<td>manual worker</td>
<td>low</td>
<td>not not text-based</td>
</tr>
</tbody>
</table>
<p>We can make some conjectures about adoption by task: :</p>
<table class="caption-top table">
<colgroup>
<col style="width: 42%">
<col style="width: 15%">
<col style="width: 42%">
</colgroup>
<thead>
<tr class="header">
<th>task</th>
<th>predicted ChatGPT use</th>
<th>reason</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Intellectual curiosity</td>
<td>high</td>
<td>novel discrete problem, similar to those on the internet</td>
</tr>
<tr class="even">
<td>Diagnosing medical problems</td>
<td>high</td>
<td>novel discrete problem, similar to those on the internet</td>
</tr>
<tr class="odd">
<td>Problems with widely-adopted systems (car, house, computer)</td>
<td>high</td>
<td>novel discrete problem, similar to those on the internet</td>
</tr>
<tr class="even">
<td>Problems with idiosyncratic systems (custom setups)</td>
<td>low</td>
<td>novel discrete problem, <em>not</em> similar to those the internet</td>
</tr>
</tbody>
</table>
<dl>
<dt>Additional things to add:</dt>
<dd>
<ol type="1">
<li><strong>High-dimensional answers.</strong> Our model assumes <em>scalar</em> answers. In fact ChatGPT gives high-dimensional outputs. I think we can say some nice things here.</li>
</ol>
</dd>
<dd>
<ol start="2" type="1">
<li><strong>Tacit knowledge.</strong> ChatGPT will be more likely to be used for domains where humans have tacit knowledge.</li>
</ol>
</dd>
</dl>
<section id="model" class="level1">
<h1>Model</h1>
<p><strong>The State of the World and Questions.</strong> The state of the world is defined by a vector of <img src="https://latex.codecogs.com/png.latex?p"> unobserved parameters, <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bw%7D%20%5Cin%20%5Cmathbb%7BR%7D%5Ep">. A question is a vector of <img src="https://latex.codecogs.com/png.latex?p"> binary features, <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D%20%5Cin%20%5C%7B-1,%201%5C%7D%5Ep">. The true answer to a question <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D"> is a scalar <img src="https://latex.codecogs.com/png.latex?a"> determined by the linear relationship: <img src="https://latex.codecogs.com/png.latex?a%20=%20%5Cbm%7Bq%7D'%5Cbm%7Bw%7D%20=%20%5Csum_%7Bk=1%7D%5Ep%20q_k%20w_k"></p>
<p><strong>Agents and Information.</strong> There is a set of agents, indexed by <img src="https://latex.codecogs.com/png.latex?i%20%5Cin%20%5Cmathcal%7BI%7D">. Each agent <img src="https://latex.codecogs.com/png.latex?i"> possesses an information set <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_i">, which consists of <img src="https://latex.codecogs.com/png.latex?n_i"> questions they have previously encountered, along with their true answers. We can represent this information as a pair <img src="https://latex.codecogs.com/png.latex?(%5Cbm%7BQ%7D_i,%20%5Cbm%7Ba%7D_i)">:</p>
<ul>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbm%7BQ%7D_i"> is an <img src="https://latex.codecogs.com/png.latex?n_i%20%5Ctimes%20p"> matrix where each row is a question vector. Let the <img src="https://latex.codecogs.com/png.latex?j">-th question for agent <img src="https://latex.codecogs.com/png.latex?i"> be <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D_%7Bi,j%7D'">, so that: <img src="https://latex.codecogs.com/png.latex?%5Cbm%7BQ%7D_i%20=%20%5Cbegin%7Bbmatrix%7D%20%5Cbm%7Bq%7D_%7Bi,1%7D'%20%5C%5C%20%5Cvdots%20%5C%5C%20%5Cbm%7Bq%7D_%7Bi,n_i%7D'%20%5Cend%7Bbmatrix%7D%20=%20%5Cbegin%7Bbmatrix%7D%20q_%7Bi,1,1%7D%20&amp;%20%5Ccdots%20&amp;%20q_%7Bi,1,p%7D%20%5C%5C%20%5Cvdots%20&amp;%20%5Cddots%20&amp;%20%5Cvdots%20%5C%5C%20q_%7Bi,n_i,1%7D%20&amp;%20%5Ccdots%20&amp;%20q_%7Bi,n_i,p%7D%20%5Cend%7Bbmatrix%7D"></li>
<li><img src="https://latex.codecogs.com/png.latex?%5Cbm%7Ba%7D_i"> is an <img src="https://latex.codecogs.com/png.latex?n_i%20%5Ctimes%201"> vector of the corresponding answers. The answers are generated according to the true model: <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Ba%7D_i%20=%20%5Cbm%7BQ%7D_i%20%5Cbm%7Bw%7D"></li>
</ul>
<p><strong>Beliefs.</strong> All agents share a common prior belief about the state of the world, assuming the weights <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bw%7D"> are drawn from a multivariate Gaussian distribution: <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bw%7D%20%5Csim%20N(%5Cbm%7B0%7D,%20%5CSigma)"> where <img src="https://latex.codecogs.com/png.latex?%5CSigma"> is a <img src="https://latex.codecogs.com/png.latex?p%20%5Ctimes%20p"> positive-semidefinite covariance matrix. A common assumption we will use is an isotropic prior, where <img src="https://latex.codecogs.com/png.latex?%5CSigma%20=%20%5Csigma%5E2%20%5Cbm%7BI%7D_p"> for some scalar <img src="https://latex.codecogs.com/png.latex?%5Csigma%5E2%20%3E%200">. This implies that, a priori, the weights are uncorrelated and have equal variance.</p>
<p>Given their information set <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BD%7D_i">, agent <img src="https://latex.codecogs.com/png.latex?i"> forms a posterior belief about <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bw%7D">. When a new question <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D_%7B%5Ctext%7Bnew%7D%7D"> arises, the agent uses their posterior distribution to form an estimate of the answer, <img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D_%7B%5Ctext%7Bnew%7D%7D%20=%20%5Cbm%7Bq%7D_%7B%5Ctext%7Bnew%7D%7D'%20%5Cmathbb%7BE%7D%5B%5Cbm%7Bw%7D%20%5Cmid%20%5Cmathcal%7BD%7D_i%5D">.</p>
</section>
<section id="propositions" class="level1">
<h1>Propositions</h1>
<p><strong>Proposition 1 (Posterior over <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bw%7D"> given <img src="https://latex.codecogs.com/png.latex?%5Cbm%7BQ%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Ba%7D">).</strong> <em>The agent’s posterior mean and variance will be:</em> <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Chat%7B%5Cbm%20w%7D&amp;=%20%5CSigma%20%5Cbm%7BQ%7D%5E%7B%5Ctop%7D(%5Cbm%7BQ%7D%5CSigma%20%5Cbm%7BQ%7D%5E%7B%5Ctop%7D)%5E%7B-1%7D%5Cbm%20a%5C%5C%0A%20%20%20%20%20%20%5CSigma_%7B%5Cmid%20a%7D%20&amp;=%5CSigma-%5CSigma%20%5Cbm%7BQ%7D%5E%7B%5Ctop%7D(%5Cbm%7BQ%7D%5CSigma%20%5Cbm%7BQ%7D%5E%7B%5Ctop%7D)%5E%7B-1%7D%5Cbm%7BQ%7D%5CSigma.%0A%20%20%20%5Cend%7Baligned%7D%0A%20%20%20"></p>
<p><em>Proof.</em> The derivation follows from the standard formula for conditional Gaussian distributions. We begin by defining the joint distribution of the weights <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bw%7D"> and the answers <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Ba%7D">. The weights and answers are jointly Gaussian: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Bpmatrix%7D%20%5Cbm%7Bw%7D%20%5C%5C%20%5Cbm%7Ba%7D%20%5Cend%7Bpmatrix%7D%20%5Csim%20N%5Cleft(%0A%20%20%20%20%20%20%5Cbegin%7Bpmatrix%7D%20%5Cbm%7B0%7D%20%5C%5C%20%5Cbm%7B0%7D%20%5Cend%7Bpmatrix%7D,%0A%20%20%20%20%20%20%5Cbegin%7Bpmatrix%7D%0A%20%20%20%20%20%20%20%20%20%5CSigma%20&amp;%20%5CSigma%20%5Cbm%7BQ%7D'%20%5C%5C%0A%20%20%20%20%20%20%20%20%20%5Cbm%7BQ%7D%5CSigma%20&amp;%20%5Cbm%7BQ%7D%5CSigma%20%5Cbm%7BQ%7D'%0A%20%20%20%20%20%20%5Cend%7Bpmatrix%7D%0A%20%20%20%5Cright)%0A%20%20%20"> where the covariance terms are derived as follows: - <img src="https://latex.codecogs.com/png.latex?Cov(%5Cbm%7Bw%7D,%20%5Cbm%7Bw%7D)%20=%20%5CSigma"> (prior covariance) - <img src="https://latex.codecogs.com/png.latex?Cov(%5Cbm%7Ba%7D,%20%5Cbm%7Ba%7D)%20=%20Cov(%5Cbm%7BQ%7D%5Cbm%7Bw%7D,%20%5Cbm%7BQ%7D%5Cbm%7Bw%7D)%20=%20%5Cbm%7BQ%7D%20Cov(%5Cbm%7Bw%7D,%20%5Cbm%7Bw%7D)%20%5Cbm%7BQ%7D'%20=%20%5Cbm%7BQ%7D%5CSigma%20%5Cbm%7BQ%7D'"> - <img src="https://latex.codecogs.com/png.latex?Cov(%5Cbm%7Bw%7D,%20%5Cbm%7Ba%7D)%20=%20Cov(%5Cbm%7Bw%7D,%20%5Cbm%7BQ%7D%5Cbm%7Bw%7D)%20=%20Cov(%5Cbm%7Bw%7D,%20%5Cbm%7Bw%7D)%5Cbm%7BQ%7D'%20=%20%5CSigma%20%5Cbm%7BQ%7D'"></p>
<p>The conditional mean <img src="https://latex.codecogs.com/png.latex?E%5B%5Cbm%7Bw%7D%7C%5Cbm%7Ba%7D%5D"> is given by the formula: <img src="https://latex.codecogs.com/png.latex?E%5B%5Cbm%7Bw%7D%7C%5Cbm%7Ba%7D%5D%20=%20E%5B%5Cbm%7Bw%7D%5D%20+%20Cov(%5Cbm%7Bw%7D,%5Cbm%7Ba%7D)Var(%5Cbm%7Ba%7D)%5E%7B-1%7D(%5Cbm%7Ba%7D%20-%20E%5B%5Cbm%7Ba%7D%5D)"></p>
<p>Substituting the values from our model (<img src="https://latex.codecogs.com/png.latex?E%5B%5Cbm%7Bw%7D%5D%20=%20%5Cbm%7B0%7D">, <img src="https://latex.codecogs.com/png.latex?E%5B%5Cbm%7Ba%7D%5D%20=%20%5Cbm%7B0%7D">): <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbm%7Bw%7D%7D%20=%20%5Cbm%7B0%7D%20+%20(%5CSigma%20%5Cbm%7BQ%7D')(%5Cbm%7BQ%7D%5CSigma%20%5Cbm%7BQ%7D')%5E%7B-1%7D(%5Cbm%7Ba%7D%20-%20%5Cbm%7B0%7D)%20=%20%5CSigma%20%5Cbm%7BQ%7D'(%5Cbm%7BQ%7D%5CSigma%20%5Cbm%7BQ%7D')%5E%7B-1%7D%5Cbm%7Ba%7D"></p>
<p>This gives us the posterior mean of the weights. The posterior covariance is given by: <img src="https://latex.codecogs.com/png.latex?Var(%5Cbm%7Bw%7D%7C%5Cbm%7Ba%7D)%20=%20Var(%5Cbm%7Bw%7D)%20-%20Cov(%5Cbm%7Bw%7D,%5Cbm%7Ba%7D)Var(%5Cbm%7Ba%7D)%5E%7B-1%7DCov(%5Cbm%7Ba%7D,%5Cbm%7Bw%7D)%20=%20%5CSigma%20-%20%5CSigma%20%5Cbm%7BQ%7D'(%5Cbm%7BQ%7D%5CSigma%20%5Cbm%7BQ%7D')%5E%7B-1%7D%5Cbm%7BQ%7D%5CSigma."> <img src="https://latex.codecogs.com/png.latex?%5Csquare"></p>
<p><strong>Proposition 2 (Expected error for a given question).</strong> <em>The expected squared error for a new question <img src="https://latex.codecogs.com/png.latex?%5Cbm%20q"> is:</em> <img src="https://latex.codecogs.com/png.latex?%20%5Cmathbb%7BE%7D%5B(%5Cbm%20q'(%5Cbm%20w%20-%20%5Chat%7B%5Cbm%20w%7D))%5E2%5D%20=%20%5Cbm%20q'%20%5CSigma_%7B%5Cmid%20a%7D%20%5Cbm%20q%20"> <em>For an isotropic prior where <img src="https://latex.codecogs.com/png.latex?%5CSigma%20=%20%5Csigma%5E2%20%5Cbm%7BI%7D">, the error is proportional to the squared distance of <img src="https://latex.codecogs.com/png.latex?%5Cbm%20q"> from the subspace spanned by the previously seen questions in <img src="https://latex.codecogs.com/png.latex?%5Cbm%7BQ%7D">:</em> <img src="https://latex.codecogs.com/png.latex?%20%5Cmathbb%7BE%7D%5B(%5Cbm%20q'(%5Cbm%20w%20-%20%5Chat%7B%5Cbm%20w%7D))%5E2%5D%20=%20%5Csigma%5E2%20%5C%7C(%5Cbm%7BI%7D-%5Cbm%7BP_Q%7D)%5Cbm%20q%5C%7C%5E2%20"> <em>where <img src="https://latex.codecogs.com/png.latex?%5Cbm%7BP_Q%7D"> is the projection matrix onto the row-span of <img src="https://latex.codecogs.com/png.latex?%5Cbm%7BQ%7D">.</em></p>
<p><em>Proof.</em> The prediction error is <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D'%5Cbm%7Bw%7D%20-%20%5Cbm%7Bq%7D'%5Chat%7B%5Cbm%7Bw%7D%7D%20=%20%5Cbm%7Bq%7D'(%5Cbm%7Bw%7D%20-%20%5Chat%7B%5Cbm%7Bw%7D%7D)">. The expected squared error is the variance of this prediction error. <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cmathbb%7BE%7D%5B(%5Cbm%20q'(%5Cbm%20w%20-%20%5Chat%7B%5Cbm%20w%7D))%5E2%5D%20&amp;=%20%5Cmathbb%7BE%7D%5B%5Cbm%20q'(%5Cbm%20w%20-%20%5Chat%7B%5Cbm%20w%7D)(%5Cbm%20w%20-%20%5Chat%7B%5Cbm%20w%7D)'%5Cbm%20q%5D%20%5C%5C%0A&amp;=%20%5Cbm%20q'%20%5Cmathbb%7BE%7D%5B(%5Cbm%20w%20-%20%5Chat%7B%5Cbm%20w%7D)(%5Cbm%20w%20-%20%5Chat%7B%5Cbm%20w%7D)'%5D%20%5Cbm%20q%20%5C%5C%0A&amp;=%20%5Cbm%20q'%20Var(%5Cbm%20w%20%5Cmid%20%5Cbm%20a)%20%5Cbm%20q%20=%20%5Cbm%20q'%20%5CSigma_%7B%5Cmid%20a%7D%20%5Cbm%20q%0A%5Cend%7Baligned%7D%0A"> This proves the first part of the proposition. For the second part, we assume an isotropic prior <img src="https://latex.codecogs.com/png.latex?%5CSigma%20=%20%5Csigma%5E2%5Cbm%7BI%7D">. Substituting this into the expression for <img src="https://latex.codecogs.com/png.latex?%5CSigma_%7B%5Cmid%20a%7D"> from Proposition 1: <img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5CSigma_%7B%5Cmid%20a%7D%20&amp;=%20%5Csigma%5E2%5Cbm%7BI%7D%20-%20(%5Csigma%5E2%5Cbm%7BI%7D)%5Cbm%7BQ%7D'(%5Cbm%7BQ%7D(%5Csigma%5E2%5Cbm%7BI%7D)%5Cbm%7BQ%7D')%5E%7B-1%7D%5Cbm%7BQ%7D(%5Csigma%5E2%5Cbm%7BI%7D)%20%5C%5C%0A&amp;=%20%5Csigma%5E2%5Cbm%7BI%7D%20-%20%5Csigma%5E4%20%5Cbm%7BQ%7D'(%5Csigma%5E2%5Cbm%7BQ%7D%5Cbm%7BQ%7D')%5E%7B-1%7D%5Cbm%7BQ%7D%20%5C%5C%0A&amp;=%20%5Csigma%5E2%5Cbm%7BI%7D%20-%20%5Csigma%5E4%20(%5Csigma%5E2)%5E%7B-1%7D%20%5Cbm%7BQ%7D'(%5Cbm%7BQ%7D%5Cbm%7BQ%7D')%5E%7B-1%7D%5Cbm%7BQ%7D%20%5C%5C%0A&amp;=%20%5Csigma%5E2(%5Cbm%7BI%7D%20-%20%5Cbm%7BQ%7D'(%5Cbm%7BQ%7D%5Cbm%7BQ%7D')%5E%7B-1%7D%5Cbm%7BQ%7D)%0A%5Cend%7Baligned%7D%0A"> Let <img src="https://latex.codecogs.com/png.latex?%5Cbm%7BP_Q%7D%20=%20%5Cbm%7BQ%7D'(%5Cbm%7BQ%7D%5Cbm%7BQ%7D')%5E%7B-1%7D%5Cbm%7BQ%7D">, which is the projection matrix onto the row space of <img src="https://latex.codecogs.com/png.latex?%5Cbm%7BQ%7D">. Then <img src="https://latex.codecogs.com/png.latex?%5CSigma_%7B%5Cmid%20a%7D%20=%20%5Csigma%5E2(%5Cbm%7BI%7D%20-%20%5Cbm%7BP_Q%7D)">. The expected squared error is: <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5B(%5Cbm%20q'(%5Cbm%20w%20-%20%5Chat%7B%5Cbm%20w%7D))%5E2%5D%20=%20%5Cbm%20q'%20%5Csigma%5E2(%5Cbm%7BI%7D%20-%20%5Cbm%7BP_Q%7D)%20%5Cbm%20q%20=%20%5Csigma%5E2%20%5Cbm%20q'(%5Cbm%7BI%7D%20-%20%5Cbm%7BP_Q%7D)%5Cbm%20q%0A"> Since <img src="https://latex.codecogs.com/png.latex?%5Cbm%7BI%7D%20-%20%5Cbm%7BP_Q%7D"> is an idempotent projection matrix, <img src="https://latex.codecogs.com/png.latex?%5Cbm%20q'(%5Cbm%7BI%7D%20-%20%5Cbm%7BP_Q%7D)%5Cbm%20q%20=%20%5Cbm%20q'(%5Cbm%7BI%7D%20-%20%5Cbm%7BP_Q%7D)'(%5Cbm%7BI%7D%20-%20%5Cbm%7BP_Q%7D)%5Cbm%20q%20=%20%5C%7C(%5Cbm%7BI%7D%20-%20%5Cbm%7BP_Q%7D)%5Cbm%20q%5C%7C%5E2">. Thus, <img src="https://latex.codecogs.com/png.latex?%0A%5Cmathbb%7BE%7D%5B(%5Cbm%20q'(%5Cbm%20w%20-%20%5Chat%7B%5Cbm%20w%7D))%5E2%5D%20=%20%5Csigma%5E2%20%5C%7C(%5Cbm%7BI%7D-%5Cbm%7BP_Q%7D)%5Cbm%20q%5C%7C%5E2%0A"> <img src="https://latex.codecogs.com/png.latex?%5Csquare"></p>
<p><strong>Proposition 3 (Error decreases with more independent questions).</strong> <em>The average expected squared error over all possible new questions <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D"> decreases linearly with the number of linearly independent questions in the training set <img src="https://latex.codecogs.com/png.latex?%5Cbm%7BQ%7D">. Specifically, with an isotropic prior <img src="https://latex.codecogs.com/png.latex?%5CSigma%20=%20%5Csigma%5E2%20%5Cbm%7BI%7D">, the average error is:</em> <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D_%7B%5Cbm%7Bq%7D%7D%5B%5Ctext%7Berror%7D(%5Cbm%7Bq%7D)%5D%20=%20%5Csigma%5E2%20(p%20-%20%5Coperatorname%7Brank%7D(%5Cbm%7BQ%7D))"> <em>where the expectation is taken over new questions <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D"> with i.i.d. components drawn uniformly from <img src="https://latex.codecogs.com/png.latex?%5C%7B-1,1%5C%7D">.</em></p>
<p><em>Proof.</em> The proof proceeds in two steps. First, we write the expression for the error for a given new question <img src="https://latex.codecogs.com/png.latex?%5Cbm%20q">. Second, we average this error over the distribution of all possible questions.</p>
<ol type="1">
<li><p><strong>Predictive error for a fixed <img src="https://latex.codecogs.com/png.latex?%5Cbm%20q">.</strong> From Proposition 2, the expected squared error for a specific new question <img src="https://latex.codecogs.com/png.latex?%5Cbm%20q">, given an isotropic prior <img src="https://latex.codecogs.com/png.latex?%5CSigma%20=%20%5Csigma%5E2%20%5Cbm%7BI%7D">, is: [ (q) = [(q’(w - ))^2] = ^2 q’(-)q ] where <img src="https://latex.codecogs.com/png.latex?%5Cbm%7BP_Q%7D%20=%20%5Cbm%7BQ%7D'(%5Cbm%7BQ%7D%5Cbm%7BQ%7D')%5E%7B-1%7D%5Cbm%7BQ%7D"> is the projection matrix onto the row-span of <img src="https://latex.codecogs.com/png.latex?%5Cbm%7BQ%7D">.</p></li>
<li><p><strong>Average over random new questions.</strong> We now take the expectation of this error over the distribution of new questions <img src="https://latex.codecogs.com/png.latex?%5Cbm%20q">. The components of <img src="https://latex.codecogs.com/png.latex?%5Cbm%20q"> are i.i.d. uniform on <img src="https://latex.codecogs.com/png.latex?%5C%7B-1,1%5C%7D">, which implies that <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5B%5Cbm%20q%5D%20=%20%5Cbm%200"> and <img src="https://latex.codecogs.com/png.latex?%5Cmathbb%7BE%7D%5B%5Cbm%20q%20%5Cbm%20q'%5D%20=%20%5Cbm%7BI%7D_p">. The average error is: [</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%5Cmathbb%7BE%7D_%7B%5Cbm%20q%7D%5B%5Ctext%7Berror%7D(%5Cbm%20q)%5D%20&amp;=%20%5Cmathbb%7BE%7D_%7B%5Cbm%20q%7D%5B%5Csigma%5E2%20%5Cbm%20q'(%5Cbm%7BI%7D-%5Cbm%7BP_Q%7D)%5Cbm%20q%5D%20%5C%5C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Csigma%5E2%20%5Cmathbb%7BE%7D_%7B%5Cbm%20q%7D%5B%5Coperatorname%7Btr%7D(%5Cbm%20q'(%5Cbm%7BI%7D-%5Cbm%7BP_Q%7D)%5Cbm%20q)%5D%20%5C%5C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Csigma%5E2%20%5Cmathbb%7BE%7D_%7B%5Cbm%20q%7D%5B%5Coperatorname%7Btr%7D((%5Cbm%7BI%7D-%5Cbm%7BP_Q%7D)%5Cbm%20q%20%5Cbm%20q')%5D%20%5C%5C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Csigma%5E2%20%5Coperatorname%7Btr%7D((%5Cbm%7BI%7D-%5Cbm%7BP_Q%7D)%5Cmathbb%7BE%7D_%7B%5Cbm%20q%7D%5B%5Cbm%20q%20%5Cbm%20q'%5D)%20%5C%5C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Csigma%5E2%20%5Coperatorname%7Btr%7D(%5Cbm%7BI%7D-%5Cbm%7BP_Q%7D)%20%5C%5C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Csigma%5E2%20(%5Coperatorname%7Btr%7D(%5Cbm%7BI%7D)%20-%20%5Coperatorname%7Btr%7D(%5Cbm%7BP_Q%7D))%0A%5Cend%7Baligned%7D">
<p>] The trace of the identity matrix is <img src="https://latex.codecogs.com/png.latex?p">. The trace of a projection matrix is the dimension of the subspace it projects onto, so <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7Btr%7D(%5Cbm%7BP_Q%7D)%20=%20%5Coperatorname%7Brank%7D(%5Cbm%7BQ%7D)">. Thus, the average error is: [ _{q}[(q)] = ^2 (p - ()) ] Since the rank of <img src="https://latex.codecogs.com/png.latex?%5Cbm%7BQ%7D"> increases with each linearly independent question added, the average error decreases linearly until <img src="https://latex.codecogs.com/png.latex?%5Coperatorname%7Brank%7D(%5Cbm%7BQ%7D)=p">, at which point it becomes zero. <img src="https://latex.codecogs.com/png.latex?%5Csquare"></p></li>
</ol>
<p><strong>Proposition 4 (Posterior in two-stage estimation).</strong> <em>We consider a two-stage process. First, an agent (the “computer,” <img src="https://latex.codecogs.com/png.latex?C">) with training data <img src="https://latex.codecogs.com/png.latex?(%5Cbm%7BQ%7D_C,%20%5Cbm%7Ba%7D_C)"> forms an estimate for the answer to a new question <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D">. Second, another agent (the “human,” <img src="https://latex.codecogs.com/png.latex?H">) with their own training data <img src="https://latex.codecogs.com/png.latex?(%5Cbm%7BQ%7D_H,%20%5Cbm%7Ba%7D_H)"> observes the computer’s estimate and updates their own belief.</em></p>
<p><em>The human has a prior over the weights <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bw%7D%20%5Csim%20N(%5Cbm%7B0%7D,%20%5CSigma)">. After observing their own data, the human’s posterior for <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bw%7D"> is <img src="https://latex.codecogs.com/png.latex?N(%5Chat%7B%5Cbm%7Bw%7D%7D_H,%20%5CSigma_H)">, where from Proposition 1:</em> <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Chat%7B%5Cbm%7Bw%7D%7D_H%20&amp;=%20%5CSigma%20%5Cbm%7BQ%7D_H%5E%7B%5Ctop%7D(%5Cbm%7BQ%7D_H%5CSigma%20%5Cbm%7BQ%7D_H%5E%7B%5Ctop%7D)%5E%7B-1%7D%5Cbm%7Ba%7D_H%20%5C%5C%0A%20%20%20%20%20%20%5CSigma_H%20&amp;=%20%5CSigma%20-%20%5CSigma%20%5Cbm%7BQ%7D_H%5E%7B%5Ctop%7D(%5Cbm%7BQ%7D_H%5CSigma%20%5Cbm%7BQ%7D_H%5E%7B%5Ctop%7D)%5E%7B-1%7D%5Cbm%7BQ%7D_H%5CSigma%0A%20%20%20%5Cend%7Baligned%7D"> <em>The human’s initial estimate for the answer to a new question <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D"> is <img src="https://latex.codecogs.com/png.latex?%5Cmu_H%20=%20%5Cbm%7Bq%7D'%5Chat%7B%5Cbm%7Bw%7D%7D_H"> with variance <img src="https://latex.codecogs.com/png.latex?%5Csigma_H%5E2%20=%20%5Cbm%7Bq%7D'%5CSigma_H%20%5Cbm%7Bq%7D">.</em></p>
<p><em>The computer has its own training data <img src="https://latex.codecogs.com/png.latex?(%5Cbm%7BQ%7D_C,%20%5Cbm%7Ba%7D_C)">. It provides an estimate <img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D_C%20=%20%5Cbm%7Bq%7D'%5Chat%7B%5Cbm%7Bw%7D%7D_C"> for the true answer <img src="https://latex.codecogs.com/png.latex?a%20=%20%5Cbm%7Bq%7D'%5Cbm%7Bw%7D">. The human observes <img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D_C"> and updates their posterior for <img src="https://latex.codecogs.com/png.latex?a">. We assume the computer’s observations may be noisy, such that <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Ba%7D_C%20=%20%5Cbm%7BQ%7D_C%5Cbm%7Bw%7D%20+%20%5Cbm%7B%5Cepsilon%7D_C"> with <img src="https://latex.codecogs.com/png.latex?%5Cbm%7B%5Cepsilon%7D_C%20%5Csim%20N(0,%20s_C%5E2%20%5Cbm%7BI%7D)">.</em></p>
<p><em>We analyze the human’s final posterior for <img src="https://latex.codecogs.com/png.latex?a"> under different assumptions about what the human knows about the computer’s process.</em></p>
<p><strong>Proposition 4.1 (Updating with minimal information).</strong> <em>Assume the human has no knowledge of the computer’s training set <img src="https://latex.codecogs.com/png.latex?%5Cbm%7BQ%7D_C"> but believes the computer’s estimate is unbiased with a known mean squared error <img src="https://latex.codecogs.com/png.latex?%5Ctau%5E2">. That is, <img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D_C%20=%20a%20+%20%5Ceta">, where <img src="https://latex.codecogs.com/png.latex?%5Ceta%20%5Csim%20N(0,%20%5Ctau%5E2)"> and is independent of <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bw%7D">.</em></p>
<p><em>Upon observing <img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D_C">, the human’s posterior for <img src="https://latex.codecogs.com/png.latex?a"> is:</em> <img src="https://latex.codecogs.com/png.latex?%20a%20%5Cmid%20%5Chat%7Ba%7D_C%20%5Csim%20N%5Cleft(%20%5Cmu_H%20+%20%5Calpha(%5Chat%7Ba%7D_C%20-%20%5Cmu_H),%20(1-%5Calpha)%5Csigma_H%5E2%20%5Cright)%20"> <em>where <img src="https://latex.codecogs.com/png.latex?%5Calpha%20=%20%5Cfrac%7B%5Csigma_H%5E2%7D%7B%5Csigma_H%5E2%20+%20%5Ctau%5E2%7D%20%5Cin%20%5B0,1%5D">. The human’s new estimate is a weighted average of their own initial estimate and the computer’s estimate. The weight <img src="https://latex.codecogs.com/png.latex?%5Calpha"> placed on the computer’s estimate is higher when the computer is believed to be more accurate (smaller <img src="https://latex.codecogs.com/png.latex?%5Ctau%5E2">) or when the human’s own estimate is more uncertain (larger <img src="https://latex.codecogs.com/png.latex?%5Csigma_H%5E2">).</em></p>
<p><strong>Proposition 4.2 (Knowledge of computer’s questions).</strong> <em>Assume the human knows the computer’s training questions <img src="https://latex.codecogs.com/png.latex?%5Cbm%7BQ%7D_C"> and its noise level <img src="https://latex.codecogs.com/png.latex?s_C%5E2">, but not the observed answers <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Ba%7D_C">.</em></p>
<p><em>The human can model the computer’s estimate as <img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D_C%20=%20%5Cbm%7Bq%7D'%5Cbm%7BP%7D%5Cbm%7Bw%7D%20+%20%5Cbm%7Bq%7D'%5Cbm%7B%5Czeta%7D">, where <img src="https://latex.codecogs.com/png.latex?%5Cbm%7BP%7D%20=%20%5CSigma%20%5Cbm%7BQ%7D_C'(%5Cbm%7BQ%7D_C%5CSigma%20%5Cbm%7BQ%7D_C'%20+%20s_C%5E2%5Cbm%7BI%7D)%5E%7B-1%7D%5Cbm%7BQ%7D_C"> and <img src="https://latex.codecogs.com/png.latex?%5Cbm%7B%5Czeta%7D%20=%20%5CSigma%20%5Cbm%7BQ%7D_C'(%5Cbm%7BQ%7D_C%5CSigma%20%5Cbm%7BQ%7D_C'%20+%20s_C%5E2%5Cbm%7BI%7D)%5E%7B-1%7D%5Cbm%7B%5Cepsilon%7D_C">.</em></p>
<p><em>The pair <img src="https://latex.codecogs.com/png.latex?(a,%20%5Chat%7Ba%7D_C)"> is jointly Gaussian, conditional on the human’s data. The posterior for <img src="https://latex.codecogs.com/png.latex?a"> is:</em> <img src="https://latex.codecogs.com/png.latex?%20a%20%5Cmid%20%5Chat%7Ba%7D_C%20%5Csim%20N%5Cleft(%20%5Cmu_H%20+%20%5Ckappa(%5Chat%7Ba%7D_C%20-%20%5Cmu_C),%20%5Csigma_H%5E2%20-%20%5Ckappa%5Csigma_%7BHC%7D%20%5Cright)%20"> <em>where:</em> - <img src="https://latex.codecogs.com/png.latex?%5Cmu_C%20=%20%5Cbm%7Bq%7D'%5Cbm%7BP%7D%5Chat%7B%5Cbm%7Bw%7D%7D_H"> (human’s expectation of computer’s estimate) - <img src="https://latex.codecogs.com/png.latex?%5Csigma_%7BHC%7D%20=%20%5Cbm%7Bq%7D'%5CSigma_H%20%5Cbm%7BP%7D'%20%5Cbm%7Bq%7D"> (covariance) - <img src="https://latex.codecogs.com/png.latex?%5Csigma_C%5E2%20=%20%5Cbm%7Bq%7D'%5Cbm%7BP%7D%5CSigma_H%20%5Cbm%7BP%7D'%20%5Cbm%7Bq%7D%20+%20%5Cbm%7Bq%7D'%5CSigma_%5Czeta%20%5Cbm%7Bq%7D"> (variance of computer’s estimate) - <img src="https://latex.codecogs.com/png.latex?%5CSigma_%5Czeta%20=%20s_C%5E2%20%5Cbm%7BP%7D%20%5CSigma%5E%7B-1%7D%20%5Cbm%7BP%7D'"> - <img src="https://latex.codecogs.com/png.latex?%5Ckappa%20=%20%5Cfrac%7B%5Csigma_%7BHC%7D%7D%7B%5Csigma_C%5E2%7D"> (the weight on the computer’s prediction error)</p>
<p><em>The weight <img src="https://latex.codecogs.com/png.latex?%5Ckappa"> depends on the covariance structure, which is influenced by the overlap between the subspaces spanned by <img src="https://latex.codecogs.com/png.latex?%5Cbm%7BQ%7D_H"> and <img src="https://latex.codecogs.com/png.latex?%5Cbm%7BQ%7D_C">.</em></p>
<p><strong>Proposition 4.3 (Limiting cases).</strong> <em>The framework of Proposition 4.2 nests two extreme cases:</em> 1. <strong>Oracle Trust:</strong> <em>If the human believes the computer’s estimate is perfect (e.g., <img src="https://latex.codecogs.com/png.latex?s_C%5E2%20%5Cto%200"> and <img src="https://latex.codecogs.com/png.latex?%5Cbm%7BQ%7D_C"> spans the relevant subspace), then <img src="https://latex.codecogs.com/png.latex?%5Ckappa%20%5Cto%20%5Csigma_H%5E2%20/%20(%5Cbm%7Bq%7D'%5Cbm%7BP%7D%5CSigma_H%20%5Cbm%7BP%7D'%5Cbm%7Bq%7D)">, and the posterior variance collapses towards zero. In the simplified Kalman model, if <img src="https://latex.codecogs.com/png.latex?%5Ctau%5E2%20%5Cto%200">, then <img src="https://latex.codecogs.com/png.latex?%5Calpha%20%5Cto%201">, and the human adopts the computer’s answer, <img src="https://latex.codecogs.com/png.latex?a%20%5Cmid%20%5Chat%7Ba%7D_C%20%5Cto%20N(%5Chat%7Ba%7D_C,%200)">.</em> 2. <strong>Total Skepticism:</strong> <em>If the human believes the computer provides no information (e.g., <img src="https://latex.codecogs.com/png.latex?%5Csigma_%7BHC%7D%20%5Cto%200"> because <img src="https://latex.codecogs.com/png.latex?%5Cbm%7BQ%7D_C"> is irrelevant to <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D">), then <img src="https://latex.codecogs.com/png.latex?%5Ckappa%20%5Cto%200">. In the Kalman model, if <img src="https://latex.codecogs.com/png.latex?%5Ctau%5E2%20%5Cto%20%5Cinfty">, then <img src="https://latex.codecogs.com/png.latex?%5Calpha%20%5Cto%200">. In both cases, the human ignores the computer’s estimate and reverts to their original posterior, <img src="https://latex.codecogs.com/png.latex?a%20%5Cmid%20%5Chat%7Ba%7D_C%20%5Csim%20N(%5Cmu_H,%20%5Csigma_H%5E2)">.</em></p>
</section>
<section id="related-literature" class="level1">
<h1>Related Literature</h1>
<section id="agrawal-et-al.-2018-exploring-the-impact-of-artificial-intelligence-prediction-versus-judgment" class="level2">
<h2 class="anchored" data-anchor-id="agrawal-et-al.-2018-exploring-the-impact-of-artificial-intelligence-prediction-versus-judgment">Agrawal et al.&nbsp;(2018) “Exploring the Impact of Artificial Intelligence: Prediction versus Judgment”</h2>
<p>https://www.nber.org/system/files/working_papers/w24626/w24626.pdf</p>
</section>
<section id="kleinberg-et-al.-2017-human-decisions-and-machine-predictions" class="level2">
<h2 class="anchored" data-anchor-id="kleinberg-et-al.-2017-human-decisions-and-machine-predictions">Kleinberg et al.&nbsp;(2017) “Human Decisions and Machine Predictions”</h2>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{cunningham2025,
  author = {Cunningham, Tom},
  title = {A {Model} of {ChatGPT}},
  date = {2025-06-14},
  url = {tecunningham.github.io/posts/2025-06-14-model-of-chatgpt.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-cunningham2025" class="csl-entry quarto-appendix-citeas">
Cunningham, Tom. 2025. <span>“A Model of ChatGPT.”</span> June 14, 2025.
<a href="https://tecunningham.github.io/posts/2025-06-14-model-of-chatgpt.html">tecunningham.github.io/posts/2025-06-14-model-of-chatgpt.html</a>.
</div></div></section></div> ]]></description>
  <guid>tecunningham.github.io/posts/2025-06-14-model-of-chatgpt.html</guid>
  <pubDate>Sat, 14 Jun 2025 07:00:00 GMT</pubDate>
</item>
<item>
  <title>On Deriving Things</title>
  <dc:creator>Tom Cunningham</dc:creator>
  <link>tecunningham.github.io/posts/2020-10-02-on-deriving-things.html</link>
  <description><![CDATA[ 





<style>
   dl {display: grid; grid-template-columns: max-content auto;
       }
   dt {grid-column-start: 1; width: 5cm;  padding-bottom: 30px;
      margin-right: 0px;
      padding-right: 5px;
      border-top: 1px solid black; }
   dd {grid-column-start: 2; margin-left: 2em;  padding-bottom: 60px; 
      margin-left: 0px;
      padding-left: 5px;
      border-top: 1px solid black; }
</style>
<dl>
<dt>I’ve spent a lot of time trying to prove things.</dt>
<dd>
With diagrams and algebra, back and forth between clipboard whiteboard blackboard &amp; keyboard.
</dd>
<dt>I can’t talk about what it’s like for a good mathematician but I can talk about it for a hack.</dt>
<dd>
I can prove true &amp; interesting things occasionally but only after wallowing in it for a long time, and after a half-dozen proofs of things that I later realize are not interesting or not true.
</dd>
<dt>The representation of a problem becomes gradually more abstract.</dt>
<dd>
I start with a full set of equations, then I gradually omit things that seem inessential, and I introduce a new symbol which feels like it denotes something but I’m not sure what, and then another higher level of abstraction, a jenga tower to the ceiling. I’m holding it very delicately and it’s crucial not to lose that concentration as I can see it teeter. Sometimes I stare out the window too long and when I look back down at the page the symbols no longer make sense: was this a matrix and that a vector? Why was I multiplying them together in this weird way?
</dd>
<dt>I take a problem and present it to my intuition to see if there’s a glimmer of recognition.</dt>
<dd>
<p>I show it from different angles: vary the notation, write worked examples, draw arrow diagrams, draw venn diagrams, hoping that one of these approaches will trigger a memory of something similar.</p>
<p>I walk around the house checking the windows and doors. I check whether the answers are in the right units: dollars/person, quantity/year, and I check whether the signs go in the right direction.</p>
</dd>
<dt>I lose track of which way the inequality should be pointing, left or right.</dt>
<dd>
<p>My intuition doesn’t give me a strong signal so I just choose one and stick with it hoping that once I arrive at the conclusion I can work backwards and fix the directions.</p>
<p>Margaret Bray told me that, when she worked correcting Stiglitz’s proofs in the 1970s, she would sometimes find an odd number of sign errors instead of an even number. When that happened she would tell him it and he would quickly rewrite the discussion of that result: substituting an intuitive explanation of why the effect of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> on F was negative for what had been an explanation of why the effect of <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> on F was positive.</p>
</dd>
<dt>Sometimes the goal is to maximize the distance between the assumptions and the conclusion.</dt>
<dd>
<p>I’m trying to make it seem more impressive. At first the starting point and end point are within sight of each other, and I hack away at either side, loosening the assumptions, tightening the conclusions, until the path stretches as far as possible.</p>
<p>I sometimes end up chasing what <em>looks</em> impressive. I twist the wording of the assumptions to make them seem weaker than they are, the conclusions to seem stronger, or hide ancillary assumptions in the body of the derivation. Looking back to some of my old papers I see signs of this and I’m ashamed of it.</p>
</dd>
<dt>There are a half-dozen different things I need to keep ambiguous.</dt>
<dd>
I’m not sure which disambiguation will be the right one - whether to use strict or weak preferences; whether the numbering should start at zero or one; whether naturals or wholes or integers. Making a decision at one stage has implications for others. This part is Sudoku, finding something which fits many constraints.
</dd>
<dt>I switch between two modes: is this true? How do I show everyone it’s true?</dt>
<dd>
In the second mode it’s just trying to keep everything straight to nail it down but then occasionally a sickening feeling arises that it’s not my clumsiness that’s holding me back but perhaps the thing isn’t true after all, and I have to return to the first mode.
</dd>
<dt>Sometimes I’m stumbling through equations trying to derive one from another but I have lost track of what they mean.</dt>
<dd>
Like going through a room in the dark grasping for a door handle. I want to go back and draw some diagrams to get an intuition of what’s going on, some mental picture, but sometimes I try and try and nothing comes.
</dd>
<dt>My coauthor suggests modifying an assumption in the setup.</dt>
<dd>
Taking apart a Lego car to rearrange the unsatisfactory front wheels but it’s hard to anticipate all the downstream consequences of that; I don’t remember all the considerations I had in mind when I put together the front wheels.
</dd>
<dt>Eventually the relief of finding the hidden door.</dt>
<dd>
Suddenly I see the inequalities holding down variables like saxophone keys, which gives me the lemma: a condition for a set of inequalities between the elements of two vectors to imply an inequality between the sums of the vectors. I don’t need to write it down, I can take a break and go for a run, it’s enough to remember the idea of saxophone keys. Later I find out this is called Hall’s marriage theorem.
</dd>
</dl>
<center>
*
</center>
<dl>
<dt>I’ll be writing out an equation for the 3rd time and suddenly a dusty image will be released.</dt>
<dd>
<p>When I quiet my brain to concentrate on just one thing the background noise becomes audible. I can hear the mice scratching in the walls. Not just the familiar memories, unfamiliar long lost memories come out too.</p>
<p>A sparse park in Manchester, a man who blanked me in a corridor in London, a cheese shop in Stockholm.</p>
</dd>
<dt>My desk is covered with notes: should write up what I have or push on towards the summit?</dt>
<dd>
It’s a dangerous tradeoff. In retrospect it seems like I’ve often made the wrong decision: too-often tried for the summit, failed, and then left without a proper writeup. I come back to make another attempt, see my old footsteps and regret that I didn’t spend more time hammering nails into the rock.
</dd>
<dt>Sometimes a gap opens up between what’s important and what’s orthodox.</dt>
<dd>
If I make an assumption that is somewhat stronger than typical then everything is much more elegant and it feels like I can keep focus on what’s important. But then I get memories of being in seminars and seeing the speaker making an unorthodox assumption and how the energy drains out of the room.
</dd>
<dt>For a while I tried to solve things falling asleep.</dt>
<dd>
Once, in Goodenough College in 2008, as my mind untethered itself from its dock I suddenly glimpsed that the integral could be taken vertically instead of horizontally. The problem was solved. For a few months afterwards I would put my book aside before I was ready to go to sleep and bring to mind some other unsolved problem but it never worked as well again.
</dd>
<dt>More memories are released.</dt>
<dd>
<p>Beer at the dreary Birkbeck bar: we were all grasping around for things to say to each other to fill up an evening with. When we heard the kids roaring by the pool table we all looked up.</p>
<p>Saturday morning on the way back from the vegetable market in Farringdon. I sit down with a sandwich in a churchyard and I see my German neighbors are there too. I’m self conscious that my loneliness is exposed.</p>
<p>Making Will’s grandmother a shandy of beer and lemonade. Noticing the shadows of the roses in her garden, how they would change when I stood near them, the proximity of my own shadow would somehow make the shadows of the roses come into focus.</p>
</dd>
</dl>
<center>
*
</center>
<dl>
<dt>Perhaps 1/3 of the time is actually concentrating on the problem.</dt>
<dd>
If it’s writing or programming I can just bring up a window and type away. If it’s deriving things then my mind is constantly drifting, trying to find some other path to go down – in reverie, doodling Zeus, refilling my pen, or looking something up.
</dd>
<dt>On a good day it’s like swimming in cold water.</dt>
<dd>
<p>I don’t want to get in but once I’m in I don’t want to get out.</p>
<p>Once my chair is firmly pushed in at the desk I don’t want to get up to get a snack, to check the mail, I don’t want to go to the bathroom, I’ll be working through proofs while shaking my legs trying to stretch my bladder out.</p>
<p>I procrastinate starting work on a revision but once the document is open and the seal is broken I see things to fix and it’s difficult to stop.</p>
<p>When daughter was a baby she would push away a bottle of warm milk until the nipple was in her lips and then she’d clutch it tight while she’d drink.</p>
</dd>
<dt>On a bad day I’m rotating slices of a Rubik’s cube.</dt>
<dd>
I’m checking to see if it’s solved then rotating again in some other direction.
</dd>
<dt>Often I’ll over-estimate what I can accomplish.</dt>
<dd>
I’ll have some cocky confidence that, this time, I can prove something I’ve failed to prove before. Or I’ll have a sketch and decide that it’s good-enough, that I can fill in the details later. Then later get a sickening feeling discovering that it’s really nothing, the bits omitted from the sketch were exactly the difficult parts. The voice in my head that causes these over-optimistic judgments causes other griefs in my life I think.
</dd>
<dt>I get flashes of recollection of other peoples’ seminars and papers.</dt>
<dd>
They seemed so boring and now my paper seems so similar to theirs.
</dd>
<dt>The ideal state of mind has both (1) the clarity of fresh eyes; and (2) the suppleness of familiarity.</dt>
<dd>
For that reason it makes sense to spend an entire day thinking about it, not little blocks of hours.
</dd>
</dl>
<dl>
<dt>Weaselly unworthy thoughts bubble up.</dt>
<dd>
I find myself thinking that a reader will be impressed by the quantity and the complexity of the derivation and give me credit for that.
</dd>
<dt>When I’m trying to concentrate on something my weasel thinks of something I could order on Amazon.</dt>
<dd>
I can suppress that. My weasel tries to get me to look up how to do a LaTeX symbol that would be useful (delta over equals). I can suppress that. The weasel sees a pair of brackets that could have an extra space. I give in. I fix the brackets. But when I’m fixing brackets I’m more vulnerable to each of the other temptations, and then after a while the weasel tells me it’s almost lunchtime. I look back on my morning it looks like swiss cheese.
</dd>
<dt>I’m a grown adult but my concentration is still not under my control.</dt>
<dd>
I put out treats for it, entreating it like a dog. Teach it bad habits. Give in to its whims.
</dd>
<dt>When I switched from programming to studying economics I missed those numb hours that would pass by.</dt>
<dd>
<p>Some people seem to get into that state when they’re deriving things but it’s difficult for me to achieve that.</p>
<p>When you’re programming you get incremental feedback: you can see the mountain peak and you’re slowly getting closer to it. With proofs you’re going through the jungle and you don’t know if you’re getting closer or farther away. You could be on the brink of emerging into a clearing or it could be months more in the forest.</p>
</dd>
<dt>Why am I doing this?</dt>
<dd>
As I’m feeling around a problem I think of a guy I knew from seminars at Harvard who would pause when a new slide came up and then nod quietly when he understood. Why do I spend my time at this when so many other people are better at this than me? Am I doing this for intrinsic or extrinsic reasons? One instinct is to untangle the threads of means and ends to see where they lead, another instinct tells me to leave it tangled, and I trust the second instinct more.
</dd>
<dt>I look up from the corner I’m stitching.</dt>
<dd>
I see the quilt goes to the end of the bed, out the window, across the fields all the way to the horizon.
</dd>
<dt>Reading through an old draft like a landlord returning to his estate after a long trip.</dt>
<dd>
To be reminded of the state of things. I’m more often surprised unpleasantly than pleasantly. My memory must be pickling things in a sweet vinegar.
</dd>
<dt>Coming back to writing a paper after 5 years at Facebook I had a different kind of confidence.</dt>
<dd>
Old anxieties were about failing to do things in the orthodox way. Now I feel that if it makes sense to me I won’t be embarrassed by it. I might be wrong but I’m not a fraud.
</dd>
<dt>I’m comparing myself, and it seems unhealthy, but then also necessary.</dt>
<dd>
<p>Am I clearer sighted than I was last month? My decision about whether to attempt to solve this problem depends on that judgment.</p>
<p>I see that the problem I’m attempting is one that was notably not solved in a paper by Benabou and Tirole: whether I should give up now depends on how I assess my ability compared to theirs.</p>
<p>I’ve spent two hours trying out different matrices to see if they have a certain property, looking for a generalization about which matrices satisfy it, I still haven’t got one and now I don’t recall why I felt this was important – how much do I trust my decision to go down this road, and at what point do I go back to reconsider whether this problem is worth solving?</p>
</dd>
<dt>In Tel Aviv I would swim in the sea in the mornings.</dt>
<dd>
By the third week I was still going down to the beach, getting in the water, and starting to swim, but after 30 seconds I would let my legs fall to the sand and just walk through the water staring into space, thinking. After a while I would remember to start swimming again.
</dd>
</dl>
<section id="some-things-i-proved" class="level1">
<h1>Some things I proved</h1>
<p>It’s likely that some of these had been proved before, but they each seemed useful enough to prove in the time and place.</p>
<dl>
<dt><a href="https://joshkim.org/files/InterpretingExperiments.pdf">The interpretation of an experimental outcome will depend on other outcomes.</a></dt>
<dd>
Your estimate of the treatment effect on one outcome will depend on the observed effect on the other outcome, and the weight will be proportional to the difference in covariances between the treatment effects and the noise (if everything is joint Normal).
</dd>
<dt><a href="https://tecunningham.github.io/posts/2023-10-23-pareto-frontiers-experiments-ranking.html">If item values are Normally distributed then selections will look like ellipses.</a></dt>
<dd>
If you’re ranking items by predictions, e.g.&nbsp;ranking by <img src="https://latex.codecogs.com/png.latex?p(A)+%5Cbeta%20p(B)">, and the probabilities have a joint Gaussian distribution, then the overall tradeoff between total <img src="https://latex.codecogs.com/png.latex?A"> and total <img src="https://latex.codecogs.com/png.latex?B"> will be an ellipse, equal to an isovalue of the joint distribution of <img src="https://latex.codecogs.com/png.latex?F(p(A),p(B))">.
</dd>
<dt><a href="https://scholar.google.com/citations?view_op=view_citation&amp;hl=en&amp;user=MDB_DgkAAAAJ&amp;citation_for_view=MDB_DgkAAAAJ:WF5omc3nYNoC">Modularity in the brain will cause characteristic inconsistencies in decisions.</a></dt>
<dd>
If information is dispersed in the brain, and aggregated sequentially, then (a) mistakes will occur when there are interactions in your posterior (nonseparabilities), and (b) certain characteristic inconsistencies which will reveal those nonseparabilities.
</dd>
<dt><a href="https://jondequidt.com/pdfs/paper_implicit.pdf">Implicit preferences can be inferred from choices.</a> (with Jon de Quidt)</dt>
<dd>
Observing choices over outcomes which differ in attributes can reveal the difference between implicit and explicit preferences – where implicit preferences can be due to either (a) tacit knowledge, (b) signaling motives, or (c) constraints on which decisions are allowable.
</dd>
<dt><a href="https://www.dropbox.com/scl/fi/qise3mwej01yuq0sxcrdy/CM-15-03-10.pdf?rlkey=nn07itezc1h6fikyaylhx3v30&amp;e=1&amp;dl=0">Noisy signalling advantages senders.</a> (with Ines Moreno de Barreda)</dt>
<dd>
Suppose you’ll admitting a student to your PhD program only if they have a GRE score sufficient to imply their ability is above median. If students can exert effort to inflate their scores then you’ll end up admitting more than half of the students, even if you rationally adjust for the inflation.
</dd>
</dl>
<center>
*
</center>
<p><img src="tecunningham.github.io/posts/images/2024-12-06-08-28-17.png" class="img-fluid"></p>
</section>
<section id="anatomy-of-a-mistake" class="level1">
<h1>Anatomy of a Mistake</h1>
<p><strong>I missed a malignant ambiguity because there was <em>another</em> ambiguity next to it, and that other ambiguity was benign.</strong></p>
<p>There were two versions of a claim, and I was a little vague about which I was assuming:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Ctheta(x,x')%20=%20%5Ctheta(x'-x)%5Ctag%7BA%20weak%7D"></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Ctheta(x,x')%20=%20%5Ctheta(%7Cx'-x%7C)%5Ctag%7BA%20strong%7D"></p>
<p>My conclusion required the strong version but my assumptions only justified the weak version.</p>
<p>Why did I miss this? Because there was <em>another</em> ambiguity that was floating around, claim B:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Ctheta(x,x')%20-%20%5Ctheta(x+1,x')%20%3E%200.%20%5Ctag%7BB%7D"></p>
<p>It happens that assuming <img src="https://latex.codecogs.com/png.latex?%5Cmu=0"> (never mind what that is) implies both (B) and (A-strong). My thought process:</p>
<ol type="1">
<li><p>I don’t want to restrict to <img src="https://latex.codecogs.com/png.latex?%5Cmu=0">.</p></li>
<li><p>I knew that (B) is not true for all <img src="https://latex.codecogs.com/png.latex?%5Cmu">, but it is true for sufficiently small values, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?%7C%5Cmu%7C%3Ck"> for some <img src="https://latex.codecogs.com/png.latex?k">.</p></li>
<li><p>So I was juggling two versions of an assumption about <img src="https://latex.codecogs.com/png.latex?%5Cmu"> (<img src="https://latex.codecogs.com/png.latex?%5Cmu"> is zero, <img src="https://latex.codecogs.com/png.latex?%5Cmu"> is small), and two versions of (A).</p></li>
<li><p>When my mind wandered onto whether claim (A strong) was justified I was reassured by remembering that it’ll work when <img src="https://latex.codecogs.com/png.latex?%5Cmu=0"> (true for both claim A and B), and additionally that for <img src="https://latex.codecogs.com/png.latex?%5Cmu=%5Cvarepsilon"> it’ll be OK (true only of claim B).</p></li>
</ol>
<p>The key thing: if claim B had not have interfered I wouldn’t have made this mistake.</p>
<p>In the same way a pickpocket will wait for you to move before taking something off you, because the friction you feel on your buttock is attributed to the walking, not to the wallet leaving your pocket.</p>


</section>

 ]]></description>
  <guid>tecunningham.github.io/posts/2020-10-02-on-deriving-things.html</guid>
  <pubDate>Thu, 30 Jan 2025 08:00:00 GMT</pubDate>
</item>
<item>
  <title>Premature Optimization and the Valley of Confusion</title>
  <dc:creator>Tom Cunningham</dc:creator>
  <link>tecunningham.github.io/posts/2024-05-10-premature-optimization.html</link>
  <description><![CDATA[ 





<p><strong>When formalizing tradeoffs our decisions typically get worse before they get better.</strong></p>
<p>Those who climb the mountain of efficiency first pass through the valley of confusion.</p>
<p>Data scientists are often asked to provide a formula to calculate the expected costs and benefits of a decision but this is hard. There are often many subtleties which we grasp intuitively but do not know how to formalize. For this reason using a simple model is often worse than no model.</p>
<p>Some examples of common mistakes:</p>
<ul>
<li>Causal effects assumed to be equal to correlations.</li>
<li>Decisions depend on a sharp threshold of statistical significance.</li>
<li>Decisions depend on average rather than marginal value.</li>
<li>Lack of accounting for opportunity cost.</li>
<li>Lack of accounting for option value.</li>
<li>Lack of accounting for network effects.</li>
<li>Confusion over whether an outcome is intrinsically desirable or instrumentally desirable.</li>
<li>Estimating the topline impact of product improvements with estimates from AB tests (i.e.&nbsp;accounting only on effects for existing users, not new users).</li>
</ul>
<p>These are all avoidable mistakes but it takes time and experience to model them correctly. In fact many of the tools we use to formalize optimization problems are relatively recent discoveries: over the last few centuries we figured out how to write down probabilities and expected value, linear programming, and dynamic programming. But before these discoveries we were still able to make quite subtle and complex decisions – we were able to build pyramids, welfare states, aeroplanes – our informal and intuitive methods were sufficient.</p>
<p><strong>This is independent of whether decisions <em>should</em> be formalized.</strong> There are some situation where no formalization of costs and benefits will be sufficient, for various reasons. My point is different: even when we truly care only about a well-defined outcome it’s often better to rely on our instincts than to use a model if we don’t have time to deeply invest in that model.</p>
<p>It’s a common idea that formalizing the costs and benefits of a decision can lead to worse decision-making. People talk about “MacNamara’s fallacy,” but this can be taken in two ways: whether problems are <em>intrinsically</em> unquantifiable or just <em>difficult</em> to quantify. In tech companies I think many problems are quantifiable in principle but it’s sufficiently hard that it’s sensible to leave off quantifying until you’re confident that you have a good model.</p>
<p><strong>The process of formalizing a decision often looks like a dialogue.</strong> When you try to write down a model to represent the tradeoffs of a situation very often it feels like a dialogue with your intuition: you first write a simple model, and see that it implies that we should be making a radically different decision. You try to think through, intuitively, the implications of that decision, and see that there is some important factor that the model is missing, and so revise the model, and then look again at the implications.</p>
<p><strong>Avoiding the valley of confusion.</strong> I have been talking about the choice between using judgment or relying on a model. There is an alternative route: instead of replacing judgment the goal should be to <em>augment</em> it. Broadly speaking by trying to summarize data in a way that helps inform intuitions about tradeoffs. If we continue this process we may end up at a comprehensive formal model which we can trust.</p>
<p>Some examples:</p>
<ul>
<li><p>If we are trying to improve decisions about which experiments to ship then we can build a table of prior experiments, and for each new experiment summarize how it compares with prior ones. (My post on <a href="https://tecunningham.github.io/posts/2023-04-18-experiment-interpretation-extrapolation.html">experiment interpretation</a> expands on this.)</p></li>
<li><p>If we are trying to improve decisions about the value of metrics as surrogates for causal effects, we can start by visualizing the distributions of each metric and the correlations between them to help build judgment about how they relate to user experience.</p></li>
<li><p>If we are trying to improve evaluation of a given initiative we should start by mapping out prior initatives and compare their inputs (e.g.&nbsp;headcount, time) and outputs (metrics), to benchmark estimates.</p></li>
</ul>
<p>General considerations that apply to all evaluations:</p>
<ol type="1">
<li>All models should be described as fundamentally aids to human judgment, and it’s always legitimate to override their recommendations.</li>
<li>Model output should not be described with a single number but a visualization to show how the broad patterns in data.</li>
<li>We should summarize what decisions other people have made in similar situations, both inside and outside the company.</li>
</ol>



 ]]></description>
  <guid>tecunningham.github.io/posts/2024-05-10-premature-optimization.html</guid>
  <pubDate>Fri, 10 May 2024 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Peer Effects, Culture, and Taxes</title>
  <dc:creator>Tom Cunningham</dc:creator>
  <link>tecunningham.github.io/posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html</link>
  <description><![CDATA[ 





<style>
    h1 {  border-bottom: 4px solid black;}
    h2 {  border-bottom: 1px solid #ccc;}
</style>
<p><strong>In short:</strong><sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;First version January 23 2023. Thanks to comments and conversations with many, incl.&nbsp;Jon de Quidt, David Atkin, Donald Kenkel, Dmitry Taubinsky, Charlotte Paul, &amp; Ruth Cunningham.</p></div></div><ol type="1">
<li><p><strong>The majority of variation in preferences is due to the influence of peers.</strong> Peoples’ tastes reflect what they are accustomed to, either from their upbringing or from their contemporaries. Tastes for food, alcohol, tobacco, music, art, and leisure, vary much more between societies than within societies. This is a banal observation but I think when combined with standard economic analysis it has fairly radical implications for policy.</p></li>
<li><p><strong>Peer influences imply that group behaviour will respond to changes in the environment with a lag.</strong> Contemporary preferences will reflect the <em>prior</em> environment of a group, because peer effects will cause persistence. If your society once had a reason to choose potatoes over bread then the preference for potatoes will linger after the reason has disappeared.</p></li>
<li><p><strong>The slow adjustment of preferences implies welfare can be improved.</strong> Peer effects are an externality and so taxes which offset it will hasten convergence and raise welfare. This would justify taxes to discourage goods that have been recently been discovered to be unhealthy: tobacco, sugar, trans fats &amp; saturated fats.</p></li>
</ol>
<section id="summary" class="level1 page-columns page-full">
<h1>Summary</h1>
<p><strong>Most variation in preferences is due to peer effects.</strong> Around the world there is substantial variation in choices over food, clothes, education, working hours, fertility, etc., much of which does not seem to be a consequence of economic constraints, but must reflect a difference in preferences. We can decompose variation in preferences into (1) genetic, (2) shared environment (upbringing and peers), and (3) idiosyncratic differences.</p>
<p>The majority of the variation seems to be due to shared environment because most of the variation is <em>between</em> group rather than <em>within</em> group. I.e. we can predict most of a person’s preferences from knowing the preferences of their peers, and it’s implausible those differences are genetic. I discuss evidence further below.</p>
<div class="cell page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-23-peer-effects-norms-culture-sin-taxes_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="288"></p>
<figcaption>Aggregate demand will be more elastic than individual demand</figcaption>
</figure>
</div>
</div></div></div>
<p><strong>Implication: aggregate demand will be more elastic than individual demand.</strong> Peer effects causes everyone to bias their behaviour towards what other people choose, as a consequence each individual’s response to changes in the environment (e.g.&nbsp;an increase in price) will be damped in comparison to the aggregate response.</p>
<p><strong>Implication: today’s preferences reflect yesterday’s prices.</strong> If peer influence is based substantially on peers’ historical behaviour (as opposed to being forward-looking) this will tend to <em>retard</em> the adjustment of decisions to circumstances, so that a society’s set of decisions at any given point in time will reflect the prior history of circumstances faced by that society.</p>
<div class="cell page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-23-peer-effects-norms-culture-sin-taxes_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="288"></p>
<figcaption>Peer effects will cause inefficiently slow adjustment to price changes.</figcaption>
</figure>
</div>
</div></div></div>
<p><strong>Implication: adjustment is inefficient.</strong> Peer effects mean that consumption decisions have an externality: when you increase your consumption it will cause others to increase theirs. In the long-run this will not cause consumption to be too-high or too-low, but it will make adjustment to a change in circumstances slower than would be efficient.</p>
<p><strong>Implication for policy: hasten adjustment.</strong> Everyone could be made better off with a tax to correct that externality and hasten adjustment to the new equilibrium. When we discover new information about the health effects of some good (tobacco, sugar, saturated fats), peer effects will cause an inefficiently slow adjustment of consumption, and everyone would be made better off by a temporary tax to hasten that adjustment process.</p>
</section>
<section id="a-simple-model-of-peer-effects" class="level1 page-columns page-full">
<h1>A Simple Model of Peer Effects</h1>
<p><strong>In short:</strong> With peer effects the aggregate demand curve will be more elastic than the individual demand curve. Additionally long-run aggregate demand will be more elastic than short-run aggregate demand.</p>
<p>I give a fuller derivation of this model below, here I just state the setup and the key results.</p>
<div class="cell page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-23-peer-effects-norms-culture-sin-taxes_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="288"></p>
<figcaption>Each person will choose a level of consumption between their exogenous preference and the population-average consumption (represented by the dashed line).</figcaption>
</figure>
</div>
</div></div></div>
<p><strong>Setup:</strong> Suppose each person has a pre-existing ideal level of consumption (of alcohol, sugar, tobacco, etc.), but they additionally get disutility from departing from the average level of consumption among their peers. Assume the reference-point is the <em>lagged</em> level of consumption among peers.</p>
<p><strong>Implications:</strong></p>
<ol type="1">
<li><p><strong>Long-run average consumption will be independent of peer effects.</strong> In the long-run equilibrium peer effects will not systematically bias consumption up or down: the average consumption will be equal to the average pre-existing preference.</p></li>
<li><p><strong>Aggregate elasticity will be higher than individual elasticity.</strong> Society as a whole will be more sensitive to changes in price (or changes in information) than each individual will be.</p></li>
<li><p><strong>Long-run aggregate elasticity will be higher than short-run aggregate elasticity.</strong> When the price changes (or information changes) then each individual will respond but their response will be retarded due to peer effects from historical levels of consumption. Thus aggregate consumption will take some time to fully reflect changes in the environment.</p></li>
<li><p><strong>Aggregate demand will respond inefficiently slowly.</strong> When the price changes then aggregate consumption will gradually adjust, but that rate of adjustment will be slower than the utilitarian (welfare-maximizing) rate of adjustment. The implication is not that we should tax unhealthy goods, but that we should tax goods which we have recently <em>learned</em> to be unhealthy, to help quicken the convergence. When we discover negative effects of saturated fat, tobacco, sugar, then we should tax people to bring them quickly to the new equilibrium (and when when we learn things are surprisingly healthy then we should subsidize them).</p></li>
<li><p><strong>Biases will be magnified.</strong> If there is a bias in decision-making, e.g.&nbsp;myopia regarding health effects, then the aggregate effect of that bias will be larger than the individual effect. Thus a tax calibrated to the <em>individual</em> effects of myopia would be too small.</p></li>
<li><p><strong>Variation in consumption will be higher than optimal.</strong> When someone shifts their consumption farther from the mean they have a negative externality on everyone else, and so welfare would be improved if everyone conformed somewhat more than they do. This implication was surprising to me but seems to follow from the model.</p></li>
</ol>
<section id="relation-to-literature-on-health-taxes" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="relation-to-literature-on-health-taxes">Relation to Literature on Health Taxes</h2>
<p>The key point of this note is that the existence of peer effects would justify a tax on unhealthy goods, independent of any direct externalities or internalities from consumption, along a <em>transition path</em> when we learn new information about that good. I haven’t found discussion of this point in the economics literature on health taxation. The literature in public health often seems to make this argument implicitly but I think it deserves to be made much more clearly.</p>
<p><strong>Most countries have substantial restriction on unhealthy goods (“sin taxes”).</strong> Most countries have substantial taxes on tobacco and alcohol, outlaw many drugs, carcinogens, trans fats, and many countries are planning to introduce a tax on sugar.</p>
<p><strong>The economics literature is not in consensus about the appropriate taxation of unhealthy goods.</strong> E.g. DeCicca (2022) reviews the evidence on tobacco taxes and conclude that they are higher than would be justified by the purely economic externalities, e.g.&nbsp;through healthcare (“evidence on the magnitude of the externalities does not support current tax levels.”) They then consider the evidence for “internalities,” i.e.&nbsp;myopic decision-making by consumers, and say that the evidence and theoretical framework is too sparse to draw a conclusion (“the empirical evidence on the magnitudes of the internalities from smoking is surprisingly thin.”)</p>
<p><strong>Discussion of optimal taxation rarely discusses peer effects.</strong> DeCicca et al.&nbsp;(2022) mention the existence of an empirical literature on the peer effects on smoking, but don’t discuss the findings, or whether it would have implications for setting taxes.</p>
<p>Kenkel et al.&nbsp;(2002) calibrate a model of rational addiction with peer effects. They show that peer effects will tend to magnify distortions due to other externalities, and so increase the size of the efficient tax for a given <em>individual</em> elasticity. In their model peer effects only magnify existing distortions, they do not justify a tax by themselves. However that paper does not discuss the role of adjustment to new information (the focus of this note), a case in which peer effects will cause inefficiencies even without other externalities.</p>
<p>Allcott et al.&nbsp;(2020) argue for a 2c/oz tax on soda based on (1) a 1c/oz externality due to healthcare costs, and (2) a 1c/oz “internality” due to peoples’ myopic decision-making. They do not (I believe) discuss a peer-effect justification for taxation, which would constitute an additional separate justification for a soda or sugar tax.</p>
<p><strong>The Public Health literature discusses peer effects, but it’s unclear what role they play in setting policy.</strong> Since the 1990s the Public Health literature has put a lot of emphasis on “community health” or “population health” interventions, in part due to the perceived importance of social norms, AKA peer effects. However this literature rarely includes an explicit calculation of costs and benefits to justify a given magnitude of intervention. As a consequence it’s often unclear whether peer effects are thought to be relevant due to magnifying some other distortion (e.g.&nbsp;a fiscal externality or myopic decision-making), or due to retarding the aggregate adjustment to new information. However I believe the informal reasoning used in public health essentially appeals to the argument I am making: they argue that peoples’ health would be substantially better off with small lifestyle changes, but they are held back by social norms.<sup>2</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;A highly-cited paper by epidemiologist Geoffrey Rose (2001) argues that public health interventions should focus on community-wide interventions rather than just on high-risk individuals, in part because of peer influences. He says <em>“Eating, smoking, exercise and all our other life-style characteristics are constrained by social norms. If we try to eat differently from our friends it will not only be inconvenient, but we risk being regarded as cranks or hypochondriacs.”</em></p></div></div></section>
<section id="application-to-tobacco-taxes" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="application-to-tobacco-taxes">Application to Tobacco Taxes</h2>
<p><strong>Consumption of tobacco declined very slowly after the health effects were discovered.</strong> Deaths from lung cancer increased by a factor of 10 between 1925 and 1950, and by 1950 it was fairly clear that smoking was the overwhelming cause.<sup>3</sup> Rates of smoking peaked around 1960 and have been declining by about 5%/year ever since, and are now about 1/3 of their peak. Thus most of the deaths from smoking have been among people who started smoking <em>after</em> it became clear that smoking caused cancer.</p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Doll and Bradford Hill (1950)</p></div></div><p><strong>The decline in smoking was primarily due to a change in beliefs.</strong> DeCicca et al.&nbsp;(2022) document that in the US the tax rates on cigarettes stayed relatively constant between 1960 and 2022, and they say most studies estimate that cigarette demand is fairly insensitive to price (estimates of elasticity between -0.05 and -0.35). They also find relatively weak effects from other regulations, e.g.&nbsp;bans on advertising, bans on smoking in public places.</p>
<p>Thus the large aggregate decline in smoking seems to have been driven by a change in tastes, which presumably was downstream from a change in beliefs about health effects. That change in tastes has propagated very slowly presumably due to peer effects (AKA “norms” or “culture”).</p>
<p>In retrospect the decline was inefficiently slow: multiple generations got a taste for tobacco from their peers and their parents, and then found the habit difficult to shake. This implies an externality from smoking. In retrospect welfare would’ve been higher if we’d had high taxes in the 1960s to hasten the decline of smoking, and indeed in a simplified model it could’ve been Pareto improving: each person would lose utility from smoking less, but offset by a gain from their peers smoking less.</p>
<p>Suppose we come to believe that saturated fat or sugar has a comparable health effect to tobacco. Then in the long-run we expect their use will decline, but it might take 50 years for that to happen. Thus it would be efficient to apply a temporary tax to correct the peer-effect externality. </p>
</section>
</section>
<section id="evidence-on-peer-effects" class="level1 page-columns page-full">
<h1>Evidence on Peer Effects</h1>
<p>This section has a quick sketch of lines of evidence for the importance of peer effects in consumption decisions, but it is not as well organized as it could be.</p>
<p><strong>Adult preferences reflect childhood exposure.</strong> Whether you prefer rice, bread, or potatoes; prefer olive oil or butter; prefer shoes on or off; prefer country or rock n roll; is mostly determined by which one you were brought up with. The cleanest evidence I know of is Atkin (2013) and Atkin (2015) who shows that Indians choice of food is highly sensitive to upbringing: those who are brought up eating rice prefer rice, those who are brough up eating wheat prefer wheat, and the effects are so strong that they contribute substantially to malnutrition.</p>
<p><strong>There is more between-society variation in consumption than can be rationalized by between-society variation in prices.</strong> Northerners use butter, southerners use olive oil.</p>
<p><strong>Changes in average consumption are primarily due to between-cohort changes, rather than within-cohort.</strong> E.g. DeCicca (2022) says that the decline in smoking in the 20th century was primarily due to lower rates of uptake by each new generation rather than an acceleration of the decline in smoking by existing generations.<sup>4</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;Note that this pattern could be rationalized either by (1) within-cohort peer effects, or (2) within-person intertemporal complementarities. However note that the existence of within-person intetemporal complementarities will tend to increase the persistence of contemporaneous peer effects.</p></div></div><p><strong>Consumption is sensitive to changes in peer behaviour.</strong> If your friends cut back on smoking, drinking, and eating donuts, you’ll likely cut back too.</p>
<section id="long-run-effects-on-culture" class="level2">
<h2 class="anchored" data-anchor-id="long-run-effects-on-culture">Long-run effects on culture</h2>
<p>Nunn (2020) gives many examples of contemporary cultural traits which appear to reflect differences in historical environments that no longer exist today.</p>
<ul>
<li><p>Becker (2020): There are large contemporary differences in culture between societies which live in lands well-suited for pastoralism vs agriculture.</p></li>
<li><p>Alesina et al.&nbsp;(2011): <em>“the descendants of societies that traditionally practiced plough agriculture, today have lower rates of female participation in the workplace, in politics, and in entrepreneurial activities, as well as a greater prevalence of attitudes favoring gender inequality.”</em></p></li>
</ul>
<p>However these types of argument are notoriously difficult to evaluate: identification requires assuming many other causal channels are zero, and researchers typically examine a lot of data before finding an association that could be publishable.</p>
</section>
<section id="evidence-from-migration" class="level2">
<h2 class="anchored" data-anchor-id="evidence-from-migration">Evidence from Migration</h2>
<p>Alesina and Guiliano (2013) say</p>
<blockquote class="blockquote">
<p>“when immigrants move to a place with different institutions, overwhelmingly their cultural values change gradually, if ever, but rarely within two generations.</p>
</blockquote>
</section>
<section id="evidence-from-heritability" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="evidence-from-heritability">Evidence from Heritability</h2>
<p><strong>Twin studies show fairly high heritability of health behaviors.</strong> I give below some very rough estimates for the contribution of shared environment to adult health behaviors from twin studies. The shared environment estimate comes from the degree of correlation between fraternal twins (precisely: the excess relative to 50% of the correlation between identical twins).<sup>5</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;<strong>Vink et al.&nbsp;(2015) <a href="https://pubmed.ncbi.nlm.nih.gov/15971021/">Heritability of smoking initiation and nicotine dependence</a></strong> – study of Dutch twins. They cite other studies with similar decomposition for smoking inititation. “individual differences in smoking initiation were explained by genetic (44%), shared environmental (51%) and unique environmental (5%).” <br><br> <strong>Verhulst (2015) <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4345133/">The heritability of alcohol use disorders: a meta-analysis of twin and adoption studies</a></strong>: “The best-fit estimate of the heritability of AUD was 0.49 [95% confidence interval (CI) 0.43–0.53], and the proportion of shared environmental variance was 0.10 (95% CI 0.03–0.16).” <br><br> <strong>Maes (1997) <a href="https://link.springer.com/article/10.1023/A:1025635913927#citea">Genetic and Environmental Factors in Relative Body Weight and Human Adiposity</a>:</strong> <em>“genetic factors explain 50 to 90% of the variance in BMI”</em>. They don’t seem to give a preferred estimate for shared environment contribution, I’m going to say 10% based on skimming this and other papers.</p></div></div><table class="caption-top table">
<colgroup>
<col style="width: 28%">
<col style="width: 25%">
<col style="width: 22%">
<col style="width: 22%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>heritability (<img src="https://latex.codecogs.com/png.latex?h%5E2">)</th>
<th>shared env (<img src="https://latex.codecogs.com/png.latex?c%5E2">)</th>
<th>unique env (<img src="https://latex.codecogs.com/png.latex?e%5E2">)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>smoking initiation</td>
<td>45%</td>
<td>50%</td>
<td>5%</td>
</tr>
<tr class="even">
<td>alcohol use disorder</td>
<td>50%</td>
<td>10%</td>
<td>40%</td>
</tr>
<tr class="odd">
<td>obesity</td>
<td>70%</td>
<td>10%</td>
<td>20%</td>
</tr>
</tbody>
</table>
<p><strong>However twin studies under-state the importance of peer effects.</strong> The “shared environment” will pick up the influence of parents and mutual friends of siblings, however the “unique environment” will also include the peer effects from each sibling’s idiosyncratic friendships.</p>
<p>More importantly, twin studies decompose the variance in behaviours for a given population (usually a single country) at a particular time. However for most of these behaviors the within-society variance is small relative to the between-society variance. Thus twin decompositions of variance will only pick up the contribution of <em>local</em> peer effects (your family or neighborhood), not society-wide peer effects.</p>
</section>
</section>
<section id="relation-with-other-economics-literature" class="level1 page-columns page-full">
<h1>Relation with Other Economics Literature</h1>
<p><strong>Economics literature on social norms mostly treats preferences as fixed.</strong> There is a lot of economic literature on “social norms”, and is applied to behaviors like generosity, bargaining, fertility, &amp; medical treatment.<sup>6</sup> However I believe the majority of this literature treats preferences as fixed so that norms are modelled as multiple equilibria (Schelling (1978), Axelrod (1986), Mackie (1996)), or with a raw preference to conform (e.g.&nbsp;Young, 2005), or a raw preference to sanction those who deviate from a norm. I model norms as essentially absorption of preferences. The key conceptual distinction is how your behavior changes when you change peer groups: purely conventional norms ought to adapt immediately – you drive on the other side of the road when you go to another country. But acquired preferences should remain: you still like the music, the food, the literature, of your home country.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;Young (2005) is a good review.</p></div></div><p><strong>Relation with the Becker-Murphy (1988) model of addiction.</strong> Becker and Murphy claim that consumption of addictive goods (tobacco, heroin) is consistent with rational choice given that there is an inter-temporal complementarity: consuming at time <img src="https://latex.codecogs.com/png.latex?t"> increases the value of consuming at <img src="https://latex.codecogs.com/png.latex?t+1">. They say there’s evidence that people are clear-sighted about the complementarity: when people anticipate a cigarette tax in the future then they smoke less now.</p>
<p>Becker-Murphy is about within-person complementarities while the model in this note is about between-person complementarities. The between-person complementarities generate an externality, and therefore (unlike in Becker-Murphy) a reason to introduce a tax.</p>
<p>In addition it seems likely that a person’s <em>earliest</em> choices are partly naive about long-term effects: when a child or adolescent eats, smokes, or drinks alcohol, they are unlikely to fully anticipate the long-run consequences on their health or on their future tastes. If this is true this would be an additional reason (independent of peer effects) to tax or restrict youthful consumption.<sup>7</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;Becker (1992) is a long essay about habits and addiction that acknowledges the existence of acquired preferences though doesn’t seem to pursue the implications. Becker says <em>“[a]ltruistic parents … may refrain from smoking even when that gives them much pleasure because their smoking raises the likelihood that the children will smoke.”</em>.</p></div></div></section>
<section id="discussion" class="level1 page-columns page-full">
<h1>Discussion</h1>
<p><strong>Concrete implications:</strong> (1) If you brought up a child in a world where nobody smoked, drank, or ate twinkies, then they would not lose much pleasure but they would live a substantially longer life – a kid who never acquires a sweet tooth will live a life both happy and healthy. (2) If we had banned cigarettes in the 1960s then one generation of smokers would have been unhappy but each subsequent generation would have grown up with no cravings or desire to smoke and substantially better health (smoking reduces life expectancy by 5 years).</p>
<p><strong>Peer effects due to rational imitation.</strong> It can be rational to imitate what you see others do because their choices are informative about common costs and benefits (AKA “rational herding”, Banerjee (1992)).</p>
<p>However a basic model of rational herding would not generate the type of peer effects we see in practice. A pure informational peer effect implies that you should look at the behaviour of all peers not just nearby ones, e.g.&nbsp;you should be influenced by the worldwide participation in smoking instead of just in your local group. Also you should be most influenced by those who you believe to have the most information, e.g.&nbsp;the smoking rate among doctors or epidemiologists. In contrast peoples’ choices seem most influenced by their direct proximate peers.</p>
<p>If peer effects are due to rational herding then aggregate responses to changes in the environment will be sensitive to the nature of the change. E.g. when all prices are common knowledge then the aggregate and individual responses to a change will be the same. However when each person has some independent signal about the health effects of smoking then the aggregate effect of information can be far higher than the individual change.</p>
<p><strong>Peer effects due to environmental spillovers.</strong> Peer effects in consumption can exist due to increasing returns in production: e.g.&nbsp;if most of your peers prefer X over Y then most shops will sell only X, and most social practices will accommodate X, make you more likely to consume X. If you move to a town where the majority smoke, drink, and eat unhealthy foods, then it becomes more difficult to live as a non-smoker, non-drinker, and healthy eater.<sup>8</sup> </p>
<div class="no-row-height column-margin column-container"><div id="fn8"><p><sup>8</sup>&nbsp;Some theory and data from media markets is discussed in Anderson and Waldfogel (2005). There must be a literature on consumption spillovers in other markets but I haven’t seen it.</p></div><div id="fn9"><p><sup>9</sup>&nbsp;Galef (1998) discusses history and evidence for imitation. He seems to say that there’s been a lot of conceptual confusion about the definition of imitation and the evidence sufficient to demonstrate it: <em>“[i]t is somewhat surprising that almost 100 years of study of social learning in animals has failed to produce a clear answer to the question of whether animals can in fact learn to do an act from seeing it done,”</em>. There is also good discussion in the Wikipedia page for <a href="https://en.wikipedia.org/wiki/Social_learning_in_animals">social learning in animals</a>.</p></div></div><p><strong>Peer effects due to a hardwired desire.</strong> Evolution may have given us a hard-wired preference to do what we observe others doing. Other animals seem to imitate each other: e.g.&nbsp;bird-song and food choices seem to be heavily influenced by observation of others.<sup>9</sup> The reason we tend to influenced by peers might be for good informational reasons, as discussed above it’s generally reasonable to do what we observe others do. But these imitative instincts might be hard-wired, which would explain why we are influenced far more by near peers than by far peers. This is the basic perspective of evolutionary psychology: Tooby and Cosmides (1992) argue that humans are built with cognitive architecture that is adapative in the evolutionary environment although often maladaptive in the modern environment.</p>
<p><strong>It mostly doesn’t matter <em>why</em> we have peer effects.</strong> Below I discuss some different reasons why our choices might be attracted towards the choices of our peers. However for some questions it doesn’t matter what the cause is: if we can measure the slope of the individual and aggregate demand curves then we can (1) make predictions about the effect of interventions; and (2) measure the welfare effect of different policies (using the area under the demand curve). This is less true for <em>informational</em> peer effects, in which case the effect of a change in environment will depend on whether it is common knowledge, and surplus cannot be identified with the area under the demand curve.</p>
<p><strong>Q: How do peer effects interact with other externalities?</strong> Suppose there’s a $1 externality from consuming a good, then should we correct that with a $1 tax, given that we know the individual and aggregate elasticities will be different? There’s an argument that the tax should be larger than $1, because when you increase your consumption it will cause everyone else to increase their consumption too, so you cause more than $1 of harm.</p>
<p><strong>Q: would cohort-specific taxes be optimal?</strong> New Zealand has a plan to continuously raise the minimum legal age for buying tobacco so that people born after 2008 would never be allowed to buy tobacco.<sup>10</sup>. Not clear to me whether the model gives an answer to this.</p>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;<a href="https://www.theguardian.com/world/2022/dec/13/new-zealand-passes-world-first-tobacco-law-to-ban-smoking-by-2025">article</a></p></div></div></section>
<section id="models" class="level1 page-columns page-full">
<h1>Models</h1>
<section id="summary-of-models" class="level2">
<h2 class="anchored" data-anchor-id="summary-of-models">Summary of Models</h2>
<p>I describe three models of peer effects, each an extension of the last.</p>
<ol type="1">
<li><p><strong>Static model.</strong> Implies (1) aggregate elasticity greater than individual elasticity; (2) decentralized decision-making will cause too much variation in behaviour relative to optimal.</p></li>
<li><p><strong>Two-generation model.</strong> Suppose the second generation wishes to stay close to the first generation’s consumption. Then the first generation imposes an externality on the second generation, and they will fail to respond to anticipated future changes in price (or information).</p></li>
<li><p><strong>Infinite generation model.</strong> We can compare the rate of adjustment chosen by agents (ignoring their externality on future generations) and the efficient rate of adjustment, and therefore the appropriate tax to restore efficiency.</p></li>
</ol>
<p>To add:</p>
<ol type="1">
<li><p><strong>An overlapping generations model.</strong></p></li>
<li><p><strong>A fixed-cost model.</strong> I believe that this peer-effects model is isomorphic to a model with spillovers due to production with fixed costs, i.e.&nbsp;a Hotelling monopolist who must choose a point on a line, as in Anderson and Waldfogel (2005). However I would like to confirm this.</p></li>
<li><p><strong>Matrix of effects.</strong> I think you can characterize the theory with two derivatives: the externality and the strategic complementarity. I think it would be useful to give examples of the 9 permutations (pos/zero/negative; pos/zero/negative).</p></li>
</ol>
</section>
<section id="model-static-peer-effects" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="model-static-peer-effects">Model: Static Peer Effects</h2>
<p><strong>Summary.</strong> Suppose you buy <img src="https://latex.codecogs.com/png.latex?x_i"> at price <img src="https://latex.codecogs.com/png.latex?p">, and utility depends both on distance from ideal-point, <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bx%7D_i"> and distance from avg level of consumption, <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Bx%7D">. Then we’ll see that:</p>
<ol type="1">
<li><p>Aggregate price elasticity will be higher than individual price elasticity because when one person moves it gives everyone else a reason to move.</p></li>
<li><p>Average consumption is the same comparing social planner to decentralized.</p></li>
<li><p>Welfare-optimal consumption would be more compressed then decentralized consumption, because when we inflict an externality whenever we depart from the mean.</p></li>
<li><p>Average consumption moves <em>slowly</em>. Suppose everyone’s ideal points changes due to new information about health but they take last period’s mean consumption as the reference point. Then the society’s consumption will not adjust all the way to the new equilibrium.</p></li>
</ol>
<div class="cell page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-23-peer-effects-norms-culture-sin-taxes_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="288"></p>
<figcaption>Each person will choose a level of consumption between their exogenous preference and the population-average consumption (represented by the dashed line).</figcaption>
</figure>
</div>
</div></div></div>
<p><strong>Consumption with spillovers.</strong> Suppose you choose your level of consumption, <img src="https://latex.codecogs.com/png.latex?x_i">, and utility depends both on distance from ideal-point and distance from avg level of consumption: <img src="https://latex.codecogs.com/png.latex?u_i(x_i)%0A%20%20%20%20%20%20=%20%20-%20%5Cfrac%7B%5Calpha%7D%7B2%7D%7B%5Cutt%7B(x_i-%5Chat%7Bx%7D_i)%7D%7Bdistance%20from%7D%7Bideal%20point%7D%7D%5E2%0A%20%20%20%20%20%20%20%20%20-%5Cfrac%7B%5Cgamma%7D%7B2%7D%7B%5Cutt%7B(x_i-%5Cbar%7Bx%7D)%7D%7Bdistance%20from%7D%7Bpopn%20mean%7D%7D%5E2%20%20%20-%20px_i.%0A%20%20%20%20%20%20%20%20%20"></p>
<p><strong>We get optimal consumption,</strong> a function of ideal-point, avg consumption, and price: <img src="https://latex.codecogs.com/png.latex?%5Cutt%7Bx_i%5E*%7D%7Butility-maximizing%7D%7Bconsumption%7D%20=%0A%20%20%20%20%20%20%5Cfrac%7B%5Calpha%7D%7B%5Calpha+%5Cgamma%7D%5Chat%7Bx%7D_i%20+%0A%20%20%20%20%20%20%5Cfrac%7B%5Cgamma%7D%7B%5Calpha+%5Cgamma%7D%5Cbar%7Bx%7D%20-%0A%20%20%20%20%20%20%5Cfrac%7B1%7D%7B%5Calpha+%5Cgamma%7Dp.%0A%20%20%20%20%20%20"></p>
<p><strong>We get average consumption (<img src="https://latex.codecogs.com/png.latex?%5Cbar%7Bx%7D=%5Cfrac%7B1%7D%7Bn%7D%5Csum%20x_i%5E*">):</strong> <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Cbar%7Bx%7D%20&amp;=%20%5Cfrac%7B%5Calpha%7D%7B%5Calpha+%5Cgamma%7D%5Cfrac%7B1%7D%7Bn%7D%5Csum%20%5Chat%7Bx%7D_i%20+%20%5Cfrac%7B%5Cgamma%7D%7B%5Calpha+%5Cgamma%7D%5Cbar%7Bx%7D%20-%20%5Cfrac%7B1%7D%7B%5Calpha+%5Cgamma%7Dp%20%5C%5C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Cutt%7B%5Cfrac%7B1%7D%7Bn%7D%5Csum%20%5Chat%7Bx%7D_i%7D%7Bavg%20ideal%7D%7Bpoint%7D-%20%5Cfrac%7B1%7D%7B%5Calpha%7Dp.%0A%20%20%20%5Cend%7Baligned%7D%0A%20%20%20"></p>
<p><strong>Observations:</strong></p>
<div class="cell page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-23-peer-effects-norms-culture-sin-taxes_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="288"></p>
</figure>
</div>
</div></div></div>
<ul>
<li>Average consumption will be more price-sensitive than individual consumption (<img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B%5Calpha%7D%3E%5Cfrac%7B1%7D%7B%5Calpha+%5Cgamma%7D">).</li>
<li>The average level of consumption is independent of the strength of spillovers (<img src="https://latex.codecogs.com/png.latex?%5Cgamma">): spillovers just compress the variance, they don’t change the average level of consumption.</li>
<li>Suppose your utility depends on <em>prior</em> aggregate consumption, then you get slow convergence to equilibrium (I derive a dynamic model below).</li>
</ul>
<p><strong>Finally we derive welfare-maximizing consumption:</strong> <img src="https://latex.codecogs.com/png.latex?U%20=%20%5Csum_i%20u_i(x_i)%0A%20%20%20%20%20%20=%20%5Csum_i%5Cleft(%20-%20%5Cfrac%7B%5Calpha%7D%7B2%7D(x_i-%5Chat%7Bx%7D_i)%5E2%0A%20%20%20%20%20%20%20%20%20-%5Cfrac%7B%5Cgamma%7D%7B2%7D(x_i-%5Cfrac%7B1%7D%7Bn%7D%5Csum_jx_j)%5E2%20%20%20-%20px_i%5Cright)%0A%20%20%20"></p>
<p>The first-order condition for <img src="https://latex.codecogs.com/png.latex?x_i"> wil be: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BdU%7D%7Bdx_i%7D%0A%20%20%20%20%20%20=%20-%20%5Cutt%7B%5Calpha(x_i-%5Chat%7Bx%7D_i)%7D%7BMC%20of%20departing%7D%7Bfrom%20ideal%20point%7D%0A%20%20%20%20%20%20%20%20%20-%5Cutt%7B%5Cgamma(x_i-%5Cbar%7Bx%7D)%7D%7BMC%20of%20departing%7D%7Bfrom%20avg%7D%0A%20%20%20%20%20%20%20%20%20-%20p%0A%20%20%20%20%20%20%20%20%20+%5Cutt%7B%5Csum_j%5Cfrac%7B1%7D%7Bn%7D%5Cgamma(x_j-%5Cbar%7Bx%7D)%7D%7Bmarginal%20peer%20externality%7D%7Bon%20others%7D=0%0A%20%20%20"></p>
<p>So we have: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%5Cutt%7Bx_i%5E*%7D%7Bwelfare-maximizing%7D%7Bconsumption%20for%20$i$%7D%20&amp;=%0A%20%20%20%20%20%20%5Cfrac%7B%5Calpha%7D%7B%5Calpha+2%5Cgamma%7D%5Chat%7Bx%7D_i%20+%0A%20%20%20%20%20%20%5Cfrac%7B2%5Cgamma%7D%7B%5Calpha+2%5Cgamma%7D%5Cbar%7Bx%7D%20-%0A%20%20%20%20%20%20%5Cfrac%7B1%7D%7B%5Calpha+2%5Cgamma%7Dp.%5C%5C%0A%20%20%20%5Cutt%7B%5Cbar%7Bx%7D%5E*%7D%7Bavg%20welfare-max%7D%7Bconsumption%7D%0A%20%20%20%20%20%20&amp;=%20%5Cfrac%7B1%7D%7Bn%7D%5Csum_i%5Chat%7Bx%7D_i-%5Cfrac%7B1%7D%7B%5Calpha%7Dp.%0A%20%20%20%5Cend%7Baligned%7D%0A%20%20%20"></p>
<p><strong>Observations:</strong></p>
<ul>
<li>The average consumption is the same, whether decentralized or welfare-maximizing, but with welfare-maximizing consumption everyone has moved closer to the mean.</li>
</ul>
<p><strong>Q: adding direct externalities?</strong> Suppose each unit of consumption inflicts $1 non-peer externality on others, e.g.&nbsp;smog, second-hand smoke, congestion, etc. Without peer effects then a $1 tax would appropriately correct for this. Would the efficient tax also be $1 in this model?</p>
</section>
<section id="extension-direct-externalities" class="level2">
<h2 class="anchored" data-anchor-id="extension-direct-externalities">Extension: Direct Externalities</h2>
<p>Now we will add a direct externality cost, e.g.&nbsp;representing second-hand smoke or fiscal externality (I believe the results would be the same for an internality, e.g.&nbsp;myopic consumption choice). We will simplify the utility function in some other ways: everyone has the same ideal-point (at zero), and we ignore price:</p>
<p><img src="https://latex.codecogs.com/png.latex?u_i%20=%20-%5Cutt%7Bx_i%5E2%7D%7Beveryone's%20ideal%7D%7Bpoint%20is%20zero%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20-%20%5Cutt%7B%5Cfrac%7B%5Cgamma%7D%7B2%7D(x_i-%5Cbar%7Bx%7D)%5E2%7D%7Bpeer%7D%7Beffects%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20-%20%5Cutt%7B%5Ctheta%20%5Cbar%7Bx%7D%7D%7Bdirect%7D%7Bexternality%7D.%0A%20%20%20%20%20%20"></p>
<p>If there were no peer effects then everyone would simply set <img src="https://latex.codecogs.com/png.latex?x_i=0">, but the welfare-maximizing choice would be to set <img src="https://latex.codecogs.com/png.latex?x_i=%5Ctheta">, at which point the marginal personal cost of increasing consumption is equal to the marginal externality.</p>
<p>With peer effects the decentralized choice of <img src="https://latex.codecogs.com/png.latex?x_i"> will ignore the externality and will simply bias their choice of <img src="https://latex.codecogs.com/png.latex?x_i"> towards the population mean: <img src="https://latex.codecogs.com/png.latex?x_i%5E*%20=%20%5Cfrac%7B%5Cgamma%7D%7B%5Cgamma+1%7D%5Cbar%7Bx%7D."></p>
<p>The unique equilibrium will be at <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Bx%7D=0">, independent of the strength of peer effects.</p>
<p>Now we can derive the welfare-maximizing choice of <img src="https://latex.codecogs.com/png.latex?x_i"> with peer effects: <img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%20%20%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%20%20%20U%20%20&amp;=%20%5Cfrac%7B1%7D%7B2%7D%5Csum_i%20x_i%5E2%20-%20%5Csum_i%20%5Cfrac%7B%5Cgamma%7D%7B2%7D(x_i-%5Cbar%7Bx%7D)%5E2%20-%20%5Ctheta%20%5Csum_i%20x_i%20%5C%5C%0A%20%20%20%20%20%20%20%20%20%5Cfrac%7BdU%7D%7Bdx_i%7D%20%20%20&amp;=%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5Cutt%7Bx_i%7D%7Bmarginal%20cost%7D%7Bof%20departing%20from%200%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20-%20%5Cutt%7B%5Cgamma%20(x_i-%5Cbar%7Bx%7D)%7D%7Bmarginal%20cost%7D%7Bof%20departing%20from%20avg%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20+%20%5Cutt%7B%5Csum_j%20%5Cfrac%7B1%7D%7Bn%7D%5Cgamma%20(x_j-%5Cbar%7Bx%7D)%7D%7Bmarginal%20peer%20harm%7D%7Bto%20others%7D%0A%20%20%20%20%20%20%5Cend%7Baligned%7D%0A%20%20%20"> The third term above (“marginal peer harm to others”) will always be equal to zero (it simplifies to <img src="https://latex.codecogs.com/png.latex?%5Cgamma(%5Cbar%7Bx%7D-%5Cbar%7Bx%7D)">). T</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20-%20%5Cutt%7B%5Ctheta%7D%7Bmarginal%7D%7Bexternality%7D%20=0%20%5C%5C%0A%20%20%20%20%20%20%20%20%20%20%20%20&amp;=%20x_i%20-%20%5Cgamma%20(x_i-%5Cbar%7Bx%7D)%20-%20%5Ctheta%20=%200%20%5C%5C%0A%20%20%20%20%20%20%20%20%20x_i%20&amp;=%20%5Cfrac%7B-%5Cgamma%5Cbar%7Bx%7D+%5Ctheta%7D%7B1-%5Cgamma%7D%20%5C%5C%0A%20%20%20%20%20%20%20%20%20x_i%20&amp;=%20%5Cut%7B%5Ctheta%7D%7Bexternality%7D%20+%20%5Cutt%7B(%5Cfrac%7B%5Cgamma%7D%7B1-%5Cgamma%7D)(%5Ctheta-%5Cbar%7Bx%7D)%7D%7Bpull%20up%7D%7Brest%20of%20population%7D%20%5C%5C%0A%20%20%20%20%20%20%5Cend%7Baligned%7D%0A%20%20%20"></p>
<p>Observations:</p>
<ul>
<li>In equilibrium we can set <img src="https://latex.codecogs.com/png.latex?x_i=%5Cbar%7Bx%7D"> and we will have <img src="https://latex.codecogs.com/png.latex?x_i=%5Ctheta">, i.e.&nbsp;the welfare-maximizing choice is <em>independent</em> of the strength of peer effects, because they’ll all be zero whatever value you choose.</li>
<li>However if we hold fixed everyone else’s choice (take <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Bx%7D"> as given), then your choice effectively has two externalities: (1) the direct externality <img src="https://latex.codecogs.com/png.latex?%5Ctheta">, (2) the .</li>
</ul>
</section>
<section id="model-peer-effects-with-two-generations" class="level2">
<h2 class="anchored" data-anchor-id="model-peer-effects-with-two-generations">Model: peer effects with two generations</h2>
<p>For the dynamic models I assume peer effects operate solely through the <em>prior</em> generation’s actions: i.e.&nbsp;there’s some cost of departing from what your ancestors do. I assume there are no peer effects <em>within</em> a generation, and so can treat each generation as having just one agent.</p>
<p><strong>Two generation model.</strong> Suppose the second generation gets disutility from departing from the first generation’s level of consumption (“habituation”): <img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20U(x_1,x_2)%20%20%20%20%20&amp;=%20u_1%20+%20%5Cbeta%20u_2%20%5C%5C%0A%20%20%20%20%20%20u_1(x_1)%20%20%20%20%20%20%20&amp;=%20%5Cutt%7B(x_1-%5Chat%7Bx%7D_1)%5E2%7D%7Bdeviation%7D%7Bfrom%20ideal%7D%20%5C%5C%0A%20%20%20%20%20%20u_2(x_1,x_2)%20%20%20&amp;=%20%5Cutt%7B(x_2-%5Chat%7Bx%7D_2)%5E2%7D%7Bdeviation%7D%7Bfrom%20ideal%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20+%20%5Cgamma%20%5Cut%7B(x_2-x_1)%5E2%7D%7Bhabitutation%7D%5C%5C%0A%20%20%20%5Cend%7Baligned%7D%0A"></p>
<p><strong>Results:</strong></p>
<ol type="1">
<li><strong>The decentralized solution is inefficient.</strong> The first generation doesn’t care about the second generation, &amp; so hurts the second generation.</li>
<li><strong>The centralized solution would alter first generation’s consumption.</strong> We push the first generation a bit towards the second generation’s ideal-point. If we have a perfect ability to tax and rebate then we can implement the centralized solution with a tax.</li>
<li><strong>Roughly: efficient tax would bring you 1/3 of the way towards long-run ideal point.</strong> Suppose the half-life of adjustment is 1 generation (<img src="https://latex.codecogs.com/png.latex?%5Cgamma=1">) and there is no discounting (<img src="https://latex.codecogs.com/png.latex?%5Cbeta=1">). Then the first generation consumption should be 1/3 of the way towards the long-term ideal point.</li>
</ol>
<p><strong>Decentralized solution.</strong> The first generation just chooses their ideal point, the second generation is somewhere between the habituation point and their own ideal point. If <img src="https://latex.codecogs.com/png.latex?%5Cgamma=1"> then they’re half-way, meaning the half-life of adjustment is one generation.</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20x_1%20%20&amp;=%20%5Chat%7Bx%7D_1%5C%5C%0A%20%20%20x_2%20%20&amp;=%20%5Chat%7Bx%7D_1%5Cfrac%7B%5Cgamma%7D%7B1+%5Cgamma%7D+%20%5Chat%7Bx%7D_2%5Cfrac%7B1%7D%7B1+%5Cgamma%7D%0A%5Cend%7Baligned%7D">
<p><strong>Centralized solution.</strong> We now adjust the first-generation consumption towards the second-generation’s ideal point (derivation below):</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20x_1%20%20&amp;=%20%5Chat%7Bx%7D_1%5Cfrac%7B1+%5Cgamma%7D%7B1+%5Cgamma+%5Cgamma%5Cbeta%7D+%20%5Chat%7Bx%7D_2%20%5Cfrac%7B%5Cgamma%5Cbeta%7D%7B1+%5Cgamma+%5Cgamma%5Cbeta%7D%5C%5C%0A%20%20%20x_2%20%20&amp;=%20%5Chat%7Bx%7D_1%5Cfrac%7B%5Cgamma%7D%7B1+%5Cgamma+%5Cgamma%5Cbeta%7D+%20%5Chat%7Bx%7D_2%5Cfrac%7B1+%5Cgamma%5Cbeta%7D%7B1+%5Cgamma+%5Cgamma%5Cbeta%7D%0A%5Cend%7Baligned%7D">
<p><strong>Observations:</strong></p>
<ol type="1">
<li>If no spillovers (<img src="https://latex.codecogs.com/png.latex?%5Cgamma=0">) then <img src="https://latex.codecogs.com/png.latex?x_1=%5Chat%7Bx%7D_1">, <img src="https://latex.codecogs.com/png.latex?x_2=%5Chat%7Bx%7D_2">.</li>
<li>If we don’t care about the future (<img src="https://latex.codecogs.com/png.latex?%5Cbeta=0">) then we get the decentralized solution again.</li>
</ol>
<p><strong>Derivation:</strong></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Cfrac%7BdU%7D%7Bdx_1%7D%20%20%20&amp;=%20(x_1%20-%20%5Chat%7Bx%7D_1)%20-%20%5Cgamma%5Cbeta(x_2-x_1)%20=%200%20%5C%5C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20x_1%20%20%20&amp;=%20%5Cfrac%7B%5Chat%7Bx%7D_1+%5Cgamma%5Cbeta%20x_2%7D%7B1+%5Cgamma%5Cbeta%7D%20%5C%5C%0A%20%20%20%20%20%20%5Cfrac%7BdU%7D%7Bdx_2%7D%20%20%20&amp;=%20(x_2%20-%20%5Chat%7Bx%7D_2)%20+%20%5Cgamma(x_2-x_1)%20=%200%20%5C%5C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20x_2%20%20%20&amp;=%20%5Cfrac%7B%5Chat%7Bx%7D_2+%5Cgamma%20x_1%7D%7B1+%5Cgamma%7D%20%5C%5C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Cfrac%7B%5Chat%7Bx%7D_2+%5Cgamma%20%5Cfrac%7B%5Chat%7Bx%7D_1+%5Cgamma%5Cbeta%20x_2%7D%7B1+%5Cgamma%5Cbeta%7D%7D%7B1+%5Cgamma%7D%20%5C%5C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Chat%7Bx%7D_2%5Cfrac%7B1%7D%7B1+%5Cgamma%7D%20+%20%5Chat%7Bx_1%7D%5Cfrac%7B%5Cgamma%7D%7B(1+%5Cgamma)(1+%5Cgamma%5Cbeta)%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20+%20x_2%5Cfrac%7B%5Cgamma%5E2%5Cbeta%7D%7B(1+%5Cgamma)(1+%5Cgamma%5Cbeta)%7D%20%5C%5C%0A%20%20%20%20%20%20x_2%5Cfrac%7B(1+%5Cgamma)(1+%5Cgamma%5Cbeta)-%5Cgamma%5E2%5Cbeta%7D%7B(1+%5Cgamma)(1+%5Cgamma%5Cbeta)%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Chat%7Bx%7D_2%5Cfrac%7B1%7D%7B1+%5Cgamma%7D%20+%20%5Chat%7Bx_1%7D%5Cfrac%7B%5Cgamma%7D%7B(1+%5Cgamma)(1+%5Cgamma%5Cbeta)%7D%20%5C%5C%0A%20%20%20%20%20%20x_2%20(1+%5Cgamma+%5Cgamma%5Cbeta)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Chat%7Bx%7D_2(1+%5Cgamma%5Cbeta)+%5Chat%7Bx%7D_1%5Cgamma%20%5C%5C%0A%20%20%20%20%20%20x_2%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Chat%7Bx%7D_2%5Cfrac%7B1+%5Cgamma%5Cbeta%7D%7B1+%5Cgamma+%5Cgamma%5Cbeta%7D%20+%20%5Chat%7Bx%7D_1%5Cfrac%7B%5Cgamma%7D%7B1+%5Cgamma+%5Cgamma%5Cbeta%7D%20%5C%5C%0A%20%20%20%20%20%20x_1%20%20%20%20%20%20&amp;=%20%5Chat%7Bx%7D_1%5Cleft(%5Cfrac%7B1%7D%7B1+%5Cgamma%5Cbeta%7D+%5Cfrac%7B%5Cgamma%5E2%5Cbeta%7D%7B(1+%5Cgamma%5Cbeta)(1+%5Cgamma+%5Cgamma%5Cbeta)%7D%5Cright)%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20+%20%5Chat%7Bx%7D_2%20%5Cfrac%7B%5Cgamma%5Cbeta%7D%7B1+%5Cgamma%5Cbeta%7D%5Cfrac%7B1+%5Cgamma%5Cbeta%7D%7B1+%5Cgamma+%5Cgamma%5Cbeta%7D%5C%5C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Chat%7Bx%7D_1%5Cfrac%7B1+%5Cgamma%7D%7B1+%5Cgamma+%5Cgamma%5Cbeta%7D+%20%5Chat%7Bx%7D_2%20%5Cfrac%7B%5Cgamma%5Cbeta%7D%7B1+%5Cgamma+%5Cgamma%5Cbeta%7D%0A%20%20%20%5Cend%7Baligned%7D%0A"></p>
</section>
<section id="model-peer-effects-with-infinite-generations" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="model-peer-effects-with-infinite-generations">Model: peer effects with infinite generations</h2>
<div class="cell page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-23-peer-effects-norms-culture-sin-taxes_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="288"></p>
<figcaption>Peer effects will cause inefficiently slow adjustment to price changes.</figcaption>
</figure>
</div>
</div></div></div>
<p>We model an infinite series of agents. Each has an ideal-point of zero, but pays some adjustment cost for departing from the prior level of consumption (<img src="https://latex.codecogs.com/png.latex?x_%7Bt-1%7D">). Given some starting point (<img src="https://latex.codecogs.com/png.latex?x_0%3E0">) we wish to know how rapidly consumption will converge to its long-run equilibrium.</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%5Cutt%7Bu_t%7D%7Bagent%20$t$%7D%7Butility%7D%20&amp;=%20-%5Cut%7Bx_%7Bt%7D%5E2%7D%7Bcost%7D%20-%20%5Cutt%7B%5Cgamma(x_%7Bt%7D-x_%7Bt-1%7D)%5E2%7D%7Badjustment%7D%7Bcost%7D%20%20%5C%5C%0A%20%20%20%5Cutt%7BU%7D%7Bsocial%7D%7Bwelfare%7D%20&amp;=%20-%5Csum_%7Bt=1%7D%5E%5Cinfty%20%5Cbeta%5Et%20u_t%0A%5Cend%7Baligned%7D">
<p>Each agent individually will simply bias their consumption towards last period’s consumption, and so the convergence factor will be simply <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cgamma%7D%7B%5Cgamma+1%7D">: <img src="https://latex.codecogs.com/png.latex?x_t=%5Cfrac%7B%5Cgamma%7D%7B%5Cgamma+1%7Dx_%7Bt-1%7D."></p>
<p>However when maximizing social welfare we also need to take into account the effect on future utility. By taking the first-order condition with respect to <img src="https://latex.codecogs.com/png.latex?x_t"> we can derive a Euler-equation relationship between <img src="https://latex.codecogs.com/png.latex?x_%7Bt-1%7D">, <img src="https://latex.codecogs.com/png.latex?x_t">, and <img src="https://latex.codecogs.com/png.latex?x_%7Bt+1%7D">:</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Cfrac%7BdU%7D%7Bdx_t%7D%20%20%20%0A%20%20%20%20%20%20%20%20%20&amp;=%20%5Cgamma%5Cbeta%5Et%20(x_t-x_%7Bt-1%7D)%20-%20%5Cgamma%5Cbeta%5E%7Bt+1%7D(x_%7Bt+1%7D-x_t)+%5Cbeta%5Et%20x_t%20=0%20%5C%5C%0A%20%20%20%20%20%20%20%20%20&amp;=%20(%5Cgamma+%5Cgamma%5Cbeta+1)x_t%20-%5Cgamma%20x_%7Bt-1%7D-%5Cgamma%5Cbeta%20x_%7Bt+1%7D%5C%5C%0A%20%20%20%20%20x_t%20&amp;=%20x_%7Bt-1%7D%5Cfrac%7B%5Cgamma%7D%7B%5Cgamma+%5Cgamma%5Cbeta+1%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20+%20x_%7Bt+1%7D%5Cfrac%7B%5Cgamma%5Cbeta%7D%7B%5Cgamma+%5Cgamma%5Cbeta+1%7D%0A%5Cend%7Baligned%7D">
<p>If we conjecture exponential convergence, meaning <img src="https://latex.codecogs.com/png.latex?x_%7Bt+1%7D=%5Ctheta%20x_t">, we can solve for the convergence factor <img src="https://latex.codecogs.com/png.latex?%5Ctheta">: <img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20x_t%20&amp;=%20%5Ctheta%5E%7B-1%7Dx_t%20%5Cfrac%7B%5Cgamma%7D%7B%5Cgamma+%5Cgamma%5Cbeta+1%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20+%20%5Ctheta%20x_t%5Cfrac%7B%5Cgamma%5Cbeta%7D%7B%5Cgamma+%5Cgamma%5Cbeta+1%7D%20%5C%5C%0A%20%20%20%20%5Cfrac%7B%5Cgamma+%5Cgamma%5Cbeta+1%7D%7B%5Cgamma%7D%20&amp;=%20%5Ctheta%5E%7B-1%7D+%5Ctheta%5Cbeta%20%5C%5C%0A%20%20%20%20%5Ctheta%5E2%5Cut%7B%5Cbeta%7D%7Ba%7D%20-%5Ctheta%20%5Cut%7B%5Cfrac%7B%5Cgamma+%5Cgamma%5Cbeta+1%7D%7B%5Cgamma%7D%7D%7Bb%7D%20+%5Cut%7B1%7D%7Bc%7D%20&amp;=%200%20%5C%5C%0A%20%20%20%5Cend%7Baligned%7D%0A"></p>
<p>Applying the quadratic formula we get a solution for <img src="https://latex.codecogs.com/png.latex?%5Ctheta">: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Ctheta%20%20%20&amp;=%5Cfrac%7B-b%5Cpm%5Csqrt%7Bb%5E2-4ac%7D%7D%7B2a%7D%5C%5C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20&amp;=%20%5Cfrac%7B-%5Cfrac%7B%5Cgamma+%5Cgamma%5Cbeta+1%7D%7B%5Cgamma%7D%5Cpm%5Csqrt%7B%5Cfrac%7B%5Cgamma+%5Cgamma%5Cbeta+1%7D%7B%5Cgamma%7D%5E2-4%5Cbeta%7D%7D%7B2%5Cbeta%7D%0A%20%20%20%5Cend%7Baligned%7D%0A"></p>
<p><strong>Observations:</strong></p>
<div class="cell page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-23-peer-effects-norms-culture-sin-taxes_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="288"></p>
<figcaption>Simulation to confirm the solution. We let <img src="https://latex.codecogs.com/png.latex?%5Cbeta=%5Cgamma=1">, implying a law of motion of <img src="https://latex.codecogs.com/png.latex?x_%7Bt+1%7D=3x_t-x_%7Bt-1%7D">, and as predicted <img src="https://latex.codecogs.com/png.latex?x"> declines by a factor of 0.38 each generation.</figcaption>
</figure>
</div>
</div></div></div>
<ul>
<li><p>The solution seems to be irreducibly quadratic. If <img src="https://latex.codecogs.com/png.latex?%5Cbeta=1"> then we can get a slightly simpler expression for the relationship between <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> and <img src="https://latex.codecogs.com/png.latex?%5Cgamma">: <img src="https://latex.codecogs.com/png.latex?%5Ctheta%5E2%20-%5Ctheta%5Cfrac%7B2%5Cgamma+1%7D%7B%5Cgamma%7D+1=0"> <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20=%20%5Cfrac%7B%5Cfrac%7B2%5Cgamma+1%7D%7B%5Cgamma%7D%5Cpm%5Csqrt%7B%5Cleft(%5Cfrac%7B2%5Cgamma+1%7D%7B%5Cgamma%7D%5Cright)%5E2-4%7D%7D%7B2%7D"></p></li>
<li><p>If <img src="https://latex.codecogs.com/png.latex?%5Cbeta=1"> (no discounting) and <img src="https://latex.codecogs.com/png.latex?%5Cgamma=1"> (habituation and ideal-point are equally strong):</p>
<ul>
<li><p>Decentralized convergence: will not be forward-looking, and so each period will be half-way between prior and the long-run equilibrium: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bx_%7Bt+1%7D%7D%7Bx_t%7D=0.5">.</p></li>
<li><p>Centralized convergence: <img src="https://latex.codecogs.com/png.latex?%5Ctheta%20=%20%5Cfrac%7B3%5Cpm%5Csqrt%7B3%5E2-4%7D%7D%7B2%7D=%5Cfrac%7B3%7D%7B2%7D%5Cpm%5Cfrac%7B%5Csqrt%7B5%7D%7D%7B2%7D%0A%20%20%20%20=%20(2.618,%200.381966)%0A"></p>
<p>We choose the root that is below 1 (the other root would be explosive), and so <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bx_%7Bt+1%7D%7D%7Bx_t%7D=%5Ctheta%5Csimeq%200.38">, i.e.&nbsp;socially optimal convergence will be only somewhat faster than decentralized convergence in this case. If we add time-discounting I believe the difference between the two rates of convergence will get smaller.</p></li>
</ul></li>
<li><p>If <img src="https://latex.codecogs.com/png.latex?%5Cbeta=0"> (infinite discounting) then <img src="https://latex.codecogs.com/png.latex?%5Ctheta=%5Cfrac%7B%5Cgamma%7D%7B%5Cgamma+1%7D">, i.e.&nbsp;the centralized solution is the same as the decentralized solution, because we do not care about future generations.</p></li>
<li><p>If <img src="https://latex.codecogs.com/png.latex?%5Cbeta=1"> (no discounting) and <img src="https://latex.codecogs.com/png.latex?%5Cgamma=2"> (adjustment costs strong) then decentralized convergence will have <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bx_%7Bt+1%7D%7D%7Bx_t%7D=%5Cfrac%7B2%7D%7B3%7D"> and centralized convergence will have <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bx_%7Bt+1%7D%7D%7Bx_t%7D=%5Ctheta=%5Cfrac%7B1%7D%7B2%7D">.</p></li>
</ul>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<p>Allcott, Lockwood, Taubinsky (2019) “Should We Tax Sugar-Sweetened Beverages? An Overview of Theory and Evidence” in <em>Journal of Economic Perspectives</em>.</p>
<p>Anderson and Waldfogel (2005) “Preference Externalities in Media Markets” in <em>Handbook of Media Economics</em></p>
<p>Atkin (2013, AER) <a href="https://www.mit.edu/~atkin/Tastes_and_Nutrition.pdf">Trade, Tastes and Nutrition in India</a>**</p>
<ul>
<li>Gives a model of habit formation, shows it fits Indian data, derives implications for trade policy.</li>
</ul>
<blockquote class="blockquote">
<p>“Household tastes evolve over time to favor foods consumed as a child.”</p>
</blockquote>
<blockquote class="blockquote">
<p>“households in the Indian state of West Bengal devoted 48 percent of their food expenditure to rice and 5 percent to wheat in 1987-88. Despite facing similar prices, households in the state of Rajasthan devoted 30 percent of their food expenditure to wheat and 1 percent to rice … If the average household in West Bengal allocated their expenditure on rice and wheat in the same proportion as households in Rajahstan, they would have obtained 23 percent more calories. … These unrealized nutritional gains are striking given that over 50 percent of children in West Bengal were classified as underweight around this time, and presumably these additional calories would have brought nutritional benefits”</p>
</blockquote>
<p>Atkin (2015, AER) “The Caloric Costs of Culture: Evidence from Indian Migrants”</p>
<blockquote class="blockquote">
<p>“I … show that migrants bring their origin-state food preferences with them.”</p>
</blockquote>
<p>Berkowitz (2005) “An Overview of the Social Norms Approach”</p>
<p>Galef (1998) “Imitations in Animals” in Zentall and Galef (eds.) <em>Social Learning Psychological and Biological Perspectives</em>.</p>
<p>Nunn (2020) “History as Evolution” in the <em>Handbook of Historical Economics.</em></p>
<p>Nunn (2022) “On the Causes and Consequences of Cross-Cultural Differences: An Economic Perspective”</p>
<p>Rose, Geoffrey (2001) “Sick individuals and sick populations”, <em>International Journal of Epidemiology</em>.</p>
<p>Tooby and Cosmides (1992) “The Psychological Foundations of Culture”</p>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{cunningham2024,
  author = {Cunningham, Tom},
  title = {Peer {Effects,} {Culture,} and {Taxes}},
  date = {2024-04-28},
  url = {tecunningham.github.io/posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-cunningham2024" class="csl-entry quarto-appendix-citeas">
Cunningham, Tom. 2024. <span>“Peer Effects, Culture, and Taxes.”</span>
April 28, 2024. <a href="https://tecunningham.github.io/posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html">tecunningham.github.io/posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html</a>.
</div></div></section></div> ]]></description>
  <guid>tecunningham.github.io/posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html</guid>
  <pubDate>Sun, 28 Apr 2024 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Bloodhounds and Bulldogs</title>
  <dc:creator>Tom Cunningham</dc:creator>
  <link>tecunningham.github.io/posts/2023-10-24-manifesto-perception-judgment-decision-making.html</link>
  <description><![CDATA[ 





<p>This note contains some ideas about hierarchical structure in perception, judgment and decision-making that I haven’t seen explained clearly elsewhere.</p>
<section id="summary" class="level1 page-columns page-full" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> Summary</h1>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="tecunningham.github.io/posts/images/bloodhound.jpeg" class="img-fluid"> <br><br><br></p>
</div></div><div class="cell page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-10-24-manifesto-perception-judgment-decision-making_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div></div></div>
<ol type="1">
<li><p><strong>A model of encapsulated inference (“bloodhound”).</strong> Many puzzling phenomena in perception, judgment, and decision-making can be explained if we assume that initial judgments are formed <em>pre-consciously</em> in a way that both (a) incorporates information not accessible to the conscious brain, and (b) does not incorporate information that is accessible to the conscious brain. This “double encapsulation” of judgment is a necessary condition for biases to occur in a model of the sequential aggregation of information. As an analogy we can think of someone tracking a scent with a bloodhound on a leash.Suppose the dog has private information, i.e.&nbsp;it know things that the human does not: this itself will not lead to inefficient decisions. However when the human also has private information, known only to her, then there can be inefficient decision-making relative to the benchmark of decisions based on pooling information between the person and the dog.</p></li>
<li><p><strong>Applied to perception.</strong> Many existing explanation of perceptual biases and illusions can be stated in terms of sequential aggregation of information. A prototypical case occurs when a perceptual cue that is ordinarily informative appears in a context where we are consciously aware that it is uninformative. Because the informational value of the cue is known only to the pre-conscious system the cue will still affect our judgment even when we know it to be irrelevant. There is an additional set of biases in judging the strength of raw sensory stimuli, e.g.&nbsp;the light faling on your eye. If the conscious brain does not have direct access to those stimuli then it will be reverse-inferred from the signals received from the encapsulated processes, which will induce spillovers between signals and result in contrast effects and cross-modal effects.</p></li>
<li><p><strong>Applied to judgment.</strong> The model says that biases in factual and logical judgments (anchoring, framing, etc.) occur in the same way as perceptual biases. A prototypical bias occurs when an ordinarily-informative cue appears in a situation where we know consciously that it is irrelevant. In short: it is rational to trust our instincts, even though our instincts can be sensitive to features of the environment that we know to be irrelevant.</p></li>
<li><p><strong>Applied to decision-making.</strong> We can treat decision-making as a special case of judgment, but we are judging relative value of each alternative. As in judgment we rationally trust our intuitions of value, despite being uncertain about what influences them. This can cause inconsistencies in decision-making when we are in unusual contexts. Many people have attempted to formalize decision biases (ambiguity aversion, prospect theory, etc.) but I believe those formalizations have limited value because (1) the biases’ fundamental causes are statistical regularities in the environment, and so the biases will be context-dependent rather than universal; and (2) people are self-reflective about their biases and act to correct them, so treating them as hard-wired into utility or perception will give misleading predictions.</p></li>
<li><p><strong>I discuss a little the history of the study of human judgment.</strong> There have been many dramatic shifts in paradigm and many laws have been proposed which later were found to generalize poorly outside their original domain. There is still today a lack of consensus around the basic reasons why human judgment is good in some domains, bad in others.</p></li>
<li><p><strong>Observation: in most cases human judgment is a close approximation of efficient inference.</strong> Computers rarely outperform humans when given the same information. In the history of psychology many discoveries of laws in human judgment have turned out to reflect, on closer inspection, optimal responses to the environment. However there are some cases where judgment makes simple mistakes – illusions, biases, anomalies – and so some theory is needed to explain why those happen.</p></li>
<li><p><strong>Encapsulated preferences (“bulldog”).</strong> I briefly discuss a separate feature of decision-making: the existence of encapsulated <em>preferences</em>, distinct from encapsulated <em>information</em>. Encapsulated preferences are hard-wired instincts that are rational with respect to a broader set of goals given a limited information-set. As an analogy think of a person with a bulldog on a leash who pulls you where they want to go: now the person is influenced by their dog in two ways: by the information received and by the dog’s own preferences.</p></li>
</ol>
</section>
<section id="history" class="level1 page-columns page-full" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> History</h1>
<p><strong>The study of human judgment is intellectual quicksand.</strong> Many historical paradigms for studying human thought and decision-making have left little trace. Many smart people spent decades working on theories that seem, in retrospect, confused and unnecessarily complicated. There are still multiple schools of thought on most major questions. I believe the most significant progress it the general acceptance that human judgment typically corresponds to optimal inference, discussed further below.</p>
<p>Two factors that might explain the lack of progress: (1) scientists’ own introspection becomes distorted such that it confirms their own theories; (2) the language we use to talk about perception and judgment is highly ambiguous, and this makes it possible for theories to survive a long time, appearing to accommodate many facts, while being in practice unfalsifiable.</p>
<p><strong>Patterns have often failed to generalize.</strong> There are many standardized experimental tasks used to study judgment and decision-making: choice between stimuli (psychophysics); the Stroop task; the implicit association task; the Wason card task; the Iowa gambling task; learning with reinforcement; choice between gambles; two-player games.</p>
<p>Many laws have been proposed based on patterns found in these tasks, but there has been a consistent failure for these patterns to generalize outside the specific tasks, and even within these tasks the laws have often been found to be sensitive to minor tweaks of context, e.g.&nbsp;instrumental conditioning, lateral inhibition, probability-weighting, loss-aversion, reciprocity. One interpretation is that behavior is very context-sensitive, and generalization is difficult. Another interpretation, which I think is the principal explanation, is that behavior is typically well-calibrated for the cues in a given situation, meaning that behavior in a given situation reveals more about the statistics of that situation than about the wiring of decision-making processes.<sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;I give a more detailed argument for this interpretation of the history of psychology in <em><a href="https://tecunningham.github.io/posts/2017-04-15-the-mechanical-and-the-rational.html">The Repeated Failure of the Laws of Behaviour</a></em>, and in <em><a href="https://tecunningham.github.io/posts/2016-04-30-relative-thinking.html">Relative Thinking</a></em>.</p></div></div></section>
<section id="a-model-of-inference" class="level1 page-columns page-full" data-number="3">
<h1 data-number="3"><span class="header-section-number">3</span> A Model of Inference</h1>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://latex.codecogs.com/png.latex?%5Cxymatrix@R-2pc%7B%0A%20%20%20%20%20%20&amp;%20%5Cboxed%7Bx_%7B1%7D%7D%5Car%5Bdddr%5D%20%5C%5C%0A%20%20%20%20%20%20&amp;%20%5Cboxed%7Bx_%7B2%7D%7D%5Car%5Bddr%5D%20%5C%5C%0A%20%20%20%20%20%20&amp;%20%5Cboxed%7Bx_%7B3%7D%7D%5Car%5Bdr%5D%20%5C%5C%0A%20%20%20%20%20%20v%5Car%5Buuur%5D%5Car%5Buur%5D%5Car%5Bur%5D%5Car%5Br%5D%5Car%5Bdr%5D%5Car%5Bddr%5D%5Car%5Bdddr%5D%20&amp;%20%5Cboxed%7Bx_%7B4%7D%7D%5Car%5Br%5D%20&amp;%20%5Chat%7Bv%7D%5C%5C%0A%20%20%20%20%20%20&amp;%20%5Cboxed%7Bx_%7B5%7D%7D%5Car%5Bur%5D%20%20%20%5C%5C%0A%20%20%20%20%20%20&amp;%20%5Cboxed%7Bx_%7B6%7D%7D%5Car%5Buur%5D%20%5C%5C%0A%20%20%20%20%20%20&amp;%20%5Cboxed%7Bx_%7B7%7D%7D%5Car%5Buuur%5D%20%5C%5C%7D%0A%20%20%20"></p>
</div></div><p>Let us start with a model of judgment as rational inference. There is some unobserved fact about the world, <img src="https://latex.codecogs.com/png.latex?v">, a set of observations which relate to that fact, <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bx%7D=(x_1,%5Cldots,x_n)">, and an estimate formed by the brain using those observations: <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bv%7D=E%5Bv%7Cx_1,%5Cldots,x_n%5D."></p>
<p>A simple prediction of this model is that influences on judgment of <img src="https://latex.codecogs.com/png.latex?v"> should correspond to regularities in the world, i.e.:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cutt%7B%5Cfrac%7Bd%5Chat%7Bv%7D%7D%7Bdx_i%7D(%5Cbm%7Bx%7D)%3E0%7D%7Bcue%20$i$%20positively%20affects%7D%7Bjudgment%20of%20value%7D%0A%20%20%20%5Ciff%0A%20%20%20%5Cutt%7B%5Ctext%7Bcorr%7D(v,x_i%7C%5Cbm%7Bx%7D_%7B-i%7D)%3E0%7D%7Bcue%20$i$%20is%20positively%20associated%7D%7Bwith%20value,%20all%20else%20equal%7D.%0A%20%20%20"></p>
<p><strong>Testing the model.</strong> The model’s predictions depend on the joint distribution of observed and unobserved characteristics in the decision-maker’s ordinary environment, <img src="https://latex.codecogs.com/png.latex?f(v,%5Cbm%7Bx%7D)">. This distribution can be difficult to credibly measure, however there have been many cases where laboratory findings about judgment turn out to be neatly rationalized by correlations in the world:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 44%">
<col style="width: 55%">
</colgroup>
<thead>
<tr class="header">
<th><strong>influence on judgment</strong></th>
<th><strong>correlation in world</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Blue objects look more distant</td>
<td>Distant objects are bluer (Rayleigh scattering)</td>
</tr>
<tr class="even">
<td>Cold objects feel heavier</td>
<td>Heavier objects transfer more heat</td>
</tr>
<tr class="odd">
<td>People repeat rewarded actions</td>
<td>Rewards are correlated over time</td>
</tr>
<tr class="even">
<td>Random rewards cause more persistence</td>
<td>Probability of reward is correlated</td>
</tr>
</tbody>
</table>
<p>In each of these cases psychologists initially discovered an influence on judgment (<img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%5Chat%7Bv%7D%7D%7Bdx_i%7D">), and proposed a mechanical explanation, e.g.&nbsp;based on how neurons are wired. Subsequently it was discovered that the influence on judgment explanations noticed that the influence could be rationalized by some correlation in the world. We can test these explanations by altering the context, finding a situation where we expect the real-world correlation between <img src="https://latex.codecogs.com/png.latex?x_i"> and <img src="https://latex.codecogs.com/png.latex?v"> to reverse, and we predict that the influence on judgment will reverse. Some examples of successful tests: (1) cold objects feel heavier, but hot objects feel heavier too, and this is consistent with the real-world association, because the effect of weight on heat-transfer depends on the <em>difference</em> from skin temperature. (2) when a rat finds a reward in a maze, it is then <em>less</em> likely to follow the same path subsequently (in contradiction to reinforcement learning), intuitively because they expect the food to be gone, i.e.&nbsp;they expect a negative correlation between past-rewards and future-rewards, instead of a positive one.</p>
<p>Feldman (2013):</p>
<blockquote class="blockquote">
<p>“[Bayesian] optimality helps explain why the perceptual system, notwithstanding its many apparent quirks and special rules, works the way it does— because these rules approximate the Bayesian posterior.”</p>
</blockquote>
<p>So far we have discussed evidence that perception and judgment are consistent with efficient use of evidence. However there are some patterns which resist rationalization, and for those we need a deeper theory.</p>
</section>
<section id="a-model-of-encapsulated-inference-bloodhound" class="level1 page-columns page-full" data-number="4">
<h1 data-number="4"><span class="header-section-number">4</span> A Model of Encapsulated Inference (“Bloodhound”)</h1>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://latex.codecogs.com/png.latex?%5Cxymatrix@R-2pc%7B%0A%20%20%20&amp;%20%5Cboxed%7Bx_%7B1%7D%7D%5Car%5Bdr%5D%5C%5C%0A%20%20%20&amp;%20%5Cboxed%7Bx_%7B2%7D%7D%5Car%5Br%5D%20&amp;%20%5Ccirc%20%5Car%5Bdr%5D%5C%5C%0A%20%20%20&amp;%20%5Cboxed%7Bx_%7B3%7D%7D%5Car%5Brr%5D%20&amp;%20%20&amp;%20%5Ccirc%20%5Car%5Bdr%5D%5C%5C%0A%20%20%20v%5Car%5Buuur%5D%5Car%5Buur%5D%5Car%5Bur%5D%5Car%5Br%5D%5Car%5Bdr%5D%5Car%5Bddr%5D%5Car%5Bdddr%5D%20&amp;%20%20%5Cboxed%7Bx_%7B4%7D%7D%5Car%5Brru%5D%20&amp;%20%20&amp;%20%20&amp;%20%5Chat%7Bv%7D%5C%5C%0A%20%20%20&amp;%20%5Cboxed%7Bx_%7B5%7D%7D%5Car%5Br%5D%20&amp;%20%5Ccirc%20%5Car%5Burr%5D%5C%5C%0A%20%20%20&amp;%20%5Cboxed%7Bx_%7B6%7D%7D%5Car%5Bur%5D%5C%5C%0A%20%20%20&amp;%20%5Cboxed%7Bx_%7B7%7D%7D%5Car%5Buuurrr%5D%7D%0A"></p>
</div></div><p>The diagram shows a hierarchical model in which information is aggregated in stages, drawing on inputs that are not themselves directly accessible to the conscious brain.</p>
<p>The intermediate nodes can be thought of as automatic unconscious judgments – e.g.&nbsp;judgments about the distance of an object, the age of a person, or the value of a bottle of wine. The conscious brain only has access to those intermediate judgments, not the raw data, and it is this separation that can cause inconsistencies in judgment.</p>
<p><br><br><br></p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="https://latex.codecogs.com/png.latex?%5Cxymatrix@R-2pc%7B%0A%20%20%20&amp;%20%5Coverbrace%7B%7D%5E%7B%5Ctext%7Blow-level%7D%7D%5C%5C%0A%20%20%20&amp;%20%5Cboxed%7Bx_%7B1%7D%7D%5Car%5Bddr%5D%20&amp;%20%5C%5C%0A%20%20%20&amp;%20%5Cldots%20%5C%5C%0A%20%20%20&amp;%20%5Cboxed%7Bx_%7Bn%7D%7D%5Car%5Br%5D%20&amp;%20%20%5Chat%7Bv%7D%5E1%20%5Car%5Bdr%5D&amp;%20%5C%5C%0Av%5Car%5Buuur%5D%5Car%5Bur%5D%5Car%5Bdr%5D%5Car%5Bdddr%5D%0A%20%20%20&amp;%20%20&amp;%20%20&amp;%20%5Chat%7Bv%7D%5E2%20&amp;%20%5C%5C%0A%20%20%20&amp;%20%5Cboxed%7Bz_%7B1%7D%7D%5Car%5Burr%5D%20&amp;%20%5C%5C%0A%20%20%20&amp;%20%5Cldots%20%5C%5C%0A%20%20%20&amp;%20%5Cboxed%7Bz_%7Bn%7D%7D%5Car%5Buuurr%5D%5C%5C%0A%20%20%20&amp;%20%5Cunderbrace%7B%7D_%7B%5Ctext%7Bhigh-level%7D%7D%7D%0A"></p>
</div></div><p>For simplicity I will assume just one encapsulated node (<img src="https://latex.codecogs.com/png.latex?%5Chat%7Bv%7D_1">) and two types of cue: “low level” (available only to the encapsulated node), and “high level” (available only to the conscious brain, <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bv%7D_2">). A bias occurs when this sequential aggregation of information reaches a different conclusion than would occur if all information was pooled:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cutt%7BE%5Bv%7CE%5Bv%7C%5Cbm%7Bx%7D%5D,%5Cbm%7Bz%7D%5D%7D%7Bencapsulated%7D%7Binference%7D%20%5Cneq%0A%20%20%20%20%20%20%5Cutt%7BE%5Bv%7C%5Cbm%7Bx%7D,%5Cbm%7Bz%7D%5D%7D%7Bpooled%7D%7Binference%7D.%0A%20%20%20"></p>
<p>Roughly speaking a bias will occur only when 3 conditions are met:<sup>2</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;The model of doubly-encapsulated inference is formalized in my paper <a href="https://www.dropbox.com/s/guf8u1r1z5qoc6g/paper_heuristics.pdf">“Hierarchical Aggregation of Information and Decision-Making”</a>.</p></div></div><ol type="1">
<li>The encapsulated system receives information not expected by the conscious brain (<img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bx%7D%5Cneq%20E%5B%5Cbm%7Bx%7D%5D">),</li>
<li>The conscious brain receives information not expected by the early system (<img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bz%7D%5Cneq%20E%5B%5Cbm%7Bz%7D%5D">),</li>
<li>The two pieces of information interact, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?E%5Bv%7C%5Cbm%7Bx%7D,%5Cbm%7Bz%7D%5D"> is non-separable in <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bx%7D"> and <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bz%7D">.</li>
</ol>
<p><strong>What information is accessible to each stage?</strong></p>
<p>There are two types of information that are likely private to early processing. I will give discuss evidence below that this information is mostly inaccessible to the conscious brain.</p>
<ol type="1">
<li>Raw sensory experience - the intensity of stimulation on the retina, the skin, the ear canal.</li>
<li>Associations between outcomes – e.g.&nbsp;the correlations between different cues which are used to make inferences.</li>
</ol>
<p>The information private to the conscious brain is relatively abstract understanding of the situation.</p>
<p><strong>As a metaphor we can think of a person leading a bloodhound on a leash.</strong> Both the person and the dog are trying to find the same object, but the dog’s private information is not accessible to the person except through how they pull on the leash, i.e.&nbsp;the dog’s interpretation of the information. In typical cases the encapsulation of information does not cause problems: the person simply follows where the dog leads. However when the person is aware of additional context important in interpreting the bloodhound’s own information (i.e.&nbsp;there is an interaction) then the person will make mistakes relative to what they would have known if they’d been aware of all the dog’s information.</p>
<p><strong>The idea that judgment is encapsulated or modular is not new.</strong> It’s been argued by Helmholtz (1866), Pylyshyn (1980), Fodor (1983), Sloman (1996). However most discussion concentrates on the modules using a <em>subset</em> of the information available to the conscious brain, explaining biases in our automatic judgments, but not biases in conscious judgments. I put relatively more emphasis on the existence of information accessible only to the modules, which makes distinctive predictions about when reflective judgments will exhibit biases (I will call this “doubly encapsulated” processing of information).<sup>3</sup> <sup>4</sup> </p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;<img src="tecunningham.github.io/posts/images/brain_encapsulated.png" class="img-fluid"></p></div><div id="fn4"><p><sup>4</sup>&nbsp;<img src="tecunningham.github.io/posts/images/brain_dblencapsulated.png" class="img-fluid"></p></div></div><p><strong>Why would information be encapsulated?</strong> Mammalians brains have been evolving for 65M years while the human forebrain grew rapidly in the last 200,000 years. It seems likely that our systems for conscious reasoning are somewhat separated from systems for automatic judgment.</p>
<section id="implications-of-encapsulated-inference" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="implications-of-encapsulated-inference"><span class="header-section-number">4.1</span> Implications of Encapsulated Inference</h2>
<p>Here I givae the basic predictions of the model, in subsequent sections I discuss evidence relevant to these predictions from perception, judgment, and decision-making.</p>
<ol type="1">
<li><p><strong>Judgment will degrade when presentation is unusual.</strong> We have very accurate judgments about cases which we have experience with but judgment degrades when some high-level information becomes relevant to interpretation. E.g., judgment will degrade when the same information is available but presented in an unusual way – upside-down, back to front, inverted.</p></li>
<li><p><strong>Judgment will be sensitive to irrelevant features.</strong> Judgment will be sensitive to a cue which the subjects knows to be irrelevant when (a) that cue is <em>usually</em> relevant in similar situations; (b) the fact that the cue is irrelevant in this situation is high-level information, not accessible to the encapsulated system.</p></li>
<li><p><strong>Judgment of hypotheticals will be poor.</strong> We will have poor ability to state how our judgment would change if one of the cues changed. E.g.: would this drawing look more like your cousin if the nostrils were more flared? Would you judge this candidate the same way if they were a woman as if they were a man? These are objective questions about your own judgments, but if judgment is encapsulated then people may give inaccurate answers to these questions.</p></li>
<li><p><strong>Judgment will be sensitive to comparisons.</strong> When someone is judging two objects at the same time the conscious brain will have access to two encapsulated judgments, and therefore will learn something about what cues influences judgment. Thus we should expect to see systematic comparison effects which will reveal the nature of the information that is encapsulated. We also should see irrelevant-influences affect judgment in between-subjects, but not in within-subjects, experiments. </p></li>
<li><p><strong>Persistence of biased intuitions.</strong> Even when we are consciously aware that an encapsulated judgment is incorrect the encapsulated system will still produce that inference, i.e.&nbsp;people retain a subjective <em>perception</em> which they know to be false (e.g., even after you learn that an illusion is misleading it still looks that way).</p></li>
</ol>
</section>
</section>
<section id="perception" class="level1 page-columns page-full" data-number="5">
<h1 data-number="5"><span class="header-section-number">5</span> Perception</h1>
<section id="basic-facts" class="level2 page-columns page-full" data-number="5.1">
<h2 data-number="5.1" class="anchored" data-anchor-id="basic-facts"><span class="header-section-number">5.1</span> Basic facts</h2>
<p><strong>People are good judges of objects in the world.</strong><sup>5</sup> We are very good at judging the distance, size, or weight of an object, recognizing a face or a scene. Computers have only recently become comparable to humans after decades of dedicated work.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;<img src="tecunningham.github.io/posts/images/2021-04-06-05-36-55.png" class="img-fluid"> How various visual cues affect the judgment of distance.</p></div></div><p><strong>People are poor judges of low-level perceptual stimuli.</strong> When asked to judge the magnitude of a raw sensation people are generally poor judges: e.g.&nbsp;comparing the brightness of two lights, the shade of two colours, the length of two lines. For computers these tasks are trivial.</p>
<p><strong>There exist a large set of perceptual illusions.</strong> Psychologists have collected a diverse set of perceptual illusions in which people make predictable mistakes in reporting raw sensations (Muller-Lyer, Ebbinghaus, McGurk, etc.). There are still multiple active schools of thought in explaining most of these illusions.</p>
</section>
<section id="predictions-of-encapsulated-inference" class="level2 page-columns page-full" data-number="5.2">
<h2 data-number="5.2" class="anchored" data-anchor-id="predictions-of-encapsulated-inference"><span class="header-section-number">5.2</span> Predictions of Encapsulated Inference</h2>
<p>We can give a simple model of encapsulated inference:</p>
<ol type="1">
<li>Early perceptual processes infer real-world values (<img src="https://latex.codecogs.com/png.latex?v">) using information about raw sensations (<img src="https://latex.codecogs.com/png.latex?x">), and about associations between sensations and value.</li>
<li>Biases in judgment about the world occur because the early processes do not have access to high-level information available to the conscious brain (<img src="https://latex.codecogs.com/png.latex?z">).</li>
<li>Biases in judgment about raw stimuli occur because the conscious infers the value of stimuli from the outputs of the early processes <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bv%7D_1">.</li>
</ol>
<p>It is useful to make a distinction between raw sensations and their causes in the world. The causes can then be subdivided into value and noise, where “value” represents something of interest to the organism.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>sensation (<img src="https://latex.codecogs.com/png.latex?x">)</th>
<th>value (<img src="https://latex.codecogs.com/png.latex?v">)</th>
<th>noise (<img src="https://latex.codecogs.com/png.latex?e">)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>light on retina</td>
<td>reflectance of object</td>
<td>illumination</td>
</tr>
<tr class="even">
<td>size on retina</td>
<td>size of object</td>
<td>distance of object</td>
</tr>
<tr class="odd">
<td>pressure on hand</td>
<td>weight of object</td>
<td>sensitivity of hand</td>
</tr>
<tr class="even">
<td>oriention on retina</td>
<td>orientation in world</td>
<td>orientation of head</td>
</tr>
<tr class="odd">
<td>motion on retina</td>
<td>motion of object</td>
<td>motion of eye</td>
</tr>
</tbody>
</table>
<p>This gives us a series of predictions:</p>
<p><strong>Sensitivity to presentation.</strong> Peoples’ ability to judge and recognize stimuli are notably worse when the stimuli are presented in a way that we’re not accustomed to: e.g.&nbsp;upside down, inverted, tinted, or back to front.<sup>6</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;e.g.&nbsp;Valentine (1988), Galper (1970), Kemp et al (1990).</p></div></div><p><strong>Influence of high-level context.</strong> It’s useful to distinguish three different aspects of high-level context, and the predictions of double encapsulations:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Cfrac%7Bd%5Chat%7Bv%7D%5E1%7D%7Bdz%7D%20&amp;&amp;&amp;%20%5Ctext%7Bcontextual%20influences%20on%20automatic%20judgment%20of%20world%7D%5C%5C%0A%20%20%20%20%20%20%5Cfrac%7Bd%5Chat%7Bv%7D%5E2%7D%7Bdz%7D%20&amp;&amp;&amp;%20%5Ctext%7Bcontextual%20influences%20on%20conscious%20judgment%20of%20world%7D%5C%5C%0A%20%20%20%20%20%20%5Cfrac%7Bd%5Chat%7Bx%7D%5E2%7D%7Bdz%7D%20&amp;&amp;&amp;%20%5Ctext%7Bcontextual%20influences%20on%20conscious%20judgment%20of%20sensations%7D%0A%20%20%20%5Cend%7Baligned%7D%0A%20%20%20"></p>
<p>The double-encapsulation theory predicts that automatic judgments about the world are independent of contextual influences (<img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%5Chat%7Bv%7D%5E1%7D%7Bdz%7D=0">), while final judgments of both are dependent (<img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%5Chat%7Bv%7D%5E2%7D%7Bdz%7D%5Cneq0">, <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%5Chat%7Bx%7D%5E2%7D%7Bdz%7D%5Cneq%200">).</p>
<p>I believe that this is broadly consistent with the empirical evidence:</p>
<ol type="1">
<li>perceptual inferences are resistant to modulation by contextual information, except for attention effects,</li>
<li>considered judgments of both sensations and values are commonly influenced by contextual information.<sup>7</sup>.</li>
</ol>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;Firestone &amp; Scholl (2016, BBS)</p></div><div id="fn8"><p><sup>8</sup>&nbsp;Cross-modal influences are often been cited as evidence <em>against</em> the encapsulation of perception, because they show influences across perceptual areas. However the fact that they show systematic errors in assessment of sensations demonstrates the existence of vertical encapsulation (i.e.&nbsp;lack of direct access to raw stimuli), at the same time as they show that the limits of horizontal encapsulation.</p></div></div><p><strong>Judgment of raw sensations.</strong> Because the conscious brain does not directly observe the raw stimuli, it must infer them from the encapsulated system’s outputs: <img src="https://latex.codecogs.com/png.latex?E%5Chat%7Bx%7D_1=%5B%5Chat%7Bx%7D_1%7C%5Chat%7Bv%7D%5E1%5D"> and <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bx%7D_2=E%5B%5Chat%7Bx%7D_2%7C%5Chat%7Bv%7D%5E1%5D">. This predicts cross-modal effects in judgment of sensations, where one sensation influences estimate of another sensation:<sup>8</sup></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cunderbrace%7B%5Cfrac%7Bd%5Chat%7Bx%7D_1%7D%7Bdx_2%7D%7D_%7B%5Ctext%7Bcross-modal%7D%5Catop%5Ctext%7Beffect%7D%7D%0A%20%20%20%20%20%20%20%20%20%5Cpropto%20%5Cunderbrace%7Bcorr(x_1,v)%7D_%7B%5Ctext%7Bdiagnostic%20value%20of%20$x_1$%7D%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%5CBig/%20%5Cunderbrace%7Bcorr(x_2,v).%7D_%7B%5Ctext%7Bdiagnostic%20value%20of%20$x_2$%7D%7D%0A%20%20%20"></p>
<p>This is consistent with many laboratory examples showing “cross-modal” effects, where one stimuli (e.g.&nbsp;auditory) is influenced by some other stimuli (e.g.&nbsp;visual).</p>
<p>Examples fitting this pattern: (1) the McGurk and anti-McGurk effects; (2) the Stroop effect; (3) confusing motion of beeps and flashes; (4) confusing numerosity of beeps and flashes; (5) phoneme restoration; (6) assimilation to expectations - proofreaders’ errors; (7) Simon interference - response location &amp; stimulus location.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="tecunningham.github.io/posts/images/2024-04-15-18-43-00.png" class="img-fluid"> Contrast effects in shade, contrast, hue, and size. The central circle is identical in each case.</p>
</div></div><p><strong>Application: contrast and assimilation effects.</strong> A common finding in perception is a contrast effect, where some quality appears less-intense when placed next to a more-intense neighbor, although we also observe assimilation effects (the opposite) in some circumstances. I argue that (1) contrast effects in real-world perception are due to rational inference; (2) contrast effects in raw sensory stimuli are byproducts of double-encapsulation.</p>
<p>Suppose there are two neighboring objects, each with unobserved value (<img src="https://latex.codecogs.com/png.latex?v_1,v_2">) and additive noise:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cunderbrace%7B%5Cbinom%7Bx_1%7D%7Bx_2%7D%7D_%7Bobserved%5Catop%20signal%7D%0A%20%20%20%20%20%20=%5Cunderbrace%7B%5Cbinom%7Bv_1%7D%7Bv_2%7D%7D_%7Bunobserved%5Catop%20value%7D%0A%20%20%20%20%20%20%20%20%20+%5Cunderbrace%7B%5Cbinom%7Be_1%7D%7Be_2%7D%7D_%7Bunobserved%5Catop%20noise%7D.%0A%20%20%20"></p>
<p>If everything is normally distributed then we have the following: <img src="https://latex.codecogs.com/png.latex?%5Cunderbrace%7B%5Cfrac%7BdE%5Bv_%7B1%7D%7Cx_%7B1%7D,x_%7B2%7D%5D%7D%7Bdx_%7B2%7D%7D%7D_%7B%5Ctext%7Bassimilation/contrast%20effect%7D%7D%0A%20%20%20%20%20%20%5Cpropto%20%5Cunderbrace%7B%5Ctext%7Bcorr%7D(v_1,v_2)%7D_%7B%5Ctext%7Bcorrelation%7D%5Catop%5Ctext%7Bin%20values%7D%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20-%5Cunderbrace%7B%5Ctext%7Bcorr%7D(e_1,e_2)%7D_%7B%5Ctext%7Bcorrelation%7D%5Catop%5Ctext%7Bin%20noise%7D%7D.%0A%20%20%20"></p>
<p>In words this implies we expect an “assimilation” effect when the value is more correlated than the noise, and a “contrast” effect when the noise is more correlated than the value.</p>
<p>The equation above describes rational inferences of the encapsulated system about real-world values, but we additionally observe that contrast and assimilation effects occur in judgment of raw sensations, and for that reason they can properly be called “biases” or “illusions” relative to the full-information case. If we assume the conscious brain infers the sensations from the posteriors of the encapsulated system (<img src="https://latex.codecogs.com/png.latex?%5Chat%7Bx%7D_1%5Cpropto%20%5Chat%7Bv%7D_1%5E1">), then we get the same comparative static:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bd%5Chat%7Bx%7D_1%7D%7Bdx_%7B2%7D%7D%5Cpropto%20%5Ctext%7Bcorr%7D(v_1,v_2)-%5Ctext%7Bcorr%7D(e_1,e_2)."></p>
<p>I believe this gives a good account of the existence of contrast and assimilation effects across a range of cases:</p>
<ol type="1">
<li>Adelson (1993) gives a series of examples demonstrating these effects in lightness illusions: contrast effects occur when context implies that noise is more correlated than value, and assimilation effects occur when context indicates that value is more correlated than noise.<sup>9</sup></li>
<li>When judgment is affected by a dissimilar stimulus (cross-modal effects), contrast effects will tend to occur when the additional cue is associated with the stimulus, assimilation effects will occur when it is associated with the value.</li>
<li>These biases will tend to be smaller for more automatic responses - e.g.&nbsp;grasping responses - insofar as they receive signals prior to high-level processing.</li>
</ol>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;Some papers also find Bayesian assimilation with very noisy stimuli, which I think is also consistent, but need more argument.</p></div></div><p><strong>Reproduction.</strong> People find it easy to recognize patterns but hard to reproduce them, for example to paint a picture or to transcribe a melody. This is consistent with the model above insofar as the recognition is done pre-consciously. The model also predicts certain biases in reproduction, e.g.&nbsp;that people will tend to fail to account for shadows when they do a drawing, because they are inferring the raw stimuli <img src="https://latex.codecogs.com/png.latex?x"> from the inferred object <img src="https://latex.codecogs.com/png.latex?v">, and the brain’s inference has discarded most of the information it considers to be noise, such as illumination.</p>
<p><strong>Additional notes.</strong></p>
<ul>
<li><em>Differences in estimand.</em> We have assumed that the encapsulated system is estimating the same quantity that the conscious brain is trying to estimate, <img src="https://latex.codecogs.com/png.latex?v">. In many cases the conscious brain will want to estimate some other quantity, and this discrepancy will cause a distinct type of bias. I think this accounts for the size-weight bias in which people judge a larger object to be lighter, all else equal: if the encapsulated system is inferring the <em>density</em> of an object (from size and weight), and passing that estimate to the conscious brain, this would account for a negative effect of size on conscious estimates of weight.</li>
<li><em>Bimodal posteriors.</em> When there are two plausible but distinct interpretations of a given stimulus, i.e.&nbsp;when posteriors are bimodal, then people often alternate between perceiving the two interpretations, e.g.&nbsp;the Necker cube and the black/gold dress. It’s interesting that the cognitive system seems incapable of representing bimodal posteriors directly. </li>
</ul>
</section>
</section>
<section id="judgment" class="level1 page-columns page-full" data-number="6">
<h1 data-number="6"><span class="header-section-number">6</span> Judgment</h1>
<section id="basic-facts-1" class="level2 page-columns page-full" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="basic-facts-1"><span class="header-section-number">6.1</span> Basic Facts</h2>
<p><strong>There are a set of well-known judgment illusions.</strong> There are a large set of simple judgment questions which people give consistently wrong answers to.</p>
<ul>
<li><em>Logical judgment:</em> bat-and-ball, Linda, Monty Hall, mathematics problems on exames.</li>
<li><em>Factual judgment:</em> anchoring, joint-separate inconsistencies.</li>
</ul>
<p>There are many proposed explanations of these biases but there is still relatively little consensus.</p>
<p><strong>People are generally poor at making statistical generalizations about features.</strong> A variety of laboratory tests give people a set of cases and ask them to either (a) judge the strength of correlation between two features, or (b) predict an unobserved feature of a case given the observed features. People are often much worse than simple computer programs at performing these tasks.</p>
<p><strong>Judgment in some domains can be very accurate very accurate.</strong> People can learn extremely subtle judgments when playing chess and Go, such that computers have only been able to beat them relatively recently.<sup>10</sup> Other domains are harder to benchmark but it seems fair to say that judgment can be finely calibrated in domains such as interpersonal emotional judgment, medical diagnosis, music or literature.<sup>11</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;Playing a game could be seen as a combination of judgment and decision-making, but because every player shares essentially the same preferences (they prefer to win) it’s informative about judgment.</p></div><div id="fn11"><p><sup>11</sup>&nbsp;Kahneman and Klein (2009) say that human judgment tends to be good when subjects have (i) a lot of experience, and (ii) quick feedback.</p></div></div></section>
<section id="judgment-and-encapsulated-inference" class="level2 page-columns page-full" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="judgment-and-encapsulated-inference"><span class="header-section-number">6.2</span> Judgment and Encapsulated Inference</h2>
<p>The encapsulated inference model says that judgments are largely instinctual and we have limited introspection into what determines them, e.g.&nbsp;when we are guessing the price of a good, judging the likelihood of an event, judging the trustworthiness of a person, we rationally trust our instincts. This has a number of consequences.</p>
<p><strong>Limited introspection.</strong> We have limited ability to explain our judgments, e.g.&nbsp;to formalize them in a way that a computer could reproduce them.<sup>12</sup> The history of human sciences is consistent with this: e.g.&nbsp;linguists have worked for centuries on finding the rules which make a sentence grammatical, formalizing knowledge that we all posess intuitively.</p>
<div class="no-row-height column-margin column-container"><div id="fn12"><p><sup>12</sup>&nbsp;There is a long debate on what evidence would establish unconscious knowledge. Experimentalists have shown that subjects can learn a pattern but have trouble describing it. Critics have said (a) the subjects may have learned a different pattern but with the same extension (Quine), or (b) the questions asked of subjects are insufficiently detailed. In my view the problem is in the design of the experiments: implicit knowledge can be defined behaviorally, by observing when low-level and high-level information fail to be integrated.</p></div></div><p><strong>Hypothetical questions.</strong> The model predicts that people will be unable to accurately answer hypothetical questions about their own judgments, e.g.: Would you value this bottle of whisky the same if it had a different price on it? Would you like this house as much if it was a different colour?</p>
<p><strong>Influence of irrelevant cues.</strong> A common finding is that judgment is influenced by irrelevant cues, i.e.&nbsp;when told to ignore some fact people will still be moved by it. This occurs in the model when (1) the cue is integrated into preconscious judgments, (2) the conscious brain does not know how heavily the cue is weighted, and so cannot account for it.</p>
<p><strong>Effect of comparisons.</strong> When two judgments are being made at the same time – e.g., two items are being judged – then the conscious decision-maker will learn more about the unconscious knowledge. The patterns of comparison effects will reveal the nature of implicit knowledge.</p>
<p><strong>Internal consistency.</strong> We should expect that judgment anomalies will disappear in within-subject studies because the decision-maker becomes aware of the inconsistency and adjusts their judgments.</p>
<p><strong>Consistency of biases.</strong> A lot of academic literature tries to identify biases in judgment. In our analysis biases will be caused by correlations in the environment, and so we should expect them to vary or reverse from one environment to the next, and indeed we find that many biases reverse sign:</p>
<ul>
<li>the “contrast effect” vs the “assimilation effect”</li>
<li>the “gambler’s fallacy” vs the “hot hand fallacy”</li>
<li>the “recency effect” vs “confirmation bias”</li>
<li>“overweighting of low probabilities” vs “neglect of rare events”</li>
</ul>
<p><strong>Effect of incentives.</strong> We should see that incentives don’t materially affect biases, beyond the point where they simply get the person to pay attention. This is in contrast to inattention-based theories.</p>
<p><strong>Persistence of mistaken judgments.</strong> The encapsulated-inference theory predicts that after someone learns that a judgment is incorrect then the <em>feeling</em> will persist, because the encapsulated system operates independently of higher-level knowledge. Thus in many of the classic judgment illusions, even when you know the right answer the wrong answer still has some intuitive draw.<sup>13</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn13"><p><sup>13</sup>&nbsp;Sloman (1996) described this as Criterion S in arguing for two systems of reasoning: “a reasoning problem satisfies Criterion S if it causes people to simultaneously believe two contradictory responses.”</p></div></div></section>
<section id="implications-for-improving-judgments" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="implications-for-improving-judgments"><span class="header-section-number">6.3</span> Implications for Improving Judgments</h2>
<ol type="1">
<li><strong>Organize information in a way that helps your instincts to recognize patterns.</strong> For example (a) visualizing data so that visual-processing modules can be used to recognize patterns; (b) describe uncertainty in language that’s similar to the way that uncertainty is experienced: in terms of frequencies (“1 out of 5”) instead of probabilities (“20%”); (c) when looking for different interpretations of data, present it in an unfamiliar way in order to route around your unconscious inferences.</li>
<li><strong>Remove information that you know to be irrelevant.</strong> Your instincts will pick up on all available cues, even irrelevant ones. Thus attempting to ignore cues is likely to be ineffective, it’s better to physically remove irrelevant information.</li>
<li><strong>Ask yourself variations on the same question.</strong> – it will help to extract more information from your instincts.</li>
</ol>
</section>
</section>
<section id="decision-making-unfinished" class="level1 page-columns page-full" data-number="7">
<h1 data-number="7"><span class="header-section-number">7</span> Decision-Making [UNFINISHED]</h1>
<section id="background" class="level2 page-columns page-full" data-number="7.1">
<h2 data-number="7.1" class="anchored" data-anchor-id="background"><span class="header-section-number">7.1</span> Background</h2>
<p><strong>Motivating examples.</strong> Decision-making as a whole is an unwieldy topic, I’ll give some motivating examples focussed on economics:</p>
<ol type="1">
<li>Choosing which wine to buy for dinner.</li>
<li>Choosing how much of your income to spend vs save.</li>
<li>Choosing whether to move cities for a job.</li>
</ol>
<p><strong>We don’t know much about what influences decisions.</strong> There are two polar schools of thought: (1) that decisions maximize some objective outcomes; (2) that decisions are buffeted by all sorts of influences – context, custom, time of day. Concretely, given variation in choices (e.g.&nbsp;savings rate, hours worked, charitable giving, education), we can decompose into variation in (1) budgets, (2) consequentialist preferences, (3) other non-consequentialist influences. Perhaps 1/3 each.</p>
<p><strong>I will treat decision-making as judgment of value.</strong> i.e., people judge <img src="https://latex.codecogs.com/png.latex?E%5Bv%7Cx%5D">, and choice is just selecting the outcome with the highest expected-value. This allows me to use the same framework as used in perception and judgment.</p>
<p><strong>In decision-making there’s no objective standard.</strong> When discussing perception and judgment we can determine whether people are right or wrong, but it’s not the same with decision-making. Here we never observe the true value of things, we can only talk about inconsistencies among decisions, which itself requires making assumptions over what are reasonable preferences.</p>
<p><strong>We have limited insight into our goals.</strong> I think we have limited insight both into ordinary value judgments (why do I prefer this coffee-cup to that one?), and into overarching goals (why do I marry? have children?). Philosophy, after thousands of years, still has not given us clarity regarding our tradeoffs between different ends (pleasure, moral imperatives and religious imperatives, providing for others, receiving love &amp; receiving esteem).</p>
<p><strong>Evolution must play some deep role in decision-making.</strong><sup>14</sup> As with other animals, humans’ decision-making must have been shaped to favor reproduction. However I believe there remains relatively little consensus on exactly how it affects decision-making, outside some specific areas.</p>
<div class="no-row-height column-margin column-container"><div id="fn14"><p><sup>14</sup>&nbsp;I think there’s a common perception that although evolution must have importantly shaped human psychology, the evolutionary perspective has made few fundamental contributions to understanding psychology.</p></div><div id="fn15"><p><sup>15</sup>&nbsp;Some of this correlation can come from genetic effects or complementarity (when other people do X, then it’s in your interest to do X). However we see that when people migrate to a different culture in adulthood they retain a substantial part of their old preferences.</p></div></div><p><strong>We must absorb most of our preferences.</strong><sup>15</sup> A large share of variation in decisions must be due to the preferences that we absorb when we grow up: taste in music, religion, political beliefs, career, who we marry, how many children to have, where to live, all must be heavily influenced by early experiences.</p>
<p><strong>Preferences vs associations.</strong> Instead of estimating objective quantity, can estimate instrumental <em>value</em>, payoff, utility.</p>
<p><strong>There are a set of known decision biases.</strong> There are some decision problems in which peoples’ choices consistently violate norms of rationality: Allais paradoxes, Ellsberg paradoxes, small-stakes risk-aversion, relative-thinking paradoxes, time inconsistency, anchoring effects, etc. There have been many attempts to fit these anomalies into generalizations about decision-making but there has been little consensus (e.g.&nbsp;with prospect theory, inattention, relative thinking).</p>
</section>
<section id="perspective-from-encapsulated-inference" class="level2 page-columns page-full" data-number="7.2">
<h2 data-number="7.2" class="anchored" data-anchor-id="perspective-from-encapsulated-inference"><span class="header-section-number">7.2</span> Perspective from Encapsulated Inference</h2>
<p><strong>(1) Limited introspection.</strong> The model predicts that we have poor ability to introspect, e.g.&nbsp;to explain how different features influenced our decisions, or to predict what we would do in a different hypothetical situation.</p>
<p><strong>(2) Sensitivity to associations.</strong> The evaluation of an outcome should be sensitive to whatever details are informative about value, and we still respond to those details when we’re aware that they are uninformative in the current situation, insofar as that awareness is not available to the encapsulated system. Thus we expect that people may have well-calibrated judgment for usual situations, but make bad or inconsistent decisions in unusual situations. This is my interpretation of many laboratory biases such as framing, anchoring, etc..</p>
<p>The decision-theory community has developed sophisticated logic to model many decision anomalies (ambiguity aversion, relative thinking, probability weighting). In my opinion most of these anomalies are due to associations which are very sensitive to context, and formal modelling is relatively unfruitful. The imperatives of publication have, I think, caused too much effort to be put in this direction: if anomalies are context-specific then research is less publishable, for that reason academics keep trying to come up with general theories, beyond the point of plausibility.<sup>16</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn16"><p><sup>16</sup>&nbsp;Examples: (1) people avoid ambiguity not intrinsically, but because it’s associated with bad outcomes in certain classes of situation, and they seek out ambiguity in other situations; (2) people are influenced by the choice-set because it’s often informative about relative value, but the nature of the influence varies drastically between situations (<a href="../_posts/2016-04-30-relative-thinking.md">discussion here</a>).</p></div><div id="fn17"><p><sup>17</sup>&nbsp;More precisely: when stakes are high enough such that people no longer choose dominated alternatives, most of the between-person inconsistency in choices remains.</p></div></div><p><strong>(3) Consistency within decisions.</strong> The model says that violations of rationality are mostly <em>inadvertent</em>. We therefore expect decisions to be consistent <em>within</em> situations, though they may be inconsistent <em>between</em> situations. This is consistent with laboratory evidence showing that people show large framing effects, but rarely choose dominated options.<sup>17</sup></p>
<p><strong>(4) Implicit knowledge is revealed in comparisons.</strong> <sup>18</sup> When two objects are evaluated side by side, that reveals some of the unconscious information to the conscious brain, and so we should expect systematic comparison effects. In choice, we can show that characteristic intransitivities reveal implicit knowledge, e.g.&nbsp;the intransitive cycle to the right (a “figure 8”) reveals an explicit preference for male over female, but an implicit preference for male over female. Similarly if we observe that some bundle of attributes is evaluated more highly when the comparison bundle becomes more similar in some respect, this reveals implicit knowledge about the attributes (either a positive implicit association about a shared attribute, or a negative implicit association about a non-shared attribute). Jon and I formalize this logic in our paper “Implicit Preferences”.</p>
<div class="no-row-height column-margin column-container"><div id="fn18"><p><sup>18</sup>&nbsp;<img src="https://latex.codecogs.com/png.latex?%5Cxymatrix%7B%0A%20%20%20%20%20%20%20%20%20%5Cbinom%7B%5Ctext%7Bfemale%7D%7D%7B%5Ctext%7BMBA%7D%7D%5Car@%7B-%7D@/_.3pc/%5Bdr%5D%7C(.4)%7B%5Crsucc%7B135%7D%7D%0A%20%20%20%20%20%20%20%20%20&amp;%20%5Cbinom%7B%5Ctext%7Bfemale%7D%7D%7B%5Ctext%7BPhD%7D%7D%5Car@%7B-%7D@/%5E.3pc/%5Bdl%5D%7C(.4)%7B%5Crsucc%7B45%7D%7D%0A%20%20%20%20%20%20%20%20%20%5C%5C%20%5Cbinom%7B%5Ctext%7Bmale%7D%7D%7B%5Ctext%7BMBA%7D%7D%5Car@%7B-%7D@/%5E.3pc/%5Bu%5D%7C%7B%5Crsucc%7B270%7D%7D%0A%20%20%20%20%20%20%20%20%20&amp;%20%5Cbinom%7B%5Ctext%7Bmale%7D%7D%7B%5Ctext%7BPhD%7D%7D%5Car@%7B-%7D@/_.3pc/%5Bu%5D%7C%7B%5Crsucc%7B270%7D%7D%0A%20%20%20%7D"></p></div></div></section>
</section>
<section id="encapsulated-preferences" class="level1 page-columns page-full" data-number="8">
<h1 data-number="8"><span class="header-section-number">8</span> Encapsulated Preferences</h1>
<p><sup>19</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn19"><p><sup>19</sup>&nbsp;<img src="https://latex.codecogs.com/png.latex?%5Cxymatrix@R-2pc%7B%0A%20%20%20&amp;%20%5Coverbrace%7B%7D%5E%7B%5Ctext%7Blow-level%7D%7D%5C%5C%0A%20%20%20&amp;%20%5Cboxed%7Bx_%7B1%7D%7D%5Car%5Bddr%5D%20&amp;%20%5C%5C%0A%20%20%20&amp;%20%5Cldots%20%5C%5C%0A%20%20%20&amp;%20%5Cboxed%7Bx_%7Bn%7D%7D%5Car%5Br%5D%20&amp;%20%20%5Ccirc%20%5Car%5Bdr%5D&amp;%20%5C%5C%0Av%5Car%5Buuur%5D%5Car%5Bur%5D%5Car%5Bdr%5D%5Car%5Bdddr%5D%0A%20%20%20&amp;%20%20&amp;%20%20&amp;%20%5Chat%7Bv%7D%20&amp;%20%5C%5C%0A%20%20%20&amp;%20%5Cboxed%7Bz_%7B1%7D%7D%5Car%5Burr%5D%20&amp;%20%5C%5C%0A%20%20%20&amp;%20%5Cldots%20%5C%5C%0A%20%20%20&amp;%20%5Cboxed%7Bz_%7Bn%7D%7D%5Car%5Buuurr%5D%5C%5C%0A%20%20%20&amp;%20%5Cunderbrace%7B%7D_%7B%5Ctext%7Bhigh-level%7D%7D%7D"></p></div></div><p>Finally I introduce an additional model which I think is necessary to explain a lot of behaviours: <em>encapsulated preferences</em>, (as opposed to encapsulated information). As with the previous model I assume an informationally-encapsulated system, but now assume it has a direct effect on decision-making, e.g.&nbsp;as a simplification we could say the final decision is a weighted average of the output of the two systems:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Chat%7Bv%7D%20=%20%5Clambda%20%5Cunderbrace%7BE%5Bv%7C%5Cbm%7Bx%7D%5D%7D_%7B%5Ctext%7Blow-level%20judgment%7D%7D%0A%20%20%20%20%20%20+%20(1-%5Clambda)%20%5Cunderbrace%7BE%5Bv%7CE%5Bv%7C%5Cbm%7Bx%7D%5D,%5Cbm%7Bz%7D%5D%7D_%7B%5Ctext%7Bhigh-level%20judgment%7D%7D."></p>
<p>We could describe these low-level preferences as <em>hard-wired</em>, meaning that they intend to achieve the same value as high-level judgments (<img src="https://latex.codecogs.com/png.latex?v">), but they respond just to superficial stimuli, here <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bx%7D">. Put another way: we have an irreducible preference for characteristics which have historically been associated with value. In some cases that association is learned over the course of our life (learned preferences), in other cases it will be over evolutionary history (evolutionary preferences). This has a number of predictions:</p>
<ol type="1">
<li><p>Decisions fail to maximize value when the association between superficial stimuli (<img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bx%7D">) and value (<img src="https://latex.codecogs.com/png.latex?v">) is different from the historical association. This is a weaker condition than encapsulated inference, because here the high-level system cannot override the low-level inferences.</p></li>
<li><p>People will choose to avoid situations where low-level and high-level judgments conflict. This follows if the high-level system has the power to choose future situations: she’ll avoid situations in which she anticipates a divergence between the preference of the low-level and high-level system.</p></li>
</ol>
<p>This is broadly consistent with some features of human behaviour:</p>
<ol type="1">
<li><p><strong>We are tempted when in situations involving immediate benefits: food, sex, tiredness, addictive drugs.</strong> These can be interpreted as evolutionary fail-safes that exist to prevent conscious reasoning from doing too much harm. They are distinct from ordinary preferences in a utility function because they are only activated in certain contexts, and for this reason the conscious brain can work around them, just as an adult can work around a child by putting certain things out of view.</p></li>
<li><p><strong>We have an aversion to things that are <em>superficially</em> unsafe.</strong> We dislike walking on a glass bridge even if we know it’s safe, we dislike drinking from vials marked “poison” even when we know it’s not poison, we dislike the smell of rotten meat even if we know there’s no danger. This has the same pattern as the examples above, but reversed.</p></li>
<li><p><strong>Developmental pathologies are adaptive at an evolutionary timescale conditional on superficial information.</strong> There are certain diseases specific to civilization: myopia, allergies, obesity, crooked teeth. For each of these there are persuasive arguments that the developmental response is optimal given some superficial information about the situation, and so the pathology is caused by the divergence between evolutionary-scale associations and contemporary associations.</p></li>
<li><p><strong>Nervous-system responses are optimal conditional on superficial information.</strong> Many bodily reactions are not under conscious control: heart-rate, sneezing, fever, pain, headaches. We can over-ride automatic responses, e.g.&nbsp;by taking medications, but we often don’t know whether it would be wise because we don’t have access to the private information used by the automatic system, and it’s resaonable to presume that its responses are optimal.</p></li>
</ol>
<section id="additional-notes-on-encapsulated-preferences" class="level2" data-number="8.1">
<h2 data-number="8.1" class="anchored" data-anchor-id="additional-notes-on-encapsulated-preferences"><span class="header-section-number">8.1</span> Additional Notes on Encapsulated Preferences</h2>
<p><strong>Puzzle: distortion of conscious reasoning.</strong> In many cases situations which activate a visceral temptation also seems to affect our conscious thoughts: we don’t just give in to temptations, we also <em>rationalize</em> it. In the dog metaphor it’s like having a very persuasive bulldog who can talk you into things. You might still avoid the situation but once in the situation you choose whole-heartedly. It’s unclear why we would reason in this way.</p>
<p><strong>Evolved desires vs encapsulated desires.</strong> Should be careful not to confuse encapsulated preferences with a different relation, where evolution gives us a goal which we consciously strive for (eat sweet things, have sex) which are only proximal relative to the evolutionary goal of reproducing. Those proximal goals could still be ipmlemented in a single system which would make entirely consistent maximizing choices (but it would be maximizing an outcome that is only correlated with evolution’s goal).</p>
<p><strong>Characteristic behaviors which reveal encapsulated preferences:</strong></p>
<ol type="1">
<li><p><strong>Preferences are stronger when made more salient.</strong> E.g., (1) more likely to choose a croissant when you smell it; (2) you’re scared by certain things only if you can see them, when giving blood you avoid looking at the needle; (3) you’re more likely to agree to have sex if you’re aroused. Just thinking about certain things can make us aroused or happy or scared, so we can strategically choose what to think about.</p></li>
<li><p><strong>Making personal rules.</strong> We set a certain time to start &amp; finish working; set a goal number of words to write each day; only getting ice-cream if you’ve been to the gym; setting a rule for how much money to save. Implies you don’t trust your future self, so you make up a rule, even though there’s no way to enforce it.</p></li>
<li><p><strong>Strategic choices.</strong> We choose to avoid certain options. E.g., (1) buying a house to force yourself to save; (2) avoiding situations where you think you’ll be tempted. Formally you can call this “choice over choice sets”, and there’s a a fair amount of decision-theory on how multiple-selves can be identified if you can observe this kind of choice.</p></li>
<li><p><strong>Effect of distraction.</strong> You are more likely to indulge in proximal goals when you are distracted - e.g.&nbsp;manipulation of cognitive load.</p></li>
<li><p><strong>implicit preferences.</strong> More influenced by an attribute in less-direct choice sets.</p></li>
</ol>
<p>Other variation in decision by context: Decisions made with salient outcomes, or abstract outcomes; decisions made under time pressure; decisions made with cognitive load; decisions made in advance; decisions over future choice sets; choices made in different moods, or after exerting willpower; decisions made directly or indirectly; from small or large choice sets; and attitudes which are expressed by involuntary responses (response time, skin conductance, pupil dilation).</p>
</section>
<section id="why-would-we-have-encapsulated-preferences" class="level2" data-number="8.2">
<h2 data-number="8.2" class="anchored" data-anchor-id="why-would-we-have-encapsulated-preferences"><span class="header-section-number">8.2</span> Why would we have encapsulated preferences</h2>
<p>Two types: (a) evolutionary proximal goals (sugar, sex, putrid smells); (b) learned proximal goals (coffee, wine, tobacco, learned food aversions).</p>
<p><strong>Proximal preferences from evolution.</strong> A lot of human preferences make sense as proximal goals for achieving a long-run evolutionary purpose: when we get a kick out of doing X, it’s because doing X was selected for in our evolutionary history. These goals are in some sense hard-wired because people still do X even when it no longer makes any evolutionary sense to do it: (1) we enjoy sex even when it doesn’t lead to reproduction; (2) we like the taste of sugar even when we know it’s bad for us; (3) we get scared of snakes, spiders, of heights, of blood even when we see them behind glass - we are even scared of <em>pictures</em> of these things; (4) we are disgusted by smells associated with infection, even when we know there’s no risk of infection. (There are also arguments that some emotions - such as anger, jealousy, love - are hardwired for a slightly different reason - because they serve as commitment devices in social interaction - Trivers).</p>
<p>Why would these be hardwired? Or rather, instead of hardwiring <em>preferences</em>, why didn’t evolution hardwire <em>beliefs</em>? It would seem to be more efficient to just have innate knowledge: that snakes are dangerous, that sex leads to reproduction, that rotting meat is an infection risk. Then we could treat each of these things as means, not as ends. I guess there are two reasons: (1) perhaps it’s harder to preinstall knowledge than to preinstall preferences, because each brain grows differently; (2) perhaps it’s safer to preinstall preferences, to prevent them being overridden by a malfunctioning conscious brain which thinks it knows better.</p>
<p><strong>Proximal preferences from learning.</strong> We often can explain peoples’ preference for X, because X has been associated with good outcomes in the past, yet people still choose X even when they are aware it no longer has those good associations. I.e., X has turned from a means into an end in itself. This can be described as a “habit”, “learned preference.” Some examples: (1) enjoying the smell of coffee, the taste of wine, or a cigarette, when those associations must be due principally to the psychoactive drugs that have been associated with them; (2) a learned phobia; (3) a learned food aversion, even when you know it’s irrelevant (e.g.&nbsp;chemotherapy patients develop aversions to whatever food they were eating during the chemotherapy).</p>
<p>It’s hard to answer why <em>learned</em> preferences should be hard-wired.</p>
<p>Different models which get at aspects of this type of behaviour: (1) a complementarity between a cue and consumption: the consumption becomes more valuable when exposed to an associated cue (Laibson); (2) a temptation cost - you have to pay a cost to not consume something when it is possible to choose it – (Gul and Pesendorfer) (3) preferences which depend on your state of arousal - e.g.&nbsp;willingness to be violent, have sex, changes preditably with context (Loewenstein on arousal).</p>
</section>
</section>
<section id="notes-on-literature" class="level1 page-columns page-full" data-number="9">
<h1 data-number="9"><span class="header-section-number">9</span> Notes on Literature</h1>
<p><strong>Unconscious knowledge.</strong> I wrote a discussion of unconscious knowledge <a href="../_posts/2017-12-10-unconscious-influences.md">here</a>. In short: there have been many proposals of how to identify unconscious knowledge or unconscious preferences over the last 150 years, but there remains little consensus.</p>
<p><strong>Modularity / top-down influences.</strong> In the 1950s the “new look” school in psychology argued that peoples’ perceptions are distorted by their beliefs and expectations. There was then a reaction in the 80s and 90s: Fodor &amp; Pylyshyn argued that perception is encapsulated, separated from expectations, &amp; you need this to explain illusions. Both are right, and in fact two types of illusions from different types of private info. System 1’s private info causes theory-biased report of sensation. System 2’s private info causes expectations-ignorant perceptions.</p>
<p><strong>Efficient coding, predictive coding, rational inattention.</strong> A general class of theories assumes that there is some kind of noise in the processing of information, so that the conscious brain makes inferences using a noisy signal about the stimuli <img src="https://latex.codecogs.com/png.latex?x">. These models, like my model, assume an intermediary process, but one which does compression rather than inference, i.e.&nbsp;the signal minimizes error with respect to <img src="https://latex.codecogs.com/png.latex?x">, instead of <img src="https://latex.codecogs.com/png.latex?v">. I have further discussion elsewhere, but in short I think (1) efficient-coding theories do a poor job accounting for features of perception; (2) rational-inattention models do a poor job accounting for gross facts of judgment; and (3) it would be an inefficient design for the brain to pass compressed versions of raw stimuli (x), instead of inferences from those stimuli (v).<sup>20</sup></p>


<div class="no-row-height column-margin column-container"><div id="fn20"><p><sup>20</sup>&nbsp;I need to think more about the relationship with “predictive coding” theories (where only the delta from the expectation is sent). Friston’s “free energy” theory is related to these theories but I haven’t been able to properly understand it.</p></div></div></section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{cunningham2024,
  author = {Cunningham, Tom},
  title = {Bloodhounds and {Bulldogs}},
  date = {2024-04-27},
  url = {tecunningham.github.io/posts/2023-10-24-manifesto-perception-judgment-decision-making.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-cunningham2024" class="csl-entry quarto-appendix-citeas">
Cunningham, Tom. 2024. <span>“Bloodhounds and Bulldogs.”</span> April
27, 2024. <a href="https://tecunningham.github.io/posts/2023-10-24-manifesto-perception-judgment-decision-making.html">tecunningham.github.io/posts/2023-10-24-manifesto-perception-judgment-decision-making.html</a>.
</div></div></section></div> ]]></description>
  <guid>tecunningham.github.io/posts/2023-10-24-manifesto-perception-judgment-decision-making.html</guid>
  <pubDate>Sat, 27 Apr 2024 07:00:00 GMT</pubDate>
</item>
<item>
  <title>The Influence of AI on Content Moderation and Communication</title>
  <dc:creator>Tom Cunningham</dc:creator>
  <link>tecunningham.github.io/posts/2023-06-06-effect-of-ai-on-communication.html</link>
  <description><![CDATA[ 





<style>
    h1 {  border-bottom: 4px solid black;}
    h2 {  border-bottom: 1px solid gray; padding-bottom: 0px; font-size: 1.3em; color: black; }
</style>

<div class="no-row-height column-margin column-container"><div class="">
<p> <img src="tecunningham.github.io/posts/images/2023-07-07-10-34-40.png" class="img-fluid"> Thanks to many comments, esp.&nbsp;Ravi Iyer, Sahar Massachi, Tal Yarkoni, Rafael Burde, Grady Ward, Ines Moreno de Barreda, and Daniel Quigley.</p>
</div></div><section id="summary" class="level1">
<h1>Summary</h1>
<p><strong>It is difficult to anticipate the effect of AI on online communication.</strong> AI models have already had big effects on content moderation but they are now starting to have effects on content production, e.g.&nbsp;through generating synthetic spam and deepfakes. How is the balance likely to play out?</p>
<p><strong>This note discusses the likely effect of AI on online communication, summarized with a set of predictions:</strong></p>
<ol type="1">
<li><strong>The prevalence of policy-violating content on platforms will decline.</strong> Classifiers are approaching human-level performance and so the prevalence of policy-violating content (e.g.&nbsp;nudity, hate speech) should decline to very low rates. The same argument applies to censorship: governments will be able to near-perfectly identify messages that include a forbidden sentiment.</li>
<li><strong>The prevalence of “context-specific” violating content will increase.</strong> This refers to content which appears to be non-violating to the average person but is interpreted in a policy-violating way by its intended audience.</li>
<li><strong>The prevalence of “known violating” content will decrease.</strong> Content that is a match against databases of illegal sexual media (PhotoDNA), IP-protected content (ContentID), or terrorist recruitment content (GIFCT) will become less frequent because AI will make obfuscation harder (i.e.&nbsp;relatively benefit the platform over the obfuscator).</li>
<li><strong>Platforms will not be able to identify bots from their behavior.</strong> Behavioral tests like CAPTCHAs will become ineffective against sophisticated actors and so platforms will have to rely relatively more on direct proofs of identity.</li>
<li><strong>People will not be able to discriminate between real and fake media based only on the content.</strong> Since the invention of audio and visual recording in the 19th century the existence of a recording has been evidence for the event depicted having occurred. The evidentiary value of media will become much less strong because it will be impossible for a human to discriminate between real and fake.</li>
<li><strong>Platforms will not be able to discriminate between real and fake media based only on the content.</strong> It seems unlikely that AI classifiers will be able to reliably detect AI-manipulated media.</li>
<li><strong>Fake media will not have a substantial influence on politics.</strong> It has always been possible to forge documents but the influence of forgeries on politics has been limited. Documents have always been evaluted not just on their content but their provenance. Advances in the ability to forge media seem likely to be accompanied by advances in skepticism and greater reliance on provenance.</li>
<li><strong>Computer-generated content will not be hyper-persuasive.</strong> LLMs with their current architecture are unlikely to produce text that is more persuasive than a skilled human could produce. Social scientists have warned about “hyper persuasive” technologies for a century and been wrong every time.</li>
<li><strong>Communication will migrate further towards large closed platforms.</strong> The rise of synthetic content will make it harder for individuals to discriminate between reliable and unreliable information sources, and so will likely to increase the demand for intermediaries, which implies greater demand for verification and closed platforms.</li>
<li><strong>Things will get weird.</strong> Our intuitions about meaning and interpretation are based on experience with human-created representations. When computers can generate representations then strange things are likely to happen.</li>
</ol>
<p><strong>A useful distinction: internal properties vs external properties.</strong> A common thread in this argument is a distinction between two types of property:</p>
<ul>
<li>An “internal” property of a message is a function solely of the content, e.g.&nbsp;whether an image contains nudity, whether text contains hate speech, whether a joke is funny. These properties hold independent of any outside facts. AI classifiers are rapidly approaching human-level accuracy for these properties and this means that platforms (and governments) will be able to near-perfectly filter by internal properties even if content-producers have access to the same technology.</li>
<li>An “external” property of a message depends on some fact outside the message’s content: e.g.&nbsp;whether an image was computer-generated, whether a claim is true, whether a message came from a specific person. Platforms will get better at predicting external properties but they will be outpaced by motivated actors who can manipulate fakes until they become indistinguishable from genuine articles, and able to manipulate lies so they’re indistinguishable from the truth.</li>
</ul>
<p>This note additionally has short appendices on the history of forged documents and on relevant forecasting questions from Metaculus.</p>
<p><strong>What I don’t talk about.</strong> Two important topics that I do not discuss here: (1) super-intelligent AI and alignment; (2) social and cultural biases in AI.</p>
</section>
<section id="predictions" class="level1 page-columns page-full">
<h1>Predictions</h1>
<p><strong>Background on AI.</strong> Before discussing the predictions I note a couple of facts about recent progress in computer abilities:</p>
<ol type="1">
<li>Computers surpassed human abililties at mathematical calculations in the mid 20th century.</li>
<li>Over the last 50 years computers have been steadily getting better at recognizing text and images, with particularly rapid progress since around 2012. Today computers can match the best-performing humans on many classification tasks.</li>
<li>Since around 2018 computers have additionally become good at <em>synthesizing</em> media, e.g.&nbsp;creating text, images, or video, to match a description.</li>
</ol>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="tecunningham.github.io/posts/images/2023-09-19-09-55-57.png" class="img-fluid"></p>
<p>The graph above from <span class="citation" data-cites="kiela2023plottingprogress">Kiela et al. (2023)</span> shows the very recent history of computer performance: AI systems are regularly achieving human performance on benchmark tasks within a year of them being introduced, and so new benchmarks are being introduced more frequently.</p>
</div></div><section id="the-prevalence-of-policy-violating-content-will-decline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-prevalence-of-policy-violating-content-will-decline">The Prevalence of Policy-Violating Content Will Decline</h2>
<p><strong>All large internet platforms use automated systems to detect policy-violating content.</strong> All major platforms ban or suppress various types of content, e.g.&nbsp;hate speech, incitement to violence, nudity, graphic content. It has not been practical to have a human review each message because the platforms have a high volume of messages being sent with low latency. However automated systems have always been used: early systems simply checked for the appearance of prohibited words or matched against media databases, later systems used classifiers trained on human labels. See a brief history of automated content moderation <a href="https://tecunningham.github.io/posts/2023-11-18-history-automated-text-moderation.html">here</a>.</p>
<p><strong>Simple classifiers have high offline accuracy.</strong> Simple classifiers which just look for the appearance of specific words are often useful, e.g.&nbsp;certain words and phrases are highly predictive of whether text would be labelled as “toxic” or “hate speech.” However this method has many false positives (<span class="citation" data-cites="chen2022profanity">Chen (2022)</span>) and false negatives (<span class="citation" data-cites="heiner2022toxic">Heiner (2022)</span>).</p>
<p><strong>Simple classifiers are easily evaded.</strong> It is typically easy to alter a violating message such that humans still think it is violating but the classifier does not. As a consequence the accuracy of these classifiers looks much higher offline than online, as users take steps to evade them.</p>
<ul>
<li><span class="citation" data-cites="grondahl2018need">Gröndahl et al. (2018)</span> note that hate speech detectors can easily be fooled if you “insert typos, change word boundaries or add innocuous words.”</li>
<li><span class="citation" data-cites="han2020fortifying">Han and Tsvetkov (2020)</span> note that simple models are poor at detecting “veiled toxicity” which they define as including “codewords, novel forms of offense, and subtle and often unintentional manifestations of social bias such as microaggressions and condescension.”</li>
<li><span class="citation" data-cites="lees2021capturing">A. Lees et al. (2021)</span> note that simple models are poor at detecting “covert toxicity” which includes “types of toxicity that may not be immediately obvious. Covertly toxic comments may use obfuscation, code words, suggestive emojis, dark humor, or sarcasm …[and] [m]icroaggressions.” These papers evaluate models trained to identify <em>context-independent</em> toxicity, i.e.&nbsp;where the ground truth is human rating of the text alone without additional information on context or audience.</li>
</ul>
<p><strong>LLM-based classifiers are approaching human levels of performance.</strong> In August 2023 OpenAI described using GPT-4 as a content labeler (<span class="citation" data-cites="weng2023gpt4moderation">Weng, Goel, and Vallone (2023)</span>) and said “[l]abeling quality by GPT-4 is similar to human moderators with light training … [h]owever, both are still overperformed by experienced, well-trained human moderators.”</p>
<p><strong>LLM-based classifiers handle adversarial cases well.</strong> Google’s 2022 generation of text moderation models, which use transformer-based LLMs, are able to correctly classify many types of adversarial messages which are designed to evade simpler classifiers. <span class="citation" data-cites="whitlocklees2022perspective">A. W. Lees et al. (2022)</span> say their classifier performs well against “code-switching, covert toxicity, emoji-based hate, human-readable obfuscation, [and] distribution shift.” Google’s 2023 generation spam classifier uses an embedding that is “robust against typos and character-level adversarial attacks” (<span class="citation" data-cites="bursztein2023retvec">Bursztein et al. (2023)</span>).<sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Arnaud Norman <a href="https://bulkninja.notion.site/Email-Obfuscation-Rendered-almost-Ineffective-Against-ChatGPT-728fba1b948d42c6b8dfa73cb64984e4">writes about</a> how algorithms to scrape email addresses are often easy to evade, by adding special characters or other obfuscations, but that ChatGPT can straight-forwardly decode most such obfuscations.</p></div></div><p><strong>Better classifiers will lower prevalence even if they are available to adversaries.</strong> Suppose an adversarial content-producer had access to the same classifier that was used by the platform. The produced could keep testing different variants of a violating post until they found a variant that was truly violating, but not identified as violating by the classifier, i.e.&nbsp;a false negative. However as the platform’s model becomes more accurate there will be fewer possible false positives, and so the task becomes relatively more time-consuming for the adversary, and thus we should expect prevalence to decline.</p>
<p><strong>The prevalence of policy-violating content has declined dramatically.</strong> Meta reports that the prevalence of nudity, bullying, hate speech, and graphic content each declined by a factor of between 2 and 5 between 2017 and 2022, and that the share of identified-violating content that was first identified by an ML model (“proactive rate”) is approaching 100% for most categories. I think much of this decline can be attributed to improvements in the quality of classifiers.<sup>2</sup> Mark Zuckerberg has been making predictions for a long time that human raters could be substituted with AI. Although he was over-optimistic about the pace, I think he has been largely correct, e.g.&nbsp;in late 2018 he said “through the end of 2019, we expect to have trained our systems to proactively detect the vast majority of problematic content.”<sup>3</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;It is important to remember that the “proactive rate” is the share of <em>detected</em> content that is detected by AI, the share of <em>violating</em> content that is detected by AI will certainly be significantly lower but is not generally reported. See Meta’s <a href="https://transparency.fb.com/data/community-standards-enforcement/">Community Standards report</a> and <a href="http://tecunningham.github.io/2023-01-31-social-media-suspensions-data.html#meta-facebook-instagram">my visualization of this data</a>.</p></div><div id="fn3"><p><sup>3</sup>&nbsp;Zuckerberg, <a href="https://www.facebook.com/notes/751449002072082/">“A Blueprint for Content Governance and Enforcement”</a></p></div></div><p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-28-1.png" class="img-fluid"></p>
<p><strong>Employment of human moderators will likely decline.</strong> As computer accuracy improves fewer messages will need to be escalated for human review, additionally fewer humans will be needed to label training data.</p>
<p><strong>This prediction also applies to government monitoring and censorship.</strong> Many governments use some kind of automated scanning tools to intercept or censor messages based on their content, e.g.&nbsp;the US’s NSA and Cybserspace Administration of China. Better AI will allow these agencies to classify every post with reliability as high as if they had a human read each one, thus we should expect obfuscation will become a much less-effective workaround for censorship.</p>
<p><strong>This prediction would fail if there were hard limits on the performance of AI.</strong> It’s conceivable that there are ways of obfuscating content that will remain difficult for an AI to identify for a long time. However even if LLMs cannot identify violating content in real-time it seems likely they could catch up quickly. Suppose humans invent new types of obfuscation, e.g.&nbsp;misspelling words in a particular way. An LLM which is continually trained on human-labeled samples could likely learn the pattern and thus force humans to continually adopt new patterns.</p>
<p><strong>Prevalence will never decline to exactly zero because it’s inherently noisy.</strong> An AI model can never perfectly predict human-rater evaluation because humans are themselves noisy: there is both between-rater variation and within-rater variation in labelling for any given piece of content. Thus if the ground truth is human judgment then even an infallible classifier could not be used to drive prevalence all the way to zero.<sup>4</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;Strictly speaking: this will be true if no content has a probability of being labelled as positive by a human of exactly zero.</p></div></div></section>
<section id="the-prevalence-of-context-specific-violations-will-increase" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-prevalence-of-context-specific-violations-will-increase">The Prevalence of <em>Context-Specific</em> Violations Will Increase</h2>
<p><strong>Some messages have a violating significance only to their intended audience.</strong> We can define a message as violating in one of two ways: (1) has a violating significance to the average person (average citizen or average user), or (2) has a violating significance to the intended audience of that message. I will define a “contextual violation” as a message that is violating to its intended audience but not to the average person. This is stronger than just having a double meaning where both meanings are clear to all audiences. I am specifically talking about messages which are interpreted in distinct ways by different audiences. Of course contextual violations are often unstable, over time the average person will often learn the contextual meaning.</p>
<p><strong>Many messages use contextual violations.</strong><sup>5</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;A related phenomena is people using selective truths to give an impression that is false. E.g. it is common for anti-vaccination groups to post mainly true claims, but only those claims which reflect badly on vaccines. People with a bias against some ethnic group likewise often refrain from posting provably false claims but post only those true claims that reflect badly on the disliked group. Because the pool of claims that are true is enormous it is easy to collect a large set of true claims that collectively give a false impression.</p></div></div><ul>
<li>Saying “globalist” when your audience understands it to mean “jewish”</li>
<li>Saying the opposite of what is meant, e.g.&nbsp;a bigot saying excessively positive things about an ethnic group, or a pro-anorexia poster making anti-anorexic statements sarcastically.</li>
<li>Using euphemisms for illegal substances or illegal acts.</li>
<li>Using emojis of eggplants and peaches with sexual connotations.</li>
<li>Using photos without explicit nudity but which can be read as pornographic.</li>
</ul>
<p><strong>Improved detection of violations is likely to cause substitution towards contextual violations.</strong> As AI improves the ability to detect violations it seems likely that there will be at least some substitution towards context-specific violations, however as long as there is some cost to using a contextual-violation then we would expect a less than one-for-one substitution.</p>
<p><strong>Platforms could detect contextual violations if they wanted to.</strong> When doing human evaluation then platforms could either (1) provide human raters with detail about the message’s context and audience, or (2) assign human raters to messages based on their experience with that community.<sup>6</sup> Likewise AI models could be trained to include rich representation of the context. An additional advantage of adding context is that it can identify and exempt posts that violate the letter but not the spirit of the policy.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;Platforms already have some policies that include context, e.g.&nbsp;Facebook’s <a href="https://transparency.fb.com/policies/community-standards/bullying-harassment/">“Bullying and Harassment policy”</a> bans “repeatedly contacting someone in a manner that is unwanted or sexually harassing.”</p></div></div><p><strong>Platforms may not want to remove contextual violations.</strong> There are reasons why platforms may be reluctant to use context in determining violations: it is more complex, and can lead to awkward PR where the platform is shown to be censoring words and images have a harmless interpretation. Additionally platforms may care more about being seen to restrict harmful content than about the actual harm prevented.</p>
<p><strong>Contextual violations have long existed in broadcast media.</strong> There have been many cases where contextual violations have been tolerated: e.g.&nbsp;newspapers would allow classified advertisments for prostitutes if described as masseuses, vibrators if described as massage wands, contraception if described as marital aids, and abortion if described as <a href="https://slate.com/human-interest/2014/08/history-of-contraception-19th-century-classified-ads-for-abortifacients-and-contraceptives.html">“removal of obstructions”</a>. Thus it seems plausible that platforms will tolerate a substantial amount of contextually-violating content to remain.<sup>7</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;In Facebook’s Marketplace it is prohibited to list guns for sale. As a consequence people began to list gun <em>cases</em>, with the understanding that a case was standing in for a gun. Facebook then updated their policy to prohibit selling gun cases. In turn people began to list gun stickers as stand-ins for guns. See WSJ reports from <a href="https://www.wsj.com/articles/gun-sellers-are-sneaking-onto-facebooks-booming-secondhand-marketplace-11566315198">2020</a> and <a href="https://www.wsj.com/articles/gun-sellers-use-new-tactic-to-deal-on-facebook-marketplace-11598270872">2021</a>.</p></div><div id="fn8"><p><sup>8</sup>&nbsp;https://www.axios.com/2022/11/28/china-protests-blank-paper-covid</p></div></div><p><strong>Government censorship is unlikely to be constrained by context-specific violations.</strong> Once a censor discovers that a term has an anti-government significance in a certain context then they are likely to start censoring that term. E.g. China has suppressed online mentions of <a href="https://www.bbc.com/news/blogs-china-blog-40627855">Winnie the Pooh</a> because it is associated with criticism of Xi Jinping, and in 2022 Hong Kong police arrested protestors for holding blank pieces of paper.<sup>8</sup></p>
</section>
<section id="the-prevalence-of-variants-of-known-violating-content-will-decline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="the-prevalence-of-variants-of-known-violating-content-will-decline">The Prevalence of Variants of Known-Violating Content Will Decline</h2>
<p><strong>Platforms typically check content against databases of known-violating content.</strong> In addition to running classifiers on content platforms also check content against databases of known-violating content. The databases are often shared across platforms, known as “signal sharing”, e.g.&nbsp;databases of illegal sexual media (PhotoDNA), IP-protected content (Content ID), or terrorist recruitment content (GIFCT).<sup>9</sup> As a consequence sophisticated uploaders often obfuscate their content, e.g.&nbsp;by adding noise, and platforms expand their matching algorithms using fuzzy matching.</p>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;Other signal sharing programs: National Center for Missing &amp; Exploited Children Child Sexual Abuse Material (NCMEC-CSAM), Non-consensual Intimiate Imagery (StopNCII), ThreatExchange.</p></div></div><p><strong><em>Gratuitous</em> violations with known-violating content will go to zero.</strong> Suppose someone wants to violate the policy just for the sake of violating that policy, e.g.&nbsp;they want to show a shocking image. Call this “gratuitous” violations. Currently the easiest way to do this is to first find a violating piece of content, then obfuscate it. If attackers can use AI synthesis they no longer need to find existing violating content, they can synthesize new ones. The defensive technique of checking against known-violating content becomes much worse. However if defenders have AI recognition, then by the same argument as above prevalence will go to zero.</p>
<p></p>
</section>
<section id="platforms-will-not-be-able-to-identify-bots-from-their-behavior" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="platforms-will-not-be-able-to-identify-bots-from-their-behavior">Platforms Will Not Be Able to Identify Bots from Their Behavior</h2>
<p><strong>Most online platforms struggle with automated users (bots) who are disruptive in a variety of ways.</strong> One way of protecting against bots is with behavioral tests, e.g.&nbsp;a CAPTCHA test asking users an image-recognition task<sup>10</sup>, or by using on-platform behavior to detect whether a user is human. However improvements in AI mean that computers have human-level performance on image-recognition tasks, and can learn to imitate human-style behavior patterns, thus it seems likely these behavioral tests will become ineffective against sophisticated actors. <span class="citation" data-cites="searles2023empirical">Searles et al. (2023)</span> finds that most contemporary CAPTCHAs can be solved by computers with higher-than-human accuracy (p10).<sup>11</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;CAPTCHA stands for Completely Automated Public Turing test to tell Computers and Humans Apart.</p></div><div id="fn11"><p><sup>11</sup>&nbsp;Similarly behavioural fingerprinting will become ineffective against advanced actors, e.g.&nbsp;using voice recognition to verify identity.</p></div><div id="fn12"><p><sup>12</sup>&nbsp;The 3rd-party identity providers will themselves have to rely on some other ground truth when accepting signups.</p></div></div><p><strong>This does not imply that the prevalence of bots will increase.</strong> All platforms need some defense against bots so they will have to rely relatively more on other forms of authentication, such as monetary payment, offline identity credentials (government ID, credit card number), hard-to-fake metadata (unique IP address, device ID), or 3rd-party identity provider (Sign in with Google, OpenID).<sup>12</sup> Thus the barriers to signing up for a service, and especially posting on it, will become higher, but the effect on equilibrium prevalence of bots is ambiguous.</p>
</section>
<section id="platforms-will-find-it-hard-to-discriminate-between-real-and-fake-media" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="platforms-will-find-it-hard-to-discriminate-between-real-and-fake-media">Platforms Will Find It Hard to Discriminate between Real and Fake Media</h2>
<p><strong>In some cases the ground truth depends on properties outside the content.</strong> I will refer to these properties as “external” in contrast to “internal” properties which depend only on the content such as whether a picture depicts nudity. I discuss the distinction at greater length below. Some examples of external properties:</p>
<ul>
<li>Whether a piece of media was generated in the traditional way (photographing a scene, recording a sound), or has been manipulated or synthesized.</li>
<li>Whether text was written by a human.</li>
<li>Whether text was written by a specific person, e.g.&nbsp;by Shakespeare.</li>
</ul>
<p><strong>Advances in AI will help with both forgery-detection and forgery-creation.</strong> It is clear that a better statistical model of genuine artefacts will help detect forgeries but it will also help create convincing forgeries.</p>
<p><strong>Determined forgers will be able to fool humans.</strong> It seems likely that the latter effect will dominate: it will gradually become possible to camouflage computer-generated content such that neither a computer nor a human could tell them apart. If the content-producer has access to the platforms’ model then they can keep perturbing their fake media until it is labelled as non-fake.</p>
<p><strong>We cannot reliably discriminate between real and AI-generated media.</strong> As of late 2023, programs to detect synthetically generated media have relatively poor accuracy: OpenAI announced a model to detect LLM-created text in January 2023 but then <a href="https://decrypt.co/149826/openai-quietly-shutters-its-ai-detection-tool">shut it down</a> in July because of poor performance. In June 2023 the NY Times compared a variety of tools to detect computer-generated images and found that with minimal effort they could all be <a href="https://www.nytimes.com/interactive/2023/06/28/technology/ai-detection-midjourney-stable-diffusion-dalle.html">reliably fooled</a>.</p>
<p><strong>The prevalence of synthetic media will increase on unmoderated platforms.</strong> The major platforms have incentives to limit the prevalence of fake media,<sup>13</sup> and can control the prevalence even without reliable classifiers. E.g. Meta and YouTube dramatically decreased the prevalence of misinformation over 2016-2020 not primarily through real-time detection of whether a given claim is false, but by (1) adjusting ranking to penalize publishers who tend to circulate false claims; (2) punishing publishers who circulate proven-false claims. Thus I do not expect overall prevalence of fake factual media to substantially increase on the major platforms.</p>
<div class="no-row-height column-margin column-container"><div id="fn13"><p><sup>13</sup>&nbsp;The goals of platforms in content moderation are discussed in my note on ranking, <span class="citation" data-cites="cunningham2023ranking">Cunningham (2023)</span>.</p></div></div></section>
<section id="fake-media-deepfakes-will-not-have-a-substantial-influence-on-politics" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="fake-media-deepfakes-will-not-have-a-substantial-influence-on-politics">Fake Media (Deepfakes) Will Not Have a Substantial Influence on Politics</h2>
<p><strong>As synthetic media becomes common people will rely more on provenance.</strong> As it becomes cheaper to manipulate and synthesize media then people are likely to become more skeptical and rely relatively more on the <em>provenance</em> of information. Thus although synthetic media will likely circulate I do not think it will have a substantial influence on beliefs in equilibrium.</p>
<p><strong>It has always been easy to create misleading documents.</strong>It is not difficult to forge or alter documents, or edit video in a misleading way. As a consequence mainstream media organizations typically do not publish leaked materials unless they have either a chain or provenance for the leaks or independent confirmation of their content.</p>
<p><strong>Influential forgeries of documents have been historically rare.</strong> In an Appendix below I compile a simple dataset of politically influential document leaks in the US over the past 25 years and estimate around 10% of them were based on forged materials.<sup>14</sup><sup>15</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn14"><p><sup>14</sup>&nbsp;I know of two forged documents that were widely taken as true in the last 25 years, from around 15 substantial leaks that I could find: (1) the “yellowcake” letters from Iraq to Niger, cited in the 2002 US case for war against Iraq; (2) a fake G W Bush military transcript reported on by CBS and Dan Rather in 2004. It’s notable both that these cases are somewhat rare, and that each was passed through a chain of apparently reputable parties.</p></div><div id="fn15"><p><sup>15</sup>&nbsp;This argument implies that, prior to AI, anonymously leaked video would be more likely to be published and circulated than anonymously leaked documents, because video is harder to fake. In fact I cannot think of many cases of influential anonymous leaks of videos. When Trump’s “Access Hollywood” tape was leaked to the Washington Post they got confirmation before publishing it. In fact maybe leaked video has always been untrustworthy because it has always been easy to make deceptive edits.</p></div><div id="fn16"><p><sup>16</sup>&nbsp;Snopes.com has an enormous database of both false claims and misleadingly manipulated media that has circulated since 1994. A typical <a href="https://www.factcheck.org/2021/03/scicheck-video-targets-gates-with-old-clip-misleading-edit/">recent example</a> is an edit of a Bill Gates interview to make it appear he wants to use vaccination to reduce population growth.</p></div></div><p><strong>The quantity of false claims circulating on the internet is not primarily constrained by the quality of their content.</strong> A great deal of false claims already circulate on the internet, especially in loosely moderated parts: e.g.&nbsp;by email, on Telegram, 4chan, Truth Social, WhatsApp, Twitter. It’s not clear that the quality of the faked media is an important constraint on the volume that circulates. It’s not uncommon to find a clip of an interview with a politician edited to make it appear that they are admitting to a crime or secret agenda.<sup>16</sup> If people already take what they see at face value then adding deepfakes seems unlikely to change their opinions substantially. Alternatively if people are skeptical and look for corroborating sources then, again, deepfakes would be unpersuasive. It seems that deepfakes would only be influential if there are a significant population who are exposed to many lies but are not haded because the documentary evidence is not sufficiently strong.</p>
<p></p>
<p></p>
<p></p>
</section>
<section id="communication-will-migrate-towards-large-closed-platforms" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="communication-will-migrate-towards-large-closed-platforms">Communication Will Migrate Towards Large Closed Platforms</h2>
<p><strong>Small platforms will be overrun with AI-created content.</strong> In particular, AI-created bots, spam, obfuscated violating content, and fake media. This would imply that consumers will tend to migrate to larger closed platforms with more effective defences, and which have more restriction on participation. This continues a general movement over the last 20 years of communication moving from small open platforms (independent email, small forums, mailing lists, independent websites) to large closed platforms (large email providers, large social media platforms).</p>
<p><strong>People will rely more on established sources of truth.</strong> E.g. they will rely relatively more on Wikipedia, Community Notes, and mainstream recognized media sources. The ordinary content-based signs of trustworthiness will become less reliable: having a professional website, well-edited text, well-argued reasoning, and documentary evidence.</p>
<p><strong>People will rely more on cryptographic signing to verify authenticity.</strong> I am not sure how strong this effect will be: it is generally more efficient for an intermediary to verify authenticity of senders than for users to do it themselves. I think we’ve seen that in other domains: (1) PGP signing of email has been less important than email providers filtering spam and phishing; (2) SSL certificates in browsers have been less important than browsers giving warnings for suspected phishing sites (e.g.&nbsp;Google’s <a href="https://safebrowsing.google.com">safe browsing</a> database of sites with phishing or malware is used to give warnings in Chrome and Safari).</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="tecunningham.github.io/posts/images/2023-06-23-10-30-49.png" class="img-fluid"></p>
</div></div><p><strong>Pedigree will become more important in publication.</strong> As an editor accepting submissions (e.g.&nbsp;an academic journal, a literary magazine, a newspaper letters page) the quality of the work submitted is typically correlated with more superficial features such as the grammaticallity and the length. As it becomes easy to synthesize text then those superficial features will become less informative about quality and editors are likely to rely relatively more on hard-to-fake signals like the pedigree of authors: what have they published before, and which college the author went to.</p>
</section>
<section id="entertainment-will-become-largely-synthetic" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="entertainment-will-become-largely-synthetic">Entertainment will Become Largely Synthetic</h2>
<p>A classifier that can detect whether a photo is pretty can also generate a synthetic photo that is pretty, and a classifier that can detect whether a joke is funny should also be able to generate funny jokes.<sup>17</sup> On average people spend around 3 hours per day watching entertainment (TV, YouTube, TikTok, Instagram). It seems likely that trained models will be able to synthesize content that is highly engaging though it’s hard to anticipate what it will look like.</p>
<div class="no-row-height column-margin column-container"><div id="fn17"><p><sup>17</sup>&nbsp;I think language models haven’t yet been very good at jokes because they generate one word at a time (autoregressive), while jokes typically have a logical structure such that the setup is probable given the punchline, but not the other way around. When we get language models which generate text using different statistical algorithms (e.g.&nbsp;diffusion instead of autoregressive generation) then it seems likely they’ll be able to create good jokes.</p></div></div></section>
<section id="things-will-get-weird" class="level2">
<h2 class="anchored" data-anchor-id="things-will-get-weird">Things Will Get Weird</h2>
<p>Much of our common-sense understanding of media will be violated when we routinely use AI models to manipulate and synthesize artefacts. Some examples:</p>
<ul>
<li><p><strong>People will synthesize completely new violating images/videos.</strong> <span class="citation" data-cites="thiel2023generative">Thiel, Stroebel, and Portnoff (2023)</span> say that, as of early 2023, less than 1% of child sexual abuse media (CSAM) appears to be synthetically generated. However the ability to synthesize has been advancing rapidly, “to the point that some images are only distinguishable from reality if the viewer is very familiar with photography, lighting and the characteristics of diffusion model outputs … it is likely that in under a year it will become significantly easier to generate adult images that are indistinguishable from actual images.”</p></li>
<li><p><strong>Producers will synthesize content to sit on the <em>edge</em> of a category.</strong> If platforms take action whenever content passes some threshold then adversarial actors will generate or perturb content such that it sits right below the threshold. If a platform removes a photo whenever more than 50% of raters would say it depicts nudity then producers would upload photos which 49% of raters would say depicts nudity. People would upload movies which <em>almost</em> look like an existing IP-protected movie, and students might submit essays that are close to existing sources but don’t quite trigger the plagiarism detector.</p></li>
</ul>
</section>
</section>
<section id="appendix-historical-observations-on-forgeries" class="level1">
<h1>Appendix: Historical Observations on Forgeries</h1>
<p><strong>Influential leaks of US political documents since 1997:</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 43%">
<col style="width: 3%">
<col style="width: 14%">
<col style="width: 38%">
</colgroup>
<tbody>
<tr class="odd">
<td>Tripp Tapes</td>
<td>1997</td>
<td>audio</td>
<td>Linda Tripp to Kenneth Starr</td>
</tr>
<tr class="even">
<td>[<strong>FORGERY</strong>] Iraq letters to Niger (“yellowcake”)</td>
<td>2002</td>
<td>documents</td>
<td>Unknown to Italian intelligence to CIA</td>
</tr>
<tr class="odd">
<td>[<strong>FORGERY</strong>] Bush military transcripts (“Killian”)</td>
<td>2004</td>
<td>fax of 1970s memo</td>
<td>Unknown to retired colonel to Dan Rather / CBS</td>
</tr>
<tr class="even">
<td>Abu Ghraib photos</td>
<td>2004</td>
<td>photos</td>
<td>Unkown to CBS</td>
</tr>
<tr class="odd">
<td>Baghdad Airstrike (“Collateral Murder”)</td>
<td>2007</td>
<td>video</td>
<td>Chelsea Manning to Wikileaks</td>
</tr>
<tr class="even">
<td>US Iraq war logs</td>
<td>2010</td>
<td>digital docs</td>
<td>Chelsea Manning to Wikileaks</td>
</tr>
<tr class="odd">
<td>US Diplomatic cables</td>
<td>2010</td>
<td>digital docs</td>
<td>Chelsea Manning to Wikileaks</td>
</tr>
<tr class="even">
<td>Romney Fundraiser Tape (“47%”)</td>
<td>2012</td>
<td>audio</td>
<td>Bartender to Mother Jones</td>
</tr>
<tr class="odd">
<td>NSA Surveillance Leaks</td>
<td>2013</td>
<td>digital docs</td>
<td>Edward Snowden to the Guardian, WaPo</td>
</tr>
<tr class="even">
<td>DNC emails</td>
<td>2016</td>
<td>emails</td>
<td>Unknown to Wikileaks</td>
</tr>
<tr class="odd">
<td>Podesta emails</td>
<td>2016</td>
<td>emails</td>
<td>Unknown to Wikileaks</td>
</tr>
<tr class="even">
<td>Colin Powell emails</td>
<td>2016</td>
<td>emails</td>
<td>Unknown to DCLeaks</td>
</tr>
<tr class="odd">
<td>Panama papers</td>
<td>2016</td>
<td>documents</td>
<td>Unknown to Süddeutsche Zeitung</td>
</tr>
<tr class="even">
<td>Donald Trump Access Hollywood Tape</td>
<td>2016</td>
<td>video</td>
<td>Unknown to Washington Post</td>
</tr>
<tr class="odd">
<td>China Cables</td>
<td>2019</td>
<td>digital docs</td>
<td>Unknown to the ICIJ</td>
</tr>
<tr class="even">
<td>Hunter Biden laptop</td>
<td>2020</td>
<td>docs,audio,video</td>
<td>computer shop to Giuliani to NY Post</td>
</tr>
<tr class="odd">
<td>Los Angeles Council call (“changuito”)</td>
<td>2022</td>
<td>audio</td>
<td>Unknown to Reddit to LA Times</td>
</tr>
</tbody>
</table>
<p><strong>Why are forgeries not more common?</strong> I can think of three possible reasons:</p>
<ol type="1">
<li>It’s difficult to forge credible documents – e.g.&nbsp;even a simple memo is hard to fake because there are lots of small details like the letterforms and formatting and jargon used.</li>
<li>It’s easy to forge credible documents but intelligence agencies and the media won’t believe them without independent confirmation.</li>
<li>It’s easy to forge credible documents and for them to get coverage but not many people are motivated to try.</li>
</ol>
<p><strong>Other examples of influential forgeries.</strong></p>
<table class="caption-top table">
<colgroup>
<col style="width: 46%">
<col style="width: 3%">
<col style="width: 50%">
</colgroup>
<tbody>
<tr class="odd">
<td>Mark Antony’s will</td>
<td>33BC</td>
<td>read out by Octavian in the senate (disputed whether forgery)</td>
</tr>
<tr class="even">
<td>Dreyfus letters sharing military info w Germany</td>
<td>1894</td>
<td>fabricated by French military</td>
</tr>
<tr class="odd">
<td>Protocols of Elders of Zion (Jewish plans for domination)</td>
<td>1903</td>
<td></td>
</tr>
<tr class="even">
<td>Castle Document, letter to British govt in Dublin</td>
<td>1916</td>
<td>unclear source, contributed to 1916 Easter rising</td>
</tr>
<tr class="odd">
<td>Zinoviev letter from Russia to UK Labour party</td>
<td>1924</td>
<td>unclear source</td>
</tr>
<tr class="even">
<td>Tanaka Memorial (Japanese plans for world domination)</td>
<td>1929</td>
<td>unclear source</td>
</tr>
<tr class="odd">
<td>Macron emails</td>
<td>2017</td>
<td>emails</td>
</tr>
</tbody>
</table>
<p><strong>Notes.</strong></p>
<ul>
<li><p>The Macron email leaks seemed to include both real and fake content (<a href="https://www.csis.org/analysis/successfully-countering-russian-electoral-interference">ref</a>). In fact one report says that Macron’s team sent each other outrageous implausible emails as a pre-emptive defense to make any subsequent leaks seem less credible (<a href="https://www.lawfareblog.com/macron-leaks-are-they-real-and-it-russia">ref</a>).</p></li>
<li><p>The Steele dossier isn’t really a forgery: it doesn’t purport to have a different author than its true author. The problem with the dossier is that the author makes knowingly false claims of fact.</p></li>
</ul>
</section>
<section id="appendix-relevant-forecasts-from-metaculus" class="level1">
<h1>Appendix: Relevant Forecasts from Metaculus</h1>
<table class="caption-top table">
<colgroup>
<col style="width: 74%">
<col style="width: 8%">
<col style="width: 17%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th>deadline</th>
<th>Metaculus Jun 30 2023</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Will a deepfake cause damage &amp; make front page of a major news source?</td>
<td>2023</td>
<td>89%</td>
</tr>
<tr class="even">
<td>Will a deepfake be blamed by G20 politician for election loss?</td>
<td>2025</td>
<td>80%</td>
</tr>
<tr class="odd">
<td>Will AI be used in an attack on infrastructure costing &gt;$1B?</td>
<td>2025</td>
<td>4%</td>
</tr>
<tr class="even">
<td>Will AI be used in a theft of intellectual property cost &gt;$10M?</td>
<td>2025</td>
<td>30%</td>
</tr>
<tr class="odd">
<td>Will AI cause a stock exchange to halt trading for &gt;24 hours?</td>
<td>2025</td>
<td>15%</td>
</tr>
<tr class="even">
<td>Will AI be used in a major attack on voting systems in G20?</td>
<td>2025</td>
<td>10%</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>Will a deepfake about politicial running for office get 2M+ views?</td>
<td>2018</td>
<td>(resolved false)</td>
</tr>
<tr class="odd">
<td>Will a wide-scale video hoax put words in a famous figure’s mouth?</td>
<td>2017</td>
<td>(resolved false</td>
</tr>
</tbody>
</table>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-bursztein2023retvec" class="csl-entry">
Bursztein, Elie, Marina Zhang, Owen Vallis, Xinyu Jia, and Alexey Kurakin. 2023. <span>“RETVec: Resilient and Efficient Text Vectorizer.”</span> <a href="https://arxiv.org/abs/2302.09207">https://arxiv.org/abs/2302.09207</a>.
</div>
<div id="ref-chen2022profanity" class="csl-entry">
Chen, Edwin. 2022. <a href="https://www.surgehq.ai/blog/are-popular-toxicity-models-simply-profanity-detectors">https://www.surgehq.ai/blog/are-popular-toxicity-models-simply-profanity-detectors</a>.
</div>
<div id="ref-cunningham2023ranking" class="csl-entry">
Cunningham, Tom. 2023. <span>“Ranking by Engagement.”</span> <a href="http://tecunningham.github.io/2023-04-28-ranking-by-engagement.html">http://tecunningham.github.io/2023-04-28-ranking-by-engagement.html</a>.
</div>
<div id="ref-grondahl2018need" class="csl-entry">
Gröndahl, Tommi, Luca Pajola, Mika Juuti, Mauro Conti, and N. Asokan. 2018. <span>“All You Need Is "Love": Evading Hate-Speech Detection.”</span> <a href="https://arxiv.org/abs/1808.09115">https://arxiv.org/abs/1808.09115</a>.
</div>
<div id="ref-han2020fortifying" class="csl-entry">
Han, Xiaochuang, and Yulia Tsvetkov. 2020. <span>“Fortifying Toxic Speech Detectors Against Veiled Toxicity.”</span> In <em>Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>, 7732–39. Online: Association for Computational Linguistics. <a href="https://doi.org/10.18653/v1/2020.emnlp-main.622">https://doi.org/10.18653/v1/2020.emnlp-main.622</a>.
</div>
<div id="ref-heiner2022toxic" class="csl-entry">
Heiner, Scott. 2022. <span>“Real-World ML Failures: The Violence, Racism, and Sexism Uncaught by Twitter’s Content Moderation Systems.”</span> <a href="https://www.surgehq.ai/blog/25-examples-of-twitters-content-moderation-failures">https://www.surgehq.ai/blog/25-examples-of-twitters-content-moderation-failures</a>.
</div>
<div id="ref-kiela2023plottingprogress" class="csl-entry">
Kiela, Douwe, Tristan Thrush, Kawin Ethayarajh, and Amanpreet Singh. 2023. <span>“Plotting Progress in AI.”</span> <em>Contextual AI Blog</em>.
</div>
<div id="ref-whitlocklees2022perspective" class="csl-entry">
Lees, Alyssa Whitlock, Vinh Q. Tran, Yi Tay, Jeffrey Scott Sorensen, Jai Gupta, Donald Metzler, and Lucy Vasserman. 2022. <span>“A New Generation of Perspective API: Efficient Multilingual Character-Level Transformers.”</span> In. <a href="https://dl.acm.org/doi/10.1145/3534678.3539147">https://dl.acm.org/doi/10.1145/3534678.3539147</a>.
</div>
<div id="ref-lees2021capturing" class="csl-entry">
Lees, Alyssa, Daniel Borkan, Ian Kivlichan, Jorge Nario, and Tesh Goyal. 2021. <span>“Capturing Covertly Toxic Speech via Crowdsourcing.”</span> In <em>Proceedings of the First Workshop on Bridging Human<span>–</span>Computer Interaction and Natural Language Processing</em>, 14–20. Online: Association for Computational Linguistics. <a href="https://aclanthology.org/2021.hcinlp-1.3">https://aclanthology.org/2021.hcinlp-1.3</a>.
</div>
<div id="ref-searles2023empirical" class="csl-entry">
Searles, Andrew, Yoshimichi Nakatsuka, Ercan Ozturk, Andrew Paverd, Gene Tsudik, and Ai Enkoji. 2023. <span>“An Empirical Study &amp; Evaluation of Modern CAPTCHAs.”</span> <a href="https://arxiv.org/abs/2307.12108">https://arxiv.org/abs/2307.12108</a>.
</div>
<div id="ref-thiel2023generative" class="csl-entry">
Thiel, David, Melissa Stroebel, and Rebecca Portnoff. 2023. <span>“Generative ML and CSAM: Implications and Mitigations.”</span>
</div>
<div id="ref-weng2023gpt4moderation" class="csl-entry">
Weng, Lilian, Vik Goel, and Andrea Vallone. 2023. <span>“Using GPT-4 for Content Moderation.”</span> <a href="https://openai.com/blog/using-gpt-4-for-content-moderation">https://openai.com/blog/using-gpt-4-for-content-moderation</a>.
</div>
</div>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{cunningham2023,
  author = {Cunningham, Tom},
  title = {The {Influence} of {AI} on {Content} {Moderation} and
    {Communication}},
  date = {2023-12-11},
  url = {tecunningham.github.io/posts/2023-06-06-effect-of-ai-on-communication.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-cunningham2023" class="csl-entry quarto-appendix-citeas">
Cunningham, Tom. 2023. <span>“The Influence of AI on Content Moderation
and Communication.”</span> December 11, 2023. <a href="https://tecunningham.github.io/posts/2023-06-06-effect-of-ai-on-communication.html">tecunningham.github.io/posts/2023-06-06-effect-of-ai-on-communication.html</a>.
</div></div></section></div> ]]></description>
  <guid>tecunningham.github.io/posts/2023-06-06-effect-of-ai-on-communication.html</guid>
  <pubDate>Mon, 11 Dec 2023 08:00:00 GMT</pubDate>
</item>
<item>
  <title>The History of Automated Text Moderation</title>
  <dc:creator>[Integrity Institute](https://integrityinstitute.org/) collaborators: [Alex Rosenblatt](https://www.linkedin.com/in/alexrosenblatt/), [Jeff Allen](https://www.linkedin.com/in/jeff-allen-scientist/), [Ejona Varangu](https://www.linkedin.com/in/ejona-varangu/), [Dave Sullivan](https://www.linkedin.com/in/davesullivan41/), Tom Cunningham</dc:creator>
  <link>tecunningham.github.io/posts/2023-11-18-history-automated-text-moderation.html</link>
  <description><![CDATA[ 





<p><strong>This document describes five technologies for automated text moderation,</strong> each roughly correspond to an historical phase.</p>
<p><strong>As a working example we will use the detection of “toxic” comments.</strong> In practice many different definitions of “toxic” have been used in the industry, and there are a variety of related concepts, e.g.&nbsp;“hate speech” and “offensive”.</p>
<section id="keywords" class="level1">
<h1>(1) Keywords</h1>
<p>The simplest technology is to hard-code a list of words which are considered “toxic”, e.g.&nbsp;a list of curse words. This can be implemented with regular expression. This has obvious limits on the accuracy and cannot be easily maintained, however many platforms still maintain a keyword block list for some sensitive terms.</p>
</section>
<section id="simple-classifier-bag-of-words" class="level1">
<h1>(2) Simple classifier (“Bag of words”)</h1>
<p>We can collect a large set of human-labeled data on whether individual messages are toxic, and then predict toxicity from the appearance of individual words e.g.&nbsp;using logistic regression or naive Bayes. These classifiers will find that certain words are highly predictive of toxicity. Simple classifiers often have reasonable accuracy but will have many important false positives and false negatives, and they are easy to evade by rewording or misspelling text.</p>
<ul>
<li>1961: Maron (1961) proposes the Naive Bayes classifier</li>
</ul>
</section>
<section id="embedding-based-classifier-2013-2018" class="level1 page-columns page-full">
<h1>(3) Embedding-based classifier (2013-2018)</h1>
<p>These models have two stages:</p>
<ol type="1">
<li>Pretrain: for each word calculate an embedding (a vector of numbers) which predicts its likelihood of co-occurring with other words. Pairs of words which are nearby in embedding-space typically have similar meanings.</li>
<li>Train: train a model to predict toxicity of a comment using the embedding of the words in a message (e.g.&nbsp;the average embedding).</li>
</ol>
<p>An advantage over simple classifiers is that these models require much less labeled data for an equal performance, because the pre-training stage has already learned (crudely) the meanings of different words. Thus these models can identify words that are diagnostic of toxicity even if they never appeared in the toxicity training set.</p>
<p>However embedding-based classifiers are still bad at edge cases, e.g.&nbsp;when a word is used inside a negation (“is an idiot” vs “is not an idiot”), or if a word is mis-spelt, or if harmless words are used to express an meaning that is toxic (“your brain is a bowl of jello”).</p>
<ul>
<li>2013: Word2Vec: a word embedding using a 2-layer neural network, (<span class="citation" data-cites="mikolov2013efficient">Mikolov et al. (2013)</span>)</li>
<li>2014: GloVe: Global Vectors for Word Representation. They say “training is performed on aggregated global word-word co-occurrence statistics from a corpus” (<span class="citation" data-cites="pennington2014glove">Pennington, Socher, and Manning (2014)</span>).</li>
<li>2015: fastText: word embedding from FAIR. They released pre-trained models for 294 languages (<span class="citation" data-cites="joulin2016bag">Joulin et al. (2016)</span>)</li>
<li>2017: Jigsaw Perspective Toxicity API v1 from Google.<sup>1</sup></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;I couldn’t find any authoritative documentation on the architecture of this classifier: I found one reference to it using the GloVe embeddings.</p></div></div></section>
<section id="llm-based-classifiers-2018-2023" class="level1">
<h1>(4) LLM-based classifiers (2018-2023)</h1>
<p>These models have three stages:</p>
<ol type="1">
<li>Embedding: Compute embedding of each token (a token is roughly equal to a word).</li>
<li>Pretrain: Train a deep neural net to predict a token from surrounding tokens (or prior tokens), using attention (i.e.&nbsp;don’t weight all words equally) on an enormous training set of text from books and the internet.</li>
<li>Train: Train a model to predict toxicity from labeled data using the top-level neurons in the net as features.</li>
</ol>
<p>Conceptually these are similar to embeddings but (1) they can represent the meaning of entire sentences instead of just words, (2) have more layers so tend to have more sophisticated representations of meaning.</p>
<ul>
<li>2017: Transformer architecture (<span class="citation" data-cites="vaswani2017attention">Vaswani et al. (2017)</span>)</li>
<li>2018: BERT transformer LLM, this model has been widely used as base model for a variety of natural language tasks, including content moderation (<span class="citation" data-cites="devlin2018bert">Devlin et al. (2018)</span>)</li>
</ul>
</section>
<section id="zero-shot-llms-2023-" class="level1">
<h1>(5) Zero-shot LLMs (2023-)</h1>
<p>These models have three stages:</p>
<ol type="1">
<li>Embedding: Compute the embedding of each token.</li>
<li>Pretrain: Train a deep net to predict the next token from previous tokens, as above.</li>
<li>Directly ask the model whether a given message violates a given policy, e.g.&nbsp;“is the following sentence toxic? ___”</li>
</ol>
<p>Notably this method does not use any human-labeled data, it only needs to be told what type of text it is looking for. This is referred to as “zero shot”, meaning it needs zero training data. These models can also use “few shot” learning, where a small number of examples are given instead of the thousands of examples that had ordinarily been used.</p>
<p>This has big benefits: it allows you to very quickly refine policy, and the LLM can generate explanations for why it made a decision.</p>
<ul>
<li>2020: GPT-3: reasonable zero-shot performance (<span class="citation" data-cites="brown2020language">Brown et al. (2020)</span>)</li>
<li>2022: ChatGPT published: very good zero-shot performance on many tasks.</li>
<li>2023: OpenAI provides GPT-4-based content moderation tools (<span class="citation" data-cites="weng2023gpt4moderation">Weng, Goel, and Vallone (2023)</span>)</li>
<li>2023: Startups providing LLM-based content moderation: <a href="https://www.safetykit.com">SafetyKit</a>, <a href="https://www.checkstep.com/blog/">CheckStep</a>, <a href="https://thehive.ai">Hive</a>, <a href="https://getcove.com/ai">Cove</a>.</li>
<li>2023: Stanford CoPE:an open-source LLM for moderation.</li>
</ul>
</section>
<section id="discussion" class="level1">
<h1>Discussion</h1>
<p><strong>Q: now that we can use LLMs for arbitrary labeling, will we change policies?</strong></p>
<ul>
<li>Proposals are coming out of Michael Bernstein’s lab, e.g. <span class="citation" data-cites="jia2023embedding">Jia et al. (2023)</span>, in using LLMs to substantially change how content is ranked.</li>
<li>Dave Wilner has argued that because LLMs offer much greater flexibility then platforms will find it easier to write more complex policies and update them more frequently.</li>
</ul>
<p><strong>Q: what do we know about degree of accuracy across languages?</strong></p>
<ul>
<li>AI typically has a strong anglophone bias. Performance in non-English languages tends to be proportional to the distance from English, e.g.&nbsp;European languages tend to be worse. However many also noted that there is typically a large anglophone bias in human moderation. </li>
<li>Some literature shows that LLMs have good performance in languages with relatively little training data, e.g. <span class="citation" data-cites="armengolestape2021multilingual">Armengol-Estapé, Gibert Bonet, and Melero (2021)</span>. </li>
</ul>
<p><strong>Q: will censorship change when using LLMs instead of humans?</strong></p>
<ul>
<li>Jeff noted that an advantage of human censors over machine censors is that humans might exercise their judgment to refuse to censor while machines will not.</li>
</ul>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-armengolestape2021multilingual" class="csl-entry">
Armengol-Estapé, Jordi, Ona de Gibert Bonet, and Maite Melero. 2021. <span>“On the Multilingual Capabilities of Very Large-Scale English Language Models.”</span> <a href="https://arxiv.org/abs/2108.13349">https://arxiv.org/abs/2108.13349</a>.
</div>
<div id="ref-brown2020language" class="csl-entry">
Brown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. <span>“Language Models Are Few-Shot Learners.”</span> <a href="https://arxiv.org/abs/2005.14165">https://arxiv.org/abs/2005.14165</a>.
</div>
<div id="ref-devlin2018bert" class="csl-entry">
Devlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. <span>“Bert: Pre-Training of Deep Bidirectional Transformers for Language Understanding.”</span> <em>arXiv Preprint arXiv:1810.04805</em>.
</div>
<div id="ref-jia2023embedding" class="csl-entry">
Jia, Chenyan, Michelle S Lam, Minh Chau Mai, Jeff Hancock, and Michael S Bernstein. 2023. <span>“Embedding Democratic Values into Social Media AIs via Societal Objective Functions.”</span> <em>arXiv Preprint arXiv:2307.13912</em>.
</div>
<div id="ref-joulin2016bag" class="csl-entry">
Joulin, Armand, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. <span>“Bag of Tricks for Efficient Text Classification.”</span> <a href="https://arxiv.org/abs/1607.01759">https://arxiv.org/abs/1607.01759</a>.
</div>
<div id="ref-mikolov2013efficient" class="csl-entry">
Mikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. <span>“Efficient Estimation of Word Representations in Vector Space.”</span> <a href="https://arxiv.org/abs/1301.3781">https://arxiv.org/abs/1301.3781</a>.
</div>
<div id="ref-pennington2014glove" class="csl-entry">
Pennington, Jeffrey, Richard Socher, and Christopher D. Manning. 2014. <span>“GloVe: Global Vectors for Word Representation.”</span> In <em>Empirical Methods in Natural Language Processing (EMNLP)</em>, 1532–43. <a href="http://www.aclweb.org/anthology/D14-1162">http://www.aclweb.org/anthology/D14-1162</a>.
</div>
<div id="ref-vaswani2017attention" class="csl-entry">
Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. <span>“Attention Is All You Need.”</span> <em>Advances in Neural Information Processing Systems</em> 30.
</div>
<div id="ref-weng2023gpt4moderation" class="csl-entry">
Weng, Lilian, Vik Goel, and Andrea Vallone. 2023. <span>“Using GPT-4 for Content Moderation.”</span> <a href="https://openai.com/blog/using-gpt-4-for-content-moderation">https://openai.com/blog/using-gpt-4-for-content-moderation</a>.
</div>
</div></section></div> ]]></description>
  <guid>tecunningham.github.io/posts/2023-11-18-history-automated-text-moderation.html</guid>
  <pubDate>Sat, 18 Nov 2023 08:00:00 GMT</pubDate>
</item>
<item>
  <title>Thinking About Tradeoffs? Draw an Ellipse</title>
  <dc:creator>Tom Cunningham, OpenAI.</dc:creator>
  <link>tecunningham.github.io/posts/2023-10-23-pareto-frontiers-experiments-ranking.html</link>
  <description><![CDATA[ 





<style>
    h1 {  border-bottom: 4px solid black;  }
    h2 {  border-bottom: 1px solid #ccc;}
    .reveal section p {
      display: inline-block;
      font-size: 2em;
      #line-height: 1.2em;
      #vertical-align: top;
   }
</style>

<div class="no-row-height column-margin column-container"><div class="">
<p>This material was first presented at MIT CODE 2021. Thanks to <a href="https://www.linkedin.com/in/seanjtaylor/">Sean Taylor</a> among others for comments.</p>
</div></div><p><strong>Thinking about tradeoffs? draw an ellipse.</strong> When making a tradeoff between two outcomes, <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y">, it’s useful to sketch out what the tradeoff looks like, and an ellipse is often a good first-order approximation. The ellipse helps visualize the most interesting parameter: the <em>tightness</em>, i.e.&nbsp;how much the rate of tradeoff between <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> varies as you increase <img src="https://latex.codecogs.com/png.latex?X">.</p>
<p>In addition we can show that if the Pareto frontier is formed by the sum of vectors, and the vectors are drawn from a joint Normal distribution, then the expected frontier will be exactly an ellipse.</p>
<p><strong>Concrete Applications:</strong></p>
<ol type="1">
<li><p><strong>Choosing launch criteria? draw an ellipse.</strong> Suppose you have a set of features each of which has some metric impact, <img src="https://latex.codecogs.com/png.latex?%5CDelta%20X"> and <img src="https://latex.codecogs.com/png.latex?%5CDelta%20Y">. If we assume that the effects are additive then we can construct a Pareto frontier, i.e.&nbsp;a set of all the aggregate effects on <img src="https://latex.codecogs.com/png.latex?%5CDelta%20X"> and <img src="https://latex.codecogs.com/png.latex?%5CDelta%20Y"> achievable by selection from the set of features. The frontier will typically look like an ellipse. You can prove that the Pareto frontier will be exactly an ellipse if the set of experiment-effects have a joint Normal distribution.</p></li>
<li><p><strong>Choosing ranking weights? draw an ellipse.</strong> Suppose you are ranking items for a user using a set of features e.g.&nbsp;p(Like), p(Comment), etc. It is useful to sketch out the Pareto frontier, i.e.&nbsp;the set of outcomes achievable by different ranking algorithms. If the outcomes are additively separable functions of the item-features, and if the joint distribution of features is Normal, then the Pareto frontier will be an ellipse.</p></li>
<li><p><strong>Allocating headcount? draw an ellipse.</strong> When you shuffle headcount around a company it’s hard to precisely measure the impact on different goals, however I have found it useful to sketch ellipses to make explicit the tradeoffs you face. This is particularly useful for visualizing the differences between within-team vs between-team reallocation of effort.</p></li>
</ol>
<section id="tight-and-loose-tradeoffs" class="level1 page-columns page-full">
<h1>Tight and Loose Tradeoffs</h1>
<div class="cell page-columns page-full" data-hash="2023-10-23-pareto-frontiers-experiments-ranking_cache/html/unnamed-chunk-1_4bb7e47c4ed41520e4c5aeae8fd12fce">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-10-23-pareto-frontiers-experiments-ranking_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div></div></div>
<div class="cell page-columns page-full" data-hash="2023-10-23-pareto-frontiers-experiments-ranking_cache/html/unnamed-chunk-2_48508bdd7f3e3c96af1e87a67367dc69">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-10-23-pareto-frontiers-experiments-ranking_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div></div></div>
<p><strong>Suppose we care about two metrics, <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y">.</strong> E.g. suppose we care about DAU and time-spent, or revenue and retention, or engagement and misinformation.</p>
<p><strong>It is useful to draw a Pareto frontier.</strong> A Pareto frontier will show the set of achievable outcomes for X and Y, to make the tradeoff precise. If we have a well-defined objective function then we can visually represent the optimal choice where the indifference curve is tangent to the Pareto frontier.</p>
<p><strong>If your frontier is “tight” then there is not much tradeoff.</strong> The first figure shows a tight frontier, meaning that there is not much tradeoff available between X and Y. With a tight tradeoff it doesn’t matter whether we maximize X or Y or a weighted average, we’ll end up in roughly the same place anyway. Suppose we are choosing among experiments: if we observe a high positive correlation between <img src="https://latex.codecogs.com/png.latex?%5CDelta%20X"> and <img src="https://latex.codecogs.com/png.latex?%5CDelta%20Y"> then the choice of shipping criteria is relatively unimportant, most criteria would select the same experiments anyway. Suppose instead we are calibrating a recommender system: if we observe a high positive correlation between predictions of the two outcomes then the choice of weights is relatively unimportant, we would end up showing the same items anyway.</p>
<p><strong>If your frontier is “loose” then there is a lot of tradeoff.</strong> The second figure shows a loose tradeoff: in this case the outcome does depend substantially on the relative weight we put on <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y">.</p>
</section>
<section id="ellipses-for-experiments" class="level1 page-columns page-full">
<h1>Ellipses for Experiments</h1>
<div class="cell page-columns page-full" data-hash="2023-10-23-pareto-frontiers-experiments-ranking_cache/html/unnamed-chunk-3_58fce7016551619d2ab1f7892a1f0e4c">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-10-23-pareto-frontiers-experiments-ranking_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div></div></div>
<p><strong>Suppose we have a set of experiments.</strong> Each experiment has some impact on two metrics, <img src="https://latex.codecogs.com/png.latex?%5CDelta%20X"> and <img src="https://latex.codecogs.com/png.latex?%5CDelta%20Y">. We visualize such a set of experiments at right.</p>
<p>If the set of features is <em>separable</em>, meaning that the impact of each feature is independent of what other features are launched, then a natural question will be the shape of the Pareto frontier formed by all possible combination of experiments.</p>
<p>If the distribution of experiments is mean zero and joint Normal then the Pareto frontier will be an <em>ellipse</em>, and it will have exactly the shape of an isovalue of the density of experiments. Thus knowing the variance and covariance of experiment results allows us to characterize the nature of the Pareto frontier we face.</p>
</section>
<section id="ellipses-for-ranking" class="level1 page-columns page-full">
<h1>Ellipses for Ranking</h1>
<div class="cell page-columns page-full" data-hash="2023-10-23-pareto-frontiers-experiments-ranking_cache/html/unnamed-chunk-4_892222e0d622229a23671a8240941df9">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-10-23-pareto-frontiers-experiments-ranking_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div></div></div>
<p><strong>Suppose we are choosing a fixed set of items to show to a user, based on two metrics <img src="https://latex.codecogs.com/png.latex?x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2">.</strong> E.g. <code>pLike</code> and <code>pComment</code>, or <code>pDAU</code> and <code>quality</code> etc. A natural question will be the shape of the Pareto frontier formed by alternative selections of items.</p>
<p><strong>The Pareto frontier will be an ellipse.</strong> We show below that, if the predictions are well calibrated, the outcomes are independent (i.e.&nbsp;additive), and the distribution of prediction obeys a joint Normal distribution, then the Pareto frontier will be an ellipse and it will have exactly the shape of an isovalue of the density of predictions. Thus knowing the variance and covariance of predictions allows us to exactly characterize the nature of the aggregate tradeoffs we face.</p>
</section>
<section id="ellipses-for-company-strategy" class="level1 page-columns page-full">
<h1>Ellipses for Company Strategy</h1>
<p><strong>Tradeoffs are looser higher in the decision hierarchy.</strong> Suppose a company cares about two outcomes, <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y">. Many different people will be making tradeoff decisions between X and Y, we can distinguish between four objectives used at different levels in the company hierarchy: <img src="https://latex.codecogs.com/png.latex?%5Csubstack%7B%5Ctext%7Bcompany%20objective%7D%5C%5C%5Ctext%7B(choose%20headcount)%7D%7D%0A%20%20%20%20%20%20%3E%20%5Csubstack%7B%5Ctext%7Bteam%20objective%7D%5C%5C%5Ctext%7B(choose%20projects)%7D%7D%0A%20%20%20%20%20%20%3E%20%5Csubstack%7B%5Ctext%7Bshipping%20objective%7D%5C%5C%5Ctext%7B(choose%20experiments)%7D%7D%0A%20%20%20%20%20%20%3E%20%5Csubstack%7B%5Ctext%7Balgorithm%20objective%7D%5C%5C%5Ctext%7B(choose%20items)%7D%7D%0A%20%20%20%20%20%20"></p>
<p>We can think of each successive level as holding more variables fixed, and so we expect the Pareto frontiers to become successively tighter (Le Chatelier principle). We thus expect the tradeoff to be loosest at the level of overall company objectives, where we reallocate headcount. For this reason we should expect that, if the company as a whole pivots form metric <img src="https://latex.codecogs.com/png.latex?X"> to metric <img src="https://latex.codecogs.com/png.latex?Y">, the principal effect will be a reallocation of effort <em>between</em> products rather than reallocation <em>within</em> products.</p>
<p>We now walk through some of the different levels of optimization:</p>
<div class="cell page-columns page-full" data-hash="2023-10-23-pareto-frontiers-experiments-ranking_cache/html/unnamed-chunk-5_388a7f2862c721051ab6f7ad2d60f448">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-10-23-pareto-frontiers-experiments-ranking_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div></div></div>
<p><strong>Different product areas have different Pareto frontiers.</strong> Typically two different product areas will have substantially different ability to affect different metrics, and we will often observe a situation like that shown on the right: team A’s choices primarily affect metric <img src="https://latex.codecogs.com/png.latex?X">, team B’s choices primarily affect metric <img src="https://latex.codecogs.com/png.latex?Y">.</p>
<p><br><br><br></p>
<div class="cell page-columns page-full" data-hash="2023-10-23-pareto-frontiers-experiments-ranking_cache/html/unnamed-chunk-6_e37db323a9db1d4dbce81ffd97af014a">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-10-23-pareto-frontiers-experiments-ranking_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div></div></div>
<p><strong>We can also draw a <em>combined</em> Pareto frontier.</strong> Here we add up the Pareto frontiers of team A and B. In this case the combined frontier is somewhat tight, because the two constituent frontiers are tight. Neither individual Pareto frontier shows a substantial effect from changing weights (if we restrict weights to be positive), and so accordingly the combined Pareto frontier shows little response to a change in weights.</p>
<p>Note that I have drawn the frontier only approximately, the frontier achieved by combining two ellipses does not have a simple representation. When the two constituent frontiers are straight lines then the combination will be a parallelogram. (Note also that when the two frontiers are circles then the combination will be a circle).</p>
<div class="cell page-columns page-full" data-hash="2023-10-23-pareto-frontiers-experiments-ranking_cache/html/unnamed-chunk-7_78f65e5d3fc6061482728ed58021bfb8">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-10-23-pareto-frontiers-experiments-ranking_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div></div></div>
<p><strong>Greater investment will shift Pareto frontiers out.</strong> Here we visualize reallocating employees from team B (the frontier shifts in) to team A (the frontier shifts out).</p>
<p><br><br><br><br><br><br><br><br></p>
<div class="cell page-columns page-full" data-hash="2023-10-23-pareto-frontiers-experiments-ranking_cache/html/unnamed-chunk-8_af7a2ac6d9570600948403f809aee92c">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-10-23-pareto-frontiers-experiments-ranking_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div></div></div>
<p><strong>A combined company Pareto frontier will be loose.</strong> Here the green curve represents all the possible outcomes as you shift resources between team A and B: we have now turned a tight tradeoff into a loose tradeoff. In this case this represents that a change in company objectives will be reflected mainly in reallocation of effort <em>between</em> teams rather than <em>within</em> teams.</p>
<p><br><br><br><br></p>
</section>
<section id="appendix-model-for-normal-distributions" class="level1 page-columns page-full">
<h1>Appendix: Model for Normal Distributions</h1>
<p>Suppose we have a set of items with, <img src="https://latex.codecogs.com/png.latex?x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2">, distributed Normally: <img src="https://latex.codecogs.com/png.latex?%5Cbinom%7Bx_1%7D%7Bx_2%7D%5Csim%20N%5Cleft(%5Cbinom%7B0%7D%7B0%7D,%0A%20%20%20%20%20%20%5Cbegin%7Bpmatrix%7D%5Csigma_1%5E2%20&amp;%20%5Crho%5Csigma_1%5Csigma_2%20%5C%5C%20%5Crho%5Csigma_1%5Csigma_2%20&amp;%20%5Csigma_2%5E2%5Cend%7Bpmatrix%7D%5Cright)."></p>
<p>We additionally let each item have a <em>score</em>, <img src="https://latex.codecogs.com/png.latex?v">, which is simply a weighted sum of the two characteristics (normalizing the weight on the first characteristic to be 1): <img src="https://latex.codecogs.com/png.latex?v=x_1%20+%20wx_2."></p>
<p>We can write the covariance between the characteristics and the score as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?Cov%5Cbegin%7Bbmatrix%7Dx_1%5C%5Cx_2%5C%5Cv%5Cend%7Bbmatrix%7D=%0A%5Cbegin%7Bbmatrix%7D%0A%20%20%20%20%5Csigma%5E2_1%20%20%20%20%20%20%20%20%20%20%20&amp;%20%5Csigma_1%5Csigma_2%5Crho%20&amp;%20%5Csigma_1%5E2+%20w%5Crho%5Csigma_1%5Csigma_2%20%5C%5C%0A%20%20%20%20%5Csigma_1%5Csigma_2%5Crho%20&amp;%20%20%5Csigma_2%5E2%20%20%20%20%20%20%20%20%20%20&amp;%20%5Crho%5Csigma_1%5Csigma_2+w%5Csigma_2%5E2%20%5C%5C%0A%20%20%20%20%5Csigma_1%5E2+w%5Crho%5Csigma_1%5Csigma_2%20%20%20%20%20%20%20%20%20%20&amp;%0A%20%20%20%20%20%20%20%20%20%20%5Crho%5Csigma_1%5Csigma_2+w%5Csigma_2%5E2%20%20%20%20&amp;%0A%20%20%20%20%20%20%20%20%20%20%5Csigma_1%5E2+w%5E2%5Csigma_2%5E2%20+%202%5Crho%20w%5Csigma_1%5Csigma_2%20%5C%5C%0A%5Cend%7Bbmatrix%7D"></p>
<p>We wish to know the total number of actions of each type, <img src="https://latex.codecogs.com/png.latex?X_1"> and <img src="https://latex.codecogs.com/png.latex?X_2">, for a given score threshold <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Bv%7D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20X_1%20%20%20&amp;=P(v%5Cgeq%20%5Cbar%7Bv%7D)E%5Bx_1%7Cv%5Cgeq%20%5Cbar%7Bv%7D%5D%20%5C%5C%0A%20%20%20X_2%20%20%20&amp;=P(v%5Cgeq%20%5Cbar%7Bv%7D)E%5Bx_2%7Cv%5Cgeq%20%5Cbar%7Bv%7D%5D.%0A%5Cend%7Baligned%7D"></p>
<p>We first calculate the conditional expectations:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0AE%5Bx_1%7Cv%5Cgeq%20%5Cbar%7Bv%7D%5D%0A%20%20%20=&amp;%20%5Csigma_1%20%5Cfrac%7BCov(x_1,v)%7D%7B%5Csqrt%7BVar(x_1)Var(v)%7D%7D%0A%20%20%20%20%20%20%5Cfrac%7B%5Cphi(%5Cfrac%7B%5Cbar%7Bv%7D%7D%7B%5Csqrt%7BVar(v)%7D%7D)%7D%7B%5CPhi(%5Cfrac%7B%5Cbar%7Bv%7D%7D%7B%5Csqrt%7BVar(v)%7D%7D)%7D%20%5C%5C%0A%20%20%20=&amp;%20%5Csigma_1%0A%20%20%20%20%20%20%5Cfrac%7B%5Csigma_1%5E2+w%5Crho%5Csigma_1%5Csigma_2%7D%0A%20%20%20%20%20%20%20%20%20%7B%5Csqrt%7B%5Csigma_1%5E2(%5Csigma_1%5E2+w%5E2%5Csigma_2%5E2%20+%202%5Crho%20w%5Csigma_1%5Csigma_2)%7D%7D%0A%20%20%20%20%20%20%5Cfrac%7B%5Cphi(%5Cfrac%7B%5Cbar%7Bv%7D%7D%7B%5Csqrt%7BVar(v)%7D%7D)%7D%7B%5CPhi(%5Cfrac%7B%5Cbar%7Bv%7D%7D%7B%5Csqrt%7BVar(v)%7D%7D)%7D%5C%5C%0A%20%20=&amp;%20%20%5Cfrac%7B%5Csigma_1%5E2+w%5Crho%5Csigma_1%5Csigma_2%7D%0A%20%20%20%20%20%20%20%20%20%20%20%7B%5Csqrt%7B%5Csigma_1%5E2+w%5E2%5Csigma_2%5E2%20+%202%5Crho%20w%5Csigma_1%5Csigma_2%7D%7D%0A%20%20%20%5Cfrac%7B%5Cphi(%5Cfrac%7B%5Cbar%7Bv%7D%7D%7B%5Csqrt%7BVar(v)%7D%7D)%7D%7B%5CPhi(%5Cfrac%7B%5Cbar%7Bv%7D%7D%7B%5Csqrt%7BVar(v)%7D%7D)%7D%0A%5Cend%7Baligned%7D"></p>
<p>Next we will assume that the expected quantity of items is fixed. This implies that both both <img src="https://latex.codecogs.com/png.latex?P(v%5Cgeq%20%5Cbar%7Bv%7D)"> and <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cbar%7Bv%7D%7D%7B%5Csqrt%7BVar(v)%7D%7D"> will be constant, and we will define: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Cgamma%5Cequiv%0A%20%20%20%20%20%20%20%20%20&amp;%5Cfrac%7B%5Cphi%5Cleft(%5Cfrac%7B%5Cbar%7Bv%7D%7D%7B%5Csqrt%7BVar(v)%7D%7D%5Cright)%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%7B%5CPhi%5Cleft(%5Cfrac%7B%5Cbar%7Bv%7D%7D%7B%5Csqrt%7BVar(v)%7D%7D%5Cright)%7DP(v%5Cgeq%20%5Cbar%7Bv%7D)%20%5C%5C%0A%20%20%20%20%20%20X_1%20=&amp;%20%5Cfrac%7B%5Csigma_1%5E2+w%5Crho%5Csigma_1%5Csigma_2%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7B%5Csqrt%7B%5Csigma_1%5E2+w%5E2%5Csigma_2%5E2%20+%202%5Crho%20w%5Csigma_1%5Csigma_2%7D%7D%5Cgamma%20%5C%5C%0A%20%20%20%20%20%20X_2%20=&amp;%20%5Cfrac%7Bw%5Csigma_2%5E2+%5Crho%5Csigma_1%5Csigma_2%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%7B%5Csqrt%7B%5Csigma_1%5E2+w%5E2%5Csigma_2%5E2%20+%202%5Crho%20w%5Csigma_1%5Csigma_2%7D%7D%5Cgamma%0A%20%20%20%5Cend%7Baligned%7D"></p>
<p>We thus have expressions for <img src="https://latex.codecogs.com/png.latex?X_1"> and <img src="https://latex.codecogs.com/png.latex?X_2"> as a function of the relative weight <img src="https://latex.codecogs.com/png.latex?w">. We wish to rearrange these to express <img src="https://latex.codecogs.com/png.latex?X_1"> directly in terms of <img src="https://latex.codecogs.com/png.latex?X_2">. To help we turn to Mathematica, with the following input:<sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;<a href="https://www.wolframcloud.com/env/tomcunningham/Ellipses.nb">see notebook</a></p></div></div><pre><code>F1[w_,p_,s1_,s2_,g_]:=g(s1^2+w p s1 s2 )/Sqrt[s1^2+w^2 s2^2+2w p s1 s2]
F2[w_,p_,s1_,s2_,g_]:=g(w s2^2 +p s1 s2)/Sqrt[s1^2+w^2 s2^2+2w p s1 s2]
Solve[{X1==F1[w,p,s1,s2,g],X2==F2[w,p,s1,s2,g]}, {X1,w}]
Simplify[First[%1]]</code></pre>
<p>This returns a large expression:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20X_1(X_2)%20&amp;=%0A%20%20%20%20%20%20%5Cfrac%7B%0A%20%20%20%20%20%20%20%20%20%5Cgamma%5E2%20%5Crho%20%5Csigma_1%20%5Csigma_2%5E3%20X_2%0A%20%20%20%20%20%20%20%20%20-%20p%20%5Csigma_1%20%5Csigma_2%20X_2%5E3%0A%20%20%20%20%20%20%20%20%20+%20%5Cgamma%5E3%20%5Csigma_2%5E4%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Csqrt%7B%5Cfrac%7B-%5Cgamma%5E2%20(-1%20+%20p%5E2)%20%5Csigma_1%5E2%20%5Csigma_2%5E2%7D%7B%5Cgamma%5E2%20%5Csigma_2%5E2%20-%20X_2%5E2%7D%7D%0A%20%20%20%20%20%20%20%20%20-%20%5Cgamma%20%5Csigma_2%5E2%20X_2%5E2%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Csqrt%7B-%5Cfrac%7B%5Cgamma%5E2%20(-1%20+%20p%5E2)%20%5Csigma_1%5E2%20%5Csigma_2%5E2%7D%7B%5Cgamma%5E2%20%5Csigma_2%5E2%20-%20X_2%5E2%7D%7D%0A%20%20%20%20%20%20%20%20%20+%20X_2%20%5Csqrt%7B(-1%20+%20p%5E2)%20%5Csigma_1%5E2%20%5Csigma_2%5E2%20X_2%5E2%20(-%5Cgamma%5E2%20%5Csigma_2%5E2%20+%20X_2%5E2)%7D%0A%20%20%20%20%20%20%7D%7B%5Cgamma%5E2%20%5Csigma_2%5E4%20-%20%5Csigma_2%5E2%20X_2%5E2%7D%5Cend%7Baligned%7D"></p>
<p>We can however substantially simplify this: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20X_1%20&amp;=%20%5Cfrac%7B%0A%20%20%20%20%20%20%20%20%20%5Csigma_1%5Csigma_2X_2%20(%5Cgamma%5E2%20%5Crho%20%5Csigma_2%5E2%20%20-%20p%20X_2%5E2)%0A%20%20%20%20%20%20%20%20%20+%20%5Cgamma%5E2%5Csigma_2%5E2(%5Cgamma%5E2%20%5Csigma_2%5E2-%20%20X_2%5E2)%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Csqrt%7B-%5Cfrac%7B(-1%20+%20p%5E2)%20%5Csigma_1%5E2%20%5Csigma_2%5E2%7D%7B%5Cgamma%5E2%20%5Csigma_2%5E2%20-%20X_2%5E2%7D%7D%0A%20%20%20%20%20%20%20%20%20-%20X_2%5E2%5Csigma_1%5Csigma_2%20%5Csqrt%7B(p%5E2-1)%20(%5Cgamma%5E2%20%5Csigma_2%5E2-X_2%5E2)%7D%0A%20%20%20%20%20%20%7D%7B%5Cgamma%5E2%20%5Csigma_2%5E4%20-%20%5Csigma_2%5E2%20X_2%5E2%7D%20%5C%5C%0A%20%20%20&amp;=%20%5Cfrac%7B%0A%20%20%20%20%20%20%20%20%20%5Csigma_1%5Csigma_2X_2%20p(%5Cgamma%5E2%20%5Csigma_2%5E2%20%20-%20X_2%5E2)%0A%20%20%20%20%20%20%20%20%20+%20%5Cgamma%20%5Csigma_2%5E3%5Csigma_1%20%5Cgamma%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Csqrt%7B(p%5E2-1)(%5Cgamma%5E2%20%5Csigma_2%5E2%20-%20X_2%5E2)%7D%0A%20%20%20%20%20%20%20%20%20-%20X_2%5E2%5Csigma_1%5Csigma_2%20%5Csqrt%7B(p%5E2-1)%20(%5Cgamma%5E2%20%5Csigma_2%5E2-X_2%5E2)%7D%0A%20%20%20%20%20%20%7D%7B%5Csigma_2%5E2(%5Cgamma%5E2%20%5Csigma_2%5E2%20-%20X_2%5E2)%7D%20%5C%5C%0A%20%20%20&amp;=%20%5Cfrac%7B%5Csigma_1%7D%7B%5Csigma_2%7DX_2p%20+%20%5Cfrac%7B%0A%20%20%20%20%20%20%20%20%20%5Csigma_1%5Csigma_2(%5Cgamma%5E2%5Csigma_2%5E2%20-%20X_2%5E2%20)%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Csqrt%7B(p%5E2-1)%20(X_2%5E2-%5Cgamma%5E2%20%5Csigma_2%5E2)%7D%0A%20%20%20%20%20%20%7D%7B%5Csigma_2%5E2(%5Cgamma%5E2%20%5Csigma_2%5E2%20-%20X_2%5E2)%7D%20%5C%5C%0A%20%20%20&amp;=%20X_2%20%5Crho%20%5Cfrac%7B%5Csigma_1%7D%7B%5Csigma_2%7D%0A%20%20%20%20%20%20+%5Cfrac%7B%5Csigma_1%7D%7B%5Csigma_2%7D%5Csqrt%7B(p%5E2-1)%20(X_2%5E2-%5Cgamma%5E2%20%5Csigma_2%5E2)%7D.%0A%5Cend%7Baligned%7D"></p>
<div class="cell page-columns page-full" data-hash="2023-10-23-pareto-frontiers-experiments-ranking_cache/html/unnamed-chunk-9_26d4c4976c38a54dadcfd2f074fe4bde">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-10-23-pareto-frontiers-experiments-ranking_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div></div></div>
<p>We now wish to show that this curve is equal to an isovalue of the joint distribution of <img src="https://latex.codecogs.com/png.latex?x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2"> (illustrated at right). We can write the isovalue of the joint Normal distribution of <img src="https://latex.codecogs.com/png.latex?(x_1,x_2)"> for any given <img src="https://latex.codecogs.com/png.latex?k"> as follows:<sup>2</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;From Bertsekas and Tsitsiklis (2002) “Introduction to Probability”, <a href="http://athenasc.com/Bivariate-Normal.pdf">Section 4.7</a></p></div></div><p><img src="https://latex.codecogs.com/png.latex?k%20=%20%5Cfrac%7Bx_1%5E2%7D%7B%5Csigma_1%5E2%7D+%5Cfrac%7Bx_2%5E2%7D%7B%5Csigma_2%5E2%7D-2%5Crho%5Cfrac%7Bx_1x_2%7D%7B%5Csigma_1%5Csigma_2%7D."></p>
<p>Solving this quadratic we can write: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20x_1%20&amp;=%20x_2%20%5Crho%20%5Cfrac%7B%5Csigma_1%7D%7B%5Csigma_2%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5Cpm%20%5Cfrac%7B%5Csigma_1%7D%7B%5Csigma_2%7D%5Csqrt%7B-x_2%5E2+x_2%5E2%5Crho%5E2+k%5Csigma_2%5E2%7D%20%5C%5C%0A%20%20%20%20%20%20%20%20%20&amp;=%20x_2%20%5Crho%20%5Cfrac%7B%5Csigma_1%7D%7B%5Csigma_2%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5Cpm%20%5Cfrac%7B%5Csigma_1%7D%7B%5Csigma_2%7D%5Csqrt%7Bk%5Csigma_2%5E2-(1-%5Crho%5E2)x_2%5E2%7D.%0A%20%20%20%5Cend%7Baligned%7D"></p>
<p>We can see that this will be identical to the relationship between <img src="https://latex.codecogs.com/png.latex?X_1"> and <img src="https://latex.codecogs.com/png.latex?X_2"> above when <img src="https://latex.codecogs.com/png.latex?k=%5Cfrac%7B%5Csigma_2%5E2%7D%7B%5Csigma_1%5E2%7D(%5Crho%5E2-1)%5Cgamma%5E2">.</p>
</section>
<section id="appendix-simulations-for-non-normal-distributions" class="level1">
<h1>Appendix: Simulations for Non-Normal Distributions</h1>
<p>In this section I compare Pareto frontiers generated from different distribution of <img src="https://latex.codecogs.com/png.latex?(X,Y)"> from different joint distribution, and then draw the Pareto frontiers. There are a few points of interest:</p>
<ol type="1">
<li>Other distributions apart from the Normal do not have the property that the Pareto frontier is equal to an isovalue of the joint density.</li>
<li>In my examples the Pareto frontier of many other distributions does look roughly <em>elliptical</em> though not precisely an ellipse. This makes me more comfortable to use an ellipse as a first-order approximation of a Pareto frontier.</li>
<li></li>
</ol>
<p>These simulations show something slightly different from what is proved in the prior section. These simulations show that as the number of experiments is large (<img src="https://latex.codecogs.com/png.latex?N%5Crightarrow%5Cinfty">) then the Pareto frontier begins to resemble an ellipse. What was proved in the previous section is that for any given <img src="https://latex.codecogs.com/png.latex?N"> the <em>expected</em> Pareto frontier will be an ellipse, where we calculate the expected value of <img src="https://latex.codecogs.com/png.latex?X"> and <img src="https://latex.codecogs.com/png.latex?Y"> for a given rate of tradeoff.</p>
<p>In each case the left-hand plot shows the raw distribution, the right-hand plot shows the Pareto frontier.</p>
<p><strong>Joint normal with positive correlation, small <img src="https://latex.codecogs.com/png.latex?N">:</strong></p>
<div class="cell" data-hash="2023-10-23-pareto-frontiers-experiments-ranking_cache/html/unnamed-chunk-13_af094d4496b894f90166127fc7a66b06">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-10-23-pareto-frontiers-experiments-ranking_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>Joint normal with positive correlation, <img src="https://latex.codecogs.com/png.latex?N">=100:</strong></p>
<div class="cell" data-hash="2023-10-23-pareto-frontiers-experiments-ranking_cache/html/unnamed-chunk-14_7a1a4a7724d86b8e1820d30e8c4cf35a">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-10-23-pareto-frontiers-experiments-ranking_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>Joint <em>t</em> Distribution with 3 degrees of freedom, <img src="https://latex.codecogs.com/png.latex?N">=100.</strong></p>
<div class="cell" data-hash="2023-10-23-pareto-frontiers-experiments-ranking_cache/html/unnamed-chunk-15_0791ae8e12842a3be12a69778cf9344b">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-10-23-pareto-frontiers-experiments-ranking_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>Independent Laplace, <img src="https://latex.codecogs.com/png.latex?N">=100:</strong></p>
<div class="cell" data-hash="2023-10-23-pareto-frontiers-experiments-ranking_cache/html/unnamed-chunk-16_9ff74a98d3faff0bd434fe2836ef8b6b">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-10-23-pareto-frontiers-experiments-ranking_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>Common Laplace factor with independent Gaussian noise, <img src="https://latex.codecogs.com/png.latex?N">=100:</strong></p>
<div class="cell" data-hash="2023-10-23-pareto-frontiers-experiments-ranking_cache/html/unnamed-chunk-17_4b47c4fe4ae958a1626c78f30b1b9249">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-10-23-pareto-frontiers-experiments-ranking_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>Independent uniform, <img src="https://latex.codecogs.com/png.latex?N">=100:</strong></p>
<div class="cell" data-hash="2023-10-23-pareto-frontiers-experiments-ranking_cache/html/unnamed-chunk-18_c03ae8b0e073a517d43d7d0d20f8bbdd">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-10-23-pareto-frontiers-experiments-ranking_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>Common uniform factor plus independent uniform noise, <img src="https://latex.codecogs.com/png.latex?N">=100:</strong></p>
<div class="cell" data-hash="2023-10-23-pareto-frontiers-experiments-ranking_cache/html/unnamed-chunk-19_819b2d9ed5de70a785d9f4ae46503766">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-10-23-pareto-frontiers-experiments-ranking_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>



</section>


 ]]></description>
  <guid>tecunningham.github.io/posts/2023-10-23-pareto-frontiers-experiments-ranking.html</guid>
  <pubDate>Wed, 25 Oct 2023 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Experiment Interpretation and Extrapolation</title>
  <dc:creator>Tom Cunningham</dc:creator>
  <link>tecunningham.github.io/posts/2023-04-18-experiment-interpretation-extrapolation.html</link>
  <description><![CDATA[ 





<style>
    h1 {  border-bottom: 4px solid black;  }
    h2 {  border-bottom: 1px solid #ccc;}
    .header-section-number {color:black; }
    .example { border: 1px #ee9933 solid; background: #ffeecc; padding: 10px; }
</style>
<section id="introduction" class="level1 unnumbered page-columns page-full">
<h1 class="unnumbered">Introduction</h1>
<p><strong>I give a simple Bayesian way of thinking about experiments, and implications for interpretation and extrapolation.</strong></p>

<div class="no-row-height column-margin column-container"><div class="">
<p>Thanks to <a href="https://jmarkhou.com/about/">J. Mark Hou</a> for comments. <img src="tecunningham.github.io/posts/images/2023-10-13-11-56-04.png" class="img-fluid"></p>
</div></div><p><strong>Setup:</strong> The canonical tech problem is to choose a policy to maximize long-run user retention. Because the policy space is high-dimensional it’s not feasible to run experiments on every alternative (there are trillions), instead most of the decision-making is done with human intuition based on observational data, and experiments are run to confirm those intuitions.</p>
<ol type="1">
<li><p><strong>The inference problem.</strong> The basic problem of experimentation is to estimate the true effect given the observed effect. The problem can become complicated when we have a set of different observed effects, e.g.&nbsp;across experiments, across metrics, across subgroups, or across time. </p>
<p>Two common approaches are: (1) adjust confidence intervals (e.g.&nbsp;Bonferroni, always-valid, FDR-adjusted); (2) adjust point estimates based on the distribution (empirical Bayes). Both have significant drawbacks: my suggested approach is to let decision-makers make their own best-estimates of the true effects but provide them with an informative set of <em>benchmark</em> statistics so they can compare the results of any given experiment to the results from a reference group.<sup>1</sup></p></li>
<li><p><strong>The extrapolation problem.</strong> Given an effect on metric A what’s our best estimate of the effect on metric B? This problem is common to observational inference, proximal goals, and extrapolation.</p>
<p>There are three approaches to solving this: (1) using raw priors; (2) using correlation across units (surrogacy); (3) using correlation across experiments (meta-analysis). I argue that approach #3 is generally the best option but reasonable care needs to be taken in interpreting the results.</p></li>
</ol>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;If the decision-maker is not technical then a data scientist or engineer can summarize for the decision-maker their best-estimate of the true impact on long-run outcomes, taking into account the evidence from the experiment and other sources of evidence, including the distribution of effects from other experiments.</p></div></div><p>I also briefly discuss two additional problems:</p>
<ul>
<li><p><strong>The explore-exploit problem.</strong> We would like to choose which experiments to run in an efficient and automated way. I think the technical solution is relatively clear but tech companies have struggled to implement it because good execution requires some discipline. I describe a simple algorithm that is not optimal but very simple and robust.</p></li>
<li><p><strong>The culture problem.</strong> Inside tech companies people keep misusing experiments and misinterpreting the results, especially (1) running under-powered experiments, (2) selectively choosing results, and (3) looking at correlations without thinking about identification.</p>
<p>A common response is to restrict access to only a subset of experiment resuts. However this often backfires because (1) it is difficult to formally specify the right subset; (2) it reinforces a perception that experimental results can be interpreted as best-estimates of true treatment effects; (3) it reinforces a norm of selecting experimental results as arguments for a desired outcome. I think a better alternative is to explicitly frame the problem as one of predicting the true effect given imperfect evidence, and benchmark peoples’ prior performance in predicting the true effect of an intervention. (This section is unfinished, I hope to add more).</p></li>
</ul>
</section>
<section id="setup" class="level1 unnumbered page-columns page-full">
<h1 class="unnumbered">Setup</h1>
<p><strong>Firms choose their policy to maximize user retention.</strong> As a simplified model companies are choosing policies to maximize long-run retention (or revenue). A policy is, for example, a recommendation algorithm, or notification algorithm, or the text and images used in an advertisement or the UX on a signup page. Notice that policies are very high dimensional: there are millions or billions of alternatives, while we usually run only a few experiments.<sup>2</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;In fact variation in the success of tech platforms is primarily due to variation in the inflow of new users, not due to variation in retention rates. However growth in new users is driven by the attractiveness of the product and retention is a good proxy for this.</p></div></div><p><strong>Experiments and formal causal inference methods not the primary sources of causal knowledge.</strong> People already have substantial knowledge about the effects of their decisions without either randomized experiments or natural experiments (IV, RDD, etc.). We built cathedrals, aeroplanes, welfare states, we doubled human life-expectancy, &amp; WhatsApp grew to 1B users, all without randomized experiments or instrumental variables estimates. These achievements were all based on causal inference but <em>informal</em> causal inference, i.e.&nbsp;using our instinctive knowledge of how to process information without writing down or calculating the assumptions and distributions. Formal causal inference methods are useful but primarly insofar as they augment our already substantial causal abilities, and in most cases they clearly lag far behind humans intuitive ability to draw causal inferences. Inside companies the primary way people learn about causal relationships is raw data (e.g.&nbsp;dashboards) and common-sense reasoning about human behaviour.</p>
<p><strong>Experiments only solve the low-dimensional problem.</strong> In most cases the dimensionality of the policy space is far higher than the dimensionality of experiment space, thus the responsibility for choosing policies is primarily human judgment. Humans come up with a few policy variants of interest, and then run experiments to compare their performance.<sup>3</sup><sup>4</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Statistically the problem is analagous to drilling for oil: you drill test holes trying to understand the distribution underneath. A common algorithm for this is <a href="https://en.wikipedia.org/wiki/Kriging">“Kriging”</a> (the invention of Danie Krige in 1960 to model gold mining in South Africa) which in some cases is equivalent to Gaussian process regression.</p></div><div id="fn4"><p><sup>4</sup>&nbsp;There is a nice analogy to machine learning: computers have been able to outperform humans at low-dimensional prediction problems, e.g.&nbsp;linear regression, for the last 100 years, but only in the last 10 years have they caught up in high-dimensional problems like recognition of patterns in images, speech, and text. The implication is that humans have extraordinarily powerful ability to intuitively represent latent structures in high-dimensional data, and we shouldn’t expect simple algorithms to replace that ability.</p></div><div id="fn5"><p><sup>5</sup>&nbsp;For simplicitly assume the experiment doesn’t have any effect on variances or covariances of outcomes, the effects are typically small enough that it doesn’t matter.</p></div></div><p><strong>Most questions related to experiments can be expressed as conditional expectations.</strong> A good workhorse model of experimentation is the following. Suppose we have two metrics #1 and #2. Taking some set of experiments we can think of three joint distributions: the observed effects, the true effects, and the noise:<sup>5</sup></p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cutt%7B%5Cbinom%7B%5Chat%7Bt%7D_1%7D%7B%5Chat%7Bt%7D_2%7D%7D%7Bobserved%7D%7Beffects%7D%0A%20%20%20%20%20%20=%5Cutt%7B%5Cbinom%7Bt_1%7D%7Bt_2%7D%7D%7Btrue%7D%7Beffects%20(ATE)%7D%0A%20%20%20%20%20%20%20%20%20+%5Cut%7B%5Cbinom%7Be_1%7D%7Be_2%7D%7D%7Bnoise%7D%0A%20%20%20%20%20%20%20%20%20"></p>
<p>For simplicity we’ll assume everything is normally distributed and has mean zero, then we get two very simple expressions for conditional expectations, and I’ll argue that these conditional expectations serve as answers to almost all interesting experimentation questions:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20E%5Bt_1%7C%5Chat%7Bt%7D_1%5D%20&amp;=%20%5Cutt%7B%5Cfrac%7B%5Csigma_%7Bt1%7D%5E2%7D%7B%5Csigma_%7Bt1%7D%5E2+%5Csigma_%7Be1%7D%5E2%7D%7D%7Bsignal-noise%7D%7Bratio%7D%5Chat%7Bt%7D_1%0A%20%20%20%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7B(posterior%20estimate%20of%20treatment%20effect,%20AKA%20shrinkage)%7D%20%5C%5C%0A%20%20%20%20%20%20E%5Bt_2%7C%5Chat%7Bt%7D_1%5D%20&amp;=%20%5Cutt%7B%5Crho_%7Bt%7D%5Cfrac%7B%5Csigma_%7Bt2%7D%7D%7B%5Csigma_%7Bt1%7D%7D%7D%7Bcovariance%7D%7Bof%20$t_1$%20and%20$t_2$%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Cutt%7B%5Cfrac%7B%5Csigma_%7Bt1%7D%5E2%7D%7B%5Csigma_%7Bt1%7D%5E2+%5Csigma_%7Be1%7D%5E2%7D%7D%7Bsignal-noise%7D%7Bratio%20of%20$%5Chat%7Bt%7D_1$%7D%5Chat%7Bt%7D_1%0A%20%20%20%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7B(true%20effect%20on%20metric%202%20given%20observed%20effect%20on%20metric%201)%7D%0A%20%20%20%5Cend%7Baligned%7D%0A%20%20%20"></p>
<p>Once we have a clear expression in terms of conditional expectations we can add on additional considerations: nonlinearities, fat-tailed distributions, strategic problems, etc..</p>
<p></p>
</section>
<section id="the-inference-problem" class="level1 page-columns page-full" data-number="1">
<h1 data-number="1"><span class="header-section-number">1</span> The Inference Problem</h1>
<p><strong>There are a number of experiment inference problems that we often find difficult.</strong> We will discuss these as pure inference problems without worrying about strategic behaviour (e.g.&nbsp;peeking, cherry-picking).</p>
<ol type="1">
<li>Estimate the treatment effect given the observed treatment effect.</li>
<li>Estimate the long-run treatment effect knowing the short-run observed effect.</li>
<li>Estimate the treatment effect, knowing the observed effect, and additionally the distribution of observed effects across some set of experiments.</li>
<li>Estimate the treatment effect on a subgroup, knowing the observed effect, and additionally the distribution of observed effects across all other subgroups.</li>
</ol>
<p><strong>The textbook approach uses <em>p</em>-values.</strong> A common approach (NHST) is to treat the true effect as equal to the observed effect if the p-value is below 0.05, and otherwise treat the true effect as zero. This leads to all sorts of well-known difficulties.</p>
<p><strong>Empirical Bayes estimates are often imperfect.</strong> We could instead calculate empirical-Bayes conditional expectations, <img src="https://latex.codecogs.com/png.latex?E%5B%5Cbm%7Bt%7D%7C%5Chat%7B%5Cbm%7Bt%7D%7D%5D">, based on covariances from prior experiments, and treat those as the true effects. However the distribution of prior experiments is only a subset of the full information set available to the decision-maker, i.e.&nbsp;empirical Bayes is not Bayes, and very often there are idiosyncratic details about this particular experiment that are consequential.</p>
<p><strong>My recommendation: report “benchmark” statistics.</strong> The ideal decision process lets humans make a judgment about estimated treatment effects given three ingredients:</p>
<ol type="1">
<li><p><strong>Raw estimate.</strong> The point estimate and standard error.<sup>6</sup></p></li>
<li><p><strong>Benchmark statistic.</strong> We should also report a statistic comparing this observed effect to observed effects of other similar treatments. There are many ways of benchmarking and I think they are all convey the same basic information, e.g.&nbsp;the empirical-bayes shrunk estimate (and there are various shrinkage estimators), the FDR-adjusted p-value, or the fraction of statistically significant experiments. We have to use judgment in defining what a “similar” experiment is, and it’s important that we report to the end-user what class of similar experiments we’re using and how many we have. For the remainder of the section I will assume we are reporting an empirical-bayes shrunk estimate.</p></li>
<li><p><strong>Idiosyncratic details.</strong> We should additional report any information about this treatment relative to the benchmark class, that could be relevant to its effect on this metric. E.g. (1) suppose this experiment only affects iPhone users then it is rational to heavily discount any outcomes on Android use unless they are highly significant; (2) suppose this experiment is a direct replication of a prior experiment, then we will likely wish to shrink our estimates towards that prior experiment rather than towards the mean of all experiments.</p></li>
</ol>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;Equivalently, the point-estimate and p-value, or the upper and lower confidence bounds.</p></div></div><p><strong>Benchmarking solves all the problems above.</strong> An empirical-Bayes shrunk estimate represents our best guess at the true treatment effect conditional on the experiment being drawn from a given reference class.</p>
<p><strong>Useful shortcut: using the fraction of significant experiments to do shrinkage.</strong> A convenient rule of thumb for doing empirical Bayes shrinkage is to use the fraction of experiments that are statistically significant in some class. If the fraction is 5% then we should shrink all estimates to zero, if the fraction is 20% then we should shrink estimates by about 50%, and if the fraction is 1/2 then we should shrink estimates by about 20%. If everything’s Gaussian and every experiment has the same <img src="https://latex.codecogs.com/png.latex?N"> then the optimal shrinkage factor is <img src="https://latex.codecogs.com/png.latex?1-(%5Cfrac%7B1%7D%7B1.96%7D%5CPhi%5E%7B-1%7D(%5Cfrac%7Bq%7D%7B2%7D))%5E2">, where <img src="https://latex.codecogs.com/png.latex?q"> is the fraction of stat-sig experiments.</p>
<section id="strategic-problems" class="level2" data-number="1.1">
<h2 data-number="1.1" class="anchored" data-anchor-id="strategic-problems"><span class="header-section-number">1.1</span> Strategic Problems</h2>
<p><strong>There are additionally some <em>strategic</em> problems in experiment interpretation.</strong></p>
<ol type="1">
<li><p><strong>Strategic stopping (“peeking”).</strong> An engineer will wait until an experiment has a high estimated impact, or low p-value, before presenting it for launch review. A common proposed remedy is that all experiments should be evaluated after the same length of time, or that engineers should pre-specify the length of experiments.</p></li>
<li><p><strong>Selection of treatments (“winners curse”).</strong> An engineer will run a dozen variants and only present for launch review the best-performing one. A common proposed remedy is that every variant should be officially presented in launch reviews, even the poorly-performing ones.</p></li>
<li><p><strong>Selection of metrics (“cherry picking”).</strong> An engineer will choose to show the experiment results on the metrics that are favorable, not those that are unfavorable. A common proposed remedy is that the set of metrics should be standardized for all launches, or that the set of evaluation metrics should be pre-specified by the engineer (AKA a pre-analysis plan).</p></li>
</ol>
<p>I will argue that the commonly proposed remedies are highly imperfect fixes. These are complicated things to think about because the mix together issues of statistical inference and of strategic behaviour. In the discussion that follows I try to separate those out as clearly as possible.</p>
</section>
<section id="strategic-stopping" class="level2 page-columns page-full" data-number="1.2">
<h2 data-number="1.2" class="anchored" data-anchor-id="strategic-stopping"><span class="header-section-number">1.2</span> Strategic Stopping</h2>
<p><strong>I will ignore dynamic effects.</strong> For simplicity assume that all effects are constant, so the length of an experiment effectively determines just the sample size of that experiment. I.e. I will ignore time-dependent and exposure-dependent effects.</p>
<p><strong>Stopping rules are irrelevant to expected effect sizes.</strong> Suppose an experiment has a given estimate. Does it matter to your estimate of the true causal effect if you learn that the experimenter chose the sample size <img src="https://latex.codecogs.com/png.latex?N"> by a data-dependent rule, e.g.&nbsp;continuing to collect data until the estimate was statistically significant? If you are estimating the true causal effect, <img src="https://latex.codecogs.com/png.latex?E%5Bt%7C%5Chat%7Bt%7D%5D"> then it doesn’t matter, your posterior will be identical either way.<sup>7</sup> A simple proof: suppose we observe two noisy signals, <img src="https://latex.codecogs.com/png.latex?x_1"> and <img src="https://latex.codecogs.com/png.latex?x_2">: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20x_1%20&amp;=%20v%20+%20e_1%20%5C%5C%0A%20%20%20%20%20%20x_2%20&amp;=%20v%20+%20e_2%20%5C%5C%0A%20%20%20%20%20%20v,e_1,e_2%20&amp;%5Csim%20%20N(0,1)%0A%20%20%20%5Cend%7Baligned%7D"> Suppose a peeker will report <img src="https://latex.codecogs.com/png.latex?x_1"> only if <img src="https://latex.codecogs.com/png.latex?x_1%3E0">, otherwise they will report <img src="https://latex.codecogs.com/png.latex?x_1+x_2">. We can compare the expectation of <img src="https://latex.codecogs.com/png.latex?v"> given the sum, depending on whther the engineer peeked: <img src="https://latex.codecogs.com/png.latex?%5Cutt%7BE%5Bv%7Cx_1+x_2%5D%7D%7Bestimate%7D%7Bwithout%20peeking%7D%20=%0A%20%20%20%20%20%20%5Cutt%7BE%5Bv%7Cx_1+x_2%7Cx_1%3C0%5D%7D%7Bestimate%7D%7Bwith%20peeking%7D"> This holds because <img src="https://latex.codecogs.com/png.latex?x_1+x_2"> is a sufficient statistic for the distribution, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?x_1%3C0"> does not tell us any additional information. Note that peeking is not irrelevant to interpretation of a result if (1) the engineer can choose to report either <img src="https://latex.codecogs.com/png.latex?x_1"> or <img src="https://latex.codecogs.com/png.latex?x_2">, (2) the engineer can choose to report <img src="https://latex.codecogs.com/png.latex?x_1"> alone <em>after</em> observing <img src="https://latex.codecogs.com/png.latex?x_2">.<sup>8</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;This argument holds if the engineer always has to report the most-recent estimate. If they can choose to ignore later datapoints, and report an earlier result, this is essentially a “selection of metrics” case as below, and so the selection rule <em>is</em> relevant for interpretation.</p></div><div id="fn8"><p><sup>8</sup>&nbsp;See <span class="citation" data-cites="deng2016continuous">Deng et al. (2016)</span> for a fuller argument that stopping rules are irrelevant, and a review of the prior literature.</p></div></div><p><strong>Stopping rules <em>would</em> be relevant if we made decisions based on statistical-significance.</strong> A stopping rule would be relevant if we conditioned only on statistical-signficance instead of the full estimate. In other words the expected true effect, conditioning only on whether or not the estimated effect is statistically significant, will depend on the stopping rule. For example if people kept running experiments until they were significant then significant experiments would tend to have small effect sizes. However it is clearly bad practice to condition only on this binary piece of information when you have the full estimate, and if you have the full estimate then the stopping rule becomes irrelevant.</p>
<p><strong>The optimal stopping rule is data-dependent.</strong> The discussion above took a stopping rule as given, we can also ask what’s the efficient stopping rule. It’s clear that a fixed length is inefficient: we should stop an experiment sooner if it does unexpectedly well or unexpectedly badly, in both of those cases the value of collecting more information has decreased because it’s less likely to change our mind about a launch decision. Thus enforcing a static or pre-specific experiment length will lead to inefficient decision-making.</p>
<p><strong>Considering engineers’ incentives.</strong> Now consider the launch process as a game, with the engineers trying to persuade the director to launch their feature. Suppose the director’s <em>ex post</em> optimal strategy is to launch if <img src="https://latex.codecogs.com/png.latex?E%5Bt%7C%5Chat%7Bt%7D%5D%3E0">, and suppose the engineers get a bonus whenever their feature is launched. In equilibrium the engineers will keep their experiments running until <img src="https://latex.codecogs.com/png.latex?E%5Bt%7C%5Chat%7Bt%7D%5D%3E0">, which will cause a skew distribution: the distribution of posteriors will show a cluster just above the threshold. The director’s strategy is <em>ex post</em> optimal but it’s not an efficient use of experimentation resources. In this game the director would likely wish to pre-commit to a different threshold which induces more efficient effort by engineers. However a more direct solution would be to align engineers’ incentives with those of the director by rewarding them for their true impact, i.e.&nbsp;setting their bonuses proportional to <img src="https://latex.codecogs.com/png.latex?%5Cmax%5C%7BE%5Bt%7C%5Chat%7Bt%7D%5D,0%5C%7D">, instead of discontinuously rewarding them for whether or not they launched.</p>
</section>
<section id="selection-of-treatments" class="level2 page-columns page-full" data-number="1.3">
<h2 data-number="1.3" class="anchored" data-anchor-id="selection-of-treatments"><span class="header-section-number">1.3</span> Selection of Treatments</h2>
<p><strong>If you learn an experiment is the top-performing variant it should change your asssessment.</strong> Suppose we have a result <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bt%7D_1">, and we are estimating the true treatment effect, <img src="https://latex.codecogs.com/png.latex?t_1">. If we learn that another variant has a lower treatment effect, <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bt_1%7D%3E%5Chat%7Bt%7D_2">, then it is rational to update our assessment of <img src="https://latex.codecogs.com/png.latex?t_1">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cutt%7BE%5Bt_1%7C%5Chat%7Bt%7D_1,%5Chat%7Bt%7D_1%3E%5Chat%7Bt%7D_2%5D%7D%7Bassessment%20knowing%7D%7Bit's%20winner%7D%3C%0A%20%20%20%20%20%20%5Cutt%7BE%5Bt_1%7C%5Chat%7Bt%7D_1%5D%7D%7Bassessment%7D%7Bgiven%20outcome%7D%0A%20%20%20%20%20%20"></p>
<p>This will hold whenever <img src="https://latex.codecogs.com/png.latex?Cov(t_1,t_2)%3E0">, i.e.&nbsp;when we have some shared source of uncertainty about the two treatment effects.<sup>9</sup> We can write a model for this, however conditioning on this binary information (whether a variant is the winner) is not an efficient way of using the information at your disposal.</p>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;Because <img src="https://latex.codecogs.com/png.latex?t_1"> and <img src="https://latex.codecogs.com/png.latex?t_2"> represent independent experiments we’ll have <img src="https://latex.codecogs.com/png.latex?cov(e_1,e_2)=0">.</p></div><div id="fn10"><p><sup>10</sup>&nbsp;<span class="citation" data-cites="andrews2019inference">Andrews et al. (2019)</span> describes some unbiased estimates for treatment effects conditional on them being winners. In general I would say this is an inefficient use of information, because we know much more about the distribution of treatment effects than just whether a specific variant is the winner. However that paper does argue that empirical Bayes estimates struggle when the sample-size is small or when we are estimating the tails of when variants are non-exchangeable, and in those cases the unbiased estimators may be useful.</p></div></div><p><strong>It’s better to condition on the whole distribution.</strong> In almost all cases we know much more than whether <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bt%7D_1"> is the winner, we also know the value of <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bt%7D_2">, and then this reduces simply to the empirical Bayes problem, i.e.&nbsp;we simply wish to estimate: <img src="https://latex.codecogs.com/png.latex?E%5Bt_1%7C%5Chat%7Bt%7D_1,%5Cldots,%5Chat%7Bt%7D_n%5D,"> and we can do that in the usual way.<sup>10</sup> E.g. if we have a Normal prior over treatment effects then we can estimate <img src="https://latex.codecogs.com/png.latex?%5Csigma_t%5E2"> from <img src="https://latex.codecogs.com/png.latex?Var(%5Chat%7Bt%7D)"> and <img src="https://latex.codecogs.com/png.latex?%5Csigma_e%5E2">. Once we have conditioned on <img src="https://latex.codecogs.com/png.latex?%5Csigma_t%5E2"> then it becomes irrelevant whether variant 1 is the winner or not, i.e.: <img src="https://latex.codecogs.com/png.latex?E%5Bt_1%7C%5Chat%7Bt%7D_1,%5Csigma_t%5E2%5D=E%5Bt_1%7C%5Chat%7Bt%7D_1,%5Csigma_t%5E2,%5Chat%7Bt%7D_1%3E%5Chat%7Bt%7D_2%5D."></p>
<p>Put another way: the selection rule is irrelevant (just as the stopping rule is irrelevant) once we condition on the distribution of observed outcomes.</p>
<p><strong>Implication: show the distribution.</strong> If we are worried that engineers are selecting variants based on their outcomes then the simplest and cleanest fix is to calculate the distribution of variants and use that to discount any experiment results, either explicitly with an empirical Bayes estimator, or implicitly by showing the decision-maker the distribution.</p>
</section>
<section id="selection-of-metrics" class="level2" data-number="1.4">
<h2 data-number="1.4" class="anchored" data-anchor-id="selection-of-metrics"><span class="header-section-number">1.4</span> Selection of Metrics</h2>
<p><strong>Suppose engineers are selectively presenting the most favorable metrics.</strong> Suppose there are two outcome metrics from a single experiment, and the engineer will present whichever is the most favorable. Knowing this fact should rationally affect your judgment of the treatment effect on the presented metric: <img src="https://latex.codecogs.com/png.latex?%5Cutt%7BE%5Bt_1%7C%5Chat%7Bt%7D_1%5D%7D%7Bassessment%20knowing%7D%7Bonly%20metric%201%7D%20%3E%0A%20%20%20%20%20%20%5Cutt%7BE%5Bt_1%7C%5Chat%7Bt%7D_1,%5Chat%7Bt%7D_1%3E%5Chat%7Bt%7D_2%5D%7D%7Bassessment%20knowing%7D%7Bmetric%201%20beats%20metric%202%7D"></p>
<p><strong>Implication: engineers should present all outcome metrics.</strong></p>
</section>
<section id="on-launch-criteria" class="level2 page-columns page-full" data-number="1.5">
<h2 data-number="1.5" class="anchored" data-anchor-id="on-launch-criteria"><span class="header-section-number">1.5</span> On Launch Criteria</h2>
<p><strong>Choosing weights on metrics for a launch decisions involves many considerations:</strong> network effects, noise, cross-metric proxy effects, and dynamic effects. In addition launch rules serve a bureaucratic role, and engineers will often want the launch rule to be public and without discretion. To make clear decisions it’s important to peel apart these layers, I recommend these steps:</p>
<ol type="1">
<li><p><strong>Choose a set of final metrics.</strong> These are the metrics we would care about <em>if we had perfect knowledge of the experimental effect.</em> We can define tradeoffs between them, it’s convenient to express those tradeoffs in terms of percentage changes, e.g.&nbsp;we might be indifferent between 1% DAU, 2% time/DAU, and 5% prevalence of bad content.<sup>11</sup></p></li>
<li><p><strong>Choose a set of proximal metrics.</strong> These are the metrics on which we are confident we can detect our experiment’s effect, meaning the measured impact will be close to the true impact on these metrics (i.e.&nbsp;has a high signal-noise ratio). To determine whether a metric is moved we can use the fraction of a given class of experiments that have a statistically-significant effect on that metric: if the share is greater than 50% then we can be confident that the estimated effect is close to the true effect.</p></li>
<li><p><strong>Identify <em>conversion factors</em> between proximal and final metrics.</strong> These tell us the best-estimate impact on final metrics given the impact on proximal metrics. Conversion factors can be estimated either from (a) long-running tuning experiments; (b) a meta-analysis of prior experiments with similar designs.</p>
<p>A final linear launch criteria can then be expressed as a set of conversion-factor weights applied to each of the proximal metrics.<sup>12</sup></p></li>
</ol>
<div class="no-row-height column-margin column-container"><div id="fn11"><p><sup>11</sup>&nbsp;Arguably revenue or profit is a more truly final metric, and these are just proxies, but these are probably close enough to final for most purposes.</p></div><div id="fn12"><p><sup>12</sup>&nbsp;For derivation see <span class="citation" data-cites="cunningham2019interpreting">Cunningham and Kim (2019)</span>.</p></div></div></section>
<section id="comparing-launch-rules" class="level2 page-columns page-full" data-number="1.6">
<h2 data-number="1.6" class="anchored" data-anchor-id="comparing-launch-rules"><span class="header-section-number">1.6</span> Comparing Launch Rules</h2>
<div class="cell page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-04-18-experiment-interpretation-extrapolation_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div></div></div>
<div class="cell page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-04-18-experiment-interpretation-extrapolation_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Ship if sum is positive</figcaption>
</figure>
</div>
</div></div></div>
<div class="cell page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-04-18-experiment-interpretation-extrapolation_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div></div></div>
<p><strong>I find it useful to visualize different launch rules.</strong> For simplicity suppose our utility function is linear: we have two metrics, 1 and 2, and we care about them equally: <img src="https://latex.codecogs.com/png.latex?U(t_1,t_2)=t_1+t_2."> But we only observe noisy estimates <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bt%7D_1,%5Chat%7Bt%7D_2">.</p>
<p><strong><span class="citation" data-cites="kohavi2020trustworthy">Kohavi et al. (2020)</span> recommend a stat-sig shipping rule.</strong> They say (p105):</p>
<ol type="1">
<li>If no metrics are positive-significant then do not ship</li>
<li>If some are positive-significant and none are negative-significant then ship</li>
<li>If some are positive-significant and some are negative-significant then “decide based on the tradeoffs.</li>
</ol>
<p>I represent this in the first diagram (but I treat condition 3 as a non-ship). The dotted line represents <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bt%7D_1+%5Chat%7Bt%7D_2=0">.</p>
<p><strong>The stat-sig shipping rule has strange consequences.</strong> You can see that this rule will recommend shipping things even with <em>negative</em> face-value utility (<img src="https://latex.codecogs.com/png.latex?U(%5Chat%7Bt%7D_1,%5Chat%7Bt%7D_2)%3C0">), when there’s a negative outcome on the relatively noisier metric. This will still hold if we evaluate utility with shrunk estimates, when there’s equal proportional shrinkage on the two metrics, but if there’s greater shrinkage on the noisier metric it will not hold.</p>
<p><strong>Linear shipping rules are better.</strong> In the margin I illustrate (1) a rule to ship wherever the sum is positive; (2) a rule to ship wherever the sum is stat-sig positive. I have drawn the second assuming that <img src="https://latex.codecogs.com/png.latex?cov(%5Chat%7Bt%7D_1,%5Chat%7Bt%7D_2)=0">. With a positive covariance the threshold would be higher.</p>
<div class="cell page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-04-18-experiment-interpretation-extrapolation_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div></div></div>
<p><strong>The Leontief sandwich.</strong> I assumed above that our true utility function is linear. In fact tech companies often explicitly give nonlinear objective functions to teams, e.g.: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Cmax_k%20&amp;%5C%20A(k)%0A%20%20%20%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7B(goal)%7D%20%5C%5C%0A%20%20%20%20%20%20%5Ctext%7Bs.t.%7D%20&amp;%5C%20B(k)%5Cleq%20%5Cbar%7BB%7D%0A%20%20%20%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7B(guardrail)%7D%0A%20%20%20%5Cend%7Baligned%7D%0A%20%20%20"></p>
<p>This is illustrated at right, the indifference curves are L-shaped so I’ll call it a Leontief utility. Having Leontief preferences can cause some unintuitive decision-making, in particular the tradeoff between <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?B"> will varies drastically depending on your location. One important observation is that if your goal is assessed at the end of some time-point (e.g.&nbsp;at the end of the half) then optimal launch decisions will depend on your future <em>expectations</em>, e.g.&nbsp;you’d be willing to launch a feature that boosts A at the cost of B only if you expect a future launch to make up that deficit in B.</p>
<p>In practice I think it’s useful to think of this nonlinear objective function as sitting in the middle of the hierarchy of an organization, with approximately linear objective functions above and below it, i.e.&nbsp;a “Leontief sandwich.”</p>
<p>At the highest layer the CEO (or shareholders) care about all the metrics in way that is locally linear, i.e.&nbsp;they do not have sharp discontinuities in how they assess the company’s health. At the lowest layer engineers and data scientists are trying to make individual changes that achieve the Org’s overall goals, but because they only account for a small share of the overall org’s impact they can treat their objectives as locally linear (&amp; likewise in a value function we make linear tradeoffs between objectives because we’re in such a small region). Finally even for orgs which have nonlinear objective functions it’s often reasonable to think of the nonlinearities as “soft”, e.g.&nbsp;if an org comes in slightly below a guardrail the punishment is slight, and if they come in above the guardrail then they will be rewarded. This softening makes the effective objective function much closer to linear, and so I think for many practical purposes it’s reasonable to start with a linear objective function.</p>
</section>
</section>
<section id="the-extrapolation-problem" class="level1 page-columns page-full" data-number="2">
<h1 data-number="2"><span class="header-section-number">2</span> The Extrapolation Problem</h1>
<p><strong>Many problems are predicting the effect one one metric (downstream) given the effect on another metric (upstream).</strong> There are a variety of situations in which we cannot measure the effect on the downstream metric, either because it has high noise, or it is in the future:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 31%">
<col style="width: 68%">
</colgroup>
<thead>
<tr class="header">
<th>upstream</th>
<th>downstream</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>short-run revenue</td>
<td>long-run revenue</td>
</tr>
<tr class="even">
<td>click</td>
<td>purchase</td>
</tr>
<tr class="odd">
<td>engagement on content</td>
<td>response to survey (“do you like this content?”)</td>
</tr>
<tr class="even">
<td>engagement on content</td>
<td>retention</td>
</tr>
<tr class="odd">
<td>exposure to content</td>
<td>retention</td>
</tr>
<tr class="even">
<td>time on surface X</td>
<td>time on all surfaces</td>
</tr>
<tr class="odd">
<td>purchase</td>
<td>repeat purchase</td>
</tr>
<tr class="even">
<td>wait-time for delivery</td>
<td>retention</td>
</tr>
<tr class="odd">
<td>price</td>
<td>quantity purchased</td>
</tr>
</tbody>
</table>
<p></p>
<p></p>
<p></p>
<p>For concreteness we will treat the problem of predicting the long-run (LR) effect of an experiment on DAU from its short-run (SR) estimated effects on all metrics:</p>
<p><img src="https://latex.codecogs.com/png.latex?E%5B%5Cutt%7B%5CDelta%5Ctext%7BDAU%7D_%7BLR%7D%7D%7Btrue%20long-run%7D%7Beffect%20on%20DAU%7D%20%7C%0A%20%20%20%20%20%20%20%5Cutt%7B%5CDelta%20%5Cwidehat%7B%5Ctext%7BDAU%7D%7D_%7BSR%7D,%20%5Cldots,%20%5CDelta%5Cwidehat%7B%5Ctext%7Bengagement%7D%7D_%7BSR%7D%7D%7Bestimated%20short-run%20effects%7D%7B%7D%5D%0A%20%20%20%20%20%20"></p>
<p>There are two obvious ways to calculate this:</p>
<ol type="1">
<li><p><strong>Meta-analysis.</strong> We can run a regression across prior experiments: <img src="https://latex.codecogs.com/png.latex?%5CDelta%5Cwidehat%7B%5Ctext%7BDAU%7D%7D_%7BLR%7D%20%5Csim%0A%20%20%20%20%5CDelta%20%5Cwidehat%7B%5Ctext%7BDAU%7D%7D_%7BSR%7D%20+%20%5Cldots%20+%20%5CDelta%5Cwidehat%7B%5Ctext%7Bengagement%7D%7D_%7BSR%7D%0A%20%20%20"></p>
<p>However the coefficients will be biased if we use on the LHS the <em>observed</em> long-run DAU, instead of the <em>true</em> long-run DAU. This bias is often large, and in fact if you run a bunch of AA tests (where the causal effect is zero) you’ll find strong significant relationships between short-run and long-run impacts. I discuss below ways in which to adjust for this bias.</p></li>
<li><p><strong>Observational Inference.</strong> We can run a regression across users: <img src="https://latex.codecogs.com/png.latex?%5Ctext%7BDAU%7D_%7BLR%7D%20%5Csim%0A%20%20%20%20%5Ctext%7BDAU%7D_%7BSR%7D%20+%20%5Cldots%20+%20%5Ctext%7Bengagement%7D_%7BSR%7D%0A"></p>
<p>We can look at what is most predictive of long-run DAU across users. The problem here is obviously endogeneity, and so it’s worth spending time drawing a DAG and running robustness tests to carefully think through the sources of variation we’re using.</p></li>
</ol>
<section id="with-meta-analysis" class="level2 page-columns page-full" data-number="2.1">
<h2 data-number="2.1" class="anchored" data-anchor-id="with-meta-analysis"><span class="header-section-number">2.1</span> With Meta-Analysis</h2>
<p>With <img src="https://latex.codecogs.com/png.latex?n"> metrics we can write the underlying model as: <img src="https://latex.codecogs.com/png.latex?%5Cutt%7B%5Cpmatrix%7B%5Chat%7Bt%7D_1%5C%5C%5Cvdots%5C%5C%5Chat%7Bt%7D_n%7D%7D%7Bobserved%7D%7Beffects%7D%0A%20%20%20%20%20%20=%20%5Cutt%7B%5Cpmatrix%7Bt_1%5C%5C%5Cvdots%5C%5Ct_n%7D%7D%7Btrue%7D%7Beffects%7D%0A%20%20%20%20%20%20%20%20%20+%5Cutt%7B%5Cpmatrix%7Be_1%5C%5C%5Cvdots%5C%5Ce_n%7D%7D%7Bnoise%7D%7B(=user%20variation)%7D%0A%20%20%20"></p>
<p>Here we are treating <img src="https://latex.codecogs.com/png.latex?%5CDelta%20%5Ctext%7BDAU%7D_%7BSR%7D"> and <img src="https://latex.codecogs.com/png.latex?%5CDelta%20%5Ctext%7BDAU%7D_%7BLR%7D"> as two different metrics, but for some experiments we only observe the first. We thus want to estimate the effect on long-run retention (DAU<img src="https://latex.codecogs.com/png.latex?_%7BLR%7D">) given short-run metrics. <img src="https://latex.codecogs.com/png.latex?E%5B%5CDelta%5Ctext%7BDAU%7D_%7BLR%7D%20%7C%0A%20%20%20%20%20%20%20%5CDelta%20%5Cwidehat%7B%5Ctext%7BDAU%7D%7D_%7BSR%7D,%20%5Cldots,%20%5CDelta%5Cwidehat%7B%5Ctext%7Bengagement%7D%7D_%7BSR%7D%5D%0A%20%20%20"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5CDelta%5Ctext%7BDAU%7D_%7BLR%7D%20%20%20&amp;=%20%5Ctextit%7Btrue%7D%5Ctext%7B%20effect%20on%20long-run%20daily%20active%20users%20(AKA%20retention)%7D%5C%5C%0A%20%20%20%20%20%20%5CDelta%5Cwidehat%7B%5Ctext%7BDAU%7D%7D_%7BSR%7D%20&amp;=%20%5Ctextit%7Bestimated%7D%5Ctext%7B%20effect%20on%20short-run%20daily%20active%20users%7D%20%5C%5C%0A%20%20%20%20%20%20%5CDelta%5Cwidehat%7B%5Ctext%7Bengagement%7D%7D_%7BSR%7D%20&amp;=%20%5Ctextit%7Bestimated%7D%5Ctext%7B%20effect%20on%20short-run%20engagement%7D%0A%20%20%20%5Cend%7Baligned%7D%0A%20%20%20"></p>
<p><strong>Running a Regression will be Biased.</strong> The obvious thing to do is run a regression across experiments: <img src="https://latex.codecogs.com/png.latex?%5CDelta%5Cwidehat%7B%5Ctext%7BDAU%7D%7D_%7BLR%7D%20%5Csim%0A%20%20%20%20%20%20%5CDelta%20%5Cwidehat%7B%5Ctext%7BDAU%7D%7D_%7BSR%7D%20+%20%5Cldots%20+%20%5CDelta%5Cwidehat%7B%5Ctext%7Bengagement%7D%7D_%7BSR%7D%0A%20%20%20"></p>
<p>However this will be biased. The simplest way to demonstrate the bias is to show that even with AA tests (where there is zero treatment effect on either metric) we will still get a strong predictive relationship between the observed treatment effects on each of the two metrics (see figure).</p>

<div class="no-row-height column-margin column-container"><div class="">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/images/2022-04-08-09-34-41.png" class="img-fluid figure-img"></p>
<figcaption>A simulated scatter-plot showing 20 experiments, with N=1,000,000, <img src="https://latex.codecogs.com/png.latex?%5Csigma_%7Be1%7D%5E2=%5Csigma_%7Be2%7D%5E2=1">, with correlation 0.8. The experiments are all AA-tests, i.e.&nbsp;there are no true treatment effects, yet a regression of <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bt%7D_2"> on <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bt%7D_1"> will consistently yield statistically-significant coefficients of around 0.8.</figcaption>
</figure>
</div>
</div></div><p>The bias is because in the regression our LHS variable is <em>estimated</em> retention (<img src="https://latex.codecogs.com/png.latex?%5CDelta%5Cwidehat%7B%5Ctext%7BDAU%7D%7D_%7BLR%7D"> instead of <img src="https://latex.codecogs.com/png.latex?%5CDelta%5Ctext%7BDAU%7D_%7BLR%7D">), and the noise in that estimate will be correlated with the noise in the estimates of short-run metrics. In the linear bivariate case (where we have just one RHS variable) then we can write: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Cut%7B%5Cfrac%7Bcov(%5Chat%7Bt%7D_2,%5Chat%7Bt%7D_1)%7D%7Bvar(%5Chat%7Bt%7D_1)%7D%7D%7Bregression%7D%0A%20%20%20%20%20%20=%20%5Cutt%7B%5Cfrac%7Bcov(t_2,%5Chat%7Bt%7D_1)%7D%7Bvar(%5Chat%7Bt_1%7D)%7D%7D%7Bwhat%20we%7D%7Bwant%20to%20know%7D%0A%20%20%20%20%20%20%20%20%20+%20%5Cut%7B%5Cfrac%7Bcov(e_2,e_1)%7D%7Bvar(%5Chat%7Bt%7D_1)%7D%7D%7Bbias%7D%0A%20%20%20%5Cend%7Baligned%7D%0A%20%20%20"></p>
<p>The bias will be small if the short-run metrics have high signal-noise ratios (SNR), <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7Bvar(t_1)%7D%7Bvar(e_1)%7D%5Cgg%200">. A simple test for SNR ratio is the distribution of p-values: if most experiments are significant then the SNR is high. However in the typical case (1) <img src="https://latex.codecogs.com/png.latex?%5CDelta%20%5Cwidehat%7B%5Ctext%7BDAU%7D%7D_%7BSR%7D"> is the best predictor of <img src="https://latex.codecogs.com/png.latex?%5CDelta%20%5Cwidehat%7B%5Ctext%7BDAU%7D%7D_%7BLR%7D">; and (2) <img src="https://latex.codecogs.com/png.latex?%5CDelta%20%5Cwidehat%7B%5Ctext%7BDAU%7D%7D_%7BSR%7D"> has a low signal-noise ratio (i.e.&nbsp;few outcomes are stat-sig). This means the bias is large, and so results are hard to interpret.</p>
<section id="adjusting-for-the-bias" class="level3 page-columns page-full" data-number="2.1.1">
<h3 data-number="2.1.1" class="anchored" data-anchor-id="adjusting-for-the-bias"><span class="header-section-number">2.1.1</span> Adjusting for the Bias</h3>
<p>Here are some alternatives:</p>
<ol type="1">
<li><p><strong>Run a regression just using the high-SNR metrics.</strong> We could just drop <img src="https://latex.codecogs.com/png.latex?%5CDelta%5Cwidehat%7B%5Ctext%7BDAU%7D%7D_%7BSR%7D"> as a regressor because of the bias, but we lose predictive power (<img src="https://latex.codecogs.com/png.latex?R%5E2">) so it’s hard to know when this will be a good idea without an explicit model.</p></li>
<li><p><strong>Adjust for bias in linear estimator.</strong> If we want a linear estimator then we can estimate and adjust for the bias. <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%5Cutt%7B%5Cfrac%7Bcov(t_2,%5Chat%7Bt%7D_1)%7D%7Bvar(%5Chat%7Bt_1%7D)%7D%7D%7BBLUE%20for%7D%7B$t_2$%20given%20$%5Chat%7Bt%7D_1$%7D%0A%20%20%20%20%20%20&amp;=%20%5Cfrac%7Bcov(t_2,t_1)%7D%7Bvar(%5Chat%7Bt%7D_1)%7D%0A%20%20%20%20%20%20=%20%5Cut%7B%5Cfrac%7Bcov(%5Chat%7Bt%7D_2,%5Chat%7Bt%7D_1)%7D%7Bvar(%5Chat%7Bt%7D_1)%7D%7D%7Bregression%20result%7D%0A%20%20%20%20%20%20%20%20%20-%20%5Cutt%7B%5Cfrac%7Bcov(e_2,e_1)%7D%7Bvar(%5Chat%7Bt%7D_1)%7D%7D%7Bobservable%7D%7Bvariables%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>If everything is joint normal then the expectation is itself linear, and so this will be optimal. In practice the true distribution of effect-sizes is somewhat fat-tailed, which imply that the conditional expectation will be nonlinear in the observables. Nevertheless I think this is a good start. (One other complication is that the SNR is more complicated to calculate when experiments vary in their sample size).<sup>13</sup></p></li>
<li><p><strong>Use experiment splitting.</strong> You can randomly assign users in each experiment to one or other sub-experiments. You now effectively have a set of <em>pairs</em> of experiments, each of which has experiments with identical treatment effects (<img src="https://latex.codecogs.com/png.latex?%5CDelta%20%5Ctext%7BDAU%7D_%7BLR%7D">) but independent noise. Thus you can run a regression with LHS from one split, and RHS from other split, and you’ll get an unbiased estimate. Additionally you can easily fit a nonlinear model (<span class="citation" data-cites="coey2019improving">Coey and Cunningham (2019)</span> has details of how to do an experiment-splitting).<sup>14</sup></p></li>
<li><p><strong>Run a regression just using the strongest experiments.</strong> If the distribution of experiments is fat-tailed then the strongest experiments will have higher SNR, and so lower bias. A worry about this is that you’re only estimating the relationship from outliers, so nonlinearities are more of a worry. At the same time the assumption of fat-tailed treatment-effects gives reason to believe the expectation will be nonlinear. (This is roughly how I interpret the <span class="citation" data-cites="peysakhovich2018learning">Peysakhovich and Eckles (2018)</span> experiments-as-instruments paper. They propose using L0 regularization and experiment-splitting cross-validations, which I think effectively selects the strongest experiments.)</p></li>
</ol>
<div class="no-row-height column-margin column-container"><div id="fn13"><p><sup>13</sup>&nbsp;See <span class="citation" data-cites="cunningham2019interpreting">Cunningham and Kim (2019)</span>, and see <span class="citation" data-cites="tripuraneni2023choosing">Tripuraneni et al. (2023)</span> for a slightly different setup with weaker assumptions. <span class="citation" data-cites="bibaut2024nonparametric">Bibaut et al. (2024)</span> shows the relationship with LIML.</p></div><div id="fn14"><p><sup>14</sup>&nbsp;<span class="citation" data-cites="bibaut2024nonparametric">Bibaut et al. (2024)</span> gives another formalization in terms of jacknife-instrumental-variable (JIVE).</p></div></div><p><strong>Choosing a Reference Class.</strong> It is important to think about the reference-class of experiments which we use to calibrate our estimates. The long-run DAU prediction can be though of as an empirical-bayes estimate, which is our best estimate conditional on the experiment being a random draw from this class of experiments.</p>
<p>In many cases a company’s experiments will naturally fall into different classes: e.g.&nbsp;some have a very steep relationship between engagement and DAU, others have a very flat. It’s important to both (1) visualize all the experiments, so that a reference-class can be chosen sensibly; (2) calculate the <img src="https://latex.codecogs.com/png.latex?R%5E2"> across experiments, so we can have some sense of confidence in our extrapolation.</p>
</section>
</section>
<section id="observational-inference" class="level2" data-number="2.2">
<h2 data-number="2.2" class="anchored" data-anchor-id="observational-inference"><span class="header-section-number">2.2</span> Observational Inference</h2>
<p><strong>What we want to know:</strong> Given the short-run effect of a content experiment on engagement we want to predict the long-run effect on DAU. We can start with a simple regression along these lines: <img src="https://latex.codecogs.com/png.latex?%5Cutt%7B%5Ctext%7BDAU%7D_%7Bu,t+1%7D%7D%7Blong-run%7D%7Bretention%7D%20%5Csim%20%5Cutt%7B%5Ctext%7Bengagement%7D_%7Bu,t%7D%7D%7Bshort-run%7D%7Bengagement%7D"></p>
<p><strong>We could set up a DAG and discuss the surrogacy conditions.</strong> The condition are that (1) all effects of an experiment on DAU are via short-run engagement; and (2) there is no unobserved factor which affects both SR engagement and LR DAU:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cxymatrix%7B%0A%20%20%20%20%20%20&amp;%20%20*+%5BF-:%3C6pt%3E%5D%5Ctxt%7Bunobserved%7D%5Car@%7B.%3E%7D%5Bd%5D%20%5Car@%7B.%3E%7D%5Bdr%5D%20%5C%5C%0A%20%20%20%20%20%20%20%20%20*+%5BF%5D%7B%5Ctext%7Bexperiment%7D%7D%20%5Car%5Br%5D%20%5Car@%7B.%3E%7D@/_1pc/%5Brr%5D%0A%20%20%20%20%20%20%20%20%20&amp;%20*+%5BF%5D%7B%5Ctext%7BSR%20engagement%7D%7D%5Car%5Br%5D%0A%20%20%20%20%20%20%20%20%20&amp;%20*+%5BF%5D%7B%5Ctext%7BLR%20DAU%7D%7D%0A%20%20%20%20%20%20%7D%0A"></p>
<p>In fact we know that engagement doesn’t <em>literally</em> lie on the causal chain, instead we think engagement is a good proxy for <em>content</em> which might lie on the causal chain.</p>
<p>In any case I find the following setup an easier way to think about the assumptions necessary for identification:</p>
<p><strong>We can write it out a simple structural model as follows</strong> (for compactness I leave out coefficients):</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Barray%7D%7Brcccccccc%7D%0A%20%20%20%5Ctext%7Bengagement%7D_%7Bu,t%7D%0A%20%20%20%20%20%20&amp;=&amp;%20%5Cutt%7B%5Ctext%7Btemperament%7D_%7Bu%7D%7D%7Buser-specific%7D%7Bpropensity%20to%20engage%7D%20%20%0A%20%20%20%20%20%20&amp;+&amp;%20%5Cutt%7B%5Ctext%7Bmood%7D_%7Bu,t%7D%7D%7Btime-varying%7D%7Bmood/holiday/etc.%7D%0A%20%20%20%20%20%20&amp;+&amp;%20%5Cutt%7B%5Ctext%7Bcontent%7D_%7Bu,t%7D%7D%7Bcontent%20seen%7D%7Bon%20platform%7D%0A%20%20%20%20%20%20&amp;+&amp;%20%5Cutt%7B%5Ctext%7Bdistractions%7D_%7Bu,t%7D%7D%7Bother%20platform%20effects%7D%7Be.g.%20messages,%20notifs%7D%5C%5C%0A%20%20%20%5Ctext%7BDAU%7D_%7Bu,t%7D%0A%20%20%20%20%20%20&amp;=&amp;%20%5Ctext%7Btemperament%7D_%7Bu%7D%0A%20%20%20%20%20%20&amp;+&amp;%5Ctext%7Bmood%7D_%7Bu,t%7D%0A%20%20%20%20%20%20&amp;+&amp;%5Cutt%7B%5Csum_%7Bs=1%7D%5E%5Cinfty%5Cbeta%5Es%5Ctext%7Bcontent%7D_%7Bu,t-s%7D%7D%7Bprior%20experience%7D%7Bw%20content%7D%0A%20%20%20%20%20%20&amp;+&amp;%5Ctext%7Bdistractions%7D_%7Bu,t%7D%5C%5C%0A%5Cend%7Barray%7D%0A"></p>
<p>Some general observations:</p>
<ol type="1">
<li><strong>We would get a more credible estimate if we could directly measure content quality.</strong> E.g. if we could use the quality of the content available to the user on the RHS, instead of just their engagement on that content. This wouldn’t get perfect identification but it would help.</li>
<li><strong>The relative shares of variation in the RHS is important.</strong> If most of the variation in engagement is due to variation in content (i.e.&nbsp;high <img src="https://latex.codecogs.com/png.latex?R%5E2"> from content), then we don’t need to worry much about confounding from other effects. We can think of introducing control variables as a way of increasing the share of varation in engagement due to content.</li>
<li><strong>We should control for distractions.</strong> If we have measures of app-related events that don’t affect content-seen but do affect engagement, e.g.&nbsp;notifications, messages, then we should use those as controls. This will increase the relative share of variation in engagement due to content.</li>
<li><strong>Controlling for pre-treatment outcomes changes variation used.</strong> If we control for <code>engagement</code><img src="https://latex.codecogs.com/png.latex?_%7Bt-1%7D"> this will change the relative contribution of each factor in the variation of engagement. Specifically it will reduce the share of the terms with higher autocorrelation. Thus by definition <code>temperament</code> will reduce its contribution. However it’s unclear whether <code>mood</code> or <code>content</code> has higher autocorrelation, and so controlling for pre-treatment could either increase or decrease the relative contribution of <code>content</code>. It’s probably worth doing some simple decomposition of variation in engagement into (1) user, (2) content, and (3) mood (the residual), both statically and over time.</li>
<li><strong>Univariate linear prediction is usually pretty good.</strong> In my experience you can get a fairly good prediction of most user-level metrics with a linear function of the lagged values. If you use a multivariate or nonlinear function you’ll get a better fit but only by a small amount (one exception: when predicting discrete variables like DAU it’s useful to use a continuous lagged variable like time-spent). So I’m skeptical that adding more regressors or adding nonlinearity will significantly change the estimates or the credibility of the estimates.</li>
<li><strong>Estimand is not <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> but <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B1-%5Cbeta%7D">.</strong> Suppose we see that 1 unit of engagement causes a certain increase in DAU over the following weeks. We then want to apply that estimate to an experiment which <em>permanently</em> increases engagement by 1 unit. We thus should take the integral over all the subsequent DAU effects. In the simple exponential case the effect of a shock at period <img src="https://latex.codecogs.com/png.latex?t"> on DAU at period <img src="https://latex.codecogs.com/png.latex?t+s"> will be <img src="https://latex.codecogs.com/png.latex?%5Cbeta%5Es">, and so the cumulative effect on all subsequent periods will be <img src="https://latex.codecogs.com/png.latex?1+%5Cbeta+%5Cbeta%5E2+%5Cldots=%5Cfrac%7B1%7D%7B1-%5Cbeta%7D">.</li>
<li><strong>Autocorrelation in content makes things messier.</strong> If there is significant autocorrelation in content then the interpretation of <code>DAU~engagement</code> is more difficult. E.g. if we see that engagement on <img src="https://latex.codecogs.com/png.latex?t"> is correlated with DAU on <img src="https://latex.codecogs.com/png.latex?t+1"> this could be because either (1) content on <img src="https://latex.codecogs.com/png.latex?t"> content caused the DAU on <img src="https://latex.codecogs.com/png.latex?t+1">, or (2) good content on <img src="https://latex.codecogs.com/png.latex?t"> is correlated with good content on <img src="https://latex.codecogs.com/png.latex?t+1">, which in turn causes DAU on <img src="https://latex.codecogs.com/png.latex?t+1">. I don’t think controlling for pre-treatment levels or trends solves this.</li>
</ol>
</section>
</section>
<section id="appendix-the-explore-exploit-problem" class="level1 unnumbered page-columns page-full">
<h1 class="unnumbered">Appendix: The Explore-Exploit Problem</h1>
<p><strong>What experiments should you run?</strong> The prior sections have been just about interpretation of existing experiments, we can now turn to the choice of which experiment to run. The space of all possible experiments is immensely high dimensional and thus most of this process uses human judgment. However in some cases we can reduce the space to a small number of dimensions and use an algorithm to explore that space. We can call this process a “bandit” or “explore exploit” or “adaptive experimentation” or “gradient descent” problem (though gradient descent is typically pure exploration with no exploitation).</p>
<p><strong>Typical cases for explore-exploit:</strong></p>
<ul>
<li>Tuning parameters on a recommendation algorithm to maximize retention.</li>
<li>Tuning parameters on video or audio streaming to maximize satisfaction and retention.</li>
<li>Tuning parameters on ad bidding to maximize net profit.</li>
<li>Exploring different components of quality in recommendations:
<ul>
<li>Content quality</li>
<li>Producer quality</li>
<li>User-topic interest</li>
</ul>
In each case showing some content that is <em>less</em> interesting to the user, but in return for learning more information.</li>
</ul>
<p><strong>I will focus just on tuning parameters in a recommendation algorithm.</strong></p>
<p><strong>Tuning projects have a high failure rate.</strong> I should say that I am not an expert on explore-exploit algorithms and many others have deeper professional experience than I do. However I have seen multiple tuning projects either abandoned because of complexity, or fail to find a set of parameters which yields a non-trivial improvement on metrics. Speaking broadly I think the problems were overly-complicated designs, under-powered experiments, lags in effects, ill-defined outcome variables, or improper use of short-term proxies for long-term outcomes.</p>
<p></p>
<div class="cell page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-04-18-experiment-interpretation-extrapolation_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>If <img src="https://latex.codecogs.com/png.latex?%5Cbeta_i%5E*"> is already close to the global optimum then there will be not much loss from perturbing some users because the loss function should be flat in that neighborhood.</figcaption>
</figure>
</div>
</div></div></div>
<p><strong>Recommendation: a simple tuning algorithm using weather stations.</strong> Here is a crude but easy-to-execute method for dynamically optimizing parameters. It’s less efficient than other algorithms but it’s easy to describe, easy to implement (it uses the existing AB-test system), and easy to visualize and see that it’s working as intended. In short: for each parameter we set up two permanent “weather stations” treatments: 1/3 of users get a slightly higher value, and 1/3 of users get a slightly lower value.</p>
<p>Suppose we have <img src="https://latex.codecogs.com/png.latex?n"> parameters to tune <img src="https://latex.codecogs.com/png.latex?(%5Cbeta_1,%5Cldots,%5Cbeta_n)">: we run <img src="https://latex.codecogs.com/png.latex?n"> orthogonal experiments, each of which partitions the all users into 3 equal-sized buckets, with either (1) <img src="https://latex.codecogs.com/png.latex?%5Cbeta_n=%5Cbeta_n%5E*"> , (2) <img src="https://latex.codecogs.com/png.latex?%5Cbeta_n=%5Cbeta_n%5E*-%5Cvarepsilon_n">, (3) <img src="https://latex.codecogs.com/png.latex?%5Cbeta_n=%5Cbeta_n%5E*+%5Cvarepsilon_n">, where <img src="https://latex.codecogs.com/png.latex?%5Cbeta_n%5E*"> is the current production level of <img src="https://latex.codecogs.com/png.latex?%5Cbeta">. If <img src="https://latex.codecogs.com/png.latex?n=2"> then users would be assigned as such:</p>
<div class="cell page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-04-18-experiment-interpretation-extrapolation_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>If we start at a point above the global optimum then the “low” group benefits and the “high” group suffers, but we can see that any short-term cost will be outweighed by long-term benefit.</figcaption>
</figure>
</div>
</div></div></div>
<table class="caption-top table">
<colgroup>
<col style="width: 29%">
<col style="width: 29%">
<col style="width: 11%">
<col style="width: 29%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: right;"></th>
<th><img src="https://latex.codecogs.com/png.latex?%5Cbeta_1-%5Cvarepsilon_1"></th>
<th><img src="https://latex.codecogs.com/png.latex?%5Cbeta_1"></th>
<th><img src="https://latex.codecogs.com/png.latex?%5Cbeta_1+%5Cvarepsilon_1"></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: right;"><img src="https://latex.codecogs.com/png.latex?%5Cbeta_2-%5Cvarepsilon_2"></td>
<td>1/9</td>
<td>1/9</td>
<td>1/9</td>
</tr>
<tr class="even">
<td style="text-align: right;"><img src="https://latex.codecogs.com/png.latex?%5Cbeta_2"></td>
<td>1/9</td>
<td>1/9</td>
<td>1/9</td>
</tr>
<tr class="odd">
<td style="text-align: right;"><img src="https://latex.codecogs.com/png.latex?%5Cbeta_2+%5Cvarepsilon_2"></td>
<td>1/9</td>
<td>1/9</td>
<td>1/9</td>
</tr>
</tbody>
</table>
<p>The size of the perturbations <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon_i"> are easy to adjust dynamically as the data comes in: we can start small and keep increasing until we see a stat-sig difference in the outcome. We monitor the trajectory of each bucket continuously, and once/month make a formal decision about whether to adjust the production parameters, e.g.&nbsp;increasing <img src="https://latex.codecogs.com/png.latex?%5Cbeta_n"> to <img src="https://latex.codecogs.com/png.latex?%5Cbeta_n+%5Cvarepsilon_n"> or lowering it to <img src="https://latex.codecogs.com/png.latex?%5Cbeta_n-%5Cvarepsilon_n">. When interpreting these experiments it is important to monitor the full trajectory of outcomes over time, ideally a visualization will show a large matrix of trajectories, with one cell for each combination of experiment-bucket and metric.</p>
<p><strong>We can use the data generated to explore other aspects:</strong> (1) whether there are significant interaction effects between the different experiments (e.g.&nbsp;if the users who have both increasing <img src="https://latex.codecogs.com/png.latex?%5Cbeta_1"> and <img src="https://latex.codecogs.com/png.latex?%5Cbeta_2"> have a different effect), and (2) whether there are significant heterogeneities in outcomes across subgroups.</p>
<p><strong>This is the simplest general framework I know of for continuous optimization of a set of parameters.</strong> I think that simplicity is by far the most important criterion: I have seen a long history of optimization projects get tangled in complexity and fail. Because of the past history of failures I think it’s crucial to do the simplest and most transparent thing at each point until you have a steady rhythm and track record of making progress.</p>
<p><strong>The hard work is the choice of parameters to tune.</strong> Once you have a small set of parameters to tune it’s not too hard to find the global optimum. However in typical problems there are thousands or millions or billions of possible parameters, how should you choose which ones to tune?</p>
<p></p>
<p></p>
<p></p>
</section>
<section id="appendix-difficult-cases" class="level1 unnumbered">
<h1 class="unnumbered">Appendix: Difficult Cases</h1>
<div class="example">
<p><strong>Example: Selection of experiments.</strong> Your team goal is to maximize <code>podcast_time</code>, and you want to know what other teams are hurting that metric. You find the 10 experiments with the biggest negative effect. Should you take their estimated effects at face value?</p>
</div>
<ol type="1">
<li><strong>Classical advice is to adjust p-values for the number of experiments you selected from (Bonferroni correction).</strong> But from a Bayesian point of view it’s irrelevant whether these 10 experiments are taken from a pool of 10 or 1000 experiments.</li>
<li><strong>The set of experiments <em>is</em> informative about appropriate shrinkage.</strong> You can use the pool of experiments to estimate the appropriate shrinkage, <img src="https://latex.codecogs.com/png.latex?E%5Bt%7C%5Chat%7Bt%7D%5D">. E.g. if we assume a Normal distribution we can quickly calculate a shrinkage estimate from the average effect and from the fraction of experiments that are statistically significant.</li>
<li><strong>Shrinkage should depend on plausibility of the effect.</strong> You can look at how much each of these experiments moves their primary outcomes. Suppose a music-ranking experiment decreases podcast time-spent by 0.4s, and increases music time-spent by 0.2s: the more-than-proportional side-effect seems unlikely, so there is reason to discount (shrink) the likely effect significantly.</li>
<li><strong>Shrink less if the effect is very significant.</strong> If the effect-size is 4 standard-errors then, because the distribution of treatment, this is much more likely to be due to treatment than to noise, and so the effect does not require much shrinkage.</li>
</ol>
<div class="example">
<p><strong>Example: Selection of Experiments #2.</strong> An engineer has an experiment with effect +1% (±0.5%) on your goal metric. They mention that they ran 20 other experiments, and this is the experiment with the biggest effect.</p>
</div>
<p><strong>Recommendation: shrink heavily towards the average effect.</strong></p>
<ol type="1">
<li><p><strong>Finding out about other experiments with smaller effects means you should shrink more.</strong> Finding out about the 20 other experiments is evidence about the size of the typical effect, and you should shrink towards that average. If the engineers are only showing you their best ones, that is reason to shrink your estimates.</p></li>
<li><p><strong>It matters how selection was done.</strong> Suppose the engineer chose the highest-effect one by chance, not intention. You should still shrink by the same amount: the distribution is evidence, not the selection rule. However if they had some independent reason for expecting this experiment would be the most effective, that is relevant evidence.</p></li>
</ol>
<div class="example">
<p><strong>Example: Subgroup Outcomes.</strong> You see that the overall time-spent of a feature holdout is -3.5% (±0.5%), but in Korea it’s -9%(±2%). How seriously should you take the Korean effect?</p>
</div>
<p><strong>Recommendation: take it seriously, because (a) very significant, and (b) there is high between-country variance.</strong></p>
<ol type="1">
<li><p><strong>Is this effect plausible?</strong> I.e., do we have reason to expect the effect of this feature to vary a lot by country, and in particular in Korea? We <em>do</em> generally think user behaviour varies a lot by country.</p></li>
<li><p><strong>How significance is this effect?</strong> The effect is 9 standard-errors – i.e., extremely significant – which makes it much less likely to be noise (<img src="https://latex.codecogs.com/png.latex?p">=.00001).</p></li>
<li><p><strong>How much variance in effect is between-country vs within-country?</strong> Suppose we see that 1/2 of the countries have effects that are significantly different from the global average effect, this implies that there is a fair amount of variance in effect-sizes, and so reasonable that Korea should be such an outlier.</p></li>
</ol>
<div class="example">
<p><strong>Example: Multiple Outcomes.</strong>Your experiment increases <code>music_time</code>, which you expected, and increases <code>podcast_time</code>, which you did not expect.</p>
</div>
<p><strong>Implication:</strong> The positive effect on <code>podcast_time</code> is <em>bad</em> news about <code>music_time</code>. If outcomes are positively correlated across units but not across treatments then: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BdE%5Bt_1%7C%20%5Chat%7Bt%7D_1,%5Chat%7Bt%7D_2%5D%7D%7Bd%5Chat%7Bt%7D_2%7D%20%3C%200."> In this case good news about one outcome is bad news about the other.</p>
<div class="example">
<p><strong>Example: Multiple Outcomes #2.</strong> You run an experiment on movie ranking intended to increase watches, and it works. You additionally see an increase in comments-given. Should the increase in comments give you more confidence or less confidence in the increase in likes?</p>
</div>
<p><strong>Recommendation: Good news is bad news, if the side-effect is unexpected.</strong></p>
<ul>
<li><strong>If the experiment was expected to increase both metrics</strong> - e.g.&nbsp;by increasing overall time spent on feed - then this is good news: it is additional evidence for the effect on likes.</li>
<li><strong>If the experment was expected to have a null or negative effect on comments</strong> – e.g.&nbsp;by boosting like-able posts at the expense of comment-able posts – then this is bad news: the positive effect on comments is likely due to noise, and it should make us expect greater noise in the measure of likes.</li>
</ul>
<p>Given two treatment effects <img src="https://latex.codecogs.com/png.latex?t_1"> and <img src="https://latex.codecogs.com/png.latex?t_2">, and two outcomes, <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bt%7D_1,%5Chat%7Bt%7D_2">, and two noise variables, <img src="https://latex.codecogs.com/png.latex?e_1,e_2"> then we have the following (in the Gaussian case):</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7BdE%5Bt_1%7C%5Chat%7Bt%7D_1,%5Chat%7Bt%7D_2%5D%7D%7Bd%5Chat%7Bt%7D_2%7D%20%5Cpropto%20%5Ctext%7Bcovariance%7D_%7Bt_1,t_2%7D-%5Ctext%7Bcovariance%7D_%7Be_1,e_2%7D.%0A%20%20%20"></p>
</section>
<section id="references" class="level1 unnumbered">




</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-andrews2019inference" class="csl-entry">
Andrews, I., Kitagawa, T., McCloskey, A., 2019. Inference on winners. National Bureau of Economic Research.
</div>
<div id="ref-bibaut2024nonparametric" class="csl-entry">
Bibaut, A., Kallus, N., Lal, A., 2024. Nonparametric jackknife instrumental variable estimation and confounding robust surrogate indices. arXiv preprint arXiv:2406.14140.
</div>
<div id="ref-coey2019improving" class="csl-entry">
Coey, D., Cunningham, T., 2019. Improving treatment effect estimators through experiment splitting, in: The World Wide Web Conference. ACM, pp. 285–295.
</div>
<div id="ref-cunningham2019interpreting" class="csl-entry">
Cunningham, T., Kim, J., 2019. Interpreting experiments with multiple outcomes.
</div>
<div id="ref-deng2016continuous" class="csl-entry">
Deng, A., Lu, J., Chen, S., 2016. <a href="https://api.semanticscholar.org/CorpusID:13511503">Continuous monitoring of a/b tests without pain: Optional stopping in bayesian testing</a>. 2016 IEEE International Conference on Data Science and Advanced Analytics (DSAA) 243–252.
</div>
<div id="ref-kohavi2020trustworthy" class="csl-entry">
Kohavi, R., Tang, D., Xu, Y., 2020. Trustworthy online controlled experiments: A practical guide to a/b testing. Cambridge University Press.
</div>
<div id="ref-peysakhovich2018learning" class="csl-entry">
Peysakhovich, A., Eckles, D., 2018. Learning causal effects from many randomized experiments using regularized instrumental variables, in: Proceedings of the 2018 World Wide Web Conference. International World Wide Web Conferences Steering Committee, pp. 699–707.
</div>
<div id="ref-tripuraneni2023choosing" class="csl-entry">
Tripuraneni, N., Richardson, L., D’Amour, A., Soriano, J., Yadlowsky, S., 2023. Choosing a proxy metric from past experiments. arXiv preprint arXiv:2309.07893.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{cunningham2023,
  author = {Cunningham, Tom},
  title = {Experiment {Interpretation} and {Extrapolation}},
  date = {2023-10-17},
  url = {tecunningham.github.io/posts/2023-04-18-experiment-interpretation-extrapolation.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-cunningham2023" class="csl-entry quarto-appendix-citeas">
Cunningham, T., 2023. Experiment Interpretation and Extrapolation [WWW
Document]. URL <a href="https://tecunningham.github.io/posts/2023-04-18-experiment-interpretation-extrapolation.html">tecunningham.github.io/posts/2023-04-18-experiment-interpretation-extrapolation.html</a>
</div></div></section></div> ]]></description>
  <guid>tecunningham.github.io/posts/2023-04-18-experiment-interpretation-extrapolation.html</guid>
  <pubDate>Tue, 17 Oct 2023 07:00:00 GMT</pubDate>
</item>
<item>
  <title>An AI Which Imitates Humans Can Beat Humans</title>
  <dc:creator>Tom Cunningham</dc:creator>
  <link>tecunningham.github.io/posts/2023-09-05-model-of-ai-imitation.html</link>
  <description><![CDATA[ 






<div class="no-row-height column-margin column-container"><div class="">
<p><img src="tecunningham.github.io/posts/20230825131919.png" class="img-fluid"> Thanks to comments from many, especially <a href="http://www.giorgiomartini.com">Giorgio Martini</a>, Grady Ward, <a href="https://robdonnelly.me">Rob Donnelly</a>, <a href="https://sites.google.com/site/inesmorenodebarreda/">Inés Moreno de Barreda</a>, and Colin Fraser.</p>
</div></div><style>p { text-indent: -2em; margin-left: 2em; }</style>
<p><strong>If we train AIs to <em>imitate</em> humans, will they ever <em>beat</em> humans?</strong> AI has caught up to human performance on many benchmarks, largely by learning to predict what humans would do. It seems important to know whether this is a ceiling or we should expect them to shoot out ahead of us. Will LLMs be able to write superhumanly-persuasive prose? Will image models be able to see things in photos that we cannot? There is a lot of technical literature on imitation learning in AI but I haven’t found much discussion of this point (<span class="citation" data-cites="bowman2023eight">Bowman (2023)</span> is a notable exception).</p>
<p></p>
<p></p>
<p><strong>In a formal model I derive five mechanisms by which imitative AI can beat humans.</strong></p>
<ol type="1">
<li><strong>Noise.</strong> Different humans give different answers to a question, and so if an LLM can consistently give the average answer it will do better than the average human (the error of the average being always smaller than the average of the error).</li>
<li><strong>Specialization.</strong> People tend to write about what they know, and so an LLM which learns to predict the typical answer to a given question will sound like a specialist in all areas: it will answer questions about water like a hydrologist and questions about bugs like an entomologist (although it will also answer questions about astrology like an astrologist).</li>
<li><strong>Interpolation.</strong> An LLM will interpolate responses from different humans, and this interpolation can be functionally equivalent to inference, meaning an LLM will sometimes be able to reliably answer questions that <em>no</em> human can answer.</li>
<li><strong>Priors.</strong> If an LLM has different priors than a human then they could uncover hidden structure that humans do not, e.g.&nbsp;an LLM trained on human observations of astronomical events could conceivably recover cycles in those events, and so give superior predictions to the human.</li>
<li><strong>Tacit knowledge.</strong> The majority of human knowledge is tacit, meaning it is used in forming judgments but we do not have conscious access to that knowledge. If AI models can accurately predict human judgments then the weights in those models effectively contain that tacit knowledge, and so the model can be re-engineered to use that knowledge in ways that humans cannot.</li>
</ol>
<p><strong>The evidence is unclear.</strong> There are many reasons why this could theoretically occur but I couldn’t find much evidence for superhuman performance: many benchmarks which we use to evaluate ML models have human labels as the ground truth, meaning we wouldn’t know when computers do pass us by.</p>
<p><strong>This blog post contains:</strong></p>
<ol type="1">
<li>A graphical argument illustrating the five mechanisms.</li>
<li>A deeper discussion of each of the five mechanisms.</li>
<li>A brief overview of the AI-human gap in various tasks.</li>
<li>Discussion of applications, related literature, and complications.</li>
<li>A simple formal model with a derivation of each of the five mechanisms.</li>
</ol>
<section id="setup-modelling-questions-and-answers" class="level1">
<h1>Setup: Modelling Questions and Answers</h1>
<p><strong>For concreteness I will describe humans and computers answering questions about the world.</strong> However I think the basic framework applies generally to performing tasks or following instructions. Some examples of questions and answers:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 93%">
<col style="width: 6%">
</colgroup>
<thead>
<tr class="header">
<th>question</th>
<th>answer</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>What’s the capital of Switzerland?</td>
<td>Bern</td>
</tr>
<tr class="even">
<td>What’s the best response if white plays c4?</td>
<td>Nf6</td>
</tr>
<tr class="odd">
<td>Does this picture (🐄) depict a cow?</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>How much more likely is someone to buy a Coke after hearing the slogan “Coke refreshes”?</td>
<td>0.1%</td>
</tr>
</tbody>
</table>
</section>
<section id="graphical-argument" class="level1">
<h1>Graphical Argument</h1>
<p>Here I illustrate all the core points in a graphical framework. For simplicity I am representing a set of questions and answers which can be represented by a pair of numbers, e.g.&nbsp;asking what is elevation of a point along a given line of latitude? I treat the human and the computer as having smooth priors about the world which determines how they interpolate and extrapolate from their observations.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>(1) Let the curve above represent the truth about the world.</strong> Each question about the world is a point on the x-axis, and the answer to each question is represented by the curve.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>(2) A human forms beliefs about the world (red curve).</strong> The human asks two questions and gets two answers (red dots) and from these they form estimates of the answer to every other question (red line).</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>(3) A computer learns to predict the human’s answers (green curve).</strong> The human records some questions and their answers (green dots), and the computer learns to predict the human’s answers (green curve). Here I have illustrated a very favorable case, where the human has shared all her observations with the computer, and both the human and computer have the same priors. If the human gives inconsistent answers (imagine a thickening of the green line) then the computer will do better than the human by having less noise (the error of the average always being smaller than the average error).</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>(4) Add another human (blue curve).</strong> Suppose we have an additional human who asks some different questions (blue dots) and so forms different beliefs (blue line). Both humans’ beliefs are are accurate in the neighborhood of their own experience.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>(5) Let the computer predict answers from both humans (green curve).</strong> Now both humans record their experiences and the computer tries to predict human answers (green curve). Here we can see:</p>
<ol type="1">
<li><strong>Specialization.</strong> The computer’s predictions can match the humans’ responses in each of their domains of expertise</li>
<li><strong>Interpolation.</strong> The computer is better than <em>both</em> humans in the intermediate region, i.e.&nbsp;the computer effectively combines information from both humans.</li>
</ol>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>(6) Let the computer have superior priors.</strong> If the computer and human have different priors then they will make different extrapolations from the same dataset. Suppose the world has a strong cyclical structure, as shown in the black oscillating line. The human does not appreciate the regularity and fits their datapoints with a simple nearest-neighbor algorithm, but the computer, with different priors, could get a superior fit to the true model. A simple hypothetical: suppose we trained a language model to predict records of astronomical observations, it could conceivably discover cycles in these observations even if no human was aware of those cycles, such that computer predictions of out-of-sample human observations of the world would constitute super-human predictions about the world.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p><strong>(7) Let the human have tacit knowledge.</strong> Finally, suppose the human always knows correct the answer when they see the question (red), but their conscious understanding of the relationship (pink) is imperfect. When asked abstract questions about the world they cannot use their tacit knowledge, e.g.&nbsp;if asked what is the maximum of this function they would choose the maximum of the pink curve (conscious beliefs), not the red curve (tacit beliefs). However the computer could learn the tacit knowledge from observing sufficiently many answers, and then algorithmically find the maximum of this curve, substantially outperforming the human at these abstract questions.</p>
</section>
<section id="five-reasons-for-superhuman-performance" class="level1 page-columns page-full">
<h1>Five Reasons for Superhuman Performance</h1>
<p><strong>(1) Noise.</strong> For many tasks there is very high within-human and between-human variation, so any model which is deterministic will have a substantial advantage. Thus averaging multiple answers tends to do much better (the “wisdom of crowds”, and the “crowd within”). A computer with a deterministic outcome will thus have a substantial advantage. <span class="citation" data-cites="zhang2024transcendence">Zhang et al. (2024)</span> shows that an LLM which is trained to predict chess moves of a group of players can outperform any of the players in that group - notably the effect is stronger when the model is trained on non-expert chess players, where the errors might be expected to be uncorrelated.</p>
<p><strong>(2) Specialization.</strong> We can see clear evidence of specialization in LLMs: they will answer questions about fish like an icthyologist, and questions about Ukraine like a Ukrainian. There is a nice discussion of this with many examples by <a href="https://ryxcommar.com/2023/03/28/chatgpt-as-a-query-engine-on-a-giant-corpus-of-text/">ryxcommar</a>:</p>
<blockquote class="blockquote">
<p>“When you ask ChatGPT a more intelligent question, you get a more intelligent answer. Just like how you ask ChatGPT a more Spanish question, you get a more Spanish answer.</p>
</blockquote>
<p>This works very well as long as the people who talk most about a topic tend to be the people who are most knowledgeable about that topic. If the reverse was true then AI would perform worse than the average person. In some cases it does seem to be true that the people who are most talkative are the least accurate, e.g.&nbsp;for conspiracy theories, politically partisan issues, or pseudosciences.<sup>1</sup> Thus we can predict that asking an LLM about these issues will tend to give low-quality answers, and indeed if you ask GPT-4 “what are some characteristics of Virgoes?” it will give a quite factual answer listing the traits of Virgoes.<sup>2</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Analogically, in music or visual art, there might be some genres where the people who create artworks are uniquely bad at it, and so in these genres imitative AI would learn to make artworks worse than the average human would make.</p></div><div id="fn2"><p><sup>2</sup>&nbsp;As of Oct 2023.</p></div></div><p>The same point applies for general supervised learning: suppose we train an image model to recognize tumors, and the training set includes examples from different radiologists, each who is an expert in their area (e.g.&nbsp;a pediatric radiologist labels the scans from children, a vetenarian radiologist labels the scans from animals), then the trained model could outperform any single radiologist.</p>
<p><strong>(3) Interpolation.</strong> I have not yet come up with a crisp question which an LLM can accurately answer but no human can, however there are <em>tasks</em> which LLMs can perform which it likely no human can perform without help:</p>
<ul>
<li><p>Recent LLMs (e.g.&nbsp;GPT, Bard) can transpose styles very easily, e.g.&nbsp;writing a Shakespearean sonnet about a particular episode of a particular television show, which arguably cannot be done by any human being.<sup>3</sup></p></li>
<li><p><span class="citation" data-cites="armengolestape2021multilingual">Armengol-Estapé, Gibert Bonet, and Melero (2021)</span> show that GPT-3 does fairly well at answering questions and producing text in Catalan, despite Catalan constituting only 35M words in the training set (0.02% of the total), implying that it can answer questions for which the answer is known by no Catalan speaker. In principle language models could translate between a pair of languages for which there exists no common speaker but I do not know of any explicit confirmation of this.</p></li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;Thanks to Giorgio Martini for this point.</p></div><div id="fn4"><p><sup>4</sup>&nbsp;thanks to Rob Donnelly for first suggesting this point to me.</p></div></div><p><strong>(4) Priors.</strong> When making extrapolation from the same set of data humans and computers will given different answers because they have different priors, and in some cases computer priors might give a better fit.<sup>4</sup></p>
<p>In fact there is a literature from the 1950s showing that computers with linear regression models do better than humans in learning to predict from novel datasets, and indeed better than experts in making clinical judgments using a small number of cues (<span class="citation" data-cites="camerer1991processperformance">Camerer and Johnson (1991)</span>). This is surprising because it took an additional 70 years for computers to catch up with human ability in making many other judgments. I think the key difference is that the 1950s results apply to low-dimensional cases (<img src="https://latex.codecogs.com/png.latex?p%3Cn">), while only recently have we taught computers to deal with high-dimensional data (<img src="https://latex.codecogs.com/png.latex?p%5Cgg%20n">).</p>
<p>Unfortunately I don’t know of any clear-cut examples which show an LLM outperforming a human because of superior priors. Here is a hypothetical: suppose the training data of an LLM included a large set of scientific observations, and those observations contain some underlying pattern which was not recognized by any contemporary scientist. If you ask the LLM about the existence of patterns then it should answer like a scientist and say there is no known pattern. However if you ask the LLM to predict new observations then those predictions may obey the pattern, as a consequence the LLM could be used to systematically map out prediction, which may make it easier to identify the pattern. Thus you could imagine an LLM would correctly predict the position of the stars.</p>
<p>There is an interesting sublety in how the AI ought to be prompted to elicit superhuman knowledge. Consider these two prompts:</p>
<ol type="1">
<li>“Q: where will Venus be on June the 16th? ___”</li>
<li>“On June the 16th Venus was observed to be ___”</li>
</ol>
<p>If the AI learned superior priors to the human then we should expect it to answer these prompts differently: it would answer prompt #1 using the human model, and answer prompt #2 using the true model.</p>
<p></p>
<p><strong>(5) Tacit Knowledge.</strong></p>
<p>The relative success of machine learning over symbolic AI has often been connected to the importance of tacit human knowledge. I have written a lot about the importance of tacit knowledge in human decision-making, especially <span class="citation" data-cites="cunningham2015hierarchical">Cunningham (2015)</span> and this <a href="https://tecunningham.github.io/posts/2017-12-10-unconscious-influences.html">post</a>, see also <span class="citation" data-cites="cunningham2022implicit">Cunningham and De Quidt (2022)</span>. In memory a related phenomenon is the “recognition recall” gap: people are significantly better at recognizing whether they saw a word before than in recalling that word (<span class="citation" data-cites="macdougall1904recognition">MacDougall (1904)</span>).</p>
<p><span class="citation" data-cites="stiennon2022learning">Stiennon et al. (2022)</span> train models to summarize text, based not on human summaries but on human evaluations of summaries. The model produces summaries that are preferred to human-produced summaries 70% of the time, i.e.&nbsp;superhuman production by training on human feedback. However this isn’t quite an apples-to-apples comparison because it’s unclear what the goals were of the humans who produced the baseline summaries: the human raters had explicit rubrics, but the human summarizers weren’t explicitly incentivized on those rubrics (as far as I can tell).</p>
<p>In addition I think that “inversion of tacit knowledge” is a reasonable description of image synthesis by neural nets: models are first trained to recognize images given captions, and then a new image can be synthesized to match a given caption, e.g.&nbsp;through a diffusion algorithm. Here there’s a striking asymmetry: algorithms can approximately match average human performance in recognition, but they far outperform human performance in construction of new artefacts.</p>
</section>
<section id="evidence-on-superhuman-performance" class="level1 page-columns page-full">
<h1>Evidence on Superhuman Performance</h1>
<section id="timeline" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="timeline">Timeline</h2>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="tecunningham.github.io/posts/images/2023-09-19-09-55-57.png" class="img-fluid"></p>
<p>The following table shows the year in which a computer (or mechanical device) could match performance with the best human:</p>
<table class="caption-top table">
<tbody>
<tr class="odd">
<td>arithmetic</td>
<td>1642</td>
</tr>
<tr class="even">
<td>chess</td>
<td>1997</td>
</tr>
<tr class="odd">
<td>Jeopardy</td>
<td>2005</td>
</tr>
<tr class="even">
<td>image recognition (ImageNet)</td>
<td>2015</td>
</tr>
<tr class="odd">
<td>handwriting recognition (MNIST)</td>
<td>2015</td>
</tr>
<tr class="even">
<td>question answering (SQuAD1.1)</td>
<td>2019</td>
</tr>
<tr class="odd">
<td>difficult math questions (MATH)</td>
<td>2023</td>
</tr>
<tr class="even">
<td>coding problems (MBPP)</td>
<td>(not yet)</td>
</tr>
</tbody>
</table>
</div><div id="fn5"><p><sup>5</sup>&nbsp;<span class="citation" data-cites="kiela2021dynabench">Kiela et al. (2021)</span> also say that “models that achieve super-human performance on benchmark tasks (according to the narrow criteria used to define human performance) nonetheless fail on simple challenge examples and falter in real-world scenarios.”</p></div></div><p><strong>Computers have hit the ceiling on most benchmarks.</strong> <span class="citation" data-cites="kiela2023plottingprogress">Kiela et al. (2023)</span> documents that most computer benchmarks have become “saturated,” i.e.&nbsp;computers get close-to-perfect performance, and that recently the speed of saturation has become quicker (see graph on right). They say identify only a single benchmark where performance is not close to the human baseline, and most of the models they discuss are imitation learning. As a consequence some work has moved to evaluating models against “adversarial” benchmarks where the problems are chosen specifically to fool computers (e.g.&nbsp;Dynabench, <span class="citation" data-cites="kiela2021dynabench">Kiela et al. (2021)</span>).<sup>5</sup></p>
<p><strong>On some tasks human performance <em>defines</em> success.</strong> On some tasks human performance effectively is the ground truth, and so by definition computers could never beat humans. This is roughly true for text comprehension: a sentence has a given meaning if and only if the average person believes it has that meaning. When we observe computer outperformance on this type of benchmark it is because either (1) there is human variation and the computer output is more consistent; or (2) computers outperform amateur humans but the ground truth is expert humans.</p>
</section>
<section id="performance-by-task" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="performance-by-task">Performance by Task</h2>
<p><strong><em>Arithmetic</em>: computers passed humans 300 years ago.</strong> Machines have been used to do calculations since the 17th century, e.g.&nbsp;<a href="https://en.wikipedia.org/wiki/Pascal's_calculator">Pascal’s calculator</a> from 1642.</p>

<div class="no-row-height column-margin column-container"><div class="">
<table class="caption-top table">
<tbody>
<tr class="odd">
<td>Backgammon</td>
<td>1979</td>
</tr>
<tr class="even">
<td>Chess</td>
<td>1997</td>
</tr>
<tr class="odd">
<td>Jeopardy</td>
<td>2005</td>
</tr>
<tr class="even">
<td>Atari games</td>
<td>2013</td>
</tr>
<tr class="odd">
<td>Go</td>
<td>2016</td>
</tr>
<tr class="even">
<td>Starcraft</td>
<td>2019</td>
</tr>
</tbody>
</table>
<p>(<a href="https://historyofyesterday.com/the-brutal-history-of-ai-defeating-every-human/">source</a>)</p>
</div></div><p><strong><em>Playing games</em>: computers passed humans over the last 45 years.</strong> See the table in the margin for games. I am not aware of any well-known games in which computers cannot reliably beat the best humans.</p>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="tecunningham.github.io/posts/images/2023-09-18-08-13-45.png" class="img-fluid"> (<a href="https://ourworldindata.org/brief-history-of-ai">source</a>)</p>
</div></div><p><strong><em>Image recognition</em>: computers surpassed humans in the 2010s.</strong> With the qualifications above about the limitations of benchmark tasks.</p>
<p><strong><em>Question answering</em>: computers surpassed humans in the 2010s.</strong> With the qualifications above about the limitations of benchmark tasks.</p>
<p><strong><em>Facial recognition</em>: computers seem to be equivalent to experts.</strong> <span class="citation" data-cites="towler2023facial">Towler et al. (2023)</span> say <em>“naturally skilled super-recognizers, trained forensic examiners and deep neural networks, … achiev[e] equivalent accuracy.”</em></p>
<p><strong><em>Coding</em>: computers still below expert.</strong> See the benchmarks on <a href="https://paperswithcode.com/task/code-generation">PapersWithCode</a>, also a graph on <a href="https://ourworldindata.org/grapher/ai-performance-coding-math-knowledge-tests">OurWorldInData</a>, specifically <a href="https://paperswithcode.com/dataset/apps">APPS</a> and <a href="https://paperswithcode.com/sota/code-generation-on-mbpp">MBPP</a>. The best-performing computers are still imperfect at solving these coding challengers (which presumably can be solved by an expert programmer), but progress is rapid.</p>
<p><strong><em>Writing persuasive text</em>: computer comparable to average human.</strong> A number of recent papers compare the persuasive power of LLM-generated text to human-generated text (<span class="citation" data-cites="bai2023persuade">Bai et al. (2023)</span>, <span class="citation" data-cites="goldstein2023persuasive">Goldstein et al. (2023)</span>, <span class="citation" data-cites="hackenburg2023persuasive">Hackenburg and Margetts (2023)</span>, <span class="citation" data-cites="matz2023personalized">Matz et al. (2023)</span>, <span class="citation" data-cites="palmer2023large">Palmer and Spirling (2023)</span>, <span class="citation" data-cites="qin2023large">Qin et al. (2023)</span>). They all find that LLMs do relatively well, but none show clear signs of computer superiority.</p>
<p><strong><em>Writing creative blurbs</em>: computer comparable to average human.</strong> <span class="citation" data-cites="koivisto2023creativity">Koivisto and Grassini (2023)</span> compared GPT4 to online recruited humans (£2 for a 13 minute task) in giving “creative” uses for everyday items. The prompt was to “come up with original and creative uses for an object”, objects were “rope”, “box”, “pencil” and “candle.” The responses were rated by humans for their “creativity” or “originality.” GPT-4 responses were perhaps 1SD above the average human score, but the difference was smaller when choosing just the best response for each user.</p>
<p><strong><em>Summarizing text</em>: computer beats average human.</strong> Two recent papers found that LLM-generated summaries, trained with feedback, were preferred by humans to human-generated summaries (<span class="citation" data-cites="stiennon2022learning">Stiennon et al. (2022)</span> using RLHF and <span class="citation" data-cites="lee2023rlaif">Lee et al. (2023)</span> using RLHF). However in both cases it wasn’t clear to me exactly how the human summarizers were incentivized, and whether they were trying to perform the same task as the LLMs.<sup>6</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;<span class="citation" data-cites="lee2023rlaif">Lee et al. (2023)</span> say <em>“RLAIF summaries are preferred over the reference [human-written] summaries 79% of the time, and RLHF are preferred over the reference summaries 80% of the time.”</em></p></div><div id="fn7"><p><sup>7</sup>&nbsp;<span class="citation" data-cites="hendrycks2021measuring">Hendrycks et al. (2021)</span> says “We found that a computer science PhD student who does not especially like mathematics attained approximately 40% on MATH, while a three-time IMO gold medalist attained 90%”</p></div></div><p><strong><em>Doing math problems</em>: computer comparable to expert.</strong> The latest score on the <a href="https://paperswithcode.com/sota/math-word-problem-solving-on-math">MATH benchmark</a> is 84%, compared to 90% by a three-time IMO gold medalist. The scores have been rising very rapidly so it seems likely that computers will soon surpass humans.<sup>7</sup></p>
<p></p>
</section>
</section>
<section id="discussion" class="level1 page-columns page-full">
<h1>Discussion</h1>
<p><strong>I will use “superhuman” to mean the AI can answer some question better than any human can.</strong> We can formalize “superhuman” ability in a variety of ways. The notation is introduced more fully below, but briefly <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D"> represents a question, <img src="https://latex.codecogs.com/png.latex?a(%5Cbm%7Bq%7D)"> represents the correct answer, <img src="https://latex.codecogs.com/png.latex?%5Cbar%7Ba%7D(%5Cbm%7Bq%7D)"> represents the computer’s answer, and <img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D_i(%5Cbm%7Bq%7D)"> represents the answer of human <img src="https://latex.codecogs.com/png.latex?i">. We assume squared error loss, and the expected error can be interpreted as either over the universe of all questions, or some subset of questions: <img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Ctext%7Bweak:%7D&amp;&amp;%20%20%20%20%5Cut%7BE%5B(a(q)-%5Cbar%7Ba%7D(q))%5E2%5D%7D%7Berror%20of%20computer%7D%0A%20%20%20%20%20%20%20%20%20&amp;%5Cleq%20%20%5Cut%7B%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi=1%7D%5EmE%5B(a(q)-%5Chat%7Ba%7D_i(q))%5E2%5D%7D%7Bavg%20error%20of%20human%7D%5C%5C%0A%20%20%20%20%20%20%5Ctext%7Bmedium:%7D&amp;&amp;%20%20%5Cut%7BE%5B(a(q)-%5Cbar%7Ba%7D(q))%5E2%5D%7D%7Berror%20of%20computer%7D%0A%20%20%20%20%20%20%20%20%20&amp;%5Cleq%20%20%5Cut%7BE%5Cleft%5B(a(q)-%5Cfrac%7B1%7D%7Bm%7D%5Csum_%7Bi=1%7D%5Em%5Chat%7Ba%7D_i(q))%5E2%5Cright%5D%7D%7Berror%20of%20avg%20human%7D%5C%5C%0A%20%20%20%20%20%20%5Ctext%7Bstrong:%7D&amp;&amp;%20%20%5Cut%7BE%5B(a(q)-%5Cbar%7Ba%7D(q))%5E2%5D%7D%7Berror%20of%20computer%7D%0A%20%20%20%20%20%20%20%20%20&amp;%5Cleq%20%20%5Cut%7B%5Cmin_%7Bi=1,%5Cldots,m%7DE%5B(a(q)-%5Chat%7Ba%7D_i(q))%5E2%5D%7D%7Berror%20of%20best%20human%7D%5C%5C%0A%20%20%20%20%20%20%5Ctext%7Bsuper-strong:%7D&amp;&amp;%20%5Cut%7BE%5B(a(q)-%5Cbar%7Ba%7D(q))%5E2%5D%7D%7Berror%20of%20computer%7D%0A%20%20%20%20%20%20%20%20%20&amp;%5Cleq%20%20%5Cut%7BE%5B%5Cmin_%7Bi=1,%5Cldots,m%7D%5C%7B(a(q)-%5Chat%7Ba%7D_i(q))%5E2%5C%7D%5D%7D%7Berror%20of%20best%20human%20by%20question%7D%5C%5C%0A%20%20%20%5Cend%7Baligned%7D%0A%20%20%20"></p>
<p>Note that we cannot rank benchmark #2 and #3: the error of the best human could be either higher or lower than the error of the average human. The most interesting question is whether AI can exhibit “super strong” superhuman performance. It seems clear that imitative learning can easily lead to superhuman performance by all the other 3 definitions through (1) reducing noise, and (2) combining expertise. However super-strong superhuman performance would require either (1) interpolation, (2) superior priors, or (3) using tacit human knowledge.</p>
<p><strong>Training models with custom-written answers is still imitative learning.</strong> Recent LLMs don’t train just on predicting existing text (books, internet, twitter) they also use datasets of instructions and responses generated by paid raters (<span class="citation" data-cites="ouyang2022training">Ouyang et al. (2022)</span>). We can still call this imitation but it’s putting relatively more weight on imitating the responses of specific set of people, the paid raters. This fine-tuning significantly improves performance on most benchmarks but I think it also has costs: the model is now predicting output of a specific set of people (i.e.&nbsp;non-expert paid raters), and so conceivably will do less well at incorporating niche information available to an expert.</p>
<p><strong>Training on human evaluations is using human tacit knowledge.</strong> Recent LLMs are not purely imitative, e.g.&nbsp;OpenAI’s GPT models are trained with human <em>evaluation</em> of their responses (called reinforcement learning with human feedback (RLHF)), and they find that it dramatically increases performance on instruction-following benchmarks.<sup>8</sup> The key difference is that the goal now reflects how humans <em>rate</em> responses rather than how humans <em>generate</em> responses. In some domains the two functions might be identical but in others there’s a clear difference, I would argue that tacit knowledge is the core difference.</p>
<div class="no-row-height column-margin column-container"><div id="fn8"><p><sup>8</sup>&nbsp;E.g. see OpenAI’s 2022 InstructGPT (<span class="citation" data-cites="ouyang2022training">Ouyang et al. (2022)</span>). In fact they run reinforcement learning against a model trained to predict the human evaluation of outputs, and other papers run reinforcement learning against LLM-produced evaluations, RLAIF (<span class="citation" data-cites="lee2023rlaif">Lee et al. (2023)</span>).</p></div></div><p><strong><span class="citation" data-cites="bowman2023eight">Bowman (2023)</span> on super-human performance by LLMs.</strong> I have found surprisingly little online or academic discussion about whether LLMs will hit a ceiling defined by human performance. A good paper by <span class="citation" data-cites="bowman2023eight">Bowman (2023)</span> has a section titled “human performance on a task isn’t an upper bound on LLM performance.” He says LLMs can outperform humans for two reasons: (1) “they are trained on far more data than any human sees,” and (2) “they are often given additional training using reinforcement learning … which trains them to produce responses that humans find helpful without requiring humans to demonstrate such helpful behavior.” I think these correspond to two of the five reasons I identified (specialization and tacit knowledge).</p>
<p><strong>For some tasks human-level performance is the ceiling by definition.</strong> The ground truth about language interpretation is humans interpretation, and so it is hard to see how a computer could exhibit superhuman performance (in the super-strong sense).<sup>9</sup> In benchmarks for natural language understanding the labels are typically written by the human authors of the benchmark, so it would be impossible to observe superhuman performance (<span class="citation" data-cites="tedeschi2023s">Tedeschi et al. (2023)</span>). A similar point applies to content moderation the definition of ground truth is typically either majority-vote among paid human raters, or the reflective judgment of a senior human employee, thus a computer could only outperform in the weak senses above.<sup>10</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;There are exceptions but I don’t think they are quantitatively important. Consider the a sentence like “Buffalo buffalo Buffalo buffalo buffalo buffalo Buffalo buffalo.” This sentence has at least one well-defined meaning according to the typical rules of English but arguably no human would correctly identify that meaning unless specifically prompted. A computer trained only on human comprehension, i.e.&nbsp;data which did not contain such outlier sentences, could plausibly identify its meaning.</p></div><div id="fn10"><p><sup>10</sup>&nbsp;Many recent language models do outperform the average human baseline on language understanding tasks, but <span class="citation" data-cites="tedeschi2023s">Tedeschi et al. (2023)</span> argue that for a variety of reasons the strength of these results is significantly exaggerated.</p></div></div><p></p>
<p><strong>Computers could outperform humans on recognition tasks, but we haven’t tested them yet.</strong> Most benchmarks for media recognition, e.g.&nbsp;testing for object detection in photos or speech recognition in audio, use human labels as the ground truth. However humans can be mistaken: they might think a photo has a dog in it when it does not or vice versa. Thus human judgment is not the ground truth. We could create a test set to measure superhuman performance either by (1) creating new media (e.g.&nbsp;taking new photos of dogs instead of using existing photos which humans identify as having a dog); (2) obfuscating existing media (e.g.&nbsp;blurring existing photos of dogs). By the argument in this note a classifiers trained only on human-provided labels could exhibit superhuman performance in such a test set, most plausibly through better priors: learning characteristic signs of dogs that humans do not.<sup>11</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn11"><p><sup>11</sup>&nbsp;Defining the ground truth in a recognition task is somewhat complicated because a given arrangement of pixels is consistent with an infinite variety of objects having caused that arrangement. We talk about an image representing an object in the world only because we have strong priors about the world which allow us to make that inference. So the ground truth in a recognition task must be something like “in ordinary circumstances, what is the probability that these pixels would be caused by a scene with a dog in them.”</p></div></div><p><strong>Self-play has helped performance in playing games.</strong> A common trick to teach computers to play games well is to have them play themselves (self-play), this has been used to get superhuman performance in Backgammon, Chess, Go, &amp; Minecraft. However this is not imitation learning: here the computer is trained against a non-human ground truth, the computer directly observes whether they have won the game. However there are analogues of self-play in imitative models: (1) training models to generate images, and to discriminate between computer-generated and real images (generative adversarial nets, GAN); (2) training an LLM to produce text based on LLM-generated feedback (RLAIF).</p>
<p><strong>The graphical model underplays the importance of model architecture.</strong> The graphical model shown above represents both questions and answers as unidimensional, and it makes it seem that a small sample is sufficient to get a reasonably good approximation of the true function. In reality the questions of interest are very high dimensional and most model architectures fail to generalize well at all. Neural nets, especially those with a transformer structure, have had remarkable success in fitting the data, leading to leaps in performance.</p>
<div class="cell page-columns page-full">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-09-05-model-of-ai-imitation_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div></div></div>
<p><strong>Venn diagram representation.</strong> This diagram shows an alternative way of representing some of the core claims: in general the questions answerable by an LLM will not be a subset of the questions answered in the training set, or even those answerable by the people who contributed to the training set. The Venn diagram’s disadvantage, relative to the visualizations above, is that it does not represent the mechanics of <em>why</em> the LLM can outperform humans, while the diagram above can be use to separately show five distinct reasons (averaging error, specialization, interpolation, different priors, and using tacit knowledge).</p>
<p><strong>Imitation learning has problems in dynamic situations.</strong> The discussion in this note has been about a purely static problem of supplying answers to questions, but text generation can also be considered as a dynamic problem of sequentially generating tokens. A common observation regarding dynamic imitation learning is that pure prediction of expert behaviour (“behavioural cloning”) is not very robust, because the algorithm does not know what to do in situations not observed before (out of distribution), and this has been used to explain weaknesses in the behaviour of autoregressive generative text models.<sup>12</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn12"><p><sup>12</sup>&nbsp;<span class="citation" data-cites="cundy2023sequencematch">Cundy and Ermon (2023)</span> say “[the] simple behaviour cloning approach results in a compounding error problem, where the further the trained model gets from the typical expert states, the worse the model performs, incurring increasing error.” I also found <a href="https://web.stanford.edu/class/cs237b/pdfs/lecture/cs237b_lecture_12.pdf">these notes</a> from Stanford’s CS273B useful.</p></div></div></section>
<section id="linear-model" class="level1 page-columns page-full">
<h1>Linear Model</h1>
<p><strong>Here I give a more formal model and derive some results.</strong> I wrote this model before coming up with the graphical argument above. There is a substantial overlap in implications, but I think there is some value in this linear model in the precision with which we define each quantity. The model has three steps:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cxymatrix@C=.5cm@R=0cm%7B%0A%20%20%20%20%20%20%5Ctext%7Bworld%7D%0A%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7Bhuman%7D%0A%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7BLLM%7D%5C%5C%0A%20%20%20%20%20%20*+%5BF:%3C5pt%3E%5D%7B%5Cbm%7Bw%7D%7D%20%5Car%5Brr%5D%5E%7B%5Cbm%7Ba%7D=Q%5Cbm%7Bw%7D%7D%0A%20%20%20%20%20%20&amp;&amp;%20*+%5BF:%3C5pt%3E%5D%7B%5Chat%7B%5Cbm%7Bw%7D%7D%7D%20%5Car%5Brr%5D%5E%7B%5Chat%7B%5Cbm%7Ba%7D%7D=%5Chat%7BQ%7D%5Chat%7B%5Cbm%7Bw%7D%7D%7D%0A%20%20%20%20%20%20&amp;&amp;%20*+%5BF:%3C5pt%3E%5D%7B%5Cbar%7B%5Cbm%7Bw%7D%7D%7D%20%5Car%5Brr%5D%5E%7B%5Ctilde%7Ba%7D=%5Ctilde%7Bq%7D'%5Cbar%7B%5Cbm%7Bw%7D%7D%7D%0A%20%20%20%20%20%20&amp;&amp;%20%5C%20%20%5C%5C%0A%20%20%20%20%20%20%5Ctxt%7Bunobserved%5C%5Ctruth%5C%5Cabout%5C%5Cthe%5C%5Cworld%7D%0A%20%20%20%20%20%20&amp;%20%5Ctxt%7Banswers%5C%5Cto%5C%5Chuman%5C%5Cquestions%7D%0A%20%20%20%20%20%20&amp;%20%5Ctxt%7Bbeliefs%5C%5Cformed%5C%5Cby%5C%5Chuman%7D%0A%20%20%20%20%20%20&amp;%20%5Ctxt%7Btext%5C%5Cwritten%5C%5Cby%5C%5Chuman%7D%0A%20%20%20%20%20%20&amp;%20%5Ctxt%7BLLM%5C%5Cmodel%5C%5Cof%5C%5Chuman%5C%5Ctext%7D%0A%20%20%20%20%20%20&amp;%20%5Ctxt%7BLLM's%5C%5Canswers%5C%5Cto%5C%5Cnew%5C%5Cquestions%7D%0A%20%20%20%7D%0A%20%20%20"></p>
<p><strong>Questions and answers.</strong> A question is defined by a set of binary attributes (<img src="https://latex.codecogs.com/png.latex?q_1,%5Cldots,q_p%5Cin%5C%7B-1,1%5C%7D">), and the answer is a linear function of those attributes given some unobserved weights <img src="https://latex.codecogs.com/png.latex?w_1,%5Cldots,w_p">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Cut%7B%5Cbmatrix%7Ba%5E1%20%5C%5C%20%5Cvdots%20%5C%5C%20a%5En%7D%7D%7Banswers%7D%0A%20%20%20%20%20%20%20%20%20=%20%5Cut%7B%5Cbmatrix%7Bq_1%5E1%20w_1%20+%20%5Cldots%20q_p%5E1w_p%20%5C%5C%20%5Cvdots%20%5C%5C%20q_1%5En%20w_1%20+%20%5Cldots%20q_p%5Enw_p%7D%7D%7Bquestions%7D%0A%20%20%20%5Cend%7Baligned%7D%0A%20%20%20"></p>
<p><strong>Human beliefs.</strong> After observing a set of question and their real-world answers the human will form beliefs about the weights <img src="https://latex.codecogs.com/png.latex?w_1,%5Cldots,w_p">. We can explicitly write the human posteriors if we assume their priors are Gaussian and i.i.d. (<img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bw%7D%5Csim%20N(0,%5Csigma%5E2I)">):<sup>13</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn13"><p><sup>13</sup>&nbsp;I am assuming <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bw%7D"> has zero-mean and is i.i.d. just to cut down on notation, the results all hold for the more general multivariate Normal case.</p></div></div><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Cbm%7Ba%7D%20%20%20%20%20%20%20%20&amp;=%20Q%5Cbm%7Bw%7D%0A%20%20%20%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7B(questions%20%5C&amp;%20answers%20given%20true%20weights%20$%5Cbm%7Bw%7D$)%7D%5C%5C%0A%20%20%20%20%20%20%5Chat%7B%5Cbm%7Bw%7D%7D%20&amp;=%20Q'(QQ')%5E%7B-1%7D%5Cbm%7Ba%7D%0A%20%20%20%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7B(human%20estimate%20of%20weights%20$%5Cbm%7Bw%7D$%20given%20$Q$%20and%20$%5Cbm%7Ba%7D$)%7D%0A%20%20%20%5Cend%7Baligned%7D">
<p>I give a derivation of the human posteriors below.</p>
<p>I will assume that the number of unobserved weights is large relative to the human’s experience (<img src="https://latex.codecogs.com/png.latex?p%5Cgg%20n">), so the human will gradually learn more about reality as she observes the answer to more questions. She will be able to perfectly answer any question she’s seen before, but will never learn the full set of weights.</p>
<p><strong>Computer beliefs.</strong> Suppose that humans write down some set of questions, <img src="https://latex.codecogs.com/png.latex?%5Chat%7BQ%7D">, and then record their best guesses at the answers. These could be questions that the humans already know the answer to (<img src="https://latex.codecogs.com/png.latex?%5Chat%7BQ%7D%5Csubseteq%20Q">), or they could be new questions that they are guessing the answer to. We then use those questions and answers to train a computer, and the computer likewise assumes a linear model with i.i.d. Gaussian weights. Note that the computer is being trained to predict human responses, not to predict properties of the world. We can write the computer-estimated weights as follows:</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Chat%7B%5Cbm%7Ba%7D%7D%20%20%20%20%20%20%20%20&amp;=%20%5Chat%7BQ%7D%5Chat%7B%5Cbm%7Bw%7D%7D%0A%20%20%20%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7B(human-generated%20questions%20%5C&amp;%20answers)%7D%5C%5C%0A%20%20%20%20%20%20%5Cbar%7B%5Cbm%7Bw%7D%7D%20&amp;=%20%5Chat%7BQ%7D'(%5Chat%7BQ%7D%5Chat%7BQ%7D')%5E%7B-1%7D%5Chat%7B%5Cbm%7Ba%7D%7D%0A%20%20%20%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7B(computer%20estimate%20of%20human%20weights%20$%5Chat%7B%5Cbm%7Bw%7D%7D$)%7D%0A%20%20%20%5Cend%7Baligned%7D">
<p><strong>Computer answers.</strong> Finally we can ask the computer a new question, <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bq%7D">, and observe its answer:</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Ctilde%7Ba%7D%20%20%20%20%20%20%20%20&amp;=%20%5Ctilde%7B%5Cbm%7Bq%7D%7D'%5Cbar%7B%5Cbm%7Bw%7D%7D%0A%20%20%20%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7B(computer%20answer%20to%20a%20novel%20question%20$%5Ctilde%7B%5Cbm%7Bq%7D%7D$)%7D%5C%5C%0A%20%20%20%5Cend%7Baligned%7D">
<section id="model-implications" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="model-implications">Model Implications</h2>
<dl>
<dt><strong>If one human records all their observations then the computer will perfectly imitate them.</strong></dt>
<dd>
Suppose that there is one human and they write down all of their observations, <img src="https://latex.codecogs.com/png.latex?%5Chat%7BQ%7D=Q">. Because the computer and human have the same priors, and observe the same data, then they will therefore end up with the same estimated weights (<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbm%7Bw%7D%7D=%5Cbar%7B%5Cbm%7Bw%7D%7D">), and so the computer will answer every question exactly as the human does, though neither knows the truth (<img src="https://latex.codecogs.com/png.latex?%5Cbar%7B%5Cbm%7Bw%7D%7D%5Cneq%5Cbm%7Bw%7D">).
</dd>
</dl>
<p><strong>If humans are noisy then the computer will outperform them.</strong> Suppose humans report their answers with some i.i.d. noise <img src="https://latex.codecogs.com/png.latex?%5Cepsilon">. If the computer observes sufficiently many answers for each question then the noise will be washed out and they will outperform.</p>
<dl>
<dt><strong>If humans record a subset of their observation then the computer will perform worse.</strong></dt>
<dd>
Suppose humans only write down some of their observations, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?%5Chat%7BQ%7D"> is a row-wise subset of <img src="https://latex.codecogs.com/png.latex?Q">. Then computers and humans will give the same answers for any question in the training set, but outside of that set computers will generally do worse than humans. And so for any question <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D%5Cnot%5Cin%5Chat%7BQ%7D"> the computer will do worse in expectation: <img src="https://latex.codecogs.com/png.latex?E%5B%5Cut%7B(%5Cbm%7Bq%7D(%5Cbm%7Bw%7D-%5Cbar%7B%5Cbm%7Bw%7D%7D))%5E2%7D%7Bcomputer%20error%7D%5D%5Cgeq%0A%20%20%20%20%20E%5B%5Cut%7B(%5Cbm%7Bq%7D(%5Cbm%7Bw%7D-%5Chat%7B%5Cbm%7Bw%7D%7D))%5E2%7D%7Bhuman%20error%7D%5D."> Note that we are fixing the question <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D"> and taking the expectation over all possible worlds, <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bw%7D">. I think you could probably rewrite this such that, in the world we are in, we should observe worse average performance across a set of questions, but I think you’d need to add some conditions to make sure that the questions are sufficiently independent (e.g.&nbsp;if there was a single weight <img src="https://latex.codecogs.com/png.latex?w_q"> which dominated all the other weights then the computer might beat the human by accident).
</dd>
<dt><strong>If there are two humans then the computer will outperform them both.</strong></dt>
<dd>
Suppose there are two humans who each observe answers to different question, <img src="https://latex.codecogs.com/png.latex?Q_A"> and <img src="https://latex.codecogs.com/png.latex?Q_B">, and they both write them all down, so <img src="https://latex.codecogs.com/png.latex?%5Cbar%7BQ%7D=(%5Csmallmatrix%7BQ_A%5C%5CQ_B%7D)"> and <img src="https://latex.codecogs.com/png.latex?%5Cbar%7B%5Cbm%7Ba%7D%7D=(%5Csmallmatrix%7BQ_A%5Cbm%7Bw%7D%5C%5CQ_B%5Cbm%7Bw%7D%7D)">. Now the computer has a strictly larger set of observations than either human, and so if we let <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbm%7Bw%7D%7D(i)"> represent the weights of human <img src="https://latex.codecogs.com/png.latex?i%5Cin%5C%7BA,B%5C%7D">, then for any question <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D"> we can write:<br>
<img src="https://latex.codecogs.com/png.latex?%20E%5B%5Cut%7B(%5Cbm%7Bq%7D(%5Cbm%7Bw%7D-%5Cbar%7B%5Cbm%7Bw%7D%7D))%5E2%7D%7Bcomputer%20error%7D%5D%5Cleq%0A%20%20E%5B%5Cut%7B(%5Cbm%7Bq%7D(%5Cbm%7Bw%7D-%5Chat%7B%5Cbm%7Bw%7D%7D(i)))%5E2%7D%7Bhuman%20error%7D%5D.%0A%20%20%20">
</dd>
</dl>
<p><strong>If there are multiple humans then the computer can answer question no human can answer.</strong> Suppose two humans observe the answers to the following questions:</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20Q_A%20&amp;=%20%5Cbmatrix%7B1%20&amp;%201%20&amp;%201%20%5C%5C%201%20&amp;%20-1%20&amp;%201%7D%20%5C%5C%0A%20%20%20%20%20%20Q_B%20&amp;=%20%5Cbmatrix%7B1%20&amp;%201%20&amp;%201%20%5C%5C%201%20&amp;%201%20&amp;%20-1%7D%0A%20%20%20%5Cend%7Baligned%7D">
<p>The first human will learn the exact value of <img src="https://latex.codecogs.com/png.latex?w_2"> (<img src="https://latex.codecogs.com/png.latex?%5Chat%7Bw%7D_2=w_2">), and the second human will learn the exact value of <img src="https://latex.codecogs.com/png.latex?w_3">, but neither will learn both values, and so neither could predict the answer to this question with perfect confidence:</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Ctilde%7Bq%7D%20&amp;=%20%5Cbmatrix%7B1%20&amp;%20-1%20&amp;%20-1%7D%20%5C%5C%0A%20%20%20%5Cend%7Baligned%7D">
<p>However if they both recorded their observations then the computer observes <img src="https://latex.codecogs.com/png.latex?%5Cbar%7B%5Cbm%7Ba%7D%7D=(%5Csmallmatrix%7BQ_1%5Cbm%7Bw%7D%5C%5CQ_2%5Cbm%7Bw%7D%7D)">, and so the computer will be able to infer both <img src="https://latex.codecogs.com/png.latex?w_2"> and <img src="https://latex.codecogs.com/png.latex?w_3">, and thus will be able to perfectly answer the question <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7Bq%7D"> above. We can see this behaviour in LLMs: they sometimes combine a pair of facts or a pair of abilities which no single human has access to, e.g.&nbsp;when an LLM translates between a pair of languages for which there exists no human speaker of both.</p>
<p><strong>If humans write outside their expertise then the computer will do worse.</strong> In the cases above we assumed that the two humans recorded only what they directly observed, <img src="https://latex.codecogs.com/png.latex?%5Chat%7BQ%7D%5Csubseteq%20Q">. This means the computer essentially had a window directly to the world. However the humans could instead have written down their estimated answers to other questions, for which they have never observed the ground truth. Suppose both humans wrote down answers to every possible question, <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D%5Cin%5C%7B-1,1%5C%7D%5Ep">, then we could conjecture that the computer would learn the average of the two humans’ weights:<sup>14</sup> <img src="https://latex.codecogs.com/png.latex?%5Cbar%7B%5Cbm%7Bw%7D%7D=%5Cfrac%7B1%7D%7B2%7D%5Chat%7B%5Cbm%7Bw%7D%7D_A+%5Cfrac%7B1%7D%7B2%7D%5Chat%7B%5Cbm%7Bw%7D%7D_B."> Here the computer will do worse than the two humans on the original questions, <img src="https://latex.codecogs.com/png.latex?Q_A"> and <img src="https://latex.codecogs.com/png.latex?Q_B">. The implication is that LLMs work so well only because people tend to write about what they know. Put another way, when an LLM answers a question, it will not predict the answer given by the average person, but will predict the answer given by people who are likely to answer that question in the real world. Luckily there tends to be a positive correlation between having knowledge about a domain, and writing about that domain.</p>
<div class="no-row-height column-margin column-container"><div id="fn14"><p><sup>14</sup>&nbsp;We would have to augment the computer’s learning rule to allow for noise in answers - I need to confirm that the weighting will be exactly 1/2.</p></div><div id="fn15"><p><sup>15</sup>&nbsp;This is related to the “generator-discriminator” gap, but specific to knowledge rather than to logical implication.</p></div></div><p><strong>If humans have tacit knowledge, then computers can outperform in choosing a question to maximize the answer.</strong> We can model tacit knowledge with two separate sets of human weights:<sup>15</sup></p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Chat%7B%5Cbm%7Bw%7D%7D%5ET%20%20%20&amp;=%20%5Ctext%7Btacit%20knowledge%7D%5C%5C%0A%20%20%20%20%20%20%5Chat%7B%5Cbm%7Bw%7D%7D%5EE%20&amp;=%20%5Ctext%7Bexplicit%20knowledge%7D%5C%5C%0A%20%20%20%5Cend%7Baligned%7D">
<p>When the human encounters a new question <img src="https://latex.codecogs.com/png.latex?%5Ctilde%7B%5Cbm%7Bq%7D%7D"> they will use their tacit knowledge to form an estimate of the answer, <img src="https://latex.codecogs.com/png.latex?%5Chat%7Ba%7D=%5Ctilde%7B%5Cbm%7Bq%7D%7D'%5Chat%7B%5Cbm%7Bw%7D%7D%5ET">. But they have limited ability to introspect about that capacity, and so when asked how they make their judgments they can report only <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbm%7Bw%7D%7D%5EE">. For simplicity assume tacit knowledge is perfect (<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbm%7Bw%7D%7D%5ET=%5Cbm%7Bw%7D">), and explicit knowledge is imperfect (<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbm%7Bw%7D%7D%5EE%5Cneq%20%5Cbm%7Bw%7D">).</p>
<p>The distinction becomes important when we want to create a new <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D">. Here it’s useful to interpret <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D"> as an artefact, e.g.&nbsp;a text or image, and interpret <img src="https://latex.codecogs.com/png.latex?a=%5Cbm%7Bq%7D'%5Cbm%7Bw%7D"> as a property of that artefact, e.g.&nbsp;how persuasive is the text, or how attractive is the image. Suppose we want to choose <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D%5Cin%5C%7B-1,1%5C%7D%5En"> to maximize <img src="https://latex.codecogs.com/png.latex?a">. If we had perfect access to our beliefs <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bw%7D%5ET"> this would be simple, however if we have access only to imperfect explicit knowledge <img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbm%7Bw%7D%7D%5EE">, the artefact which maximizes that function will not generally be the one which maximizes <img src="https://latex.codecogs.com/png.latex?a">. This represents an asymmetry in human cognition: we can recognize certain patterns (whether text is persuasive, whether a picture is pretty), without being able to produce those patterns.</p>
<p>Here the computer model is less constrained. Suppose the computer has observed sufficiently many questions such that they have perfectly learned human tacit knowledge, <img src="https://latex.codecogs.com/png.latex?%5Cbar%7B%5Cbm%7Bw%7D%7D=%5Chat%7B%5Cbm%7Bw%7D%7D%5ET">. If computation is costless we could query every single <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bq%7D%5Cin%5C%7B-1,1%5C%7D%5Ep"> to find the highest <img src="https://latex.codecogs.com/png.latex?a">. In the real-world we use a diffusion algorithm, or reinforcement learning against human or computer evaluation, to find an artefact with a high <img src="https://latex.codecogs.com/png.latex?a">.</p>
</section>
<section id="derivation" class="level2">
<h2 class="anchored" data-anchor-id="derivation">Derivation</h2>
<p><strong>Setup.</strong></p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20Q%20&amp;=%20%5Cbmatrix%7Bq_1%5E1%20&amp;%20%5Cldots%20&amp;%20q%5E1_p%20%5C%5C%20&amp;%20%5Cddots%20%5C%5C%20q%5En_1%20&amp;%20%5Cldots%20&amp;%20q%5En_p%7D%0A%20%20%20%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7B(matrix%20of%20$n$%20questions,%20each%20with%20$p$%20parameters)%7D%20%5C%5C%0A%20%20%20%20%20%20%5Cbm%7Bw%7D'%20%20&amp;=%20%5Cbmatrix%7Bw_1%20%5Cldots%20w_p%7D%0A%20%20%20%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7B(vector%20of%20$p$%20unobserved%20weights)%7D%5C%5C%0A%20%20%20%20%20%20%5Cbm%7Ba%7D%20%20%20%20&amp;=%20%5Cbmatrix%7Ba%5E1%20%5C%5C%20%5Cvdots%20%5C%5C%20a%5En%7D%0A%20%20%20%20%20%20%20%20%20=%20%5Cbmatrix%7Bq_1%5E1%20w_1%20+%20%5Cldots%20q_p%5E1w_p%20%5C%5C%20%5Cvdots%20%5C%5C%20q_1%5En%20w_1%20+%20%5Cldots%20q_p%5En%20w_p%7D%0A%20%20%20%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7B(vector%20of%20$n$%20observed%20answers)%7D%5C%5C%0A%20%20%20%5Cend%7Baligned%7D">
Written more compactly:
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20Q%20%20%20%20%20%20&amp;%5Cin%20%5C%7B-1,1%5C%7D%5E%7Bn%5Ctimes%20p%7D%0A%20%20%20%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7B($n$%20questions,%20each%20has%20$p$%20binary%20parameters)%7D%5C%5C%0A%20%20%20%20%20%20%5Cbm%7Bw%7D%20&amp;%5Csim%20N(0,%5CSigma)%0A%20%20%20%20%20%20%20%20%20&amp;&amp;%20(p%5Ctimes%201%5Ctext%7B%20vector%20of%20true%20parameters%20of%20the%20world)%7D%5C%5C%0A%20%20%20%20%20%20%5Cut%7B%5Cbm%7Ba%7D%7D%7B$n%5Ctimes1$%7D%20%20%20&amp;=%20%5Cut%7BQ%7D%7B$n%5Ctimes%20p$%7D%5Cut%7B%5Cbm%7Bw%7D%7D%7B$p%5Ctimes1$%7D%0A%20%20%20%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7B(answers%20provided%20by%20the%20world)%7D%5C%5C%0A%20%20%20%5Cend%7Baligned%7D">
<p><strong>Human posteriors.</strong> Given you observe a subset of a set of multivariate normal variables there is a simple expression for your posteriors over the remaining unobserved variables (e.g.&nbsp;see <a href="https://cs.nyu.edu/~roweis/notes/gaussid.pdf">here</a>).</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Chat%7B%5Cbm%7Bw%7D%7D%20&amp;=%20E%5B%5Cbm%7Bw%7D%7CQ,%5Cbm%7Ba%7D%5D%0A%20%20%20%20%20%20%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7B(human%20beliefs%20about%20the%20world)%7D%5C%5C%0A%20%20%20%20%20%20%20%20%20&amp;=%20%5Cut%7B%5CSigma%20Q'%7D%7B$Cov(%5Cbm%7Bw%7D,%5Cbm%7Ba%7D)$%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20(%5Cut%7BQ%5CSigma%20Q'%7D%7B$Var(%5Cbm%7Ba%7D)$%7D)%5E%7B-1%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%5Cbm%7Ba%7D%0A%20%20%20%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7B(from%20the%20Schur%20complement)%7D%0A%20%20%20%5Cend%7Baligned%7D">
<p>We can use the same formula to calculate computer beliefs.</p>
</section>
<section id="additional-observations" class="level2">
<h2 class="anchored" data-anchor-id="additional-observations">Additional Observations</h2>
<p>These are a few miscellaneous results additional results that helped me with intuition for the working of this model.</p>
<strong>With one observation and two weights.</strong> Suppose <img src="https://latex.codecogs.com/png.latex?n=1,%20p=2">, then we have:
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20Q%20%20&amp;=%20%5Cbmatrix%7Bq_1%20&amp;%20q_2%7D%20%5C%5C%0A%20%20%20%20%20%20%5Cbm%7Ba%7D'%20%20&amp;=%20%5Cbmatrix%7Ba%7D%20%5C%5C%0A%20%20%20%20%20%20%5Cbm%7Bw%7D'%20%20&amp;=%20%5Cbmatrix%7Bw_1%20&amp;%20w_2%20%7D%20%5C%5C%0A%20%20%20%20%20%20%5CSigma%20&amp;=%20%5Cbmatrix%7B%5Csigma_1%5E2%20&amp;%20%5Crho%20%5C%5C%20%5Crho%20&amp;%20%5Csigma_2%5E2%7D%5C%5C%0A%20%20%20%20%20%20%5CSigma%20Q'%20&amp;=%20%5Cbmatrix%7B%20%5Csigma_1%5E2q_1%20+%20%5Crho%20q_2%20%5C%5C%20%5Crho%20q_1%20+%20%5Csigma_2%5E2%20q_2%20%7D%20%5C%5C%0A%20%20%20%20%20%20Q%5CSigma%20Q'%20&amp;=%20%5Cbmatrix%7B%20%5Csigma_1%5E2q_1%5E2%20+%202%5Crho%20q_1q_2%20+%20%5Csigma_2%5E2%20q_2%5E2%7D%20%5C%5C%0A%20%20%20%20%20%20%5Chat%7B%5Cbm%7Bw%7D%7D=%5CSigma%20Q'(Q%5CSigma%20Q')%5E%7B-1%7D%5Cbm%7Ba%7D%0A%20%20%20%20%20%20%20%20%20&amp;=%20%5Cbmatrix%7B%20%5Cfrac%7B%5Csigma_1%5E2q_1%20+%20%5Crho%20q_2%7D%7B%5Csigma_1%5E2q_1%5E2%20+%202%5Crho%20q_1q_2%20+%20%5Csigma_2%5E2%20q_2%5E2%7D%20%5C%5C%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5Cfrac%7B%5Crho%20q_1%20+%20%5Csigma_2%5E2%20q_2%7D%7B%5Csigma_1%5E2q_1%5E2%20+%202%5Crho%20q_1q_2%20+%20%5Csigma_2%5E2%20q_2%5E2%7D%7D%20a%0A%20%20%20%5Cend%7Baligned%7D">
<p>We can normalize <img src="https://latex.codecogs.com/png.latex?q_1=q_2=1">, then we have: <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bw%7D_1%20=%20%5Cfrac%7B%5Csigma_1%5E2+%5Crho%7D%7B%5Csigma_1%5E2+2%5Crho+%5Csigma_2%5E2%7Da,"> Here we are dividing up responsibility for the answer (<img src="https://latex.codecogs.com/png.latex?a">) into the contributions of each component, nice and simple.</p>
<strong>With two observations and one weight.</strong> Here we’re <em>over-identified</em>.
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20Q%20%20&amp;=%20%5Cbmatrix%7Bq%5E1%20%5C%5C%20q%5E2%7D%20%5C%5C%0A%20%20%20%20%20%20%5Cbm%7Ba%7D%20%20&amp;=%20%5Cbmatrix%7Ba%5E1%20%5C%5C%20a%5E2%7D%20%5C%5C%0A%20%20%20%20%20%20%5Cbm%7Bw%7D%20%20&amp;=%20%5Cbmatrix%7Bw%20%7D%20%5C%5C%0A%20%20%20%20%20%20%5CSigma%20&amp;=%20%5Cbmatrix%7B%5Csigma%5E2%20%7D%5C%5C%0A%20%20%20%20%20%20%5CSigma%20Q'%20&amp;=%20%5Cbmatrix%7B%20%5Csigma%5E2%20q%5E1%20&amp;%20%5Csigma%5E2%20q%5E2%20%7D%20%5C%5C%0A%20%20%20%20%20%20Q%5CSigma%20Q'%20&amp;=%20%5Cbmatrix%7B%20%5Csigma%5E2%20q%5E1q%5E1%20&amp;%20%5Csigma%5E2q%5E1q%5E2%20%5C%5C%20%5Csigma%5E2q%5E1q%5E2%20&amp;%20%5Csigma%5E2q%5E2q%5E2%7D%0A%20%20%20%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7B(this%20matrix%20doesn't%20have%20an%20inverse)%7D%0A%20%20%20%5Cend%7Baligned%7D">
<strong>With noise.</strong> Suppose we only observe the answers with random noise, then we have
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Cut%7B%5Cbm%7Ba%7D%7D%7B$n%5Ctimes1$%7D%20%20%20&amp;=%20%5Cut%7BQ%7D%7B$n%5Ctimes%20p$%7D%5Cut%7B%5Cbm%7Bw%7D%7D%7B$p%5Ctimes1$%7D%0A%20%20%20%20%20%20%20%20%20+%20%5Cut%7B%5Cbm%7Be%7D%7D%7B$n%5Ctimes%201$%7D%20%5C%5C%0A%20%20%20%20%20%20%5Cbm%7Be%7D%20&amp;%5Csim%20N(%5Cbm%7B0%7D,s%5E2I_n)%20&amp;&amp;%20%5Ctext%7B(i.i.d.%20noise%20with%20variance%20$s%5E2$)%7D%5C%5C%0A%20%20%20%20%20%20Cov(%5Cbm%7Bw%7D,%5Cbm%7Ba%7D)%20%20%20&amp;=%20%5CSigma%20Q'%20%5C%5C%0A%20%20%20%20%20%20Var(%5Cbm%7Ba%7D)%20&amp;=%20Q%5CSigma%20Q'%20+%20s%5E2I_n%20%5C%5C%0A%20%20%20%20%20%20E%5B%5Cbm%7Bw%7D%7CQ,%5Cbm%7Ba%7D%5D%20%20%20&amp;=%20%5CSigma%20Q'(Q%5CSigma%20Q'%20+%20s%5E2I_n)%5E%7B-1%7D%5Cbm%7Ba%7D%0A%20%20%20%5Cend%7Baligned%7D">
<strong>Compare to Bayesian linear regression.</strong> We can compare this result to Bayesian linear regression (e.g.&nbsp;<a href="https://en.wikipedia.org/wiki/Bayesian_linear_regression">Wikipedia</a>):
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Cbar%7B%5Cbeta%7D%20%20&amp;=%20%5CSigma%20Q'(Q%5CSigma%20Q'%20+%20s%5E2I_n)%5E%7B-1%7D%5Cbm%7Ba%7D%0A%20%20%20%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7B(our%20result)%7D%20%5C%5C%0A%20%20%20%20%20%20%5Ctilde%7B%5Cbeta%7D%20&amp;=%20(Q'Q+s%5E%7B2%7D%5CSigma%5E%7B-1%7D)%5E%7B-1%7DQ'%5Cbm%7Ba%7D%0A%20%20%20%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7B(standard%20Bayesian%20linear%20regression)%7D%5C%5C%0A%20%20%20%5Cend%7Baligned%7D">
<p>I <em>believe</em> that these can be shown to be equivalent by the <a href="https://en.wikipedia.org/wiki/Woodbury_matrix_identity">matrix inversion lemma</a>, though I haven’t confirmed this. There is a concise proof in an online note from Utah State University.</p>
<p><strong>Extension: quadratic forms.</strong> Instead of answers being linear in question-features (<img src="https://latex.codecogs.com/png.latex?a=q'w">) we could suppose they’re quadratic, <img src="https://latex.codecogs.com/png.latex?a=q'Wq">, with <img src="https://latex.codecogs.com/png.latex?W"> a matrix having dimension <img src="https://latex.codecogs.com/png.latex?p%5E2">. I am not sure whether we could still get an analytic solution for posteriors. One way to visualise <img src="https://latex.codecogs.com/png.latex?W"> is that each bit in <img src="https://latex.codecogs.com/png.latex?q"> adds an “L” (a row and a column) to the matrix, and <img src="https://latex.codecogs.com/png.latex?a"> is the sum of the cells where both the row and the column are activated.</p>
<p><strong>Extension: binary answers.</strong> In some cases it is natural to think of the answer, <img src="https://latex.codecogs.com/png.latex?a">, as binary instead of continuous. We might be able to reinterpret the model with <img src="https://latex.codecogs.com/png.latex?a"> representing the log-odds ratio of a binary outcome. Alternatively there might be a way of having a beta-binomial conjugate prior over the probability of <img src="https://latex.codecogs.com/png.latex?a">.</p>
<section id="whos-closer" class="level3">
<h3 class="anchored" data-anchor-id="whos-closer">Who’s closer</h3>
<p>Suppose two people, <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?B">, have observed different training sets <img src="https://latex.codecogs.com/png.latex?Q%5EA"> and <img src="https://latex.codecogs.com/png.latex?Q%5EB">, then we can characterize their expected error for the answer of a new question <img src="https://latex.codecogs.com/png.latex?q"> (where <img src="https://latex.codecogs.com/png.latex?q%5Cnot%5Cin%20Q%5EA">, <img src="https://latex.codecogs.com/png.latex?q%5Cnot%5Cin%20Q%5EB">):</p>
<p><strong>Setup.</strong> Let the true weights be <img src="https://latex.codecogs.com/png.latex?%5Cbm%7Bw%7D%5Csim%20N(0,%5CSigma)"> and let the two individuals observe <img src="https://latex.codecogs.com/png.latex?%5Cbm%20a%5Ei%20=%20Q%5Ei%5Cbm%20w%5C;(i%5Cin%5C%7BA,B%5C%7D)">. Their posterior mean is<br>
<img src="https://latex.codecogs.com/png.latex?%5Chat%7B%5Cbm%20w%7D%5E%7B%5C,i%7D=%20%5CSigma(Q%5Ei)'%5C!%5Cbigl(Q%5Ei%5CSigma(Q%5Ei)'%5Cbigr)%5E%7B-1%7D%5Cbm%20a%5Ei"> and their posterior covariance is<br>
<img src="https://latex.codecogs.com/png.latex?%5CSigma_%7B%5Cmid%20i%7D=%20%5CSigma-%5CSigma(Q%5Ei)'%5C!%5Cbigl(Q%5Ei%5CSigma(Q%5Ei)'%5Cbigr)%5E%7B-1%7DQ%5Ei%5CSigma."></p>
<p><strong>Expected error for a new question <img src="https://latex.codecogs.com/png.latex?q">.</strong> Both people answer the fresh question by <img src="https://latex.codecogs.com/png.latex?%5Chat%20a_i%20=%20q'%5Chat%7B%5Cbm%20w%7D%5E%7B%5C,i%7D">, while the truth is <img src="https://latex.codecogs.com/png.latex?a=q'%5Cbm%20w">. Conditioning on the training set we therefore have<br>
<img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%20%20%5Cmathbb%7BE%7D%5C!%5Cleft%5B(a-%5Chat%20a_i)%5E2%5C;%5Cmiddle%7C%5C;Q%5Ei%5Cright%5D%0A%20%20%20%20%20%20%20%20%20=%5Cmathbb%7BE%7D%5C!%5Cleft%5B(q'(%5Cbm%20w-%5Chat%7B%5Cbm%20w%7D%5E%7B%5C,i%7D))%5E2%5C;%5Cmiddle%7C%5C;Q%5Ei%5Cright%5D%0A%20%20%20%20%20%20%20%20%20=%20q'%5CSigma_%7B%5Cmid%20i%7Dq.%20%20%5Ctag%7B1%7D%0A%20%20%20"></p>
<p><strong>Isotropic prior.</strong> If we specialise to <img src="https://latex.codecogs.com/png.latex?%5CSigma=%5Csigma%5E%7B2%7DI_p"> then<br>
<img src="https://latex.codecogs.com/png.latex?%5CSigma_%7B%5Cmid%20i%7D=%20%5Csigma%5E%7B2%7D%5C!%5Cbigl(I-P%5E%7Bi%7D%5Cbigr)"> with the orthogonal projector<br>
<img src="https://latex.codecogs.com/png.latex?P%5E%7Bi%7D=%20(Q%5Ei)'%5Cbigl(Q%5Ei(Q%5Ei)'%5Cbigr)%5E%7B-1%7DQ%5Ei"> onto the row–span of <img src="https://latex.codecogs.com/png.latex?Q%5Ei">. Hence<br>
<img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%20%20%5Cmathrm%7Berr%7D_i(q)%5Cequiv%5Cmathbb%7BE%7D%5C!%5Cleft%5B(a-%5Chat%20a_i)%5E2%5C;%5Cmiddle%7C%5C;Q%5Ei%5Cright%5D%0A%20%20%20%20%20%20=%5Csigma%5E%7B2%7D%5Cbigl%5C%7C(I-P%5E%7Bi%7D)%5C,q%5Cbigr%5C%7C%5E%7B2%7D.%0A%20%20%20"></p>
<p>In words, the expected squared error is exactly the squared length of the component of <img src="https://latex.codecogs.com/png.latex?q"> that is <strong>orthogonal</strong> to the set of questions that person <img src="https://latex.codecogs.com/png.latex?i"> has already encountered. It is zero if <img src="https://latex.codecogs.com/png.latex?q"> is contained in the span of their past questions and grows with the distance of <img src="https://latex.codecogs.com/png.latex?q"> from that span.</p>
<p><strong>Who’s closer?</strong> Person <img src="https://latex.codecogs.com/png.latex?A"> is expected to be more accurate than <img src="https://latex.codecogs.com/png.latex?B"> precisely when<br>
<img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%20%20%20%20%20q'%5CSigma_%7B%5Cmid%20A%7Dq%20%5C;%3C%5C;%20q'%5CSigma_%7B%5Cmid%20B%7Dq%0A%20%20%20%20%20%20%20%20%5Cqquad%5Cbigl(%5Ctext%7Bequivalently%20%7D%0A%20%20%20%20%20%20%20%20%5C%7C(I-P%5E%7BA%7D)q%5C%7C%20%3C%20%5C%7C(I-P%5E%7BB%7D)q%5C%7C%5Cbigr).%0A%20%20%20"> Thus the tie-breaker between the two forecasters is which training set provides a better <strong>projection</strong> of the new question <img src="https://latex.codecogs.com/png.latex?q">.</p>
</section>
</section>
</section>
<section id="references" class="level1">




</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-armengolestape2021multilingual" class="csl-entry">
Armengol-Estapé, Jordi, Ona de Gibert Bonet, and Maite Melero. 2021. <span>“On the Multilingual Capabilities of Very Large-Scale English Language Models.”</span> <a href="https://arxiv.org/abs/2108.13349">https://arxiv.org/abs/2108.13349</a>.
</div>
<div id="ref-bai2023persuade" class="csl-entry">
Bai, Hui, Jan G Voelkel, johannes C Eichstaedt, and Robb Willer. 2023. <span>“Artificial Intelligence Can Persuade Humans on Political Issues.”</span> OSF Preprints. <a href="https://doi.org/10.31219/osf.io/stakv">https://doi.org/10.31219/osf.io/stakv</a>.
</div>
<div id="ref-bowman2023eight" class="csl-entry">
Bowman, Samuel R. 2023. <span>“Eight Things to Know about Large Language Models.”</span> <em>arXiv Preprint arXiv:2304.00612</em>.
</div>
<div id="ref-camerer1991processperformance" class="csl-entry">
Camerer, Colin, and Eric J. Johnson. 1991. <span>“The Process-Performance Paradox in Expert Judgment - How Can Experts Know so Much and Predict so Badly?”</span> In. <a href="https://api.semanticscholar.org/CorpusID:67971809">https://api.semanticscholar.org/CorpusID:67971809</a>.
</div>
<div id="ref-cundy2023sequencematch" class="csl-entry">
Cundy, Chris, and Stefano Ermon. 2023. <span>“SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling with Backtracking.”</span> <em>arXiv Preprint arXiv:2306.05426</em>.
</div>
<div id="ref-cunningham2015hierarchical" class="csl-entry">
Cunningham, Tom. 2015. <span>“Hierarchical Aggregation of Information and Decision-Making.”</span> <em>Unpublished Manuscript, Columbia University</em>.
</div>
<div id="ref-cunningham2022implicit" class="csl-entry">
Cunningham, Tom, and Jonathan De Quidt. 2022. <span>“Implicit Preferences.”</span> <a href="http://jondequidt.com/pdfs/paper_implicit.pdf">http://jondequidt.com/pdfs/paper_implicit.pdf</a>.
</div>
<div id="ref-goldstein2023persuasive" class="csl-entry">
Goldstein, Josh A, Jason Chao, Shelby Grossman, Alex Stamos, and Michael Tomz. 2023. <span>“Can AI Write Persuasive Propaganda?”</span> SocArXiv. <a href="https://doi.org/10.31235/osf.io/fp87b">https://doi.org/10.31235/osf.io/fp87b</a>.
</div>
<div id="ref-hackenburg2023persuasive" class="csl-entry">
Hackenburg, Kobi, and Helen Margetts. 2023. <span>“Evaluating the Persuasive Influence of Political Microtargeting with Large Language Models.”</span> OSF Preprints. <a href="https://doi.org/10.31219/osf.io/wnt8b">https://doi.org/10.31219/osf.io/wnt8b</a>.
</div>
<div id="ref-hendrycks2021measuring" class="csl-entry">
Hendrycks, Dan, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. <span>“Measuring Mathematical Problem Solving with the Math Dataset.”</span> <em>arXiv Preprint arXiv:2103.03874</em>.
</div>
<div id="ref-kiela2021dynabench" class="csl-entry">
Kiela, Douwe, Max Bartolo, Yixin Nie, Divyansh Kaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vidgen, et al. 2021. <span>“Dynabench: Rethinking Benchmarking in NLP.”</span> <em>arXiv Preprint arXiv:2104.14337</em>.
</div>
<div id="ref-kiela2023plottingprogress" class="csl-entry">
Kiela, Douwe, Tristan Thrush, Kawin Ethayarajh, and Amanpreet Singh. 2023. <span>“Plotting Progress in AI.”</span> <em>Contextual AI Blog</em>.
</div>
<div id="ref-koivisto2023creativity" class="csl-entry">
Koivisto, Mika, and Simone Grassini. 2023. <span>“Best Humans Still Outperform Artificial Intelligence in a Creative Divergent Thinking Task.”</span> <em>Scientific Reports</em> 13 (1): 13601. <a href="https://doi.org/10.1038/s41598-023-40858-3">https://doi.org/10.1038/s41598-023-40858-3</a>.
</div>
<div id="ref-lee2023rlaif" class="csl-entry">
Lee, Harrison, Samrat Phatale, Hassan Mansoor, Kellie Lu, Thomas Mesnard, Colton Bishop, Victor Carbune, and Abhinav Rastogi. 2023. <span>“RLAIF: Scaling Reinforcement Learning from Human Feedback with AI Feedback.”</span> <em>arXiv Preprint arXiv:2309.00267</em>.
</div>
<div id="ref-macdougall1904recognition" class="csl-entry">
MacDougall, Robert. 1904. <span>“Recognition and Recall.”</span> <em>The Journal of Philosophy, Psychology and Scientific Methods</em> 1 (9): 229–33.
</div>
<div id="ref-matz2023personalized" class="csl-entry">
Matz, Sandra, Jake Teeny, Sumer S Vaid, Gabriella M Harari, and Moran Cerf. 2023. <span>“The Potential of Generative AI for Personalized Persuasion at Scale.”</span> PsyArXiv. <a href="https://doi.org/10.31234/osf.io/rn97c">https://doi.org/10.31234/osf.io/rn97c</a>.
</div>
<div id="ref-ouyang2022training" class="csl-entry">
Ouyang, Long, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, et al. 2022. <span>“Training Language Models to Follow Instructions with Human Feedback.”</span> <em>Advances in Neural Information Processing Systems</em> 35: 27730–44.
</div>
<div id="ref-palmer2023large" class="csl-entry">
Palmer, Alexis, and Arthur Spirling. 2023. <span>“Large Language Models Can Argue in Convincing and Novel Ways about Politics: Evidence from Experiments and Human Judgement.”</span> Working paper), Technical report.
</div>
<div id="ref-qin2023large" class="csl-entry">
Qin, Zhen, Rolf Jagerman, Kai Hui, Honglei Zhuang, Junru Wu, Jiaming Shen, Tianqi Liu, et al. 2023. <span>“Large Language Models Are Effective Text Rankers with Pairwise Ranking Prompting.”</span> <a href="https://arxiv.org/abs/2306.17563">https://arxiv.org/abs/2306.17563</a>.
</div>
<div id="ref-stiennon2022learning" class="csl-entry">
Stiennon, Nisan, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. 2022. <span>“Learning to Summarize from Human Feedback.”</span> <a href="https://arxiv.org/abs/2009.01325">https://arxiv.org/abs/2009.01325</a>.
</div>
<div id="ref-tedeschi2023s" class="csl-entry">
Tedeschi, Simone, Johan Bos, Thierry Declerck, Jan Hajic, Daniel Hershcovich, Eduard H Hovy, Alexander Koller, et al. 2023. <span>“What’s the Meaning of Superhuman Performance in Today’s NLU?”</span> <em>arXiv Preprint arXiv:2305.08414</em>.
</div>
<div id="ref-towler2023facial" class="csl-entry">
Towler, Alice, James D. Dunn, Sergio Castro Martı́nez, Reuben Moreton, Fredrick Eklöf, Arnout Ruifrok, Richard I. Kemp, and David White. 2023. <span>“Diverse Types of Expertise in Facial Recognition.”</span> <em>Scientific Reports</em> 13 (1): 11396. <a href="https://doi.org/10.1038/s41598-023-28632-x">https://doi.org/10.1038/s41598-023-28632-x</a>.
</div>
<div id="ref-zhang2024transcendence" class="csl-entry">
Zhang, Edwin, Vincent Zhu, Naomi Saphra, Anat Kleiman, Benjamin L Edelman, Milind Tambe, Sham M Kakade, and Eran Malach. 2024. <span>“Transcendence: Generative Models Can Outperform the Experts That Train Them.”</span> <em>arXiv Preprint arXiv:2406.11741</em>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{cunningham2023,
  author = {Cunningham, Tom},
  title = {An {AI} {Which} {Imitates} {Humans} {Can} {Beat} {Humans}},
  date = {2023-10-06},
  url = {tecunningham.github.io/posts/2023-09-05-model-of-ai-imitation.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-cunningham2023" class="csl-entry quarto-appendix-citeas">
Cunningham, Tom. 2023. <span>“An AI Which Imitates Humans Can Beat
Humans.”</span> October 6, 2023. <a href="https://tecunningham.github.io/posts/2023-09-05-model-of-ai-imitation.html">tecunningham.github.io/posts/2023-09-05-model-of-ai-imitation.html</a>.
</div></div></section></div> ]]></description>
  <guid>tecunningham.github.io/posts/2023-09-05-model-of-ai-imitation.html</guid>
  <pubDate>Fri, 06 Oct 2023 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Sushi-Roll Model of Online Media</title>
  <dc:creator>Tom Cunningham, [Integrity Institute](https://integrityinstitute.org/)</dc:creator>
  <link>tecunningham.github.io/posts/2023-03-06-social-media-business-models-sushi-roll.html</link>
  <description><![CDATA[ 





<style>
h1 { border-bottom: 4px solid black;}
h2 { border-bottom: 1px solid #ccc;}
</style>
<div class="hidden">
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cdef%5CRR%7B%7B%5Cbf%20R%7D%7D%0A%5Cdef%5Cbold#1%7B%7B%5Cbf%20#1%7D%7D%0A"></p>
</div>
<div class="cell page-columns page-full" data-preamble="\usepackage{pgf-pie}">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/data:image/png;base64,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" class="img-fluid figure-img" width="384"></p>
</figure>
</div>
</div></div></div>
<p><strong>A model of internet media: the platform chooses the <em>composition</em>, the user chooses the <em>quantity</em>.</strong> I think this is a nice crisp way of modeling how media platforms (FB, YouTube, TikTok) make their decisions about content: they chooses the <em>mix</em> of content, i.e.&nbsp;the shares of each type, and then their users choose the <em>quantity</em>. The platform is choosing the fillings for the sushi roll and the consumer is choosing how much to eat. Their decisions jointly determine the total amount of each ingredient consumed.</p>
<p><strong>This gives a unified model of feed ranking inclusive of ad-load, revenue-sharing, producer-side effects, and advertiser demand elasticity.</strong></p>
<p>We can break down four different ways in which increasing the share of a given content-type <img src="https://latex.codecogs.com/png.latex?i"> will affect total revenue:</p>
<ol type="1">
<li><strong>Consumption incrementality.</strong> The effect on total impressions, AKA incrementality. In general platforms are always trying to find the types of content that increase total long-run consumption for a given user.</li>
<li><strong>Price.</strong> The price received by the platform for each impression of content-type <img src="https://latex.codecogs.com/png.latex?i">. For ads this is positive, but platforms also sometimes license content or pay a revenue-share, in which case the price is negative.</li>
<li><strong>Price elasticity.</strong> The effect of quantity on price. E.g. when we increase the number of ad impressions shown this will lower the market price for all other ad-impressions, and so this is a reason to limit the quantity of ads.</li>
<li><strong>Production elasticity.</strong> The effect on production by producers. Some producers will produce more content when they receive more impressions, and so this is an additional benefit of increasing their share of impressions.</li>
</ol>
<p>An efficient mix of content will choose the shares such that each type of content has the same marginal value, i.e.&nbsp;for each the type four components of value all sum to the same number.</p>
<p><strong>Related literature.</strong> There are some nice models of ad-media tradeoff in <span class="citation" data-cites="anderson2015handbook">Anderson and Jullien (2015)</span>, but I believe they don’t consider the effects on production by producers, nor the tradeoff between different types of content (though it’s a long time since I read it).</p>
<p><strong>Expressed formally:</strong> Suppose the platform chooses <img src="https://latex.codecogs.com/png.latex?x_1,%5Cldots,x_n"> which represent the impression-shares of each type of content such that <img src="https://latex.codecogs.com/png.latex?%5Csum_%7Bi=1%7D%5Enx_i=1">. User demand depends on the average quality of each type of content (<img src="https://latex.codecogs.com/png.latex?q_i">), and they have diminishing returns in each type of content. The platform receives <img src="https://latex.codecogs.com/png.latex?p_i"> for showing an impression of type <img src="https://latex.codecogs.com/png.latex?i">, but that price depends on the number of impressions-seen. We can write the maximization problem as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Cmax_%7Bx_1,%5Cldots,x_n%7D%20%5Cutt%7B%5Cleft(%5Csum_%7Bi=1%7D%5En%20q_i(x_i)x_i%5E%5Cgamma%5Cright)%7D%7Btotal%7D%7Bimpressions%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5Cutt%7B%5Cleft(%5Csum_%7Bi=1%7D%5En%20x_ip_i(x_i)%5Cright)%7D%7Bavg%20revenue%7D%7Bper%20impression%7D%0A%20%20%20%20%20%20,%5Ctext%7B%20s.t.%20%7D%5Csum_%7Bi=1%7D%5En%20x_i=1%0A%5Cend%7Baligned%7D%0A"></p>
<p>The first order condition shows us the four components of value:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x_i%7D%20=%0A%20%20%20%5Cut%7B%0A%20%20%20%20%20%20(%5Cutt%7Bq_i%20%5Cgamma%20x_i%5E%7B-(1-%5Cgamma)%7D%7D%7Bincrementality%7D%7B%7D%0A%20%20%20%20%20%20+%20%5Cutt%7Bq_i'(x_i)x_i%5E%5Cgamma%7D%7Beffect%20through%7D%7Bquality%7D)%0A%20%20%20%20%20%20%5Cutt%7B%5Cleft(%5Csum_%7Bj=1%7D%5En%20p_j%20x_j%5Cright)%7D%7Bavg%20revenue%7D%7Bper%20impression%7D%0A%20%20%20%7D%7Beffect%20on%20revenue%20through%20total%20impressions%7D%0A%20%20%20+%0A%20%20%20%5Cutt%7B%0A%20%20%20%20%20%20(%0A%20%20%20%20%20%20%20%20%20%5Cutt%7Bp_i(x_i)%7D%7Brevenue%20from%7D%7Badditional%20impressions%7D+%0A%20%20%20%20%20%20%20%20%20%5Cutt%7Bp'_i(x_i)x_i%7D%7Brevenue%20from%7D%7Bchange%20in%20price%7D%0A%20%20%20%20%20%20)%0A%20%20%20%20%20%20%5Cutt%7B%5Cleft(%5Csum_%7Bj=1%7D%5En%20q_j%20x_j%5E%7B%5Cgamma%7D%5Cright)%7D%7Btotal%7D%7Bimpressions%7D%7D%0A%20%20%20%20%20%20%7Beffect%20on%20revenue%7D%7Bthrough%20impressions%20on%20$i$%7D%0A%20%20%20%20+%20%5Cutt%7B%5Clambda%7D%7Bavg%20marginal%7D%7Beffect%7D=0%0A"></p>
<p>The final term, <img src="https://latex.codecogs.com/png.latex?%5Clambda">, is the Lagrangian, representing the average marginal value of the outside option, i.e.&nbsp;the other types of content that are being replaced. In some cases we can simplify this model and we get a closed-form solution for the optimal content composition.</p>
<p><strong>What this model doesn’t include:</strong></p>
<ul>
<li><p><em>The effect of consumption on production.</em> E.g. we sometimes want to reduce ad-load to increase production by users, or we want to show users friend posts because it increases production (“mimicry”). You could incorporate this by defining <img src="https://latex.codecogs.com/png.latex?x_%7Bi,j%7D"> as the share shown from producer <img src="https://latex.codecogs.com/png.latex?i"> to consumer <img src="https://latex.codecogs.com/png.latex?j">, and then each user’s production depends both on (1) how many impressions they get on their content; (2) composition of impressions that they give to other content.</p></li>
<li><p><em>Setting a price for production (revenue sharing).</em> In this model the platform’s choice variables are just the quantities, <img src="https://latex.codecogs.com/png.latex?x_i">. However for revenue-sharing it seems that the platform is setting a <em>price</em>, with the goal of increasing producer quality. I don’t think you can model this such that price is a function of quantity (as we do with ads). I think you need to keep track of two separate things: (1) what happens when I give this producer more distribution; (2) what happens when I pay this producer to produce.</p></li>
</ul>
<section id="more-details" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="more-details">More Details</h2>
<p>We can walk through a series of models from simple to complicated, to build up to the full sushi-roll model:</p>
<ol type="1">
<li><p><strong>Platform chooses share of ads.</strong> The platform chooses the share of impressions that are ads, and consumers choose how many total impressions to consume. If we assume the price of ads (CPMs) is fixed then the platform will set the ad-load to maximize the total number of ad-impressions. If the platform can influence the price of ads by their choice of quantity (i.e.&nbsp;they act as a monopolist) then the platform may choose to reduce ad-load to drive up CPMs.<sup>1</sup></p>
<p>In this model we’re letting the platform set the quantity of ads, but we would get the same result if the platform instead set the <em>price</em> of ads, e.g.&nbsp;they posted a specific CPM and advertisers can buy as much as they want.</p></li>
<li><p><strong>Platforms chooses shares of organic content.</strong> Suppose ad-load is fixed but platforms can vary the shares of different types of organic content. Users’ consumption depends on the quality of the content but they also have a taste for variety (i.e.&nbsp;diminishing returns in each type of content). We then get a nice closed-form solution where the share of each type of content is increasing in its relative quality. On the margin the incrementality of each type of content will be zero: i.e.&nbsp;increasing the share of that type of content will have no effect on total impressions.</p></li>
<li><p><strong>Platform choses shares of ads and organic content.</strong> Now lets treat both advertisers and organic producers as the same: each producer has a quality <img src="https://latex.codecogs.com/png.latex?q_i"> but they also will pay a certain price <img src="https://latex.codecogs.com/png.latex?p_i"> for impressions on their content. The platform takes those prices as given. We can then distinguish between three types of producer:</p>
<ul>
<li>Advertisers: <img src="https://latex.codecogs.com/png.latex?p_i%3C0">: the producer will pay the platform per impression.</li>
<li>Professional producers: <img src="https://latex.codecogs.com/png.latex?p_i%3E0">: the producer asks to be paid per impression.</li>
<li>Amateurs: <img src="https://latex.codecogs.com/png.latex?p_i=0">: there is no monetary exchange, the content is in the public domain or generated by an ordinary user.</li>
</ul>
<p>In equilibrium the share of impressions allocated to a given producer (<img src="https://latex.codecogs.com/png.latex?x_i">) will depend both on its quality <img src="https://latex.codecogs.com/png.latex?q_i"> (AKA incrementality) and the price <img src="https://latex.codecogs.com/png.latex?p_i"> that the producer sets. I don’t have a closed-form solution but we can derive a first-order condition that has a straight-forward interpretation.</p>
<p>This model is easy to state but I think is primarily applicable to <em>small</em> platforms where they take prices as given. E.g. suppose you run an app where you (1) license certain content, or use user-generated content; (2) run ads from various different ad networks, the ads vary in CPMs but they also vary in incrementality (i.e.&nbsp;how obnoxious they are to your userbase).</p>
<p>Because prices are taken as given, this model doesn’t help us calculate the optimal revenue share. It will tell us the optimal ad-load, but not taking into account the elasticity of supply from the advertiser.</p>
<p>Note that most platforms do not explicitly discriminate between advertisers based on their incrementality however they can implicitly discriminate by having a “quality score” or “organic bid”. This score is quite clearly designed to measure the incrementality of the advertisements, and so I think can be used to implement an efficient pricing scheme.</p></li>
<li><p><strong>Platform choses shares of ads and organic content, prices endogenous.</strong> We can easily extend the model above to allow the price of each type of content to depend on the quantity shown (<img src="https://latex.codecogs.com/png.latex?p_i(x_iM)">). For example the price of ads will depend on the number of ad impression shown (due to advertisers’ diminishing marginal returns from ads shown). This gives platforms a reason to restrict the quantity of ad-impressions.</p></li>
<li><p><strong>Platform chooses both shares and prices.</strong> Up to this point the platform chose only the share of each type of content.</p>
<p>Now suppose the platform can set a price to pay producers (e.g.&nbsp;“revenue share”), and it’s a homogenous price. The price should roughly depend on (1) the producer’s elasticity of quality to price (i.e.&nbsp;their cost function), and (2) the incrementality of quality on the consumer side. We could set this up with a single price for all producers, or set a producer-specific price.</p></li>
</ol>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;In the profit-maximizing solution <em>either</em> the consumer-side or advertiser-side first-order-condition will be binding. It depends on the relative elasticity of the two sides of the platform.</p></div></div></section>
<section id="model-1-platform-chooses-ad-load" class="level2">
<h2 class="anchored" data-anchor-id="model-1-platform-chooses-ad-load">Model 1: Platform Chooses Ad-Load</h2>
<p>(see previous paper)</p>
</section>
<section id="model-2-platform-chooses-organic-composition" class="level2">
<h2 class="anchored" data-anchor-id="model-2-platform-chooses-organic-composition">Model 2: Platform Chooses Organic Composition</h2>
<p>We have a model where there are <img src="https://latex.codecogs.com/png.latex?n"> producers, the platform assigns to each producer a share of total content <img src="https://latex.codecogs.com/png.latex?x_i">, with <img src="https://latex.codecogs.com/png.latex?%5Csum_ix_i=1">, and the consumer will choose how many total impressions to consume (<img src="https://latex.codecogs.com/png.latex?M">) based on the average quality, but with diminishing returns in each .</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20q_i%20&amp;%5Cin%20%5Cmathbb%7BR%7D%5E+%0A%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7Bquality%20of%20producer%20$i$%7D%20%20%5C%5C%0A%20%20%20x_i%20&amp;%5Cin%20%5B0,1%5D%0A%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7Bshare%20of%20impressions%20on%20producer%20$i$%7D%5C%5C%0A%20%20%20%5Csum_i%20x_i%20&amp;=%201%0A%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7Bshares%20must%20sum%20to%201%7D%5C%5C%0A%20%20%20M%20%20&amp;=%20%5Csum_%7Bi=1%7D%5Enq_ix_i%5E%5Cgamma%0A%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7Btotal%20impressions,%20diminishing%20returns%20in%20each%20producer,%20$0%3C%5Cgamma%3C1$%7D%20%5C%5C%0A%5Cend%7Baligned%7D"></p>
<p>The platform wishes to maximize total impressions, <img src="https://latex.codecogs.com/png.latex?M">. We want to solve for the resultant impression-share of each producer, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?x_i"> as a function of the qualities <img src="https://latex.codecogs.com/png.latex?q_1,..,q_n"> and parameter <img src="https://latex.codecogs.com/png.latex?%5Cgamma">. We get the following impression-maximizing shares:</p>
<p><img src="https://latex.codecogs.com/png.latex?x_i=%5Cfrac%7Bq_i%5E%5Cfrac%7B1%7D%7B1-%5Cgamma%7D%7D%7B%5Cgamma%5Csum_%7Bj=1%7D%5Enq_j%5E%5Cfrac%7B1%7D%7B1-%5Cgamma%7D%7D."></p>
<p><strong>Implication: impression-share will be proportional to quality.</strong> Interesting the elasticity will be <em>increasing</em> in quality: a 1% increase in quality will get a <em>more than</em> 1% increase in share of impressions, because <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B1%7D%7B1-%5Cgamma%7D%3E1">.</p>
<p><strong>Adding money.</strong> Suppose now that the platform gets paid for showing certain impressions. We can make various different assumption about the price paid:</p>
<ol type="1">
<li>Uniform homogenous price: the platform takes the price as given. This only makes sense if there are a subset of producers who are advertisers.</li>
<li>Each producer sets a payment rate per impression.</li>
<li>The platform chooses a single price for all producers to get extra impressions.</li>
</ol>
</section>
<section id="model-3-platform-chooses-composition-prices-fixed" class="level2">
<h2 class="anchored" data-anchor-id="model-3-platform-chooses-composition-prices-fixed">Model 3: Platform Chooses Composition, Prices Fixed</h2>
<p>We have a model with a consumer, a platform, and a set of <img src="https://latex.codecogs.com/png.latex?n"> producers. The platform chooses the share of content from each producer, <img src="https://latex.codecogs.com/png.latex?x_i%5Cin%5B0,1%5D"> with <img src="https://latex.codecogs.com/png.latex?%5Csum_i%20x_i=1">. The consumer chooses the total amount of impressions they consume, <img src="https://latex.codecogs.com/png.latex?M">, based on the mixture of content and the quality of each type of content <img src="https://latex.codecogs.com/png.latex?q_i">. Finally producers can set a price <img src="https://latex.codecogs.com/png.latex?p_i"> for each impression that they receive from the consumer. A positive price <img src="https://latex.codecogs.com/png.latex?p_i%3E0"> means</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20q_i%20&amp;%5Cin%20%5Cmathbb%7BR%7D%0A%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7Bquality%20of%20producer%20$i$%7D%20%20%5C%5C%0A%20%20%20p_i%20&amp;%5Cin%20%5Cmathbb%7BR%7D%0A%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7Bprice%20offered%20by%20producer%20$i$%7D%20%20%5C%5C%0A%20%20%20x_i%20&amp;%5Cin%20%5B0,1%5D%0A%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7Bshare%20of%20impressions%20on%20producer%20$i$%7D%5C%5C%0A%20%20%20%5Csum_i%20x_i%20&amp;=%201%0A%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7Bshares%20must%20sum%20to%201%7D%5C%5C%0A%20%20%20M%20%20&amp;=%20%5Csum_%7Bi=1%7D%5Enq_ix_i%5E%5Cgamma%0A%20%20%20%20%20%20&amp;&amp;%20%5Ctext%7Btotal%20impressions,%20diminishing%20returns%20in%20each%20producer,%20$0%3C%5Cgamma%3C1$%7D%20%5C%5C%0A%5Cend%7Baligned%7D%0A"></p>
<p>If the platform simply wanted to maximize total impressions, <img src="https://latex.codecogs.com/png.latex?M">, then they can derive the optimal impression-shares as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?x_i%5E*=%5Cfrac%7Bq_i%5E%5Cfrac%7B1%7D%7B1-%5Cgamma%7D%7D%7B%5Cgamma%5Csum_%7Bj=1%7D%5Enq_j%5E%5Cfrac%7B1%7D%7B1-%5Cgamma%7D%7D."></p>
<p>However we want the platform to maximize profit, which we can write as follows:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%5Ctext%7Bprofit%7D%20&amp;=%20%5Cutt%7B%5Cleft(%5Csum_%7Bi=1%7D%5En%20q_ix_i%5E%5Cgamma%5Cright)%7D%7Btotal%7D%7Bimpressions%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5Cutt%7B%5Cleft(%5Csum_%7Bi=1%7D%5En%20x_ip_i%5Cright)%7D%7Bavg%20revenue%7D%7Bper%20impression%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>I’m not sure if we can get a closed-form solution but we can at least get a first-order condition for each <img src="https://latex.codecogs.com/png.latex?x_i"> that tells us useful stuff about comparative statics:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x_i%7D%20=%0A%20%20%20%5Cut%7B%5Cutt%7Bq_i%20%5Cgamma%20x_i%5E%7B-(1-%5Cgamma)%7D%7D%7Beffect%20on%7D%7Btotal%20impressions%7D%0A%20%20%20%20%20%20%5Cutt%7B%5Cleft(%5Csum_%7Bj=1%7D%5En%20p_j%20x_j%5Cright)%7D%7Bavg%20revenue%7D%7Bper%20impression%7D%0A%20%20%20%7D%7Beffect%20on%20revenue%20through%20total%20impressions%7D%0A%20%20%20+%0A%20%20%20%5Cutt%7Bp_i%5Cutt%7B%5Cleft(%5Csum_%7Bj=1%7D%5En%20q_j%20x_j%5E%7B%5Cgamma%7D%5Cright)%7D%7Btotal%7D%7Bimpressions%7D%7D%0A%20%20%20%20%20%20%7Beffect%20on%20revenue%7D%7Bthrough%20impressions%20on%20$i$%7D%0A%20%20%20%20+%20%5Cutt%7B%5Clambda%7D%7Bavg%20marginal%7D%7Beffect%7D=0%0A"></p>
<p>Observations:</p>
<ol type="1">
<li>If producers offer more, increasing <img src="https://latex.codecogs.com/png.latex?p_i">, then <img src="https://latex.codecogs.com/png.latex?x_i"> will go down until the marginal effect on total impression declines to balance the additional revenue.</li>
<li>If <img src="https://latex.codecogs.com/png.latex?p_i%3C0">, meaning a producer charges for impressions, then they can still have a positive number of impressions if their effect on total impressions is higher than the average of other types of content. (We could have added an additional constraint that <img src="https://latex.codecogs.com/png.latex?x_i%5Cgeq%200">.)</li>
</ol>
</section>
<section id="model-4-platform-chooses-composition-monopolist" class="level2">
<h2 class="anchored" data-anchor-id="model-4-platform-chooses-composition-monopolist">Model 4: Platform Chooses Composition, Monopolist</h2>
<p>Now we allow the price of each type of content to depend on the quantity used, e.g.&nbsp;the price of ads will be higher when the quantity of ad-impressions is smaller (monopolist in the ad market). Strictly we should write <img src="https://latex.codecogs.com/png.latex?p_i(Mx_i)">, but it’s somewhat easier to write <img src="https://latex.codecogs.com/png.latex?p_i(x_i)"> and the answer should be similar for any type of content that is a small share.</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Ctext%7Bprofit%7D%20&amp;=%20%5Cutt%7B%5Cleft(%5Csum_%7Bi=1%7D%5En%20q_ix_i%5E%5Cgamma%5Cright)%7D%7Btotal%7D%7Bimpressions%7D%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5Cutt%7B%5Cleft(%5Csum_%7Bi=1%7D%5En%20x_ip_i(x_i)%5Cright)%7D%7Bavg%20revenue%7D%7Bper%20impression%7D%0A%5Cend%7Baligned%7D%0A"></p>
<p>There’s now one additional term in the first order condition:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20x_i%7D%20=%0A%20%20%20%5Cut%7B%5Cutt%7Bq_i%20%5Cgamma%20x_i%5E%7B-(1-%5Cgamma)%7D%7D%7Beffect%20on%7D%7Btotal%20impressions%7D%0A%20%20%20%20%20%20%5Cutt%7B%5Cleft(%5Csum_%7Bj=1%7D%5En%20p_j%20x_j%5Cright)%7D%7Bavg%20revenue%7D%7Bper%20impression%7D%0A%20%20%20%7D%7Beffect%20on%20revenue%20through%20total%20impressions%7D%0A%20%20%20+%0A%20%20%20%5Cutt%7B%0A%20%20%20%20%20%20(%0A%20%20%20%20%20%20%20%20%20%5Cutt%7Bp_i(x_i)%7D%7Brevenue%20from%7D%7Badditional%20impressions%7D+%0A%20%20%20%20%20%20%20%20%20%5Cutt%7Bp'_i(x_i)x_i%7D%7Brevenue%20from%7D%7Bchange%20in%20price%7D%0A%20%20%20%20%20%20)%0A%20%20%20%20%20%20%5Cutt%7B%5Cleft(%5Csum_%7Bj=1%7D%5En%20q_j%20x_j%5E%7B%5Cgamma%7D%5Cright)%7D%7Btotal%7D%7Bimpressions%7D%7D%0A%20%20%20%20%20%20%7Beffect%20on%20revenue%7D%7Bthrough%20impressions%20on%20$i$%7D%0A%20%20%20%20+%20%5Cutt%7B%5Clambda%7D%7Bavg%20marginal%7D%7Beffect%7D=0%0A"></p>
<p>The additional term represents the platform’s monopoly power with respect to the price paid. This has a natural interpretation for advertisers: showing fewer ads will drive up the price. For paid content-providers it could perhaps represent bulk discounts, I’m not sure whether this is a significant consideration.</p>
</section>
<section id="model-2-derivation" class="level2">
<h2 class="anchored" data-anchor-id="model-2-derivation">Model 2 Derivation</h2>
<p>This is derivation of model #2. (I had chatGPT help with this derivation, was very useful)</p>
<ol type="1">
<li><p><strong>Setting up the Lagrangian.</strong> The objective is to maximize the total impressions, <img src="https://latex.codecogs.com/png.latex?M">, subject to the constraint that the allocated shares of impressions sum to one. We start by writing the Lagrangian: <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D%20=%20%5Csum_%7Bi=1%7D%5En%20q_i%20x_i%5E%5Cgamma%20-%20%5Clambda%20%5Cleft(%5Csum_%7Bi=1%7D%5En%20x_i%20-%201%5Cright)"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5Clambda"> is the Lagrange multiplier associated with the constraint.</p></li>
<li><p><strong>Solving for the multiplier.</strong> To solve for the value of <img src="https://latex.codecogs.com/png.latex?%5Clambda">, we take the derivative of the Lagrangian with respect to <img src="https://latex.codecogs.com/png.latex?x_i"> and set it equal to zero: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cpartial%20%5Cmathcal%7BL%7D%7D%7B%5Cpartial%20x_i%7D%20=%20%5Cgamma%20q_i%20x_i%5E%7B%5Cgamma%20-%201%7D%20-%20%5Clambda%20=%200"></p>
<p>Rearranging this equation yields: <img src="https://latex.codecogs.com/png.latex?x_i%20=%20%5Cleft(%5Cfrac%7B%5Clambda%7D%7B%5Cgamma%20q_i%7D%5Cright)%5E%7B%5Cfrac%7B1%7D%7B%5Cgamma-1%7D%7D"></p>
<p>Taking the sum of this expression over all producers and using the constraint that the shares of impressions must sum to one, we obtain: <img src="https://latex.codecogs.com/png.latex?1%20=%20%5Csum_%7Bi=1%7D%5En%20x_i%20=%20%5Csum_%7Bi=1%7D%5En%20%5Cleft(%5Cfrac%7B%5Clambda%7D%7B%5Cgamma%20q_i%7D%5Cright)%5E%7B%5Cfrac%7B1%7D%7B%5Cgamma-1%7D%7D"></p>
<p>Simplifying this equation gives:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Clambda%5E%7B%5Cfrac%7B1%7D%7B%5Cgamma-1%7D%7D%20=%20%5Cgamma%20%5Csum_%7Bi=1%7D%5En%20q_i%5E%7B-%5Cfrac%7B1%7D%7B%5Cgamma-1%7D%7D"></p>
<p>Substituting this expression for <img src="https://latex.codecogs.com/png.latex?%5Clambda"> back into the equation for <img src="https://latex.codecogs.com/png.latex?x_i"> results in:</p>
<p><img src="https://latex.codecogs.com/png.latex?x_i%20=%20%5Cfrac%7Bq_i%5E%7B-%5Cfrac%7B1%7D%7B%5Cgamma-1%7D%7D%7D%7B%5Cgamma%20%5Csum_%7Bj=1%7D%5En%20q_j%5E%7B-%5Cfrac%7B1%7D%7B%5Cgamma-1%7D%7D%7D"></p>
<p>This is our final expression for the share of impressions on each producer as a function of the exogenous qualities and <img src="https://latex.codecogs.com/png.latex?%5Cgamma">.</p></li>
</ol>
<p><strong>Summary:</strong></p>
<p>The Lagrangian: <img src="https://latex.codecogs.com/png.latex?%5Cmathcal%7BL%7D%20=%20%5Csum_%7Bi=1%7D%5En%20q_i%20x_i%5E%5Cgamma%20-%20%5Clambda%20%5Cleft(%5Csum_%7Bi=1%7D%5En%20x_i%20-%201%5Cright)">.</p>
<p>Expression for the multiplier: <img src="https://latex.codecogs.com/png.latex?%5Clambda%5E%7B%5Cfrac%7B1%7D%7B%5Cgamma-1%7D%7D%20=%20%5Cgamma%20%5Csum_%7Bi=1%7D%5En%20q_i%5E%7B-%5Cfrac%7B1%7D%7B%5Cgamma-1%7D%7D"></p>
<p>The resultant expression for <img src="https://latex.codecogs.com/png.latex?x_i">: <img src="https://latex.codecogs.com/png.latex?x_i%20=%20%5Cfrac%7Bq_i%5E%7B-%5Cfrac%7B1%7D%7B%5Cgamma-1%7D%7D%7D%7B%5Cgamma%20%5Csum_%7Bj=1%7D%5En%20q_j%5E%7B-%5Cfrac%7B1%7D%7B%5Cgamma-1%7D%7D%7D."></p>
<p>Slightly rearranged (by me):</p>
<p><img src="https://latex.codecogs.com/png.latex?x_i=%5Cfrac%7Bq_i%5E%5Cfrac%7B1%7D%7B1-%5Cgamma%7D%7D%7B%5Cgamma%5Csum_%7Bj=1%7D%5Enq_j%5E%5Cfrac%7B1%7D%7B1-%5Cgamma%7D%7D."></p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-anderson2015handbook" class="csl-entry">
Anderson, Simon P., and Bruno Jullien. 2015. <span>“Chapter 2 - the Advertising-Financed Business Model in Two-Sided Media Markets.”</span> In <em>Handbook of Media Economics</em>, edited by Simon P. Anderson, Joel Waldfogel, and David Strömberg, 1:41–90. Handbook of Media Economics. North-Holland. https://doi.org/<a href="https://doi.org/10.1016/B978-0-444-62721-6.00002-0">https://doi.org/10.1016/B978-0-444-62721-6.00002-0</a>.
</div>
</div></section></div> ]]></description>
  <guid>tecunningham.github.io/posts/2023-03-06-social-media-business-models-sushi-roll.html</guid>
  <pubDate>Fri, 08 Sep 2023 07:00:00 GMT</pubDate>
</item>
<item>
  <title>How Much has Social Media affected Polarization?</title>
  <dc:creator>Tom Cunningham, [Integrity Institute](https://integrityinstitute.org/)</dc:creator>
  <link>tecunningham.github.io/posts/2023-07-27-meta-2020-elections-experiments.html</link>
  <description><![CDATA[ 





<style>
   h1 {  border-bottom: 4px solid black;}
   h2 {  border-bottom: 1px solid gray; padding-bottom: 0px; font-size: 14px; color: black; }
   dl {display: grid;grid-template-columns: max-content auto;}
   dt {grid-column-start: 1;}
   dd {grid-column-start: 2; margin-left: 2em;}
</style>
<p><strong>TL;DR: The experiments run by Meta during the 2020 elections were not big enough to test the theory that social media has made a substantial contribution to polarization in the US. Nevertheless there are other reasons to doubt it.</strong></p>
<section id="summary" class="level1 page-columns page-full">
<h1>Summary</h1>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="tecunningham.github.io/posts/images/2023-08-04-13-59-52.png" class="img-fluid"> Thanks to Dean Eckles, Solomon Messing, Jeff Allen, &amp; Brandon Silverman for discussion which led to this post. I put together the <a href="https://docs.google.com/spreadsheets/d/1_96kEzP9MFLcBFppVV0Bl7O3Cv9hQFxKArwS2zVCtXE/edit#gid=0">spreadsheet summary of results</a> with Dean and Solomon. See also a <a href="https://statmodeling.stat.columbia.edu/2023/07/27/new-research-on-social-media-during-the-2020-election-and-my-predictions/">post by Dean</a>.</p>
</div><div id="fn1"><p><sup>1</sup>&nbsp;<span class="citation" data-cites="allcott2019trends">Allcott et al. (2019)</span>, see below for discussion of whether these standard deviations are comparable.</p></div></div><p><strong>Three new experiments show that changing Facebook’s feed ranking algorithm for 1.5 months has an effect on affective polarization of less than 0.03 standard deviations.</strong> This is small compared to a growth of 1.1 standard deviations in nationwide affective polarization over the last 40 years.<sup>1</sup></p>
<p><strong>Small effects in these experiments are consistent with large effects in aggregate.</strong> <span class="citation" data-cites="guess2023chronological">Guess et al. (2023b)</span> says:</p>
<blockquote class="blockquote">
<p>“these findings suggest that social media algorithms may not be the root cause of phenomena such as increasing political polarization.”</p>
</blockquote>
<p>However they aggregate contribution of social media to polarization will differ from these experimental estimates in a number of ways: depth, breadth, duration, timing, category, and population. My rough attempts to account for these considerations make me think the aggregate effect is likely 10 or 20 times larger than the effects that would be measured in these experiments, and so small effects in these experiments are consistent with large effects on aggregate.</p>
<p><strong>Put simply:</strong> these experiments measure the effect of reducing exposure of an individual user (not their friends and family) to political content on Facebook by 15% for 1.5 months, and occurred in a period after Facebook had already sharply reduced the amount of partisan content circulating. Thus we should expect them to measure only a small fraction of the cumulative impact of social media, and in fact these results are consistent with social media being <em>entirely</em> responsible for the growth of polarization in the US.</p>
<p><strong>Nevertheless other evidence implies that social media has probably not made a huge contribution to US polarization.</strong> If we wish to evaluate the balance of evidence relating social media to polarization there are many other sources which are probably more informative than these experiments. I give a rough sketch below and it seems to me social media probably does not account for a majority share, mainly because (1) polarization had been growing for 20 years prior to social media’s introduction, and much of the growth since 2014 was in people without internet access; (2) a lot of partisan discourse continues to spread outside of social media, e.g.&nbsp;through cable TV and talk radio; (3) other countries do not show a similar increase in affective polarization.</p>
<p><strong>Discussion of these results has been distressingly non-quantitative.</strong> The majority of discussion of these results (in papers, editorials, on Twitter) has been about whether these changes “have an effect” or “do not have an effect.” Interpreted sympathetically these statements are compressed ways of saying “an effect larger than 0.03 standard deviations.” However I think taking this shortcut so consistently has led to far too little time thinking about what we have learned from these experiments that we didn’t already know, and what is the balance of evidence regarding the effects of social media. I give a lot of examples below.</p>
</section>
<section id="the-experiments" class="level1 page-columns page-full">
<h1>The Experiments</h1>
<p><strong>Last week’s papers reported the results of three experiments on Facebook’s News Feed.</strong> The experiments (<span class="citation" data-cites="guess2023chronological">Guess et al. (2023b)</span>, <span class="citation" data-cites="guess2023reshares">Guess et al. (2023a)</span>, <span class="citation" data-cites="nyhan2023likeminded">Nyhan et al. (2023)</span>) were run between September and December 2020, and half-way through participants were asked about their feelings towards members of their own party and the opposing party, e.g.&nbsp;<em>“how warm do you feel about Republicans on a scale of 0-100?”</em><sup>2</sup> The answers were aggregated to make an index of “affective polarization”: <img src="https://latex.codecogs.com/png.latex?%5Cxymatrix@R=0em@C=6em%7B%0A%20%20%20%20%20%20*+%5BF:%3C5pt%3E%5D%5Ctxt%7Brank%20items%20on%20News%5C%5CFeed%20chronologically%7D%20%20%5Car%5Bdr%5D%20&amp;%20%5C%5C%0A%20%20%20%20%20%20*+%5BF:%3C5pt%3E%5D%5Ctxt%7Bremove%20reshares%5C%5Con%20News%20Feed%7D%20%20%5Car%5Br%5D%20&amp;%0A%20%20%20%20%20%20%20%20%20*+%5BF:%3C5pt%3E%5D%5Ctxt%7Baffective%5C%5Cpolarization%5C%5Csurvey%7D%5C%5C%0A%20%20%20%20%20%20*+%5BF:%3C5pt%3E%5D%5Ctxt%7Bdownrank%20likeminded%5C%5Citems%20on%20News%20Feed%7D%20%20%5Car%5Bur%5D%0A%20%20%20%20%20%20%7D%0A%20%20%20"></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;Although the treatments ran for 3 months (24 Sep–23 Dec 2020), the survey responses were collected during the experiment and the average survey measure was measured after around 1.5 months of treatment: see Figure S2 in the Supplementary Appendix.</p></div></div><p><strong>All three experiments found effects on polarization of less than 0.03 standard deviations (SDs).</strong> The 95% confidence intervals on affective polarization are approximately <img src="https://latex.codecogs.com/png.latex?%5Cpm"> 0.03 SDs, and the effect-sizes are all smaller than that (i.e.&nbsp;they do not estimate a significant effect). Dean Eckles, Solomon Messing, and myself put together a <a href="https://docs.google.com/spreadsheets/d/1_96kEzP9MFLcBFppVV0Bl7O3Cv9hQFxKArwS2zVCtXE/edit#gid=0">spreadsheet summary</a> of the results from all the experiments reported so far, along with other results from the literature on political effects of media.</p>
<p>They also measured effects on a number of other off-platform outcomes: removing reshares did lower news knowledge by 0.07 standard deviations, but all other outcomes (factual discernment, issue polarization, perceived legitimacy, self-reported turnout) were not significant, and had similar-sized confidence intervals.</p>
<p></p>
<p></p>
</section>
<section id="extrapolating-to-the-cumulative-effect-of-social-media" class="level1 page-columns page-full">
<h1>Extrapolating to the Cumulative Effect of Social Media</h1>
<p><strong>Many people have interpreted these results as implying that social media has not had much effect on overall polarization.</strong> E.g. one of the experimental papers says:<sup>3</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;<span class="citation" data-cites="guess2023chronological">Guess et al. (2023b)</span></p></div></div><blockquote class="blockquote">
<p>“these findings suggest that social media algorithms may not be the root cause of phenomena such as increasing political polarization.”</p>
</blockquote>
<p><strong>Here I try to extrapolate from these experiments to the long-run aggregate effect of social media.</strong> The comparison is between two extremes but there are a lot of other intermediate estimands that we could alternatively use, e.g.&nbsp;the effect of permanantly disabling just Facebook for everybody, or the effect of temporarily disabling all social networks for an individual user.</p>
<p>These are difficult judgment calls. I have tried my best to be neutral and discuss evidence on either side but it’s likely I’m forgetting some important considerations.</p>
<p><strong>I work through six ways in which the experimental results will differ from the aggregate impact on social media:</strong></p>
<ol type="1">
<li><strong>Depth.</strong> Whether changing one feature or disabling the app entirely.</li>
<li><strong>Breadth.</strong> Whether changing the experience for one user or for all users.</li>
<li><strong>Duration.</strong> Whether changing the experience for 1.5 months or for the whole history of social media.</li>
<li><strong>Timing.</strong> Whether changing the experience in Oct 2020, or the average effect over 2004-2020.</li>
<li><strong>Category.</strong> Whether changing the experience just for Facebook or for all social media.</li>
<li><strong>Population.</strong> Whether we are estimating the effect for all US adults or just Facebook users.</li>
</ol>
<p>I try to give quantitative estimates for each of these six differences, and it makes me think that having tight confidence intervals on the effects of the experiments (plus or minus 0.03 SDs) is still consistent with the aggregate effect of social media being having an effect as large as 1 SD or more.<sup>4</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;I am considering the effect-size rather than the uncertainty, you could separately do a similar exercise to propagate up the uncertainty, and I think this would make the experiments seem even more under-powered to measure the aggregate effect.</p></div></div><p><strong>(1) <em>Depth</em>: the experiments have small effects on exposure.</strong> Each of the experiments reported have effects on overall Facebook time-spent of less than 25%, and on exposure to political material of less than 15%. Thus the effect of complete withdrawal from Facebook seems likely to be at least 2X larger than measured by any of these experiments. The most natural causal path from Facebook use to polarization is exposure to partisan or misleading political media. An additional experiment was run which deactivated peoples’ accounts but the results from that experiment are not yet public (as of August 4).</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>effects on metric<sup>5</sup></th>
<th>time-spent</th>
<th>political impression</th>
<th>cross-cutting impressions</th>
<th>untrustworthy impressions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>baseline</td>
<td>?</td>
<td>14pp</td>
<td>21pp</td>
<td>3pp</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>- rank chronologically</td>
<td>-21%</td>
<td>+12%</td>
<td>-10%</td>
<td>+60%</td>
</tr>
<tr class="even">
<td>- remove reshares</td>
<td>-5%</td>
<td>-14%</td>
<td>-3%</td>
<td>-32%</td>
</tr>
<tr class="odd">
<td>- downrank likeminded posts</td>
<td>-1%</td>
<td>-5%</td>
<td>+7%</td>
<td>?</td>
</tr>
</tbody>
</table>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;<a href="https://docs.google.com/spreadsheets/d/1_96kEzP9MFLcBFppVV0Bl7O3Cv9hQFxKArwS2zVCtXE/edit#gid=0">source data</a>.</p></div></div><p></p>
<p><strong>(2) <em>Breadth</em>: Experiments exclude network effects.</strong> The effects of social media on polarization likely work not just through direct exposure but also downstream via peoples’ interactions with friends and families, in both online and offline conversations. Thus it seems likely the aggregate effect could be 2X or larger than the individual effect.</p>
<p>Causal evidence on peer effects on attitudes is notoriously difficult to find. It’s worth mentioning that (1) the simplest rational model of beliefs implies that peer effects are overwhelmingly strong<sup>6</sup>; (2) cross-sectional variation in attitudes typically shows large between-group variation relative to within-group, consistent with strong peer effects: e.g.&nbsp;those brought up catholic tend to be catholic, those brought up protestant tend to be protestant.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;In the simplest model with common knowledge of rationality everyone will converge to the same belief, effectively pooling their information. Thus in a world of 7B people the weight each person gives to their personal experience would be 1/7B and the remainder is peer effects. The calculation becomes slightly more complicated when there are common sources of information.</p></div><div id="fn7"><p><sup>7</sup>&nbsp;The polarization survey measures were collected in wave 3, 4, and 5. From eyeballing Figure S3 (and assuming the response rate declines over time) it appears the average response would be collected around 1.5 months after the treatment began: <img src="tecunningham.github.io/posts/images/2023-08-07-07-34-48.png" class="img-fluid">.</p></div><div id="fn8"><p><sup>8</sup>&nbsp;The only dynamic effects discussion I could find was wave-by-wave results for survey questions in the supplementary appendices. I think it would be useful to show the dynamic effects for on-platform behavior because (1) there is good reason to expect the cumulative treatment effects will dramatically change over time, (2) the experiments are sufficiently well-powered that dynamic effects should be easy to observe.</p></div></div><p><strong>(3) <em>Duration</em>: the experiments only measure short-run effects.</strong> These experiments measured the effect of a News Feed change on polarization after around 1.5 months, while most American adults have been using Facebook for perhaps 10 years.<sup>7</sup> It is hard to judge how quickly we should expect polarization attitudes to respond to treatment, and I have not found useful academic literature. The national polarization trends documented in <span class="citation" data-cites="allcott2019trends">Allcott et al. (2019)</span> seem fairly stable despite a volatile news cycle suggesting attitudes change relatively slowly. If the half-life of adjustment was 1.5 months (which seems quite short to me) then the effects measured in these experiments would be half of the long-run effect. It seems likely that the effects of exposure do not decay at a constant rate: there is a short-run component that decays quickly (the effect of salience), and a long-run component that decays slowly.<sup>8</sup></p>
<p><span class="citation" data-cites="guess2023chronological">Guess et al. (2023b)</span> notes this limitation:</p>
<blockquote class="blockquote">
<p>“It is possible that such downstream effects require a more sustained intervention period … although our approximately 3-month study had a much longer duration than that of most experimental research in political communication.”</p>
</blockquote>
<p>Although if I understand correctly this is slightly misleading: the outcome variables were measured after 1.5 months exposure, not 3 months.</p>
<p><strong>(4) <em>Timing</em>: Experiments were run during the lead-up to the 2020 election.</strong> The experiments ran between September and November 2020. If we compare this to the average experience on Facebook over the previous decade there are reasons to expect relatively smaller effects on polarization:</p>
<ul>
<li><p>Following the 2016 elections facebook invested very heavily in integrity systems reducing prevalence of many types of bad content by factors of between 2X and 10X, especially misinformation and hyperpartisan political content. In May 2020 Guy Rosen <a href="https://about.fb.com/news/2020/05/investments-to-fight-polarization/">claimed</a> that Facebook had made “a number of important steps to reduce the amount of content that could drive polarization on our platform” over the prior years.</p></li>
<li><p><span class="citation" data-cites="allcott2019trends">Allcott et al. (2019)</span> estimates that exposure to misinformation on Facebook (measured by data on engagement with domains known to host misinformation) grew over 2015 and 2016, roughly doubling, then fell over 2017 and 2018, roughly halving. The data ends at the end of 2018 but I believe the trend would continue downward. <img src="tecunningham.github.io/posts/images/2023-08-02-16-19-16.png" class="img-fluid"></p></li>
<li><p>Meta’s Community Standards reports show a decline in prevalence of most types of harmful content by a factor of between 2 and 5 over roughly 2017 to 2022 (see chart <a href="https://tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data.html#meta-facebook-instagram">here</a>).</p></li>
<li><p>Prior to and during the 2020 election Facebook implemented a series of extra “break the glass” measures with the effect of suppressing extreme or fringe political content.</p></li>
<li><p>During election seasons there tends to be significantly more political content circulating. This might mean that Facebook would have a relatively larger impact on polarization in this period. However if the influence of social media on polarization depends on the <em>share</em> of exposure to partisan or polarizing content (rather than the level) then the effect would be the same in election season as outside election season.</p></li>
</ul>
<p>In fact 2020 Facebook has further reduced the prevalence of political and fringe content since 2020:</p>
<ul>
<li>The share of politics on News Feed was reduced by 50%.<sup>9</sup></li>
<li>Prevalence of hate-speech fell by factor of 5 between 2020 and 2022 (from 0.1% to 0.02%). (<a href="https://transparency.fb.com/data/community-standards-enforcement/hate-speech/facebook/">ref</a>).</li>
<li>Engagement on US right-wing politics pages has fallen by factor of 4 from 2021-2022 (<a href="https://fwiwnewsletter.substack.com/p/has-facebook-dialed-down-the-conservative">ref</a>).</li>
<li>Prevalence of engagement-bait among the top 20 most-viewed posts went from 100% to 5% between 2021Q3 to 2022Q3 (<a href="https://www.wsj.com/articles/facebooks-most-popular-posts-were-trash-here-is-how-it-cleaned-up-11669140034">ref</a>).</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;<a href="https://www.wsj.com/articles/facebook-politics-controls-zuckerberg-meta-11672929976">The WSJ</a> reported that in late 2021 <em>“Mr.&nbsp;Zuckerberg and the board chose the most drastic [option], instructing the company to demote posts on “sensitive” [(politics and health)] topics as much as possible in the newsfeed that greets users when they open the app”</em>, and that in 2022 <em>“politics accounts for less than 3% of total content views in users’ newsfeed, down from 6% around the time of the 2020 election.”</em> The article reports that these experiments reduced daily visitation (daily active users) by 0.2%.</p></div></div><p><strong>(5) <em>Category</em>: The experiments affected only Facebook, but in 2020 Facebook probably accounted for around 1/4 of all partisan political content that people are exposed to on social media.</strong> If we include YouTube, TikTok, Instagram, Twitter, Snapchat, Reddit, and the long tail of niche social networks. In contrast, if we are estimating the cumulative effect (2004-2020) then Facebook would likely comprise a significantly larger share of exposure political content.</p>
<p><strong>(6) <em>Population</em>: The experiments measure outcomes only on Facebook users.</strong> I believe that the “population average treatment effects” reported in the papers are weighted to match the Facebook-using population, not the voting population. This would be a reason for the experimental effect-size to be larger than the aggregate effect. I would guess around 2/3 of the US adult population is active once/month on Facebook, and so the aggregate effect-size could be smaller by that factor.<sup>10</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;The supplement to the Science paper mentions Facebook had 231 million monthly active users.</p></div></div><p><strong>Putting it all together.</strong> If factors 1-5 each contributed a 2X amplification, as well as factor 6 contributing a 2/3 shrinkage, then the cumulative effect of social media on polarization until 2020 would be <img src="https://latex.codecogs.com/png.latex?%5Csimeq"> 20X larger than the experimentally-measured effect, i.e.&nbsp;effective confidence intervals would be 0.6 SDs instead of 0.03 SDs. In other words these experiments would not be sufficiently well-powered to rule out social media being responsible for the <em>entire</em> growth of polarization since 2014.</p>
<p><strong>Note: adjusting for standard deviation size.</strong> The papers all report effect sizes on affective polarization in units of standard deviations. I wasn’t sure whether these are standard-deviations of the cross-sectional variance, or the residual variance after controlling for pre-treatment values. If the latter then I would guess they are perhaps half the size of the cross-section SD, based on my professional experience (and Supplementary Appendix page S-140). If correct this would halve the estimated effect-size when expressed in cross-sectional standard deviations, i.e.&nbsp;it would close the gap by a factor of 2.</p>
</section>
<section id="other-evidence-on-media-and-polarization" class="level1">
<h1>Other Evidence on Media and Polarization</h1>
<p>Here is a rough sketch of the evidence of related to affective polarization. I do not consider myself an expert on this literature and I would love corrections or additions. On balance this evidence seems to imply that social media hasn’t been the primary contributor to US affective polarization, but I think a thorough analysis of this evidence would be really valuable.</p>
<dl>
<dt>News Sources</dt>
<dd>
From Pew data I would guess social media is around 25% of all exposure to political news, probably a higher share of exposure to partisan political news. Cable TV and political talk radio probably account for similar shares of overall exposure to partisan media. This seems the strongest evidence that social media is not the primary driver of affective polarization.
</dd>
<dt>Professional Opinion</dt>
<dd>
The political science literature talks about “the paradox of minimal effects” and the economics-of-media literature generally seems to have a consensus that most persuasive effects of media are <a href="https://tecunningham.github.io/posts/2023-08-02-small-effects.html">small</a>. However this might just apply to marginal effects.
</dd>
<dt>Other Experiments</dt>
<dd>
<span class="citation" data-cites="allcott2020welfare">Allcott et al. (2020)</span> is often interpreted as finding an effect on affective polarization but it does not (see below). <span class="citation" data-cites="broockman2022crosscutting">Broockman and Kalla (2022)</span> finds a null effect. I don’t know of other good experiments on affective polarization.
</dd>
<dt>National Trends</dt>
<dd>
In the US affective polarization steadily grew 1978-2020, for a total of 1.1 SD over 40 years. Other countries do not show a consistent trend, and there is no clear connection with internet access or online news consumption.
</dd>
<dt>Demographic Trends</dt>
<dd>
Over 1996-2012 affective polarization grew the most in groups who have not increased their internet access. I’m not aware of more recent data.
</dd>
<dt>Natural Experiments</dt>
<dd>
Some papers find that roll-out of mobile internet is associated with increased support for populist parties, to a degree that mobile internet could account for perhaps 1/3 of their total support
</dd>
</dl>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion</h2>
<p><strong>Trends in affective polarization.</strong> <span class="citation" data-cites="boxell2022PolarizationTrends">Boxell et al. (2022)</span> document affective polarization across a dozen countries, 1978-2020:</p>
<p><img src="tecunningham.github.io/posts/images/2023-07-27-15-03-32.png" class="img-fluid"></p>
<ol type="1">
<li><p>In the US affective polarization index increased from around 25 to 50, <em>“an increase of 1.08 standard deviations as measured in the 1978 distribution.”</em> (I’m not sure if the SD increased).</p></li>
<li><p>Across the world there’s no clear trend: some countries increased, other countries decreased. This weakens the simple argument that polarization has increased at the same time as social media use.</p></li>
<li><p>In the US the trend seems to be almost entirely due to increasing negative feelings about the opposing party:</p>
<p><img src="tecunningham.github.io/posts/images/2023-08-03-12-51-42.png" class="img-fluid"></p></li>
</ol>
<p>The US timeseries can be seen <a href="https://electionstudies.org/data-tools/anes-guide/anes-guide.html?chart=affective_polarization_parties">online</a> from the <a href="https://electionstudies.org">ANES</a>.</p>
<p><strong>Growth in populist support.</strong> Across the world there has been a substantial growth in populist governments. <span class="citation" data-cites="guriev20213g">Guriev et al. (2021)</span> and <span class="citation" data-cites="manacorda2023mobile">Manacorda et al. (2023)</span> both argue from natural variation in mobile internet expansion that the internet has caused perhaps 1/3 to 1/2 of the increase in populist support in Europe.</p>
<p><strong>Observational data finds that much of the growth in polarization in the US was among people who were not online.</strong> <span class="citation" data-cites="boxell2017greater">Boxell et al. (2017)</span> say</p>
<blockquote class="blockquote">
<p>“the growth in polarization in recent years [1996-2012] is largest for the demographic groups least likely to use the internet and social media”</p>
</blockquote>
<p><strong>Content on Meta platforms.</strong> <span class="citation" data-cites="guess2023chronological">Guess et al. (2023b)</span> has data from the control group in their 2020 experiments:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Share of Impressions</th>
<th>Facebook</th>
<th>Instagram</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Political content</td>
<td>14%</td>
<td>5%</td>
</tr>
<tr class="even">
<td>Political news content</td>
<td>6%</td>
<td>-</td>
</tr>
<tr class="odd">
<td>Content from untrustworthy sources</td>
<td>3%</td>
<td>1%</td>
</tr>
<tr class="even">
<td>Uncivil content</td>
<td>3%</td>
<td>2%</td>
</tr>
</tbody>
</table>
<p><a href="https://www.pewresearch.org/journalism/fact-sheet/news-platform-fact-sheet/?tabId=tab-4ef8dece-845a-4b25-8637-ceb3114503c5">Pew 2022</a> has data on where people get their news from:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th></th>
<th>pct adults regularly get news from</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>television</td>
<td>65%</td>
</tr>
<tr class="even">
<td>news websites</td>
<td>63%</td>
</tr>
<tr class="odd">
<td>search</td>
<td>60%</td>
</tr>
<tr class="even">
<td>social media</td>
<td>50%</td>
</tr>
<tr class="odd">
<td>radio</td>
<td>47%</td>
</tr>
<tr class="even">
<td>print</td>
<td>33%</td>
</tr>
<tr class="odd">
<td>podcasts</td>
<td>23%</td>
</tr>
</tbody>
</table>
<p><strong>Radio show popularity.</strong> Around half of the top 20 most-listened radio shows in the US are conservative talk, with around 90M weekly listeners (this is double-counting overlapping users). <a href="https://en.wikipedia.org/wiki/List_of_most-listened-to_radio_programs">Data from 2021</a>.</p>
<p><strong>Television.</strong> Fox News is Cable TV’s most-watched network with around 5M regular viewers. (<a href="http://www.adweek.com/tvnewser/2016-ratings-fox-news-channel-is-cable-tvs-most-watched-network/315009">source from 2016</a>).</p>
<p><strong>Time spent on social media.</strong> <a href="https://www.statista.com/statistics/433871/daily-social-media-usage-worldwide/">Statista</a>: Average time-spent 150 minutes/day/person on social networks</p>
<p><strong>The academic literature has identified other possible causes of polarization.</strong> Some potential causes: southern realignment, 1968 changes to the primary system, the Obama presidency, the tea party movement (though each of these could be in part proximal causes). Martin &amp; Yurcoglu (2017) argue that a large part of recent growth is due to cable news: &gt; “the cable news channels can explain an increase in political polarization of similar size to that observed in the US population over [2000-2008]. … In absolute terms, however, this increase is fairly small.”</p>
<p>See also Haidt and Bail’s long document <a href="https://docs.google.com/document/d/1vVAtMCQnz8WVxtSNQev_e1cGmY9rnY96ecYuAj6C548/edit#heading=h.96bogdklzo1j">Social Media and Political Dysfunction: A Collaborative Review</a></p>
<p><strong>Does <span class="citation" data-cites="allcott2020welfare">Allcott et al. (2020)</span> find that Facebook use increases polarization?</strong> This paper reports on an experiment paying people to stop using Facebook for a month. They find an effect of -0.16 SDs (<img src="https://latex.codecogs.com/png.latex?%5Cpm"> 0.08) on a measure they describe as “political polarization,” however there are some subtleties:</p>
<p><img src="tecunningham.github.io/posts/images/2023-08-07-09-12-41.png" class="img-fluid"></p>
<ol type="1">
<li><p>Unlike the questions used in typical population surveys the questions were explicitly about their feelings during the period of the experiment, e.g.&nbsp;<em>“Thinking back over the last 4 weeks, how warm or cold did you feel towards the parties and the president on the feeling thermometer?”</em></p></li>
<li><p>Polarization is measured by a composite of different measures. By far the largest effect was on the “congenial news exposure” question: <em>“over the last 4 weeks how often did you see news that made you better understand the point of view of the Democrat (Republican) party?”</em> The score was the difference between the answer for their own party vs the other-side party. It seems to me that it’s not surprising that deactivating Facebook would affect one’s exposure to such news, but that this wouldn’t normally be called a measure of “polarization” in the literature. The paper mentions in a footnote that <em>“the effect on the political polarization index is robust to excluding each of the seven individual component variables,”</em> but it turns out that removing “congenial news exposure” halves the effect-size and shifts the p-value from 0.00 to 0.09 (i.e.&nbsp;from very significant to non-significant). I’m not sure I would describe this as evidence of robustness.</p></li>
<li><p>The paper finds no significant effect on their two “affective polarization” measures (-0.08 <img src="https://latex.codecogs.com/png.latex?%5Cpm"> 0.08 SD, and 0 <img src="https://latex.codecogs.com/png.latex?%5Cpm"> 0.04 SD), however the Meta2020 papers which cite <span class="citation" data-cites="allcott2020welfare">Allcott et al. (2020)</span> seem to treat it as finding that Facebook has a positive effect on “polarization” without noting that it has a null effect on <em>affective</em> polarization.</p></li>
</ol>
</section>
<section id="literature-review" class="level2">
<h2 class="anchored" data-anchor-id="literature-review">Literature Review</h2>
<ul>
<li><p><span class="citation" data-cites="zhuravskaya2020political">Zhuravskaya et al. (2020)</span> review literature on the political effects of the internet and social media. Regarding politics they say:</p>
<blockquote class="blockquote">
<p>“The spread of the internet and social media has contributed, at least in part, to the electoral success of populists in Europe and to reduced political support for the ruling parties in immature democracies and semiautocratic regimes. There is also evidence that social media can be used to mobilize voters.”</p>
</blockquote>
<p>Regarding polarization they say:</p>
<blockquote class="blockquote">
<p>“the available evidence so far is not conclusive about whether social media increases political polarization.”</p>
</blockquote></li>
<li><p><span class="citation" data-cites="guriev20213g">Guriev et al. (2021)</span> uses 3G roll-out worldwide 2008 to 2017, they find a 40pp increase in mobile internet use causes:</p>
<blockquote class="blockquote">
<p>“reduced the confidence in the national government of the region’s population by 2.5 percentage points (from the mean level of 51%), and increased the perception that the government is corrupt by 1.4 percentage points (from the mean of 77%).</p>
</blockquote>
<p>They estimate within Europe that a 50pp increase in 3G access caused a 5% drop in support for incumbents, a 5% increase in support for right-populist parties, and a 4% increase for left-populist parties.</p></li>
<li><p><span class="citation" data-cites="melnikov2021mobile">Melnikov (2021)</span> finds that 3G roll-out in the US (1) increased the rates of self-identified ; (2) increased polarization in voting <em>“the vote share of Republican candidates increases by 4.5 percentage points in Republican-leaning counties and decreases by 2.6 percentage points in Democratic-leaning counties.” The paper says “3G network coverage can account for 11.3% of the increase in polarization in political views, 37.7% of the increase in polarization in voting behavior, and, on average, 34.8% of the increase in polarization in policy preferences.”</em> The paper calculates a “persuasion rate” of around 10, meaning I think that around 10% of people who are exposed to 3G internet are persuaded.</p></li>
<li><p><span class="citation" data-cites="manacorda2023mobile">Manacorda et al. (2023)</span> study roll-out of mobile internet across Europe 2007-2017, they estimate: &gt; <em>“between one third and one half of the remarkable success of communitarian [populist] parties, which roughly doubled their support over the period, can be ascribed to enhanced access to mobile Internet technology.”</em></p></li>
</ul>
</section>
</section>
<section id="quantitative-vs-qualitative-description-of-results" class="level1 page-columns page-full">
<h1>Quantitative vs Qualitative Description of Results</h1>
<p><strong>Throughout these papers and in the public discussion the findings have been described in <em>qualitative</em> terms:</strong> i.e.&nbsp;either as “positive,” “negative,” or “neutral.” Implicitly these terms are referring to whether the results are statistically-significant (p&lt;0.05), which depends on whether the effect-size is bigger than the confidence interval. These statements only make sense given some implicit understanding of how broad the confidence intervals are, yet I do not think that implicit understanding exists: I’m fairly confident that most people reading these statements (and many people making them) do not know quantitatively what the thresholds are.<sup>11</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn11"><p><sup>11</sup>&nbsp;It’s worth stating that all of these treaments will have <em>some</em> non-zero effect, so it’s never literally correct to say “this treatement has no effect on polarization,” it can only be understood as a roundabout way of saying “this treatment has a small effect” for some definition of “small”.</p></div></div><ol type="1">
<li><p><strong>Titles and abstracts used qualitative descriptions.</strong> The titles and abstracts all used qualitative language, e.g.&nbsp;“did not reduce” or “did not significantly affect” or “had no measurable effects.” None of the abstracts of the papers gave information on the size of the effects that were ruled out.</p></li>
<li><p><strong>Hypotheses used qualitative descriptions.</strong> The pre-analysis plans contained a series of hypotheses, e.g.:</p>
<blockquote class="blockquote">
<p>H1: Decreased exposure to content shared by like-minded friends, Pages, and groups decreases affective polarization.</p>
</blockquote>
<blockquote class="blockquote">
<p>H1: Reverse chronological feed will reduce polarization and negative perceptions of out-groups.</p>
</blockquote>
<p>The terms “decrease” and “reduce” are presumably implicitly referring to the width of the confidence intervals, but I could find no discussion of how much .</p></li>
<li><p><strong>Public discussion used qualitative descriptions.</strong> Almost all discussion in editorials and on Twitter described the results in qualitative terms, whether there was an effect or not, not in quantitative terms.</p></li>
<li><p><strong>Elicitation of priors used qualitative descriptions.</strong> I was at an SSRC conference a few days before the results were released and there was a poll taken to predict the results. For “polarization” the options were (as I recall) “no effect”, “small increase”, “substantial increase”, etc., where I believe “increase” was intended to be interpreted as “statistically significant increase.” However as I recall we were not told the width of the confidence intervals when asked to make predictions. I think this is a bad way of eliciting priors: whether something is significant depends on the width of the confidence intervals as much as the effect-size. Thus an equivalent way of phrasing the question would be “do you think these experiments are sufficiently powered?”<sup>12</sup></p></li>
<li><p><strong>Power calculations used qualitative descriptions.</strong></p>
<p><span class="citation" data-cites="guess2023chronological">Guess et al. (2023b)</span> said that there was sufficient power to detect “small” effects, without explaining why they regarded 0.03 SD as small.<sup>13</sup> The supplement and pre-analysis plan do not mention “power” or seem to discuss the quantitative interpretation of these effect-sizes.</p>
<p>They do cite a previous paper:</p>
<blockquote class="blockquote">
<p>“In all cases, we could rule out effect sizes smaller than those found in previous research [citation to Allcott 2020]</p>
</blockquote>
<p>However I beleive this is a misinterpretation: <span class="citation" data-cites="allcott2020welfare">Allcott et al. (2020)</span> does test for affective polarization but they find a non-significant effect. As discussed above that paper reports a significant effect for “polarization” but the significance is due solely to the response to asking people about “congenial news exposure” over the last 4 weeks, which I think is quite different from polarization.</p>
<p>The Supplementary Appendix to <span class="citation" data-cites="guess2023chronological">Guess et al. (2023b)</span> says the sample size was chosen to detect an effect size of 1.5 percentage points in vote choice (p.&nbsp;S-139), however it is not clear why this effect size was chosen.</p>
<p><span class="citation" data-cites="guess2023reshares">Guess et al. (2023a)</span> says explicitly that they (or someone) expected a significant effect:</p>
<blockquote class="blockquote">
<p>“Contrary to expectations, the treatment does not significantly affect political polarization or any measure of individual-level political attitudes.”</p>
</blockquote>
<p>However I did not find a discussion of why they expected an effect of that size.</p></li>
</ol>
<div class="no-row-height column-margin column-container"><div id="fn12"><p><sup>12</sup>&nbsp;A similar phenomenon occurs in forecasting: if someone asks you a question for which you miss crucial context like “what is the chance of the Grockles winning the Kaplooey cup?” then you can give a good answer but it will be based on your judgment of the person asking the question, not your judgment about the substance of the question itself.</p></div><div id="fn13"><p><sup>13</sup>&nbsp;“The large samples … allowed for adequate statistical power to detect small effects (for example, for affective polarization, we were powered to detect population average treatment effects with Cohen’s d = 0.032 or larger for both Facebook and Instagram).”</p></div></div><p><strong>Good quantitative work.</strong> Some of the authors of these 2020 papers have written other papers which I think use a much more useful approach: they use observational data, are quite focussed on <em>quantitative</em> outcomes, and they perform back-of-the-envelope calculations to reconcile evidence from different sources, e.g. <span class="citation" data-cites="boxell2022PolarizationTrends">Boxell et al. (2022)</span>, <span class="citation" data-cites="boxell2017greater">Boxell et al. (2017)</span>, <span class="citation" data-cites="allcott2017social">Allcott and Gentzkow (2017)</span>.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-allcott2020welfare" class="csl-entry">
Allcott, H., Braghieri, L., Eichmeyer, S., Gentzkow, M., 2020. The welfare effects of social media. American Economic Review 110, 629–676.
</div>
<div id="ref-allcott2017social" class="csl-entry">
Allcott, H., Gentzkow, M., 2017. Social media and fake news in the 2016 election. Journal of economic perspectives 31, 211–236.
</div>
<div id="ref-allcott2019trends" class="csl-entry">
Allcott, H., Gentzkow, M., Yu, C., 2019. Trends in the diffusion of misinformation on social media. Research &amp; Politics 6, 2053168019848554.
</div>
<div id="ref-boxell2022PolarizationTrends" class="csl-entry">
Boxell, L., Gentzkow, M., Shapiro, J.M., 2022. <span class="nocase">Cross-Country Trends in Affective Polarization</span>. The Review of Economics and Statistics 1–60. <a href="https://doi.org/10.1162/rest_a_01160">https://doi.org/10.1162/rest_a_01160</a>
</div>
<div id="ref-boxell2017greater" class="csl-entry">
Boxell, L., Gentzkow, M., Shapiro, J.M., 2017. Greater internet use is not associated with faster growth in political polarization among US demographic groups. Proceedings of the National Academy of Sciences 114, 10612–10617.
</div>
<div id="ref-broockman2022crosscutting" class="csl-entry">
Broockman, D., Kalla, J., 2022. Consuming cross-cutting media causes learning and moderates attitudes: A field experiment with fox news viewers. <a href="https://doi.org/10.31219/osf.io/jrw26">https://doi.org/10.31219/osf.io/jrw26</a>
</div>
<div id="ref-guess2023reshares" class="csl-entry">
Guess, A.M., Malhotra, N., Pan, J., Barberá, P., Allcott, H., Brown, T., Crespo-Tenorio, A., Dimmery, D., Freelon, D., Gentzkow, M., 2023a. Reshares on social media amplify political news but do not detectably affect beliefs or opinions. Science 381, 404–408.
</div>
<div id="ref-guess2023chronological" class="csl-entry">
Guess, A.M., Malhotra, N., Pan, J., Barberá, P., Allcott, H., Brown, T., Crespo-Tenorio, A., Dimmery, D., Freelon, D., Gentzkow, M., González-Bailón, S., Kennedy, E., Kim, Y.M., Lazer, D., Moehler, D., Nyhan, B., Rivera, C.V., Settle, J., Thomas, D.R., Thorson, E., Tromble, R., Wilkins, A., Wojcieszak, M., Xiong, B., Jonge, C.K. de, Franco, A., Mason, W., Stroud, N.J., Tucker, J.A., 2023b. How do social media feed algorithms affect attitudes and behavior in an election campaign? Science 381, 398–404. <a href="https://doi.org/10.1126/science.abp9364">https://doi.org/10.1126/science.abp9364</a>
</div>
<div id="ref-guriev20213g" class="csl-entry">
Guriev, S., Melnikov, N., Zhuravskaya, E., 2021. 3g internet and confidence in government. The Quarterly Journal of Economics 136, 2533–2613.
</div>
<div id="ref-manacorda2023mobile" class="csl-entry">
Manacorda, M., Tabellini, G.E., Tesei, A., 2023. Mobile internet and the rise of communitarian politics. Centre for Economic Policy Research.
</div>
<div id="ref-melnikov2021mobile" class="csl-entry">
Melnikov, N., 2021. Mobile internet and political polarization. Available at SSRN 3937760.
</div>
<div id="ref-nyhan2023likeminded" class="csl-entry">
Nyhan, B., Settle, J., Thorson, E., Wojcieszak, M., Barberá, P., Chen, A.Y., Allcott, H., Brown, T., Crespo-Tenorio, A., Dimmery, D., Freelon, D., Gentzkow, M., González-Bailón, S., Guess, A.M., Kennedy, E., Kim, Y.M., Lazer, D., Malhotra, N., Moehler, D., Pan, J., Thomas, D.R., Tromble, R., Rivera, C.V., Wilkins, A., Xiong, B., Jonge, C.K. de, Franco, A., Mason, W., Stroud, N.J., Tucker, J.A., 2023. Like-minded sources on facebook are prevalent but not polarizing. Nature 620, 137–144. <a href="https://doi.org/10.1038/s41586-023-06297-w">https://doi.org/10.1038/s41586-023-06297-w</a>
</div>
<div id="ref-zhuravskaya2020political" class="csl-entry">
Zhuravskaya, E., Petrova, M., Enikolopov, R., 2020. Political effects of the internet and social media. Annual review of economics 12, 415–438.
</div>
</div></section></div> ]]></description>
  <guid>tecunningham.github.io/posts/2023-07-27-meta-2020-elections-experiments.html</guid>
  <pubDate>Mon, 07 Aug 2023 07:00:00 GMT</pubDate>
</item>
<item>
  <title>The Paradox of Small Effects</title>
  <dc:creator>Tom Cunningham, [Integrity Institute](https://integrityinstitute.org/)</dc:creator>
  <link>tecunningham.github.io/posts/2023-08-02-small-effects.html</link>
  <description><![CDATA[ 





<style>
    h1 {  border-bottom: 4px solid black;}
    h2 {  border-bottom: 1px solid gray; padding-bottom: 0px; font-size: 14px; color: black; }
</style>
<p>In summary:</p>
<ol type="1">
<li><strong>Attitudes are hard to change.</strong> Many fields in social science have adopted a doctrine of “small effects”: high quality studies tend to show that peoples’ attitudes are not very sensitive to exposure to media, or to their peers’ attitudes.</li>
<li><strong>Yet attitudes do change.</strong> We see very wide society-level variation in attitudes, which are hard to explain without peer or media effects.</li>
<li><strong>Resolution of the paradox: each effect is small, but there are a lot of them.</strong></li>
</ol>
<p>(see an earlier <a href="https://www.facebook.com/tom.cunningham.374549/posts/pfbid022GaqAxUuKobKyS6soWnQVZYevPxkGxQ6BxAiScmA47eZdU9RAJPpGi1NrXQip6Jyl">Facebook post</a>)</p>
<section id="attitudes-are-hard-to-change" class="level1">
<h1>(1) Attitudes are Hard to Change</h1>
<p>Many fields in social science tend to say that attitudes show little influence from either peer effects or from media exposure:</p>
<ul>
<li><p><span class="citation" data-cites="angrist2014perils">Angrist (2014)</span> says studies of peer effects <em>“have mostly uncovered little in the way of socially significant causal effects.”</em></p></li>
<li><p>Political scientists talk about “the paradox of minimal effects”, <span class="citation" data-cites="ansolabehere2006paradox">Ansolabehere (2006)</span> says that election campaigns <em>“seem to be inessential to understanding who wins and who loses.”</em></p></li>
<li><p>David Stromberg says <em>“the lesson from the last 50 years of media research is that it is very hard to manipulate voters … evidence of [supply side bias] effects is weak or non-existent”</em></p></li>
</ul>
<p>There are many studies which find large effects but they tend to be treated with extreme skepticism by the methodologists: they are overwhelmingly from lab experiments or observational data and so can be very biased.</p>
</section>
<section id="attitudes-do-change" class="level1">
<h1>(2) Attitudes do Change</h1>
<p><strong>Attitudes vary a huge amount across time and space:</strong></p>
<ul>
<li>Variation in political and religious attitudes.</li>
<li>Variation in attitudes towards other races, sexes, sexualities, religions.</li>
<li>Variation in preferences over food, e.g.&nbsp;for rice vs wheat vs corn.</li>
<li>Variation in preferences over how many children to have.</li>
</ul>
<p><strong>It is hard to explain this variation with individual economic circumstances:</strong> when someone migrates to another country they face different economic circumstances (different prices and income) but they typically maintain their attitudes for decades.</p>
<p><strong>It is hard to explain this variation with genetic variation,</strong> because attitudes vary so much over time, while genes move very slowly.</p>
<p>So it seems like peer and media effects must be substantial proximal determinants of attitudes.</p>
</section>
<section id="resolution-each-effect-is-small-but-there-are-many" class="level1 page-columns page-full">
<h1>(3) Resolution: Each Effect is Small, but There are Many</h1>
<p><strong>How can we resolve small treatment effects with big variation in outcomes?</strong> It makes sense if we’ve only been testing very small treatments. Each individual effect is small but there are millions of them, so collectively the effects are large.</p>
<p><strong>Peer effect studies tend to find small effects when looking at random assignment of peers,</strong> e.g.&nbsp;random assignment of roommates, but this may be because time with your roommate constitutes only a very small share of your overall exposure to other people and ideas.<sup>1</sup> Collectively that exposure must be hugely important in your attitudes.</p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Kremer and Levy (2008) say <em>“Most studies do not find effects of these predetermined characteristics on the whole sample of students … conventional peer effects on academic achievement … are not estimated to be particularly important.”</em></p></div></div><p><strong>Media studies tend to find small effects from exposure to social media or to television,</strong> but in most cases the media exposure is only a single-digit percentage-point share of their lifetime exposure to media. So the aggregate effect can be far larger than that measured in any credible experiment or natural experiment (in addition, much of the effect likely propagates through peer effects).</p>
<p>An individual campaign advertisement might have very small effects on voting intention, but an individual campaign advertisement is only a tiny share of your lifetime exposure to political communication. Small individual effects are consistent with political attitudes being overwhelmingly determined by exposure and persuasion.</p>
<p><strong>More technically:</strong> we can reconcile small effects with big variations if:</p>
<ol type="1">
<li>Effects have a long half-life, e.g.&nbsp;exposure in childhood can affect your attitudes as an adult.</li>
<li>Peer effects are propagated through many weak links instead of a few strong links: i.e.&nbsp;there are substantial influences from all of society, not just closest friends and family.</li>
<li>Persuasion works even with indirect channels, e.g.&nbsp;your political views aren’t just affected by campaign ads, but also by the implicit attitudes to politics reflected in all the media you’re exposed to.</li>
<li>Attitudes are sensitive to the <em>average</em> rather than the <em>total</em> amount of persuasive material you’re exposed to, thus marginal effects can be small while total effects are large.</li>
</ol>
<p>(As a footnote: from my time in social media companies I learned that individual peer effects are tiny, yet we also know that social media demand is <em>entirely</em> peer effects, i.e.&nbsp;people only use Facebook because other people use Facebook.)</p>
</section>
<section id="other-notes" class="level1">
<h1>Other Notes</h1>
<p><strong>The paradox of large effects.</strong> <span class="citation" data-cites="tosh2021piranha">Tosh et al. (2021)</span> discuss an opposite problem: in some fields there are many claims of large effects, but it is not possible to reconcile the aggregate variance in the data with so many large effects. E.g. they discuss a paper claiming to show that exposure to age-related words tends to lower a subject’s subsequent walking speed by 13%. If people are exposed to many such primes, and they are uncorrelated, then we should expect huge and implausible variation in peoples’ day-to-day walking speed.</p>
<p>Their problem is somewhat the opposite: they are talking about a literature which has many non-credible effects from lab experiments or observational data. Instead I’m talking about literature which has credible but small effects.</p>
</section>
<section id="references" class="level1">




</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-angrist2014perils" class="csl-entry">
Angrist, Joshua D. 2014. <span>“The Perils of Peer Effects.”</span> <em>Labour Economics</em> 30: 98–108. https://doi.org/<a href="https://doi.org/10.1016/j.labeco.2014.05.008">https://doi.org/10.1016/j.labeco.2014.05.008</a>.
</div>
<div id="ref-ansolabehere2006paradox" class="csl-entry">
Ansolabehere, Stephen. 2006. <span>“The Paradox of Minimal Effects.”</span> <em>Capturing Campaign Effects</em>, 29–44.
</div>
<div id="ref-tosh2021piranha" class="csl-entry">
Tosh, Christopher, Philip Greengard, Ben Goodrich, Andrew Gelman, Aki Vehtari, and Daniel Hsu. 2021. <span>“The Piranha Problem: Large Effects Swimming in a Small Pond.”</span> <em>arXiv Preprint arXiv:2105.13445</em>.
</div>
</div></section></div> ]]></description>
  <guid>tecunningham.github.io/posts/2023-08-02-small-effects.html</guid>
  <pubDate>Wed, 02 Aug 2023 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Ranking by Engagement</title>
  <dc:creator>Tom Cunningham</dc:creator>
  <link>tecunningham.github.io/posts/2023-04-28-ranking-by-engagement.html</link>
  <description><![CDATA[ 





<style>
   h1 {  border-bottom: 4px solid black; }
   h2 {  border-bottom: 1px solid #ccc; }
   h3 { font-weight: bold; font-size: 1em; /*margin-left: -1em;*/ }
   h3~p { margin-left: 2em; }
   h3~ul { margin-left: 2em; }
   h3~ol { margin-left: 2em; }
   body {counter-reset: h3counter; }
   h3::before {
      content: counter(h3counter) ". ";
      counter-increment: h3counter;
   }
   
   dt { font-weight: normal; }
   dt strong { font-weight: bold; }
   dd { margin-left: 20px; }
   a { color: black; }
   /* dl { display: grid; grid-auto-flow: row;}
   dt { grid-column-start: 1; }
   dd { grid-column-start: 2; } */
</style>

<div class="no-row-height column-margin column-container"><div class="">
<p> Thanks to comments from Jeff Allen, Jacquelyn Zehner, David Evan Harris, Jonathan Stray, and others. If you find this note useful for your work send me an email and tell me :)</p>
</div><div class="">
<p><img src="tecunningham.github.io/posts/images/2023-06-13-15-11-29.png" class="img-fluid"></p>
</div></div>
<p><strong>Six observations on ranking by engagement on social media platforms:</strong></p>
<ol type="1">
<li><p><strong>Platforms rank content primarily by the predicted probability of engagement.</strong> Platforms choose for each user the items they are predicted to click on, or reply to, or to retweet, etc.<sup>1</sup></p></li>
<li><p><strong>Platforms rank by engagement because it increases user retention.</strong> In experiments which compare engagement-ranked feeds to unranked feeds (“chronological” feeds) the users with engagement-ranked feeds consistently show substantially higher long-run retention (DAU) and time-spent. Platforms care about engagement not in itself but as a means to an end, and when faced with a tradeoff between engagement and retention would choose retention.</p></li>
<li><p><strong>Engagement is negatively related to quality.</strong> The content with the highest predicted engagement very often has low scores by various measures of objective quality: clickbait, spam, scams, misleading headlines, and misinformation. Intuitively this is because engagement only measures immediate appeal, and the most appealing content is often the most disappointing. Low quality content typically <em>hurts</em> retention, and as a consequence platforms often supplement their engagement-based ranking algorithms with a range of proxies for content quality.</p></li>
<li><p><strong>Sensitive content is often both engaging and retentive.</strong> Engagement-ranked feeds often increase the prevalence of various types of “sensitive” content: nudity, bad language, abuse, hate speech, hyper-partisan politics, etc.. However unlike low-quality content, reducing the prevalence of sensitive content often hurts retention, implying that sensitivity is positively correlated with retention.</p></li>
<li><p><strong>Sensitive content is often preferred by users.</strong> Platforms have tried out many experiments with asking users directly for their preferences over content. The results have been mixed, and platforms have often been disappointed to find that users express fairly positive attitudes towards content that the platform considers sensitive.</p></li>
<li><p><strong>Platforms don’t want sensitive content but don’t want to be seen to be removing it.</strong> Platform decision-makers often have principled reasons for limiting the distribution of certain types of sensitive content. Additionally there are instrumental reasons: sensitive content attracts negative attention from the media, advertisers, app stores, politicians, regulators, and investors. But platforms are also hesitant to directly target this content, especially when it has some political dimension. As a consequence platforms often target sensitive content indirectly by using proxies, and they prefer to justify their decision-making by appealing to user preferences or to user retention.</p></li>
</ol>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;In this note I’m using “engagement” to refer to individual actions not user-level metrics like time-spent or DAU.</p></div></div><div class="cell page-columns page-full" data-layout-align="center">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-04-28-ranking-by-engagement_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="288"></p>
</figure>
</div>
</div></div></div>
<p><strong>In an appendix I formalize the argument.</strong> I show that all these observations can be expressed as covariances between different properties of content, e.g.&nbsp;between the retentiveness, predicted engagement rates, and other measures of content quality. From those covariances we can derive Pareto frontiers and visualize how platforms are trading-off between different outcomes.</p>
<p><br><br><br></p>
<section id="argument-in-detail" class="level1 page-columns page-full">
<h1>Argument in Detail</h1>
<section id="i-will-bucket-attributes-of-content-into-five-types" class="level3">
<h3 class="anchored" data-anchor-id="i-will-bucket-attributes-of-content-into-five-types">I will bucket attributes of content into five types</h3>
<ol type="1">
<li><em>Engagement:</em> the predicted probability of a user clicking, commenting, retweeting, etc., on a specific piece of content.</li>
<li><em>Retentiveness:</em> the causal contribution of seeing the content on a specific user’s long-term retention (e.g.&nbsp;DAU). Unlike the other attributes this can never be directly observed, only inferred from experiments.</li>
<li><em>Quality:</em> some objective measure of quality, e.g.&nbsp;whether fact-checked, whether the headline is misleading, whether the linked website has a high ad-load, whether the source is trustworthy, etc..</li>
<li><em>Sensitivity:</em> whether the content could be offensive, harmful, corrosive – e.g.&nbsp;nudity, bad language, abuse, hate speech.</li>
<li><em>Preference:</em> the user’s response to a survey question, e.g.&nbsp;“do you want to see more of this type of content?”</li>
</ol>
<p>Note that “quality” and “sensitivity” apply to individual pieces of content, while the other three attributes apply to relationship between a user and a piece of content.</p>
</section>
<section id="social-media-platforms-rank-their-content-primarily-by-predicted-engagement" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="social-media-platforms-rank-their-content-primarily-by-predicted-engagement">Social media platforms rank their content primarily by predicted engagement</h3>
<p>The core ranking model for most social platforms is a weighted average of predicted engagement rates.<sup>2</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;I believe this is true for almost all platforms with personalized recommendations including YouTube, Netflix, and Amazon. An excellent reference for how recommenders work, with illustrations and links, is Thorburn, Bengani, &amp; Stray (2022) <a href="https://medium.com/understanding-recommenders/how-platform-recommenders-work-15e260d9a15a">“How Platform Recommenders Work”</a></p></div><div id="fn3"><p><sup>3</sup>&nbsp;For simplicity the rest of the discussion treats the causal effect of content as purely separable.</p></div></div><p>However ranking functions also include hundreds of other tweaks incorporating non-engagement features, upranking or downranking content depending on, for example, the media type (photo/text/video), the relationship between the user and the author (whether you follow this person), various predictions of of objective quality (classifiers predicting whether the content is spam, offensive, adult, misinformation, etc.), or other features (network centrality, off-platform popularity, etc.). They also often have some diversity rules to prevent the content that is shown from being too similar.<sup>3</sup></p>
<p>Ranking by popularity is common for other media: we look at lists of bestsellers, most popular, highest grossing, most watched, or top charting. Attention is limited and it would be inefficient to offer people a random selection of everything that’s available.</p>
</section>
<section id="predicted-engagement-rates-are-mostly-historical-engagement-rates" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="predicted-engagement-rates-are-mostly-historical-engagement-rates">Predicted engagement rates are mostly historical engagement rates</h3>
<p>In many cases the most important predictors of whether a user will engage with a piece of content are (1) this user’s historical rate of engagement on similar pieces of content (e.g.&nbsp;content from the same author, or of the same media-type); (2) other users’ rate of engagement on this piece of content. Platforms do use more complicated models (embeddings and neural nets), those models typically are most valuable for qualitatively new types of content, when you have relatively sparse historical data either on the user or on the item.<sup>4</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn4"><p><sup>4</sup>&nbsp;See discussion below of Covington et al.&nbsp;(2016) which describes an architecture commonly used in recommenders.</p></div></div></section>
<section id="platforms-care-primarily-about-long-run-retention-engagement-is-a-means-to-that-end" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="platforms-care-primarily-about-long-run-retention-engagement-is-a-means-to-that-end">Platforms care primarily about long-run retention, engagement is a means to that end</h3>
<p>The outcome that leadership care about the most is long-run retention, measured with metrics like Daily Active Users (DAU).<sup>5</sup> They would generally sacrifice substantial amounts of engagement in return for DAU. They also would sacrifice substantial short-term DAU if it could be shown with confidence that it would lead to higher long-term DAU.</p>
<div class="no-row-height column-margin column-container"><div id="fn5"><p><sup>5</sup>&nbsp;Most publicly-traded platforms report in their quarterly earnings just one non-financial metric: the number of active users (DAU/MAU/mDAU, etc.). I do not know of any company that publicly reports a metric of aggregate engagement.</p></div></div><p>This point is often unclear because many changes to ranking (as measured in experiments) move engagement and retention in the same direction, and move short-run and long-run metrics in the same direction, meaning that we cannot easily tell which metric is decisive. Individual teams are often given targets to increase short-term engagement but that is mainly because that metric is easier to measure.</p>
<p></p>
</section>
<section id="engagement-ranked-feeds-have-substantially-higher-long-term-retention-and-time-spent-than-chronologically-ranked-feeds" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="engagement-ranked-feeds-have-substantially-higher-long-term-retention-and-time-spent-than-chronologically-ranked-feeds">Engagement-ranked feeds have substantially higher long-term retention and time-spent than chronologically-ranked feeds</h3>
<p>Users who are given engagement-ranked feeds in experiments typically have higher long-term DAU by single-digit percentages (1%-9%), and higher long-term time-spent by double-digit percentages (10%-99%). Accounting for network effects makes the aggregate difference even larger.<sup>6</sup>.</p>
<div class="no-row-height column-margin column-container"><div id="fn6"><p><sup>6</sup>&nbsp;<span class="citation" data-cites="huszar2022twitter">Huszár et al. (2022)</span> note that since Twitter introduced a ranked timeline in 2016 they maintained an experiment with 1% of users with a chronological feed.</p></div></div><ul>
<li><p><em>Meta 2020 experiments with chronological ranking showed a 20% time-spent decline on Facebook and a 10% decline on Instagram.</em> The experiments ran for 3 months and they reported the average effect over the whole period, it is likely that the time-spent effects continued declining. The effects on DAU were not reported. See <span class="citation" data-cites="guess2023chronological">Guess et al. (2023)</span>.</p></li>
<li><p><em>A Facebook 2018 experiment showed a 3% decline in time-spent after 10 days.</em> The effects on time-spent seemed to be linearly trending down at the time the analysis was posted. Engagement (MSI) declined by about 20%, and politics impression reduced by 15% (the share of politics impressions is more complicated to calculate but seems unambiguously down). The effects on DAU were not reported (<a href="https://www.bigtechnology.com/p/facebook-removed-the-news-feed-algorithm?s=09">source</a>.). The experiment was not on a purely chronological feed: they retained “diversity rules, client-side ranking, read-state logging, comment-bumping” as well as integrity ranking rules.</p>
<p></p></li>
</ul>
</section>
<section id="engaging-content-is-often-low-quality" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="engaging-content-is-often-low-quality">Engaging content is often low quality</h3>
<p>Despite the positive relationship between engagement and retention, many studies have found that highly-engaging content is has lower-than-average quality:</p>
<ul>
<li><em>In summer 2016 half of Facebook’s most-seen posts related to the US election were misinformation.</em> As <a href="https://www.buzzfeednews.com/article/craigsilverman/viral-fake-election-news-outperformed-real-news-on-facebook">reported</a> by Craig Silverman at Buzzfeed. This exceeds the <em>average</em> rate of misinformation, i.e.&nbsp;the most-engaging posts have a much lower-than-average quality.</li>
<li><em>In 2019 Facebook’s top group and pages were run by troll farms.</em> A series of internal analyses by by Jeff Allen (<a href="https://s3.documentcloud.org/documents/21063547/oct-2019-facebook-troll-farms-report.pdf">subsequently leaked</a>) found that a substantial share of Facebook’s top pages, groups, and posts, were run by “troll farms,” whose main tactic was reposting copied content that had high engagement rates. </li>
<li><em>In 2019 Facebook’s high-quality content received lower engagement.</em> In the late 2010s Facebook maintained an internal “quality” score for content (FUSS=“Feed Unified Scoring System”). A data scientist’s analysis from 2019 (<a href="https://s3.documentcloud.org/documents/21063547/oct-2019-facebook-troll-farms-report.pdf">subsequently leaked, see p.10</a>) found that low-quality content had significantly higher predicted engagement rates.<sup>7</sup></li>
<li><em>In 2021 Facebook’s most-viewed posts were very low quality.</em> Since early 2021 FB has been releasing a dataset of their 20 most-viewed links and posts. An <a href="https://lookerstudio.google.com/u/0/reporting/28bc32fd-a067-4b4a-9be0-637e8c9bd917/page/0Z3mC?s=g7_EWEyFjrc">Integrity Institute analysis</a> by Jeff Allen has found that each quarter 60-80% of the posts fail some basic checks, either “the account behind it is anonymous, is posting unoriginal content, using spammy page or group networks, or if the post or link violated Facebook’s community standards.”</li>
<li><em>In 2020 Twitter’s ranking has mixed effects on political content.</em> Huszar et al.&nbsp;(2021) compare political content of users who are randomized to ranked vs chronological feeds. They report (1) for political parties, ranked feed tends to amplify the right-wing parties somewhat more than left-wing parties (but the same does not hold for individual politicians); (2) for US media sources, ranked feed amplifies “sources that are more partisan compared to ones rated as center”.<sup>8</sup></li>
<li><em>In 2020 FB and IG chronological-ranking experiments showed a 5-15% increase in political content.</em> <span class="citation" data-cites="guess2023chronological">Guess et al. (2023)</span> found that replacing feed-ranking algorithms with simple chronological ranking (i.e., the most-recent posts are shown first) for 3 months (1) the share of impressions that were classified as political increased by 15% on Facebook and by 5% on Instagram, the share that were classified as “political news” increased by 40% on Facebook.</li>
</ul>
<div class="no-row-height column-margin column-container"><div id="fn7"><p><sup>7</sup>&nbsp;These correlations can be difficult to interpret: suppose there is no correlation between engagement and quality in the pool of all available content, there will nevertheless be a negative correlation among the subset of content that is <em>seen</em> if the ranking algorithm penalizes low-quality content, meaning low-engagement low-quality content will never be shown to users. It is unclear from the document whether the correlation is among content that is available, or content that is seen.</p></div><div id="fn8"><p><sup>8</sup>&nbsp;Bakshy et al.&nbsp;(2015) found that Facebook’s feed-ranking doesn’t substantially change the share of cross-cutting (across-the-aisle) content seen.</p></div></div></section>
<section id="many-platforms-have-found-that-increasing-quality-helps-retention" class="level3">
<h3 class="anchored" data-anchor-id="many-platforms-have-found-that-increasing-quality-helps-retention">Many platforms have found that increasing quality helps retention</h3>
<p>Platforms have tried to address quality problems by defining measures of objective quality:</p>
<ul>
<li><p><em>Facebook uses many heuristics and classifiers to identify various types of low-quality content:</em> Facebook identifies and downranks, among other things, engagement bait, links that go to ad-farms, scraped content, titles that withhold information, and titles that exaggerate information. In each case these types of content would generate high engagement but give users a bad experience, and in most cases experiments confirmed that dowranking these types of content increases long-run user retention.</p></li>
<li><p><em>Facebook uses some metadata features to identify low-quality content.</em> E.g. Facebook calculates the <a href="https://www.cnbc.com/2019/04/10/facebook-click-gap-google-like-approach-to-stop-fake-news-going-viral.html">“click gap”</a> (the amount of organic traffic a website gets) and <a href="https://www.wired.com/story/how-facebook-wants-to-improve-the-quality-of-your-news-feed/">“broad trust”</a> (diversity of engagement across users).</p>
<p></p></li>
<li><p><em>YouTube has introduced a series of quality adjustments to ranking:</em> E.g. downranking <a href="https://blog.youtube/inside-youtube/on-youtubes-recommendation-system/">“sensationalistic tabloid content”</a> and upranking <a href="https://blog.youtube/inside-youtube/on-youtubes-recommendation-system/">“authoritative content”</a>.</p></li>
</ul>
<p>Some companies have also shifted engagement weights to put relatively more weight on “deeper” measures of engagement:</p>
<ul>
<li><em>In 2012 YouTube switched from maximizing clicks to maximizing watch-time.</em> They found it led to a short-term decrease in clicks but a long-term <a href="https://blog.youtube/inside-youtube/on-youtubes-recommendation-system/">increase in retention</a>. I believe Netflix similarly has invested a lot of time in developing “deep” measures of engagement.</li>
</ul>
<p></p>
</section>
<section id="quality-also-helps-the-producer-ecosystem." class="level3">
<h3 class="anchored" data-anchor-id="quality-also-helps-the-producer-ecosystem.">Quality also helps the producer ecosystem.</h3>
<p>There is an additional reason for prioritizing the quality of content independent of the direct effect on user retention: because prioritizing high-quality content helps foster a long-run community of creators.</p>
<p>A central fact about social media is that it relies on a tremendous amount of uncompensated labor. Most content is created for the joy of creation, with little realistic expectation of financial return. The impulse to post clearly relies on a delicate social perception or norm, and a platform could inadvertantly break this spell. I think speaking loosely Facebook mismanaged their public-content ecosystem in this way: they alienated creators in a variety of ways, especially by allowing copied content to proliferate, and high-quality creators instead posted to Instagram, Twitter, YouTube, and Tik Tok. Facebook leadership tried to attract creators with various monetary incentives but they often backfired: creators who are financially-motivated are often not the creators you want.</p>
</section>
<section id="engagement-measures-immediate-quality-and-hence-is-a-poor-proxy-for-the-quality-of-factual-claims" class="level3">
<h3 class="anchored" data-anchor-id="engagement-measures-immediate-quality-and-hence-is-a-poor-proxy-for-the-quality-of-factual-claims">Engagement measures <em>immediate</em> quality, and hence is a poor proxy for the quality of factual claims</h3>
<p>Engagement necessarily measures the immediate reaction of a user to a piece of content, and thus ranking by predicted engagement will surface content that <em>appears</em> to be good. This is fine when there is no hidden aspect to quality, e.g.&nbsp;for jokes and pictures which mostly be judged in the moment. However if we rank informational content by predicted engagement it will tend to surface the claims that are the most sensational or intriguing independent of whether they are true.</p>
<p>If apples were sold only by how they looked, and not by how they tasted, then we would be offered delicious-looking and bland-tasting apples.</p>
<p>I believe this basic mechanism explains why internet platforms typically have higher rates of exaggerated, misleading, or false content compared to traditional media (newspapers, television, etc.). Traditional media do not publish whichever headlines would maximize short-run sales because that would harm long-run sales. This also explains why platforms have found that they can substantially improve retention by building proxies for quality.</p>
</section>
<section id="a-negative-relationship-between-engagement-and-quality-can-be-caused-by-unscrupulous-publishers" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="a-negative-relationship-between-engagement-and-quality-can-be-caused-by-unscrupulous-publishers">A negative relationship between engagement and quality can be caused by unscrupulous publishers</h3>
<p>Suppose each publisher can produce a fixed number of headlines, which will vary in (1) the headline’s propensity to be clicked, and (2) whether the headline is true. There are two types of publishers:</p>
<ol type="1">
<li>Honest publishers: they choose the most-engaging headlines from within the subset that are true.</li>
<li>Dishonest publishers: they choose the most-engaging headlines from the universe of all possible headlines (irrespective of truth).</li>
</ol>
<p>In this world the most-engaging headlines will be disproportionately false compared to the average headline. In the long run consumers will learn some skepticism, and to discount headlines in proportion to how clickable they seem, but they are unlikely to learn to discriminate perfectly. There’s always a chance that an intriguing headline will be true, and so the negative correlation would persist in equilibrium.<sup>9</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn9"><p><sup>9</sup>&nbsp;A more formal version: each consumer sees a single headline with observed signal <img src="https://latex.codecogs.com/png.latex?s"> and chooses whether to click. The payoff from clicking is <img src="https://latex.codecogs.com/png.latex?s"> if it’s from an honest publisher, zero otherwise, and there’s some stochastic outside option so the consumer’s probability of clicking is continuously increasing in the expected payoff from clicking. Honest publishers report their signals drawn from <img src="https://latex.codecogs.com/png.latex?f_H(s)">, dishonest publishers choose any signal. In equilibrium the dishonest publishers’ signal distribution, <img src="https://latex.codecogs.com/png.latex?f_D(s)">, must be such that all signals with non-zero mass have have an equal click-through rate, meaning there is some <img src="https://latex.codecogs.com/png.latex?%5Ckappa"> such that for every <img src="https://latex.codecogs.com/png.latex?s%3E%5Ckappa">, <img src="https://latex.codecogs.com/png.latex?f_D(s)=%5Cfrac%7Bs-%5Ckappa%7D%7B%5Ckappa%7Df_H(s)">. Thus low click-through-rate headlines (<img src="https://latex.codecogs.com/png.latex?s%3C%5Ckappa">) are all true, but high click-through rate headlines (<img src="https://latex.codecogs.com/png.latex?s%3E%5Ckappa">) all have some share which are false. Qualitatively: if a headline is not very interesting, then you believe it; but if it’s interesting then you discount exactly inversely to how interesting it is. In this model we have (1) retentiveness (consumer surplus) is increasing with engagement; (2) quality (truth) is decreasing with engagement; (3) retentiveness (consumer surplus) would be higher if you rank by both engagement and quality (e.g.&nbsp;by removing false stories).</p></div></div></section>
<section id="platforms-have-been-slow-in-improving-the-quality-of-ranked-content" class="level3">
<h3 class="anchored" data-anchor-id="platforms-have-been-slow-in-improving-the-quality-of-ranked-content">Platforms have been slow in improving the quality of ranked content</h3>
<p>I discuss above some examples of Facebook’s slowness in addressing problems with the quality of content. I think this slowness is for two mains reasons. First, predicting engagement is a well-defined technical problem with a track record of success while evaluating content-quality is much more open-ended and difficult to validate. Hard-headed engineers often argue that a user’s preferences are revealed in their engagement and that evaluating quality is paternalistic. Secondly, platforms are nervous of being opinionated about objective quality because they don’t wish to take sides on politically delicate issues. In 2016 Facebook was criticized for using human judgment in determining what topics are “trending”, and in the wake of that criticism many projects which involved human judgment were shut down and replaced with automatic systems. Then in 2020 engineers on News Feed were told to avoid using words such as “trust” or “quality” or “authority”, and to instead use language that referred only to user preferences.</p>
<p></p>
<p></p>
</section>
<section id="sensitive-content-is-often-both-engaging-and-retentive" class="level3">
<h3 class="anchored" data-anchor-id="sensitive-content-is-often-both-engaging-and-retentive">Sensitive content is often both engaging and retentive</h3>
<p>I have defined “sensitive” content to include nudity, bad language, abuse, hate speech, hyper-partisan politics, etc.. Sensitive content often has higher-than-average engagement rates, and when content is demoted this often hurts retention, implying that sensitivity is positively correlated with retentiveness.</p>
<ul>
<li>A 2023 academic study by <a href="https://drive.google.com/file/d/1HYiBOGLNM91RBiqBlxKFvjDhJAuCxe60/view">Beknazar-Yuzbashev et al.</a> estimated that filtering the 7% most-toxic content on Facebook reduced overall Facebook content consumed by 20%. However the results look quite odd to me: they asked users to install a custom browser, which filtered out content. They estimate an extraordinarily high incrementality rate (greater than 2X), the dynamic treatment effects shown in Fig 13 are very weak, and I found it difficult to follow the arguments against differential attrition driving these results.</li>
</ul>
</section>
<section id="platforms-dont-want-to-show-sensitive-content" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="platforms-dont-want-to-show-sensitive-content">Platforms don’t want to show sensitive content</h3>
<p>Platforms are clearly prepared to pay a cost to reduce the prevalence of sensitive content, both in terms of retentiveness (DAU), and in the monetary cost of engineers, labelers, and computation.<sup>10</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn10"><p><sup>10</sup>&nbsp;Platforms often spend around 5% of their total costs on content moderation, despite the prevalence of sensitive content and the effects on retention typically being closer to 0.1% or less.</p></div></div><p>The platforms have many reasons for avoiding sensitive content, independent of its effect on retention, but internally there is often an ambiguity about the contribution of different reasons. In part decisions are driven by a feeling of moral duty to not amplify content that is harmful. However there are also many instrumental reasons, because sensitive content often causes friction with advertisers, app stores, regulators, media, employees and investors.</p>
<p>Misinformation is a somewhat special case. From what has been discussed before, misinformation can be expected to reduce retention because it’s not true. However misinformation is very often related to sensitive issues, e.g.&nbsp;partisan politics, race relations, vaccines, and often reports falsehoods that support the viewer’s political prejudices.</p>
</section>
<section id="platforms-avoid-directly-penalizing-sensitive-content" class="level3">
<h3 class="anchored" data-anchor-id="platforms-avoid-directly-penalizing-sensitive-content">Platforms avoid directly penalizing sensitive content</h3>
<p>Platforms are caught in a double bind: there are strong pressures to reduce the amount of sensitive content on their platform, but there is also a pressure not to be seen to be making judgments about the objective value or harm of content. They want a garden with no weeds but they also wish to have clean hands. This often causes a bifurcation between the nominal reason for a policy and the real reason. Some examples:</p>
<ul>
<li><p><em>Platforms often downrank engagement patterns because they correlate with sensitive content.</em> It is common to downrank posts which features a specific engagement pattern (e.g.&nbsp;certain types of sharing, certain types of downstream attributes), and the downranking is justified internally based on the correlation with measures of quality or sensitivity, e.g.&nbsp;misinformation, or hate speech, or hyperpartisan content. This is odd because it would seem to be more efficient to target the sensitive content directly, i.e.&nbsp;instead of downranking the proxy, use the proxy as a feature in a classifier, and downrank based on the classifier output. However platforms avoid this approach in part because they are nervous about the perception of being perceiving as judges of the quality of content.</p></li>
<li><p><em>Platforms speak about sensitivity rules as if they were adopted to serve the interests of their users.</em> Google’s Jigsaw group has an influential set of definitions of content quality, their <a href="https://support.perspectiveapi.com/s/about-the-api-attributes-and-languages?language=en_US">definition</a> of a “toxic” comment is <em>“a rude, disrespectful, or unreasonable comment that is likely to make people leave a discussion.”</em> This definition is worded to presuppose that rude comments cause lower retention. The definition thus allows a platform to talk about their toxicity classifiers as if they were solely serving the interests of the users exposed to toxic language.</p></li>
<li><p><em>Survey questions are chosen based on their correlation with measures of sensitive content.</em> Platforms will often try out multiple different wordings of a survey question and decide which one to use by comparing the results with their internal measures of content quality and sensitivity, leading to survey questions that are somewhat awkwardly worded (e.g.&nbsp;asking people “is this good for the world?”).</p></li>
</ul>
</section>
<section id="subjective-user-ratings-of-quality-have-a-mixed-relationship-with-objective-measures-of-quality" class="level3">
<h3 class="anchored" data-anchor-id="subjective-user-ratings-of-quality-have-a-mixed-relationship-with-objective-measures-of-quality">Subjective user ratings of quality have a mixed relationship with objective measures of quality</h3>
<p>Platforms have often tried to collect explicit user feedback about quality, e.g.&nbsp;asking “was this worth your time?”, “do you want to see more of this?”, “was this informative?”. In my experience, for most such questions, responses are highly correlated with engagement, but often show a negative correlation with objective measures of quality. E.g. people often rate misinformation as “informative” and “worth my time.”</p>
<p>Nevertheless some of these initiatives have had success in raising both objective quality and retention, e.g.&nbsp;Facebook recently launched a prompt asking “would you like to see more posts like this?” The signal from this prompt apparently increases both retention and many objective measures of quality.</p>
<p></p>
</section>
<section id="platforms-additionally-care-about-engagement-because-of-network-effects" class="level3">
<h3 class="anchored" data-anchor-id="platforms-additionally-care-about-engagement-because-of-network-effects">Platforms additionally care about engagement because of network effects</h3>
<p>I said above that platforms care about engagement primarily insofar as it’s a proxy for retention, however there is an additional reason to pay attention to engagement. When one user engages (likes, comments, retweets) this increases the value of the platform to all the other users, and so has an indirect positive effect on retention. For this reason platforms are generally willing to sacrifice some retention in return for engagement, as measured in an experiment, if the sacrifice is sufficiently small.</p>
</section>
</section>
<section id="technical-appendix-expressed-as-a-covariance-matrix" class="level1">
<h1>Technical Appendix: Expressed as a Covariance Matrix</h1>
<p><strong>We can express most of the argument above with a covariance matrix.</strong> Given a user we can give scores to each piece of content with respect to the five attributes defined above. Then we can give a reasonable characterization of the platform ranking problem with the following covariance matrix:</p>
<table class="caption-top table">
<colgroup>
<col style="width: 25%">
<col style="width: 19%">
<col style="width: 14%">
<col style="width: 10%">
<col style="width: 16%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">retentiveness</th>
<th style="text-align: center;">engagement</th>
<th style="text-align: center;">quality</th>
<th style="text-align: center;">sensitivity</th>
<th style="text-align: center;">preference</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>retentiveness</strong></td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">+</td>
<td style="text-align: center;">+</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">+</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>engagement</strong></td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">+</td>
<td style="text-align: center;">+</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>quality</strong></td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>sensitivity</strong></td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">+</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>preference</strong></td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">&nbsp;</td>
</tr>
</tbody>
</table>
<p><strong>Given this covariance matrix, we can draw Pareto frontiers and indifference curves.</strong> Each Pareto frontier represents the set of achievable tradeoffs between two outcomes. I explain below how elliptical Pareto frontiers and linear indifference curves can be derived from the covariance matrix if we assume that everything is distributed joint Normally.</p>
<p><strong>Retentiveness and engagement.</strong> We can draw a Pareto frontier between retention and engagement as below. We do not directly observe the retentiveness of content, but we know that ranking content by engagement (i.e.&nbsp;choosing the farthest right-hand point on the Pareto frontier) increases retention relative to an unranked feed, so we can infer that retentiveness and engagement are positively correlated, thus the Pareto ellipse must be upward-sloping.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-04-28-ranking-by-engagement_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="288"></p>
</figure>
</div>
</div>
</div>
<p><strong>Engagement and quality.</strong> As discussed above, we often see that (1) measures of content quality have zero or negative correlation with engagement, (2) downranking low-quality content (equivalently, upranking high-quality content) increases retention. This is somewhat surprising because engagement and retention have a positive correlation, meaning the three correlations are not transitive.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-04-28-ranking-by-engagement_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="336"></p>
</figure>
</div>
</div>
</div>
<p>We illustrate the relationship with retention here with three lines representing different levels of retention, effectively these are indifference curves of a platform that is trying to maximize retention.</p>
<p><strong>Engagement and sensitivity.</strong> Next consider “sensitive” attributes. We often see that more sensitive content has higher engagement rates, shown below as an upward-tilt to the Pareto frontier. In addition experiments that penalize sensitive content often have a negative effect on retention: this could be either due to a positive partial correlation between engagement and retentiveness, or a positive partial correlation between sensitivity and retentiveness. But in either case it seems that sensitive content does not have a strong negative effect on retention.</p>
<p>Despite these facts, most platforms still put substantial penalties on sensitive content, either directly or indirectly (as discussed above), and they pay a price in terms of both engagement and retention.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-04-28-ranking-by-engagement_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="288"></p>
</figure>
</div>
</div>
</div>
<p><strong>Preference and sensitivity.</strong> Finally consider a direct measure of user preference over content, e.g.&nbsp;asking users “is this informative?” or “would you like to see more like this?” In general user preference correlates relatively well with engagement, but it also offers incremental value for predicting retentiveness, in other words adding an additional term to the ranking function to predict user preference tends to increase retention.</p>
<p>However as discussed above, projects which collect survey questions are often focussed on the sensitivity of content rather than its retentiveness, and in that respect their findings are often mixed. Below we illustrate a case in which ranking by preference increases retentiveness but does not lower the amount of sensitive content (which platforms often desire). However platforms will offer try out many different wordings of survey questions, and each question will have somewhat different correlations.</p>
<div class="cell" data-layout-align="center">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-04-28-ranking-by-engagement_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid quarto-figure quarto-figure-center figure-img" width="288"></p>
</figure>
</div>
</div>
</div>
<section id="formal-observations" class="level2">
<h2 class="anchored" data-anchor-id="formal-observations">Formal Observations</h2>
<p>Here I describe a few formal properties of a model of ranking based on a joint-normal distribution of attributes. Fuller proofs of some of these results are <a href="https://tecunningham.github.io/posts/2023-10-23-pareto-frontiers-experiments-ranking.html">here</a>, other are in notes I’m preparing and available on request.</p>
<ol type="1">
<li><p><strong>The covariance between item attributes will determine a Pareto frontier among outcomes.</strong> Suppose we know the joint distribution of attributes and we can choose a subset with share <img src="https://latex.codecogs.com/png.latex?p"> of the distribution (e.g.&nbsp;a fixed number of impressions given a pool of possible stories to show), and we want to calculate the average value of each attribute in the subset of content shown to the user. Then we can describe the Pareto frontier over subsets, i.e.&nbsp;the set of realized average outcomes, and it will be a function of the covariances among attributes over pieces of content. With 2 attributes the Pareto frontier will be an ellipse with shape exactly equal to an isoprobability curve from the joint density.</p>
<p>The shape of the ellipse has a simple interpretation. If two attributes are positively correlated then the Pareto frontier will be <em>tight</em> meaning there is little tradeoff, i.e.&nbsp;we will have similar aggregate outcomes independent of the relative weights put on each outcome in ranking. If instead two attributes are negatively correlated then the Pareto frontier will be <em>loose</em> meaning outcomes will vary a lot with the relative weights used in ranking.</p>
<p>Our assumption that the share <img src="https://latex.codecogs.com/png.latex?p"> is fixed is equivalent to assuming that any ranking rule will get the same number of impressions. This assumption obviously has some tension with <em>retentiveness</em> being an outcome variable: if some ranking rule has low retentiveness, then we would expect lower impressions. Accounting for this would make the Pareto frontier significantly more complicated to model, for simplicity we can interpret every attribute except retentiveness as a short-run outcome. Alternatively we could interpret them as relative instead of absolute outcomes, e.g.&nbsp;as engagement/impression or engagement/DAU.</p></li>
<li><p><strong>Improving a classifiers will stretch the Pareto frontier.</strong> As a classifier gets better the average prediction will stay the same but the variance will increase, meaning the Pareto frontier will stretch out, and given a linear indifference curve we can derive the effect on outcomes.</p></li>
<li><p><strong>The joint distribution plus utility weights will determine ranking weights.</strong> If we observe only some outcomes then we can calculate the conditional expectation for other outcomes. Typically we want to know retentiveness, and we can write the conditional expectation as follows: <img src="https://latex.codecogs.com/png.latex?E%5B%5Ctext%7Bretentiveness%7D%7C%0A%20%20%20%5Ctext%7Bengagement%7D,%5Cldots,%5Ctext%7Buser%20preference%7D%5D."> This expectation has a closed-form solution when the covariance matrix is joint normal. When we have just two signals, for example engagement and quality, we can write:</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20E%5Br%7Ce,q%5D%20&amp;=%20%5Cfrac%7B1%7D%7B1-%5Cgamma%5E2%7D(%5Crho_e-%5Cgamma%5Crho_q)e%20+%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5Cfrac%7B1%7D%7B1-%5Cgamma%5E2%7D(%5Crho_q-%5Cgamma%5Crho_e)q%5C%5C%0A%20%20%20r%20%20%20%20%20&amp;=%20%5Ctext%7Bretentiveness%7D%5C%5C%0A%20%20%20e%20%20%20%20%20&amp;=%20%5Ctext%7Bengagement%20(predicted)%7D%5C%5C%0A%20%20%20q%20%20%20%20%20&amp;=%20%5Ctext%7Bquality%20(predicted)%7D%5C%5C%0A%20%20%20%5Crho_%7Be%7D%20%20%20%20%20&amp;=%20%5Ctext%7Bcovariance%20of%20engagement%20and%20retentiveness%7D%5C%5C%0A%20%20%20%5Crho_%7Bq%7D%20%20%20%20%20&amp;=%20%5Ctext%7Bcovariance%20of%20quality%20and%20retentiveness%7D%5C%5C%0A%20%20%20%5Cgamma%20%20%20%20%20&amp;=%20%5Ctext%7Bcovariance%20of%20engagement%20and%20quality%7D%0A%5Cend%7Baligned%7D">
<p>Note that the slope of the iso-retentiveness line in <img src="https://latex.codecogs.com/png.latex?(e,q)">-space will be <img src="https://latex.codecogs.com/png.latex?-%5Cfrac%7B%5Crho_e-%5Cgamma%5Crho_q%7D%7B%5Crho_q-%5Cgamma%5Crho_e%7D">.</p></li>
<li><p><strong>Experiments which vary ranking weights tell us about covariances.</strong> We can write findings from experiments as follows. First, suppose we find that retention is higher when ranked by engagement than when unranked, this can be written:</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Cutt%7BE%5Br%7Ce%3Ee%5E*%5D%7D%7Branked%20by%7D%7Bengagement%7D%20&amp;%3E%20%5Cut%7BE%5Br%5D%7D%7Bunranked%7D%0A%20%20%20%5Cend%7Baligned%7D">
<p>Here <img src="https://latex.codecogs.com/png.latex?e%5E*"> is chosen such that <img src="https://latex.codecogs.com/png.latex?P(e%3Ee%5E*)=p"> for some <img src="https://latex.codecogs.com/png.latex?p">, representing the share of potential inventory that the user consumes. This implies that engagement must positively correlate with retentiveness, <img src="https://latex.codecogs.com/png.latex?%5Crho_e%3E0">.</p>
<p>Next we can express that retention is higher when we put some weight <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> on quality:</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%5Cutt%7BE%5Br%7Ce+%5Cbeta%20q%3E%5Ckappa%5E*%5D%7D%7Branked%20by%7D%7Bengagement%20and%20quality%7D%20&amp;%3E%20%5Cutt%7BE%5Br%7Ce%3Ee%5E*%5D%7D%7Branked%20by%7D%7Bengagement%7D%0A%5Cend%7Baligned%7D">
<p>Here <img src="https://latex.codecogs.com/png.latex?%5Ckappa%5E*"> is chosen such that <img src="https://latex.codecogs.com/png.latex?P(e+%5Cbeta%20q%20%3E%20%5Ckappa%5E*)=P(e%3Ee%5E*)=p">. If <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> is fairly small then we can infer that the iso-retentiveness line is downward-sloping, implying: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Crho_e-%5Cgamma%5Crho_q%7D%7B%5Crho_q-%5Cgamma%5Crho_e%7D%3E0."></p>
<p>This implies that both engagement and quality have the same sign. I don’t think they both can be negative, so they both must be positive:</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Crho_e%20-%20%5Cgamma%20%5Crho_q%20&amp;%3E%200%20%5C%5C%0A%20%20%20%20%20%20%5Crho_q%20-%20%5Cgamma%20%5Crho_e%20&amp;%3E%200.%0A%20%20%20%5Cend%7Baligned%7D"></li>
<li><p><strong>I think it’s reasonable to treat preferences as locally linear.</strong> To have a well-defined maximization problem (with an interior solution) we need either nonlinear preferences or a nonlinear Pareto frontier. It’s always easier to treat things as linear when you can, so a relevant question is which of these two is closer to linear? Internally companies often treat their preferences as nonlinear, e.g.&nbsp;setting specific goals and guardrails, but those are always flexible and often have justifications as incentive devices. Typical metric changes are small, only single-digit percentage points, over that range the Pareto frontier does show significant diminishing returns while (it seems to me) value to the company does not.</p></li>
</ol>
</section>
</section>
<section id="appendix-literature-on-ranking-and-recommendation" class="level1">
<h1>Appendix: Literature on Ranking and Recommendation</h1>
<section id="overviews-of-recommender-systems-chronological" class="level2">
<h2 class="anchored" data-anchor-id="overviews-of-recommender-systems-chronological">Overviews of Recommender Systems (chronological)</h2>
<dl>
<dt><strong>Adomavicius and Tuzhilin (2005) “Toward the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible Extensions”</strong></dt>
<dd>
An influential overview of recommender systems (14,000 citations!). The canonical example is recommending movies to get the highest predicted rating. They use “rating” as similar to “engagement”. A more recent survey is <a href="https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00592-5">Roy and Dutta (2022)</a>.
</dd>
<dt><strong>Davidson et al.&nbsp;(2010) <a href="https://www.inf.unibz.it/~ricci/ISR/papers/p293-davidson.pdf">“The YouTube Video Recommendation System”</a></strong></dt>
<dd>
<blockquote class="blockquote">
<p>“[videos] are scored and ranked using … signals [which] can be broadly categorized into three groups corresponding to three different stages of ranking: 1) video quality, 2) user specificity and 3) diversification.””</p>
</blockquote>
</dd>
<dd>
<blockquote class="blockquote">
<p>“The primary metrics we consider include click through rate (CTR), long CTR (only counting clicks that led to watches of a substantial fraction of the video), session length, time until first long watch, and recommendation coverage (the fraction of logged in users with recommendations).”</p>
</blockquote>
</dd>
<dd>
<p>They say recommendations are good because they have high click-through rate. - <a href="https://blog.youtube/news-and-events/youtube-now-why-we-focus-on-watch-time/">A blog post from 2012</a> discusses a switch from views to watch time: <em>“Our video discovery features were previously designed to drive views. This rewarded videos that were successful at attracting clicks, rather than the videos that actually kept viewers engaged. (Cleavage thumbnails, anyone?)”</em></p>
</dd>
<dt><strong>Gomez-Uribe and Hunt (2015) <a href="https://dl.acm.org/doi/abs/10.1145/2843948">“The Netflix Recommender System: Algorithms, Business Value, and Innovation”</a></strong></dt>
<dd>
Clearly states that they evaluate AB tests using engagement, but it is regarded as an imperfect proxy for retention:
</dd>
<dd>
<blockquote class="blockquote">
<p><em>“we have observed that improving engagement—the time that our members spend viewing Netflix content—is strongly correlated with improving retention. Accordingly, we design randomized, controlled experiments … to compare the medium-term engagement with Netflix along with member cancellation rates across algorithm variants. Algorithms that improve these A/B test metrics are considered better.”</em></p>
</blockquote>
</dd>
</dl>
<p><strong>Covington et al.&nbsp;(2016) <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf">“Deep Neural Networks for YouTube Recommendations”</a></strong></p>
<p>This paper proposed a very influential architecture for content recommendation (the paper has 3000 citations). They say:</p>
<blockquote class="blockquote">
<p>“Our final ranking objective is constantly being tuned based on live A/B testing results but is generally a simple function of expected watch time per impression. Ranking by click-through rate often promotes deceptive videos that the user does not complete (“clickbait”) whereas watch time better captures engagement”</p>
</blockquote>
<p><strong>Lada, Wang, &amp; Yan (2021, FB Blog) <a href="https://tech.facebook.com/engineering/2021/1/news-feed-ranking/">How does news feed predict what you want to see?</a></strong></p>
<dl>
<dt><strong>Thorburn, Bengani, &amp; Stray (2022, Understanding Recommenders) <a href="https://medium.com/understanding-recommenders/how-platform-recommenders-work-15e260d9a15a">“How Platform Recommenders Work”</a></strong></dt>
<dd>
<p>This is an excellent short article with description and illustration of the stages in building a slate of content: moderation, candidate generation, ranking, and reranking. Includes links to many posts from platforms describing their systems.</p>
</dd>
<dd>
<p><img src="tecunningham.github.io/posts/images/2023-07-31-15-45-00.png" class="img-fluid"></p>
</dd>
<dd>
<p><img src="tecunningham.github.io/posts/images/2023-07-31-16-09-47.png" class="img-fluid"></p>
</dd>
<dt><strong>Arvin Narayanan (2023) <a href="https://knightcolumbia.org/content/understanding-social-media-recommendation-algorithms">“Understanding Social Media Recommendation Algorithms”</a></strong></dt>
<dd>
<p>A good overview of recommendation algorithms, with an in-depth discussion of Facebook’s MSI.</p>
</dd>
<dd>
<p>Criticisms of social media recommendation: (1) harm users because “implicit-feedback-based feeds cater to our basest impulses,” (2) harm creators because “engagement optimization … is a fickle overlord,” (3) harms society because “social media platforms are weakening institutions by undermining their quality standards and making them less trustworthy. While this has been widely observed in the case of news … my claim is that every other institution is being affected, even if not to the same degree.”</p>
</dd>
<dd>
<p>The technical part of the essay is excellent but I found some of the arguments about harm and social effects hard to follow.</p>
</dd>
</dl>
<p><strong><span class="citation" data-cites="kleinberg2022challenge">Kleinberg, Mullainathan, and Raghavan (2022)</span></strong></p>
</section>
<section id="proposals-for-change-chronological" class="level2">
<h2 class="anchored" data-anchor-id="proposals-for-change-chronological">Proposals for Change (chronological)</h2>
<dl>
<dt><strong>Andrew Mauboussin (2022, SurgeAI) <a href="https://www.surgehq.ai/blog/what-if-social-media-optimized-for-human-values">“Moving Beyond Engagement: Optimizing Facebook’s Algorithms for Human Values”</a></strong></dt>
<dd>
Says that the problem is <em>“the most engaging content is often the most toxic.”</em> They propose using human raters, e.g.&nbsp;ask people “did this post make you feel closer to your friends and family on a 1-5 scale?” They label a small set of FB posts as a proof of concept.
</dd>
<dt><strong>Bengani, Stray, &amp; Thorburn (2022,Medium) <a href="https://medium.com/understanding-recommenders/whats-right-and-what-s-wrong-with-optimizing-for-engagement-5abaac021851">“What’s Right and What’s Wrong with Optimizing for Engagement”</a></strong></dt>
<dd>
They define engagement as “a set of user behaviors, generated in the normal course of interaction with the platform, which are thought to correlate with value to the user, the platform, or other stakeholders.” Reviews evidence for good and bad effects of ranking by engagement.
</dd>
</dl>
<p><strong>Ovadya &amp; Thorburn (2023). <a href="https://doi.org/10.48550/arXiv.2301.09976">Bridging Systems: Open Problems for Countering Destructive Divisiveness across Ranking, Recommenders, and Governance</a></strong></p>
<dl>
<dt><strong>Stray, Iyer, Larrauri (2023) <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4429558">“The Algorithmic Management of Polarization and Violence on Social Media”</a></strong></dt>
<dd>
<ol type="1">
<li>Our overall goal should be to minimize “destructive conflict”.</li>
</ol>
</dd>
<dd>
<ol start="2" type="1">
<li>The major lever used has been content moderation: changing the visibility of content based on semantic criteria (e.g.&nbsp;downranking toxic, disallowing hate speech).</li>
</ol>
</dd>
<dd>
<ol start="3" type="1">
<li>However we should put relatively more work on system design, e.g.&nbsp;adding friction or changing the mechanics of sharing or engagement-based ranking. In part because there’s a robust correlation between content that causes destructive conflict and content that is engaging.</li>
</ol>
</dd>
</dl>
<p><strong><span class="citation" data-cites="milli2021optimizing">Milli, Belli, and Hardt (2021)</span> (2021) “From Optimizing Engagement to Measuring Value”</strong></p>
<dl>
<dt><strong>Milli, Pierson and Garg (2023) <a href="https://arxiv.org/abs/2305.17428">Choosing the Right Weights: Balancing Value, Strategy, and Noise in Recommender Systems</a></strong></dt>
<dd>
<p>I find the model a little hard to follow. </p>
</dd>
</dl>
<p></p>
<dl>
<dt><strong>Lubin &amp; Gilbert (2023) <a href="https://arxiv.org/abs/2306.07443">“Accountability Infrastructure: How to implement limits on platform optimization to protect population health”</a></strong></dt>
<dd>
<p>A very wide-ranging and loose discussion of issues related to ranking content. Makes an analogy with 19th century measures to control public health. I think the main proposal is that firms come up with metrics to measure their effect on social problems such as mental health, and regularly report on how they’re doing. They suggest requirements for platforms of different sizes:</p>
</dd>
<dd>
<table class="caption-top table">
<colgroup>
<col style="width: 2%">
<col style="width: 97%">
</colgroup>
<tbody>
<tr class="odd">
<td>1M+</td>
<td>Submitted plan for metrics and methods for evaluation of potential structural harms</td>
</tr>
<tr class="even">
<td>10M+</td>
<td>Consistent data collection on potential structural harms</td>
</tr>
<tr class="odd">
<td>50M+</td>
<td>Quarterly, enforceable assessments on product aggregate effects on structural harms, with breakouts for key subgroups</td>
</tr>
<tr class="even">
<td>100M+</td>
<td>Monthly, enforceable assessments on product aggregate effects as well as targeted assessments of specific product rollouts for any subproduct used by at least 50 million users, with breakouts for key subgroups</td>
</tr>
</tbody>
</table>
</dd>
</dl>
</section>
</section>
<section id="appendix-taxonomy-of-metrics" class="level1">
<h1>Appendix: Taxonomy of Metrics</h1>
<p>This is meant to be a parsimonious taxonomy of metrics used in a recommender. They are organized by the the types of entity they apply to. For simplicity I omit aggregations (e.g.&nbsp;a user’s like rate is just the average over likes over user-item pairs), and I omit predictions (e.g.&nbsp;an item’s pToxic is just the prediction of whether a paid rater would rate the item as toxic).</p>
<table class="caption-top table">
<colgroup>
<col style="width: 15%">
<col style="width: 24%">
<col style="width: 60%">
</colgroup>
<thead>
<tr class="header">
<th><strong>entity</strong></th>
<th><strong>type of metric</strong></th>
<th><strong>metric</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>user</td>
<td>activity</td>
<td>login (DAU/DAP)</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>time spent</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>evaluation</td>
<td>survey (“are you satisfied?”)</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>item</td>
<td>paid rater</td>
<td>policy-violating (“does this violate policy?”)</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td>quality evaluation (“does this fit quality defn?”)</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>objective features</td>
<td>recency</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>item contains author info</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td>item contains link</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>producer</td>
<td>objective features</td>
<td>off-platform popularity</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>graph statistics</td>
<td>network centrality</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>user-item</td>
<td>social interaction</td>
<td>like (heart/fav/emoji reaction)</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>comment (reply)</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td>reshare (retweet/forward)</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>downstream interactions</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>interest signal</td>
<td>linger (time spent watching)</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td>click (follow link, expand)</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>evaluation</td>
<td>star-rating</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>upvote (downvote)</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td>survey (“did you find this worth your time?”)</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>see-more</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td>dislike (see less)</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>other</td>
<td>report</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>dislike</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>user-producer</td>
<td>interest signal</td>
<td>follow (subscribe, friend)</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td>block</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="references" class="level1">




</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0">
<div id="ref-guess2023chronological" class="csl-entry">
Guess, Andrew M., Neil Malhotra, Jennifer Pan, Pablo Barberá, Hunt Allcott, Taylor Brown, Adriana Crespo-Tenorio, et al. 2023. <span>“How Do Social Media Feed Algorithms Affect Attitudes and Behavior in an Election Campaign?”</span> <em>Science</em> 381 (6656): 398–404. <a href="https://doi.org/10.1126/science.abp9364">https://doi.org/10.1126/science.abp9364</a>.
</div>
<div id="ref-huszar2022twitter" class="csl-entry">
Huszár, Ferenc, Sofia Ira Ktena, Conor O’Brien, Luca Belli, Andrew Schlaikjer, and Moritz Hardt. 2022. <span>“Algorithmic Amplification of Politics on Twitter.”</span> <em>Proceedings of the National Academy of Sciences</em> 119 (1): e2025334119. <a href="https://doi.org/10.1073/pnas.2025334119">https://doi.org/10.1073/pnas.2025334119</a>.
</div>
<div id="ref-kleinberg2022challenge" class="csl-entry">
Kleinberg, Jon, Sendhil Mullainathan, and Manish Raghavan. 2022. <span>“The Challenge of Understanding What Users Want: Inconsistent Preferences and Engagement Optimization.”</span> <a href="https://arxiv.org/abs/2202.11776">https://arxiv.org/abs/2202.11776</a>.
</div>
<div id="ref-milli2021optimizing" class="csl-entry">
Milli, Smitha, Luca Belli, and Moritz Hardt. 2021. <span>“From Optimizing Engagement to Measuring Value.”</span> In <em>Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency</em>, 714–22.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{cunningham2023,
  author = {Cunningham, Tom},
  title = {Ranking by {Engagement}},
  date = {2023-05-08},
  url = {tecunningham.github.io/posts/2023-04-28-ranking-by-engagement.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-cunningham2023" class="csl-entry quarto-appendix-citeas">
Cunningham, Tom. 2023. <span>“Ranking by Engagement.”</span> May 8,
2023. <a href="https://tecunningham.github.io/posts/2023-04-28-ranking-by-engagement.html">tecunningham.github.io/posts/2023-04-28-ranking-by-engagement.html</a>.
</div></div></section></div> ]]></description>
  <guid>tecunningham.github.io/posts/2023-04-28-ranking-by-engagement.html</guid>
  <pubDate>Mon, 08 May 2023 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Social Media Suspensions of Prominent Accounts</title>
  <dc:creator>Tom Cunningham</dc:creator>
  <link>tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data.html</link>
  <description><![CDATA[ 





<style>
    h1 {  border-bottom: 4px solid black;}
    h2 {  border-bottom: 1px solid #ccc;}
</style>
<p><strong>Tom Cunningham.</strong> (<a href="https://twitter.com/testingham"><span class="citation" data-cites="testingham">@testingham</span></a>) First version Jan 31 2023, data last updated April 2023, text updated July 2024.<sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Thanks to comments from Sahar Massachi, Katie Harbath, Nichole Sessego, David Harris, Theodora Skeadas, Eric Davis, Jimin Lee, and many others.</p></div></div><p><strong>This note describes the suspension practices of the major social media platforms.</strong> I have collected a dataset of around 200 suspensions of prominent people across 12 platforms between 2011 and early 2023, stored in a <a href="https://docs.google.com/spreadsheets/d/1-lch6CvywoV4iAz0Yb61UsC9U6pgXDyYI2qb8jN5wmI/edit#gid=0">google spreadsheet</a>. The chart below summarizes the full dataset:</p>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-1_cfd31c44aaedec9ef1a202f494e0608c">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="720"></p>
</figure>
</div>
</div>
</div>
<p><strong>The data helps illuminate what platforms are doing.</strong> It is very difficult for an outside observer to see how a platform moderates their content. The advantages of studying the suspension of prominent users are that (1) the data is public and (2) the outcomes are comparable across platforms.</p>
<p><strong>Key findings.</strong></p>
<ol type="1">
<li><p><strong>The rate of suspensions has grown over time.</strong> The increase seems to be primarily due to adoption of new policies rather than changes in user behaviour or changes in enforcement.</p></li>
<li><p><strong>Suspension practices are fairly similar across the major platforms.</strong> Meta, Twitter, and YouTube all have broadly similar policies: they each suspend users for hate speech, election misinformation, COVID misinformation, and incitement.</p></li>
<li><p><strong>The most common reasons for suspension were hate speech (15%) and COVID misinformation (12%).</strong> Platforms typically do not publicly state the reason why they suspend an account, however I was able to code the majority of cases either because the reason was clear from context, or the platform’s justification was relayed to the suspended user or a journalist. In 19% of cases I could find no reason given. For cases with a reason there were quite a wide variety: hate speech (15%), covid misinformation (12%), incitement (7%), and posting personal information (6%).</p></li>
<li><p><strong>Twitter suspended more people than other platforms.</strong> From examining cases it seems this was primarily due to differences in the type of content posted rather than differences in policies or in enforcement.</p></li>
<li><p><strong>US politicians were suspended at a much higher rate than non-US politicians.</strong> This was concentrated on Twitter and seems to be a mixture of US politicians being more active, being more likely to make policy-violating statements, and being under more scrutiny.</p></li>
<li><p><strong>Among US Federal politicians suspended, 8 were Republicans, none were Democrats.</strong> The Republicans were suspended for a variety of different reasons and on a variety of platforms. The asymmetry does not seem to me to be primarily a difference in enforcement but a higher propensity for Republicans to say or do policy-violating things.</p></li>
</ol>
<p><strong>I am working on a separate essay about <em>why</em> platforms suspend users.</strong> It is difficult to give clear reasons <em>why</em> platforms suspend users. In a separate essay I try to break down how much their action can be attributed to influence from owners, from employees, from users, from advertisers, or from governments. Having this dataset of suspensions is very useful to be able to make generalizations about platform behavior.</p>
<section id="dataset" class="level1">
<h1>Dataset</h1>
<p><strong>The figure below shows the entire dataset.</strong> Each line represents a suspension, with a given start and end date, colored according to the platform that implemented the suspension. I have highlighted the earliest suspension I observe: Courtney’s Love suspension from Twitter in 2011 for defamation. I have also highlighted all the suspensions of Donald Trump: in 2017 a rogue employee suspended him from Twitter, in January 2021 he was suspended from multiple platforms. Twitter and Meta both gave him temporary suspensions, then a number of platforms gave him indefinite suspensions. Twitter, Meta and YouTube all lifted their Trump suspensions in late 2022 or early 2023.</p>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-2_2d70775b84c329e29d4c56424a4dae31">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="720"></p>
</figure>
</div>
</div>
</div>
<p><strong>I am including only <em>newsworthy</em> suspensions.</strong> My basic criterion is to include any suspension which merits a mainstream news story. My process of collection has been collecting whatever caught my attention and supplementing that with data from Wikipedia, Ballotpedia, and Google searches (see Appendix for more discussion of sources). I believe that the data probably covers the majority of the newsworthy suspensions in the US since 2016, however for suspensions outside the US the fraction is surely lower. There is more detail on data sources in an Appendix. I have a list of other datasources for suspensions at the bottom of this document. Particular blind spots are (1) countries outside the US; (2) platforms like Twitch; (3) celebrities, especially adult performers who have high rates of being suspended, and who often have many followers.</p>
<p><strong>I have a rough database of platform policy changes.</strong> In an Appendix I have a rough history of platform policy changes, and references to other similar databases online. One thing that I hope to work on as followup is a database of policies that would allow apples-to-apples compare what content is banned across platforms and across time. I also include in the Appendix a table showing general trends of famous people being excluded from mainstream media (film, television, music).</p>
</section>
<section id="aggregate-patterns" class="level1 page-columns page-full">
<h1>Aggregate Patterns</h1>
<p><strong>The rate of suspensions has substantially increased over time.</strong> The data show a substantial increase in suspensions, from an average of 1-2 per year before mid-2017, to an average of 10-20 per year since then. The observed increase is probably partly due to growth in overall activity on the platforms and partly due to my observations being biased towards more-recent suspensions, but I think the primary cause is an expansion of the rules which would justify a ban, discussed further below.</p>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-3_8b5d72c69d8dedcf21564ae2ff8c03eb">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="720"></p>
</figure>
</div>
</div>
</div>
<p><strong>There are three prominent peaks in the history of suspensions:</strong></p>
<ul>
<li><strong>June 2020:</strong> a variety of American and British political personalities on the right were suspended for hate speech or similar reasons by Twitter and YouTube. On Twitter: Katie Hopkins, David Duke, American Renaissance (Jared Taylor). On YouTube: Stefan Molyneux, Gavin McInnes, American Renaissance (Jared Taylor), Richard Spencer, National Policy Institute.</li>
<li><strong>January 2021:</strong> many US politicians and political commentators were suspended after the capitol riot.</li>
<li><strong>November-December 2022:</strong> Twitter suspended many prominent figures for a variety of reasons under new policies introduced by Elon Musk.</li>
</ul>
<p><strong>The most suspensions are from Twitter, then Meta, then YouTube.</strong> We can see that Twitter suspension have been growing continuously since 2015, with a spike in 2021 due to the Capitol riot. Meta and YouTube also show an increase since 2017, but less-clear pattern of continuous growth.</p>
<p>In the Appendix I compare data from Twitter’s transparency reports which report <em>total</em> account suspensions on Twitter (i.e.&nbsp;suspensions of all accounts, not just prominent accounts) between 2018 and 2021. That data shows slower growth: suspensions for abuse grew by a factor of around 2, and suspensions for hateful conduct grew from 2018 to 2019 then decreased.</p>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-4_d8a14b6f7334069b8971d6412af5959a">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="720"></p>
</figure>
</div>
</div>
</div>
<p><strong>Suspension policies have become stricter.</strong> Several new policies have been added by platforms over the last 10 years which have tightened the limits on acceptable speech. Some examples (see Appendix for more details). Note that policies usually describe the type of content that will be removed, violations are only sometimes punished with an account suspension, and often platforms have a “strike” system for account suspension.</p>
<ol type="1">
<li>Misgendering/deadnaming: Twitter (2018), TikTok (2022).</li>
<li>Group superiority/inferiority: Meta (2019), YouTube (2019).</li>
<li>Holocaust denial: YouTube (2019), Twitter (2020), Meta (2020), Reddit (2020).</li>
<li>Harmful medical misinformation: Meta (2020), YouTube (2021).</li>
<li>False allegations of election fraud: Meta (2021), YouTube (2021).</li>
</ol>
<p>Broadly speaking it appears that the growth in suspension is attributable more to a change in policies than due to a change in behaviour or due to a change in application of policies. This is because it appears that a majority of suspensions over the last 5 years were taken under policies that did not exist prior to 2018.</p>
<p>Two important policies were introduced in response to external events: COVID misinformation and election deligitimization. Many accounts have been suspended under these new policies since 2020 but it is difficult to say whether this reflects a generally increased strictness of platforms.</p>
<p><strong>Suspension policies have rarely been relaxed.</strong> There are two substantial instances of relaxation of moderation policy I’m aware of:</p>
<ol type="1">
<li>A “newsworthiness” exemption for bans, i.e.&nbsp;effectively a relaxation of those bans. Newsworthiness exemption were officially introduced by Meta in 2016, Twitter in 2017, and Youtube in 2019.</li>
<li>Twitter’s relaxation of policies under Elon Musk in late-2022. Twitter unsuspended many prominent accounts, however they also suspended many accounts for new reasons, and there has been little official communication of new policies.</li>
</ol>
<p><strong>Reasons for suspension.</strong> Platforms typically do not publicly state the reason why they suspend an account, however I was able to code the majority of cases either because the reason was clear from context, or was reported by the user, or because the reason was given by the platform to a journalist. In 19% of cases I could find no reason given. For cases with a reason there were quite a wide variety:</p>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-5_ff7a82dba2e985426d66949f5f51e112">
<div class="cell-output-display">
<div id="czfldrgwuz" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#czfldrgwuz table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#czfldrgwuz thead, #czfldrgwuz tbody, #czfldrgwuz tfoot, #czfldrgwuz tr, #czfldrgwuz td, #czfldrgwuz th {
  border-style: none;
}

#czfldrgwuz p {
  margin: 0;
  padding: 0;
}

#czfldrgwuz .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#czfldrgwuz .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#czfldrgwuz .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#czfldrgwuz .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#czfldrgwuz .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#czfldrgwuz .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#czfldrgwuz .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#czfldrgwuz .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#czfldrgwuz .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#czfldrgwuz .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#czfldrgwuz .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#czfldrgwuz .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#czfldrgwuz .gt_spanner_row {
  border-bottom-style: hidden;
}

#czfldrgwuz .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#czfldrgwuz .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#czfldrgwuz .gt_from_md > :first-child {
  margin-top: 0;
}

#czfldrgwuz .gt_from_md > :last-child {
  margin-bottom: 0;
}

#czfldrgwuz .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#czfldrgwuz .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#czfldrgwuz .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#czfldrgwuz .gt_row_group_first td {
  border-top-width: 2px;
}

#czfldrgwuz .gt_row_group_first th {
  border-top-width: 2px;
}

#czfldrgwuz .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#czfldrgwuz .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#czfldrgwuz .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#czfldrgwuz .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#czfldrgwuz .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#czfldrgwuz .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#czfldrgwuz .gt_last_grand_summary_row_top {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#czfldrgwuz .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#czfldrgwuz .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#czfldrgwuz .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#czfldrgwuz .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#czfldrgwuz .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#czfldrgwuz .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#czfldrgwuz .gt_left {
  text-align: left;
}

#czfldrgwuz .gt_center {
  text-align: center;
}

#czfldrgwuz .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#czfldrgwuz .gt_font_normal {
  font-weight: normal;
}

#czfldrgwuz .gt_font_bold {
  font-weight: bold;
}

#czfldrgwuz .gt_font_italic {
  font-style: italic;
}

#czfldrgwuz .gt_super {
  font-size: 65%;
}

#czfldrgwuz .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#czfldrgwuz .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#czfldrgwuz .gt_indent_1 {
  text-indent: 5px;
}

#czfldrgwuz .gt_indent_2 {
  text-indent: 10px;
}

#czfldrgwuz .gt_indent_3 {
  text-indent: 15px;
}

#czfldrgwuz .gt_indent_4 {
  text-indent: 20px;
}

#czfldrgwuz .gt_indent_5 {
  text-indent: 25px;
}
</style>

<table class="gt_table caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-quarto-disable-processing="false" data-quarto-bootstrap="false">
<thead>
<tr class="header gt_col_headings">
<th id="reason" class="gt_col_heading gt_columns_bottom_border gt_left" data-quarto-table-cell-role="th" scope="col">reason</th>
<th id="n" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">n</th>
<th id="pct" class="gt_col_heading gt_columns_bottom_border gt_right" data-quarto-table-cell-role="th" scope="col">pct</th>
</tr>
</thead>
<tbody class="gt_table_body">
<tr class="odd">
<td class="gt_row gt_left" headers="reason">no reason given</td>
<td class="gt_row gt_right" headers="n">43</td>
<td class="gt_row gt_right" headers="pct">19</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="reason">other</td>
<td class="gt_row gt_right" headers="n">39</td>
<td class="gt_row gt_right" headers="pct">17</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="reason">hate speech</td>
<td class="gt_row gt_right" headers="n">36</td>
<td class="gt_row gt_right" headers="pct">16</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="reason">covid misinfo</td>
<td class="gt_row gt_right" headers="n">26</td>
<td class="gt_row gt_right" headers="pct">11</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="reason">incitement</td>
<td class="gt_row gt_right" headers="n">16</td>
<td class="gt_row gt_right" headers="pct">7</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="reason">personal information</td>
<td class="gt_row gt_right" headers="n">13</td>
<td class="gt_row gt_right" headers="pct">6</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="reason">election misinfo</td>
<td class="gt_row gt_right" headers="n">12</td>
<td class="gt_row gt_right" headers="pct">5</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="reason">hate group</td>
<td class="gt_row gt_right" headers="n">10</td>
<td class="gt_row gt_right" headers="pct">4</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="reason">manipulation</td>
<td class="gt_row gt_right" headers="n">10</td>
<td class="gt_row gt_right" headers="pct">4</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="reason">terrorist group</td>
<td class="gt_row gt_right" headers="n">9</td>
<td class="gt_row gt_right" headers="pct">4</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="reason">court order</td>
<td class="gt_row gt_right" headers="n">6</td>
<td class="gt_row gt_right" headers="pct">3</td>
</tr>
<tr class="even">
<td class="gt_row gt_left" headers="reason">threat</td>
<td class="gt_row gt_right" headers="n">5</td>
<td class="gt_row gt_right" headers="pct">2</td>
</tr>
<tr class="odd">
<td class="gt_row gt_left" headers="reason">accident</td>
<td class="gt_row gt_right" headers="n">4</td>
<td class="gt_row gt_right" headers="pct">2</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
<div class="cell page-columns page-full" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-6_9adbb12935db39ecf3ceef2e1985821b" data-cap-location="margin">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="720"></p>
<figcaption class="margin-caption">Reasons for suspension over time</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Suspension practices are fairly similar across the major platforms.</strong> We can see that Meta, Twitter, and YouTube all have broadly similar policies: they will suspend users for all of the following reasons:</p>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-7_026509960857f8ece1994dc66c0489f5">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid figure-img" width="720"></p>
</figure>
</div>
</div>
</div>
<p>It is clear that Twitter has suspended more people than Meta and YouTube, but the majority of suspensions fall in categories which are also enforced by Meta and YouTube.<sup>2</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn2"><p><sup>2</sup>&nbsp;One important exception is Dec 2022 spike in suspensions after Twitter adopted a new policy, suspending users for posting or alluding to already-public location information. This policy is notably stronger than Meta or YouTube.</p></div></div></section>
<section id="by-type-of-target" class="level1">
<h1>By Type of Target</h1>
<section id="politicians" class="level2">
<h2 class="anchored" data-anchor-id="politicians">Politicians</h2>
<p><strong>Among US Federal politicians only Republicans have been suspended.</strong> In the US 8 Republicans have had one or more suspension, but no Democrats. Among the Republicans the suspensions were for a variety of reasons: related to the Jan 6 riots (Trump, Barry Moore, MTG), related to COVID (Ron Johnson, Rand Paul, MTG), for misgendering (Jim Banks), for tweeting a threat (Briscoe Cain), for animal blood on a profile photo (Steve Daines), one by a rogue employee (Trump).</p>
<p>It seems to me that the asymmetry in suspensions is primarily due to Republicans being more likely to violate the policies, rather than asymmetric enforcement of existing policies. I am not aware of any cases where a Democratic politician violated one of these policies but was not suspended.</p>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-8_4b39fa579b0014b4e13751b72a03cb72">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid figure-img" width="720"></p>
</figure>
</div>
</div>
</div>
<p><strong>Suspension of national politicians outside the US has been relatively rare.</strong> My dataset contains 13 national politicians who were suspended in the world outside the US, compared to 8 in the US. This is a big asymmetry, and something of a puzzle. I have discussed this with a number of people who worked in enforcement and they attribute to a mixture of (1) less policy-violating behaviour from non-US politicians; (2) looser enforcement against non-US politicians; (3) lower overall social media usage outside the US; and (4) lower coverage of non-US politicians in my dataset.</p>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-9_82e4c14aec1e2b6dc315106cff761eb1">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img" width="720"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="us-prominent-figures" class="level2">
<h2 class="anchored" data-anchor-id="us-prominent-figures">US Prominent Figures</h2>
<p>This shows all suspensions of US “notable people”:</p>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-10_540d531ff21c87552c7eff94546ac03d">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="720"></p>
</figure>
</div>
</div>
</div>
<p><strong>Between 2015 and 2017 there were a series of alt-right personalities suspended from Twitter.</strong> The suspensions were often not for their views but their behaviour:</p>
<ul>
<li>2015: Charles Johnson from Twitter for a threat.</li>
<li>2016: Milo Yiannopoulos from Twitter for harassment, Richard Spencer from Twitter for manipulation.</li>
<li>2017: Roger Stone from Twitter for abuse.</li>
<li>2018: Alex Jones from Twitter for incitement and abuse.</li>
</ul>
<p><strong>Beginning in late 2017 more alt-right accounts were suspended.</strong> Either for hate speech, for offline behaviour, or without any public reason given:</p>
<ul>
<li>Late 2017: Baked Alaska from Twitter for hate speech.</li>
<li>2018: Owen Benjamin from Twit with no reason given, Alex Jones from FB and YouTube for hate speech.</li>
<li>2019: Nick Fuentes from Meta with no reason given.</li>
</ul>
<p><strong>Between November 2020 and January 2021 a large set of prominent figures were suspended for election-related reasons.</strong> The most suspensions were on Twitter but there were also from other platforms.</p>
<ol type="1">
<li>Since November 2022 Twitter has unsuspended a large fraction of the suspended users that I track, probably around 1/2.</li>
<li>Some people have been suspended simultaneously across multiple platforms (e.g.&nbsp;Alex Jones)</li>
</ol>
</section>
</section>
<section id="by-platform" class="level1">
<h1>By Platform</h1>
<section id="meta" class="level2">
<h2 class="anchored" data-anchor-id="meta">Meta</h2>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-11_eb80943af807be235d172b5d521a9003">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid figure-img" width="720"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="twitter" class="level2">
<h2 class="anchored" data-anchor-id="twitter">Twitter</h2>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-12_3e00afcd1ca8f01e55a23cb64471052f">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid figure-img" width="720"></p>
</figure>
</div>
</div>
</div>
<p>The following chart shows just accounts that were un-suspended under Musk, i.e.&nbsp;people with Twitter suspension that started before Oct 27 2022 and ended after that date. See below for a more fine-grained dataset of accounts unsuspended under Musk.</p>
<p>You can see that the primary original reasons for suspension were hate speech COVID misinformation. Kanye West and Nick Fuentes were re-suspended under Musk.</p>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-13_a872a3eba51490eddb3b2b48de691a94">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid figure-img" width="720"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="youtube" class="level2">
<h2 class="anchored" data-anchor-id="youtube">YouTube</h2>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-14_e58a255f4894d94b3aa8c1fe08645b71">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid figure-img" width="720"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="tik-tok" class="level2">
<h2 class="anchored" data-anchor-id="tik-tok">Tik Tok</h2>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-15_10a2c304a47181c259ef42df0c03ed44">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid figure-img" width="720"></p>
</figure>
</div>
</div>
</div>
</section>
<section id="other-platforms" class="level2">
<h2 class="anchored" data-anchor-id="other-platforms">Other Platforms</h2>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-16_3e7afd182db5d2908b5172fb0129ec8d">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid figure-img" width="720"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="by-policy" class="level1">
<h1>By Policy</h1>
<section id="hate-speech" class="level2">
<h2 class="anchored" data-anchor-id="hate-speech">Hate Speech</h2>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-17_a308a4f9cba6e8d6b0196bdeaa6f4d29">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid figure-img" width="720"></p>
</figure>
</div>
</div>
</div>
</section>
</section>
<section id="appendix-data-sources" class="level1 page-columns page-full">
<h1>Appendix: Data Sources</h1>
<p>In this Appendix I give a fairly lengthy discussion of what data there is available regarding suspensions on each platform, as well as data on other types of content moderation and data on the history of policy announcements by platforms.</p>
<section id="twitter-1" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="twitter-1">Twitter</h2>
<p><strong><a href="https://en.wikipedia.org/wiki/Twitter_suspensions">Wikipedia page on Twitter Suspensions.</a></strong> Wikipedia has a list of around 400 Twitter suspensions. I chose not to create my own database (partly drawing from Wikipedia) for a few reasons: (1) I would want to add a lot of annotations to the Wikipedia data, e.g.&nbsp;about reasons for suspension or types of suspension. (2) Parsing the data is nontrivial: date ranges are given in various formats and would require some work on a regex to parse consistently. (3) There is some missing and inconsistent data, e.g.&nbsp;it has Trump’s suspension start-date but not end-date, and the names of people are not consistent (e.g.&nbsp;sometimes “Donald Trump”, sometimes “Donald J Trump”).</p>
<p>The Wikipedia dataset shows a similar basic pattern to what I document above: a dramatic increase in the rate of suspensions around mid-2017</p>
<div class="cell page-columns page-full" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-18_1bf3413098221e085d48d034d037542c" data-cap-location="margin">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid figure-img" width="900"></p>
<figcaption class="margin-caption">Wikipedia-reported Twitter suspension by year</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell page-columns page-full" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-19_c5f74451b453bf7e848610f27526755f" data-cap-location="margin">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid figure-img" width="1080"></p>
<figcaption class="margin-caption">All Wikipedia-reported Twitter suspension, highlighting accounts with more than 1M followers (not all suspensions list the number of followers).</figcaption>
</figure>
</div>
</div>
</div>
<p><strong><a href="https://github.com/travisbrown/twitter-watch">Travis Brown: Twitter Watch</a></strong> This project appears to have data on almost all suspensions on Twitter since Feb 2022, and also tracks whether the suspension have been reversed. It does <em>not</em> include any suspensions which started prior to Feb 2022. There is a giant CSV file with 600K rows, <a href="https://github.com/travisbrown/twitter-watch/blob/main/data/suspensions.csv">suspensions.csv</a>. Some visualizations:</p>
<div class="cell page-columns page-full" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-21_9054861682c9376a5d55f46cee01c32b" data-cap-location="margin">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid figure-img" width="720"></p>
<figcaption class="margin-caption">Observations by date of suspension</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell page-columns page-full" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-22_e848dbe17f1db9aa7664f3a523f905f6" data-cap-location="margin">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-22-1.png" class="img-fluid figure-img" width="720"></p>
<figcaption class="margin-caption">Observation by date of unsuspension</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell page-columns page-full" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-23_42058d5236b1617eb5634b37d9e5d31b" data-cap-location="margin">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-23-1.png" class="img-fluid figure-img" width="720"></p>
<figcaption class="margin-caption">Observation by date of account creation</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell page-columns page-full" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-24_55268f72348608ea351ec913903bf6c5" data-cap-location="margin">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-24-1.png" class="img-fluid figure-img" width="720"></p>
<figcaption class="margin-caption">Suspensions for accounts with &gt;1M followers</figcaption>
</figure>
</div>
</div>
</div>
<p><strong><a href="https://github.com/travisbrown/unsuspensions">Travis Brown: Twitter Unsuspensions</a>.</strong> This is a collection of users who Twitter has un-suspended since Oct 27 2022 (when Musk took over). For some accounts there is a date of suspension but some have missing dates, I think suspension-date is only observed if after Feb 2022. (The content of this dataset is neither a subset nor a superset of the previous daatset). Unfortunately the dataset doesn’t have follower-count or twitter handle, so it’s not easy to join with other datasets or find the most prominent accounts.</p>
<div class="cell page-columns page-full" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-25_dd7d0641c81a59a2fbf91706339566ef" data-cap-location="margin">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-25-1.png" class="img-fluid figure-img" width="720"></p>
<figcaption class="margin-caption">Observations by date of suspension</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell page-columns page-full" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-26_02a275045ed7c275a3263524fdc975e0" data-cap-location="margin">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-26-1.png" class="img-fluid figure-img" width="720"></p>
<figcaption class="margin-caption">Observations by date of unsuspension</figcaption>
</figure>
</div>
</div>
</div>
<p><strong><a href="https://github.com/travisbrown/deleted-tweets/blob/main/data/suspended/README.md">Travis Brown: Deleted Tweets / Suspended Accounts</a>.</strong> This project scrapes profiles from the Wayback Machine, and seems to have a large set of accounts that were suspended with fairly long retention, I have not yet investigated further.</p>
<p><strong><a href="https://transparency.twitter.com/en/reports/rules-enforcement.html#2021-jul-dec">Twitter Transparency Reports</a>.</strong> This has data on the aggregate number of suspensions per half between July 2018 and Dec 2021. Note that the website is down but the CSV files can still be downloaded. </p>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-27_5fac0f31901c654db7f37a6c8624dcbb">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-27-1.png" class="img-fluid figure-img" width="720"></p>
<figcaption>Total Accounts Suspended on Twitter by Reason, 2018H2-2021H2</figcaption>
</figure>
</div>
</div>
</div>
<p><strong><a href="https://counterhate.com/research/toxic-twitter/">CounterHate list of unsuspensions</a>.</strong> The organization CounterHate has a list of 10 large accounts reinstated by Twitter since Musk’s takeover. Note I believe they incorrectly listed Rizza Islam as an account re-activated by Twitter: I can find no evidence that the acccount <span class="citation" data-cites="RizzaIslam">@RizzaIslam</span> was ever suspended, it seems to have been continuously tweeting from November 2022 through Feb 2023. I have added all 10 accounts to my database, and checked activity across all platforms.</p>
</section>
<section id="youtube-1" class="level2">
<h2 class="anchored" data-anchor-id="youtube-1">YouTube</h2>
<p><strong><a href="https://en.wikipedia.org/wiki/YouTube_suspensions">Wikipedia page on YouTube suspensions</a>.</strong> See above for reasons why I chose not to use this dataset as the primary source.</p>
<p><strong><a href="https://youtube.fandom.com/wiki/Terminated_YouTubers">Wikitubia: Terminated YouTubers</a>.</strong> A list of around 2300 YouTubers that have been permanently banned, including date of ban, subscribers, reason for ban, and citation. They don’t have a date when <em>unbanned</em>.</p>
</section>
<section id="meta-facebook-instagram" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="meta-facebook-instagram">Meta / Facebook / Instagram</h2>
<p><strong>There is no Wikipedia page of suspensions on Facebook, Instagram or WhatsApp.</strong></p>
<p><strong>Meta’s <a href="https://transparency.fb.com/data/community-standards-enforcement/">“Community Standards Enforcement Report”</a> is shown below.</strong> Meta’s data does not include any data on account suspensions, however there are a few other patterns of interest.</p>
<ol type="1">
<li><p><strong>Content actioned is relatively stable.</strong> There are fairly few notable upward or downward trends across the different types of content actioned: terrorism content actioned has increased significantly on both platforms, hate speech actions increased up to the end of 2020, then declined.</p></li>
<li><p><strong>The proactive detection rate is close to 100% for most categories.</strong> there were dramatic improvements for bullying and for hate speech over 2017-2021. Note that the proactive detection rate is the share of <em>actioned</em> content that is automatically detected, the share of <em>true</em> positives that are automatically detected is surely much lower.</p></li>
<li><p><strong>The prevalence of volations has fallen significantly.</strong> The log axis diminishes the magnitude of the decline: prevalence has fallen by a factor of 2-5 for nudity, bullying, hate speech, and graphic content. (I only show the prevalence upper bound, but the lower bound generally tracks the same course).</p></li>
</ol>
<div class="cell page-columns page-full" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-28_22f0e5bcb4c81b7617103fa0aa729687">
<div class="cell-output-display page-columns page-full">
<div class="page-columns page-full">
<figure class="figure page-columns page-full">
<p class="page-columns page-full"><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-28-1.png" class="img-fluid figure-img column-page" width="1440"></p>
</figure>
</div>
</div>
</div>
<p><strong><a href="https://theintercept.com/document/2021/10/12/facebook-dangerous-individuals-and-organizations-list-reproduced-snapshot/">Facebook’s dangerous organizations list</a>.</strong> This list was leaked in 2021 by the Intercept. Unfortunately it does not include the dates of when each organization was added. The list is organized into the following categories:</p>
<ul>
<li>Terror Organizations (e.g.&nbsp;Islamic State)</li>
<li>Crime Organizations (e.g.&nbsp;Bloods, Crips)</li>
<li>Hate Organizations (e.g.&nbsp;Aryan Nation, includes bands and websites)</li>
<li>Militarized Social Movements (e.g.&nbsp;United States Patrio Defense Force)</li>
<li>Violent Non-State Actors (e.g.&nbsp;Free Syrian Army)</li>
<li>Hate (e.g.&nbsp;David Duke)</li>
<li>Individuals: Crime (e.g.&nbsp;Denton Suggs, Gangster Disciples)</li>
<li>Individuals: Terror (e.g.&nbsp;Osama bin Laden)</li>
</ul>
</section>
<section id="tiktok" class="level2">
<h2 class="anchored" data-anchor-id="tiktok">TikTok</h2>
<p><strong><a href="https://www.tiktok.com/transparency/en/community-guidelines-enforcement-2022-3/">Community Standards Report</a>.</strong> Shows an increase in suspensions from around 1M accounts/quarter per 2020 to 6M accounts/quarter in 2023.</p>
<p><img src="tecunningham.github.io/posts/images/2023-02-27-16-29-44.png" class="img-fluid"></p>
</section>
<section id="twitch" class="level2">
<h2 class="anchored" data-anchor-id="twitch">Twitch</h2>
<p><strong><a href="https://streamerbans.com">StreamerBans</a>.</strong> They seem to have a pretty comprehensive database of bans on Twitch.</p>
</section>
<section id="other-platforms-1" class="level2">
<h2 class="anchored" data-anchor-id="other-platforms-1">Other Platforms</h2>
<ul>
<li><p><strong>Spotify.</strong> The only unambiguous suspension from Spotify I found was Alex Jones’ podcast. Spotify removed some episodes of Joe Rogan’s podcast, and <a href="https://www.nytimes.com/2018/05/10/arts/music/rkelly-spotify-accusations-xxxtentacion.html">removed</a> R Kelly and XXXtentacion’s music from playlists. They <a href="https://www.thefader.com/2022/09/27/new-study-finds-spotify-slow-to-take-down-white-supremacist-music">remove</a> some white-supremacist artists and music. They <a href="https://themusicnetwork.com/spotify-took-the-moral-high-ground-on-r-kelly-xxxtentacion-dont-be-shocked-this-has-happened-before/">removed</a> all music from the band LostProphets after their lead singer was convicted of child sexual abuse.</p></li>
<li><p><strong>Substack.</strong> I’m not aware of anybody who’s been kicked off Substack, they present themselves as very pro-free-speech.</p></li>
<li><p><strong>Reddit.</strong> I’m not aware of any data on reddit account suspensions.</p></li>
<li><p><strong>Rumble.</strong> The Rumble video-hosting platform has become quite large (they claim 70M MAU, and have a market cap of ). Their <a href="https://rumble.com/s/terms">terms of service</a> restrict content that is “abusive, inciting violence, harassing, harmful, hateful, anti-semitic, racist or threatening.” However I have not yet found a single example of a prominent user who has been suspended from Rumble.</p></li>
</ul>
</section>
<section id="other-data-sources" class="level2">
<h2 class="anchored" data-anchor-id="other-data-sources">Other Data Sources</h2>
<p><strong>Can find more suspensions by searching Wikipedia for “suspended from XXX”.</strong> E.g. <code>site:wikipedia.org "suspended from facebook"</code>. Possibly worth doing the same search for Google News.</p>
<p><strong><a href="https://socialblade.com/twitter/user/dbongino/monthly">SocialBlade</a></strong> has data on number of followers by month since 2018, across Twitter, FB, YouTube. I’m not sure how easy it would be to scrape this data. They have a paid API, they say “up to 3 years of Historical statistics on creators.” However the website seems to have data back to at least April 2018.</p>
<p><strong><a href="https://ballotpedia.org/Elected_officials_suspended_or_banned_from_social_media_platforms">Ballotpedia list of elected officials suspended from social media.</a></strong> It is an excellent resource, appears comprehensive and cites original reporting. I have added all of their data to the database as of January 2023.</p>
<p><strong>Global Internet Forum to Counter Terrorism (GIFCT).</strong> They mainly work on sharing hashes of terrorist content between platforms. They have some dicussion papers about “terror designation lists” but I don’t think they maintain any lists themselves.</p>
<p><strong><a href="https://en.wikipedia.org/wiki/Specially_Designated_Global_Terrorist">Specially Designated National / Global Terrorist (SDN/SDGT)</a>.</strong> This is a public list maintained by the US government, and consumed by a number of tech companies. The <a href="https://home.treasury.gov/policy-issues/financial-sanctions/specially-designated-nationals-and-blocked-persons-list-sdn-human-readable-lists">full history</a> is available, but it would be extremely difficult to parse.</p>
<p><strong><a href="https://www.lumendatabase.org/">Lumen</a>.</strong> This has an international database of government takedown requests. They also seem to include whether the request was honored.</p>
<p><strong>CCDH Disinformation Dozen.</strong> This is a list from March 2021 of prominent accounts who were spreading anti-vax information on social media: <a href="https://counterhate.com/research/the-disinformation-dozen/">original report</a>, <a href="https://f4d9b9d3-3d32-4f3a-afa6-49f8bf05279a.usrfiles.com/ugd/f4d9b9_7a52dea3ce19442a92a0aa62ece7c045.pdf">followup report from April 2021</a>). They also have a <a href="https://counterhate.com/research/the-toxic-ten/">“toxic ten”</a> report. It’s probably worth adding both lists to the database.</p>
</section>
</section>
<section id="appendix-codebook" class="level1">
<h1>Appendix: Codebook</h1>
<p><strong>What suspensions to include.</strong> I aim to include any suspension which merits a national news story. In the future it would be nice to also include any suspensions of accounts with more than a certain number of followers.</p>
<p><strong>Definition of a suspension.</strong> It is a suspension if any of the following are true:</p>
<ul>
<li>Your existing content is available but you cannot post new content.</li>
<li>Users cannot see your existing content anywhere on the site.</li>
<li>Users cannot post links to your domain (e.g.&nbsp;cannot link to <code>piratebay.se</code>).</li>
<li>Users cannot search for your hashtag (this applies to social movements, e.g.&nbsp;<code>#StopTheSteal</code>).</li>
<li>Your content is blocked in the region where the majority of your followers are.</li>
</ul>
<p>On Twitter users often can end their suspension by deleting a specific tweet, I count these as suspensions. The following do not count as suspensions:</p>
<ul>
<li>Your content is demonetized but still visible.</li>
<li>Your content is removed from some surfaces but still available on your profile.</li>
<li>Your content is blocked in a region where a minority of your followers are.</li>
<li>Your content is blocked only for a secondary type of sharing (e.g.&nbsp;when Instagram <a href="https://www.thethings.com/why-madonna-banned-from-instagram-live-controversial-photos/">disabled</a> Madonna’s ability to stream to Instagram Live, but not to post regular photos; e.g.&nbsp;in 2010 50 cent was <a href="https://metro.co.uk/2010/08/30/50-cent-in-twitter-war-after-x-rated-photos-see-him-banned-3436802/">suspended from Twitpic</a> but not Twitter).</li>
</ul>
<p><strong>Coding of platform.</strong> I categorize a suspension from either FB or Instagram as a suspension from “Meta”. There are some cases where a person has been suspended from one but not the other, in those cases I leave an annotation in the spreadsheet.</p>
<p><strong>Coding of reason for suspension.</strong> Most platforms do not give public statements on the reason for a suspension, however it appears that they commonly give information to journalists which is then reflected in news stories. When there is some quotation I include it in the “long reason.” The reasons are as stated by platform, I am obviously not endorsing their judgments of whether they are making correct determinations of whether the policy applies, or if the stated reason is the true reason. I have tried to compress the “short” reasons into a small number of distinct reasons using my judgment. In some cases there is no company statement but the reason is obvious (e.g.&nbsp;I coded Kanye West’s “I’m going death con 3 on jewish people” statement as “hate speech”).</p>
<p><strong>We record suspensions of <em>entities</em> not <em>accounts</em>.</strong> I try to record the suspension of the <em>person</em> or <em>group</em> rather than the account. E.g. Courtney Love’s original Twitter account (<a href="https://twitter.com/CourtneyLoveUK"><span class="citation" data-cites="CourtneyLoveUK">@CourtneyLoveUK</span></a>) was suspended in 2011 and is still suspended, but Courtney Love seems to have been using another account since at least April 2012, without any suspensions for ban-evasion, and so I count the end-date as April 2012.</p>
</section>
<section id="appendix-history-of-policy-changes" class="level1">
<h1>Appendix: History of Policy Changes</h1>
<p><strong>Here we collect platform statements of policy changes.</strong> I try to restrict to explicit statements of changes regarding what is banned or not. For each statement I give a summary, if a single statement contains multiple significant changes I list it multiple times. The list is very incomplete, I would guess it currently contains only around 1/2 of the relevant policy changes from the large platforms. Additionally there are probably many significant policy changes that are never explicitly announced.</p>
<ul>
<li><strong>Meta.</strong>
<ul>
<li>2013-05-01: <a href="https://www.theguardian.com/technology/2013/oct/23/facebook-removes-beheading-video">ban graphic violence (beheading)</a></li>
<li>2013-10-23: <a href="https://www.theguardian.com/technology/2013/oct/23/facebook-removes-beheading-video">ban graphic violence (beheading) (again)</a>: <em>“Facebook had introduced a temporary ban on such videos in May but later decided to remove the block on the grounds that the site is used to share information about world events.”</em> Then ban was reintroduced after outcry.</li>
<li>2014-06-13: <a href="https://time.com/2869849/facebook-breastfeeding-nipples/">exemption for nipples if breastfeeding</a></li>
<li>2015-12-01: <a href="https://www.washingtonpost.com/technology/2020/06/28/facebook-zuckerberg-trump-hate/">exemption for newsworthy (not announced, applied to Trump video)</a></li>
<li>2016-09-01: <a href="https://www.facebook.com/josofsky/posts/10157347245570231">exemption for nudity if historically important (“terror of war”/napalm girl)</a></li>
<li>2016-10-21: <a href="https://about.fb.com/news/2016/10/input-from-community-and-partners-on-our-community-standards/">exemption for newsworthy (publicly announced)</a></li>
<li>2019-03-27: <a href="https://about.fb.com/news/2019/03/standing-against-hate/">ban on white separatism</a> <em>“praise, support and representation of white nationalism and white separatism”</em></li>
<li>2019-09-24: <a href="https://about.fb.com/news/2019/09/elections-and-political-speech/">exemption for speech from politicians</a>, <em>“we will treat speech from politicians as newsworthy content that should, as a general rule, be seen and heard.”</em></li>
<li>2019-12-19: <a href="https://reclaimthenet.org/facebook-bans-denying-existence-gender-identity">ban on “denying existence” of someone’s gender identity</a>. The statement of the policy is not super clear but I believe this bans general statements of the form “trans women are men,” but it does not ban individual misgendering or deadnaming.</li>
<li>2020-06-04: <a href="https://about.fb.com/news/2021/06/facebook-response-to-oversight-board-recommendations-trump/">revoke exemption on speech from politicians (discretionary)</a> <em>“we will not treat content posted by politicians any differently from content posted by anyone else”</em></li>
<li>2020-08-27: <a href="https://transparency.fb.com/policies/community-standards/dangerous-individuals-organizations/">ban on support for hate events, mass murders or attempted mass murders, serial murders</a>. Replaces ban on support for “mass shooting”. This was two days after the Rittenhouse shooting on August 25.</li>
<li>2020-10-12: <a href="https://about.fb.com/news/2020/10/removing-holocaust-denial-content/">ban on holocaust denial</a>: “prohibit any content that denies or distorts the Holocaust.”</li>
<li>2020-10-26: <a href="https://www.facebook.com/business/news/facebook-ads-restriction-2020-us-election">temporary ban on political ads in US</a></li>
<li>2020-12-03: <a href="https://about.fb.com/news/2020/12/coronavirus/">ban on covid-19 vaccine misinfo</a> <em>“claims about these vaccines that have been debunked by public health experts”</em></li>
<li>2021-02-08: <a href="https://about.fb.com/news/2020/04/covid-19-misinfo-update/#removing-more-false-claims">ban on covid-19 misinfo</a> <em>“COVID-19 is man-made or manufactured”</em></li>
<li>2021-02-08: <a href="https://about.fb.com/news/2020/04/covid-19-misinfo-update/#removing-more-false-claims">ban on vaccine misinfo</a> <em>“claims such as: Vaccines are not effective at preventing the disease … It’s safer to get the disease than to get the vaccine … Vaccines are toxic, dangerous or cause autism.”</em></li>
<li>2021-05-26: <a href="https://about.fb.com/news/2020/04/covid-19-misinfo-update/#removing-more-false-claims">lift ban on covid-19 man-made claim</a> <em>“we will no longer remove the claim that COVID-19 is man-made or manufactured”</em></li>
<li>2021-06-23: <a href="https://www.brennancenter.org/our-work/analysis-opinion/facebooks-new-dangerous-individuals-and-organizations-policy-brings-more">three tiers of dangerous organizations</a></li>
</ul></li>
<li><strong>YouTube</strong>
<ul>
<li>2019-06-05: <a href="https://blog.youtube/news-and-events/our-ongoing-work-to-tackle-hate">ban on group superiority, holocaust denial, &amp; massacre denial</a>: <em>“prohibiting videos alleging that a group is superior in order to justify discrimination, segregation or exclusion based on qualities like age, gender, race, caste, religion, sexual orientation or veteran status. … we will remove content denying that well-documented violent events, like the Holocaust or the shooting at Sandy Hook Elementary, took place.”</em></li>
<li>2019-09-26: <a href="https://www.vox.com/recode/2019/9/26/20885783/facebook-twitter-youtube-policies-political-content">exemption for newsworthy</a> <em>“exemption for content with”educational, documentary/news, scientific or artistic value”</em></li>
<li>2020-09-25: <a href="https://www.npr.org/2020/09/25/916957090/google-to-halt-political-ads-after-polls-close-amid-worries-over-delayed-results">temporary ban on political ads after election</a></li>
<li>2020-10-15: <a href="https://blog.youtube/news-and-events/harmful-conspiracy-theories-youtube">ban on conspiracy theories - Qanon and pizzagate</a><em>“prohibit content that targets an individual or group with conspiracy theories that have been used to justify real-world violence.”</em></li>
<li>2021-01-07: <a href="https://twitter.com/YouTubeInsider/status/1347231471212371970">ban on false election claims</a> – accounts will get a strike if they make false election claim.</li>
<li>2021-09-19: <a href="https://blog.youtube/news-and-events/managing-harmful-vaccine-content-youtube/">ban on vaccine misinformation</a> <em>“content that falsely alleges that approved vaccines are dangerous and cause chronic health effects, claims that vaccines do not reduce transmission or contraction of disease, or contains misinformation on the substances contained in vaccines”</em></li>
</ul></li>
<li><strong>Twitter:</strong>
<ul>
<li>2009-01-18: <a href="https://web.archive.org/web/20090118211301/http://twitter.zendesk.com/forums/26257/entries/18311">first rules</a> – bans impersonation, private information, threats of violence, copyright infringement, spam, pornography in profile picture.</li>
<li>2013-08-07: <a href="https://www.npr.org/sections/alltechconsidered/2013/08/07/209602106/as-twitter-expands-reach-abuse-policy-gets-added-scrutiny">ban on targeted abuse</a>: <em>“Unlike Facebook, Twitter doesn’t ban bullying … targeted abuse and real threats of violence are verboten, but the page is kind of buried. So on Saturday, Twitter amended its rules, adding language on targeted abuse and harassment more prominently.”</em>. <a href="https://www.vice.com/en/article/z43xw3/the-history-of-twitters-rules">Vice article</a> says this was in response to abuse of UK women over the new pound notes.</li>
<li>2015-01-01: <a href="https://www.vice.com/en/article/z43xw3/the-history-of-twitters-rules">ban on “excessively violent media”</a></li>
<li>2015-04-01: <a href="https://www.vice.com/en/article/z43xw3/the-history-of-twitters-rules">ban on “threatening or promoting terrorism”</a></li>
<li>2015-04-01: <a href="https://www.vice.com/en/article/z43xw3/the-history-of-twitters-rules">ban on “promot[ing] violence against others… on the basis of race, ethnicity, national origin, religion, sexual orientation, gender, gender identity, age, or disability.”</a>. <em>“Twitter spokesperson …[said] the company does not prohibit hate speech.”‘Hateful conduct’ differs from ‘hate speech’ in that the latter focuses on words. It’s the incitement to violence that we’re prohibiting. Offensive content and controversial viewpoints are still permitted on Twitter.”</em></li>
<li>2015-08-02: <a href="https://web.archive.org/web/20150802125743/https://support.twitter.com/articles/20169997">ban on indirect threats of violence</a>. Vice says <em>“the original rules set out in 2009, which had explicitly limited the prohibition on threats to”direct” and “specific” threats.”</em> They say <em>“[prohibited] .. also the”incitement” of harassment – speech that wasn’t a threat per se, but was intended to result in threats regardless.”</em></li>
<li>2015-03-12: <a href="https://www.wired.com/2015/03/twitter-bans-revenge-porn/">ban on revenge porn</a>.</li>
<li>2017-09-01: <a href="https://www.washingtonpost.com/news/the-switch/wp/2017/09/26/twitter-explains-why-it-wont-take-down-trumps-north-korea-tweet/">newsworthiness exemption</a></li>
<li>2019-10-30: <a href="https://www.nytimes.com/2019/10/30/technology/twitter-political-ads-ban.html">ban on political ads</a></li>
<li>2020-10-14: <a href="https://www.bloomberg.com/news/articles/2020-10-14/twitter-like-facebook-to-remove-posts-denying-the-holocaust">ban on holocaust denial</a></li>
<li>2022-11-29: <a href="https://www.cnn.com/2022/12/19/tech/twitter-elon-musk-deletes-policy/index.html">ban sharing links to other platforms (lasted 1 day)</a></li>
<li>2022-12-14: <a href="https://twitter.com/TwitterSafety/status/1603165959669354496">ban sharing live location</a></li>
<li>2022-12-16: <a href="https://twitter.com/TwitterSafety/status/1619125418371723264">general reinstatement of suspended accounts</a> <em>“We did not reinstate accounts that engaged in illegal activity, threats of harm or violence, large-scale spam and platform manipulation, or when there was no recent appeal to have the account reinstated.”</em></li>
<li>2023-01-04: <a href="https://www.euronews.com/next/2023/01/04/twitter-advertising">lifts ban on political ads</a></li>
<li>2023-02-28: <a href="https://www.engadget.com/twitter-updates-violent-speech-policy-to-ban-wishes-of-harm-214320985.html">ban on wishes of harm</a> <em>“ban on violent threats, wishes of harm, glorification of violence, and incitement of violence”</em></li>
<li>2023-04-08: <a href="https://www.theverge.com/2023/4/18/23688192/twitter-harrass-transgender-users-policy">lifts ban on misgendering/deadnaming</a> removed <em>“targeted misgendering or deadnaming of transgender individuals.”</em>.</li>
</ul></li>
<li><strong>Tik Tok.</strong>
<ul>
<li>2019-10-02: <a href="https://newsroom.tiktok.com/en-us/understanding-our-policies-around-paid-ads">ban on political ads</a></li>
<li>2020-01-08: <a href="https://www.wired.com/story/tiktok-overhauls-community-guidelines/">ban on promoting discrimination</a>, “promoting or justifying exclusion, segregation, or discrimination”</li>
<li>2020-01-08: <a href="https://www.wired.com/story/tiktok-overhauls-community-guidelines/">ban on showing guns</a>, “the depiction, trade, or promotion of firearms,” with some exceptions.</li>
<li>2020-01-08: <a href="https://www.mediamatters.org/tiktok/tiktok-full-boogaloo-videos-even-though-it-prohibits-content-dangerous-individuals-and">ban on dangerous individuals and organizations</a></li>
<li>2020-01-08: <a href="https://www.mediamatters.org/fake-news/hoax-about-national-coronavirus-quarantine-spread-tiktok-despite-platforms-anti">ban on harmful misinformation</a> “misinformation that could cause harm to an individual’s health or wider public safety.”</li>
<li>2020-10-20: <a href="https://newsroom.tiktok.com/en-gb/countering-hate-on-tiktok-gb">ban on white nationalism, anti-semitic ideas, conversion therapy</a> <em>“white nationalism, white genocide theory, as well as statements that have their origin in these ideologies, and movements such as Identitarianism and male supremacy … misinformation about notable Jewish individuals and families who are used as proxies to spread antisemitism. We’re also removing content that is hurtful to the LGBTQ+ community by removing hateful ideas, including content that promotes conversion therapy and the idea that no one is born LGBTQ+.”</em></li>
<li>2022-02-09: <a href="https://www.npr.org/2022/02/09/1079643611/tiktok-bans-deadnaming-misgendering">ban on misgendering and deadnaming</a></li>
<li>2022-09-21: <a href="https://newsroom.tiktok.com/en-us/updating-our-policies-for-political-accounts">ban on fundraising by politicians</a></li>
</ul></li>
<li><strong>Reddit.</strong>
<ul>
<li>2015-07-16: <a href="https://www.buzzfeednews.com/article/charliewarzel/nothing-changes-at-reddit#.rdaWGZrbp">2015-07, new Reddit policy</a></li>
<li>2015-02-24: <a href="https://arstechnica.com/tech-policy/2015/02/reddit-bans-nude-images-posted-without-consent/">ban on non-consensual nudity</a></li>
<li>2015-08-05: <a href="https://boingboing.net/2015/08/05/reddits-new-content-policy-g.html">new policy</a>: removed “/r/CoonTown, /r/WatchNiggersDie, /r/bestofcoontown, /r/koontown, /r/CoonTownMods, /r/CoonTownMeta”.</li>
<li>2020-06-20: <a href="https://www.reddit.com/r/announcements/comments/hi3oht/update_to_our_content_policy/">updated hate speech policy</a>. <em>“communities and users that promote hate based on identity or vulnerability will be banned.”</em> They don’t explicitly mention holocaust denial in the post, but reporting indicates that they’re interpreting this to mean that holocaust denial is banned.</li>
</ul></li>
<li><strong>4chan.</strong>
<ul>
<li>2013-09-19: <a href="https://gigaom.com/2013/09/19/4chan-has-rules-now-apparently/">ban on doxxing</a></li>
<li>2014-09-03: <a href="https://arstechnica.com/tech-policy/2014/09/4chan-adopts-dmca-policy-after-nude-celebrity-photo-postings/">ban on DMCA/copyrighted content</a></li>
</ul></li>
</ul>
<section id="third-party-sources-on-platform-policies" class="level2">
<h2 class="anchored" data-anchor-id="third-party-sources-on-platform-policies">Third-Party Sources on Platform Policies</h2>
<p>There are a variety of third-party resources comparing policies across platforms, however none seem to have data comparable to the list above, i.e.&nbsp;a summary of specific content policy changes over time.</p>
<p><strong>Comparisons at a single point in time.</strong></p>
<ul>
<li><p>DNC (2020) <strong><a href="https://democrats.org/who-we-are/what-we-do/disinfo/comparative-social-media-policy-analysis/">Comparison of Misinformation Policies, 2020</a></strong></p></li>
<li><p>Consumer Reports (Aug 13 2020) <strong><a href="https://www.consumerreports.org/social-media/social-media-misinformation-policies/">Comparison of Misinformation Policies, 2020</a></strong>. Data as of 2020, with three levels: “allowed”, “sometimes”, and “prohibited”.</p></li>
<li><p>UNC CITAP (May 22 2020) <strong><a href="https://citapdigitalpolitics.com/?page_id=2508">Comparison of Misinformation Policies, 2020</a></strong>. Has tables comparing misinfo policies as of 2020, three levels: “prohibited”, “flagged”, “allowed.”</p></li>
<li><p>Election Integrity Partnership (Oct 28 2020) <strong><a href="https://www.eipartnership.net/policy-analysis/platform-policies">Comparison of Election Policies, 2020</a></strong>.</p></li>
<li><p>Carnegie Endowment (April 1 2021) <strong><a href="https://carnegieendowment.org/2021/04/01/how-social-media-platforms-community-standards-address-influence-operations-pub-84201">Existence of Policies, 2021</a></strong>. Table just marks <em>whether</em> a platform has a policy on some type of content, not nature of policy. They also they have a database of platform policies but it seems to only have data from February 2021.</p></li>
<li><p>Virality Project <strong><a href="https://www.viralityproject.org/policy-analysis/evaluating-covid-19-vaccine-policies-on-social-media-platforms">Comparison of COVID Vaccine Policies in 2021</a>.</strong></p></li>
</ul>
<p><strong>Policies tracked over time.</strong></p>
<ul>
<li><p>Mchangama, Fanlo and Alkiviadou (2023) <strong><a href="https://futurefreespeech.com/wp-content/uploads/2023/07/Community-Guidelines-Report_Latest-Version_Formated-002.pdf">Scope Creep: An Assessment of 8 Social Media Platforms’ Hate Speech Policies</a></strong>. They document the hate speech policies over time for 8 platforms using a consistent rubric. They document that hate speech policies have become more broad-reaching over time. The data is available in Excel sheets <a href="https://futurefreespeech.com/scope-creep/">here</a>.</p></li>
<li><p>Katie Harbath and Collier Fernenkes (August 2022) <strong><a href="https://bipartisanpolicy.org/blog/tech-platforms-election-database/">Election Policy Announcements, 2003-2022</a></strong>. Google spreadsheet with links to around 600 policy announcements, organized by platform, author, date, product-type, and country. Focussed on election-related policies, and they don’t include summaries of the policy announcement. They also wrote up analyses: (1) <a href="https://bipartisanpolicy.org/report/history-tech-elections/">“A Brief History of Tech and Elections”</a>; (2) <a href="https://bipartisanpolicy.org/blog/tech-midterm-election-announcements/">2022 election announcements</a>.</p></li>
<li><p>Ranking Digital Rights Index, <strong><a href="https://rankingdigitalrights.org/bts22/explore-indicators">Comparison of Privacy and Transparency Policies, 2017-2022</a></strong>. They collect perhaps 100 different indicators across around 15 tech companies, mostly related to privacy and transparency, earliest data from 2017. All the data is available.</p></li>
<li><p>GLAAD <strong><a href="https://sites.google.com/glaad.org/smsi/platform-scores?authuser=0">Comparison of LGBTQ user safety, 2021-2022</a></strong></p></li>
<li><p>CELE, <strong><a href="https://letrachica.digital">Letra Chica</a>.</strong> Tracks all public policy changes on Meta, YouTube, and Twitter. Most data from May 2020, but they go back to 2019 for Facebook by using FB’s Transparency Center. Each policy update includes a short summary of what’s changed. Tracks both Spanish and English versions. Data stored on coda.io, I think it’s queryable.</p></li>
<li><p>Linterna Verdes, <strong><a href="https://www.circuito.digital">Circuito</a></strong>. Has about 15 in-depth case studies of platform moderation decisions.</p></li>
<li><p>Humboldt Institute, <strong><a href="https://pga.hiig.de">Platform Governance Archive</a>.</strong> Comprehensive archive of ToS, Privacy Policy, and Community Guidlines, from 2004 until late 2021, for FB, IG, Twitter, and YouTube. The data will not be updated.</p></li>
<li><p><strong><a href="https://opentermsarchive.org">Open Terms Archive</a></strong>. Started by the French Ambassador for Digital Affairs, but now a collaboration. Tracks terms for many different online services in a <a href="https://github.com/OpenTermsArchive/">github repo</a>. The Platform Governance Archive has moved to be part of this project, <a href="https://github.com/OpenTermsArchive/pga-snapshots">here</a>.</p></li>
<li><p>EFF, <strong><a href="https://tosback.org">TOSback</a></strong>. Database of historical ToS documents from different services, with cross-platform comparisons. The most recent updates seem to be from May 2021, possibly was succeeded by Open Terms Archive.</p></li>
<li><p>European Commission, <strong><a href="https://zenodo.org/record/6461568#.ZD7KCi-21pS">Copyright Content Moderation and Removal</a>.</strong> This PDF report includes a lot of work which maps the copyright policies of major platforms.</p></li>
</ul>
<p><strong>Narrative histories:</strong></p>
<ul>
<li>Catherine Buni and Soraya Chemaly (2016, the Verge) <strong><a href="https://www.theverge.com/2016/4/13/11387934/internet-moderator-history-youtube-facebook-reddit-censorship-free-speech">History of Moderation</a></strong>.</li>
<li>Sarah Jeong (2016, Vice) <strong><a href="https://www.vice.com/en/article/z43xw3/the-history-of-twitters-rules">The History of Twitter’s Rules</a></strong></li>
<li>Bergen (2022) <strong><a href="https://www.penguinrandomhouse.com/books/653248/like-comment-subscribe-by-mark-bergen/">Like, Comment, Subscribe</a>.</strong> A book on the history of YouTube, it has a lot of detail on policy changes.</li>
</ul>
</section>
</section>
<section id="appendix-exclusion-of-prominent-people-from-mass-media" class="level1 page-columns page-full">
<h1>Appendix: Exclusion of Prominent People from Mass Media</h1>
<p><strong>This is a very crude history of people who’ve been “excluded” from mass media in the US.</strong></p>
<p><strong>Definition of exclusion:</strong> any impediment to work in entertainment due to your behaviour, independent of the quality of your work, either through explicit blacklisting or general sentiment. I have not taken the time to include references. In some cases there are conflicting claims about whether there was an exclusion, I generally take the person’s own judgment at face value.</p>
<p><strong>More important than exclusions of individuals were exclusions of entire classes.</strong> For much of the 20th century african americans, asians, women, &amp; openly gay people, had a pronounced difficulty getting roles in entertainment. This would be useful to document quantitatively but I think it would be more work.<sup>3</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn3"><p><sup>3</sup>&nbsp;There’s a nice visualization <a href="https://www.scmp.com/infographics/article/1917641/infographic-what-color-oscar">here</a> showing the history of oscar nominations: between 1927 and 1948 there were approximately 400 nominations for best actor or director, the only black nominee was Hattie McDaniel in 1939 for best supporting actress in Gone with the Wind.</p></div></div><p><strong>There are three basic reasons for exclusion: politics, art, or personal life.</strong></p>
<p><strong>Exclusion of people for their politics.</strong> By far the biggest exclusion was the anti-communist blacklisting of the 1940s and 1950s. Many of the most prominent people in Hollywood were out of work for a decade or more, often based on fairly weak evidence of communist associations.</p>
<p>During wars celebrities have sometimes been excluded for being disloyal: during the Vietnam war (Jane Fonda, John Lennon, Eartha Kitt), and during the Iraq War (Sean Penn, Dixie Chicks).</p>
<p>In the last 20 years a number of people have been excluded for anti-homosexual statements (Isaiah Washington, Kirk Cameron) or generally conservative statements (Gina Carano, James Woods, Stacy Dash).</p>
<p><strong>Exclusion of people for their art.</strong> These seem relatively rare. During the 1970s-1990s a few musicians were excluded for shocking or transgressive art, e.g.&nbsp;the Sex Pistols, Marilyn Manson, Body Count. Kathy Griffin has had a couple of episodes of exclusion for different statements intended to shock. In the 1980s and 1990s a number of radio hosts said shocking things and were fined or fired (e.g.&nbsp;Howard Stern).</p>
<p><strong>Exclusion of people for their personal life:</strong> Exclusions prior to the mid-2000s were mostly for more severe accusations (Roman Polanski, Woody Allen, OJ Simpson, Mel Gibson), and the exclusions were sometimes relatively mild. In the last 20 years these exclusions have become much more common, especially for sex-related accusations against men.</p>
<p><strong>Exclusion for other reasons.</strong> (1) some have been boycotted by their peers for informing on fellow artists (Eliza Kazan, Lovin Spoonful); (2) the police have temporarily boycotted a number of musicians for songs criticizing the police (NWA, Bruce Springsteen, Beyonce).</p>
<section id="exclusions-in-film-and-television" class="level2">
<h2 class="anchored" data-anchor-id="exclusions-in-film-and-television">Exclusions in Film and Television</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 18%">
<col style="width: 32%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th></th>
<th>alleged crime</th>
<th>result</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1920s</td>
<td>Fatty Arbuckle</td>
<td>rumors of immorality</td>
<td>film industry blacklisted</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>1940s</td>
<td>Orson Welles</td>
<td>communist associations</td>
<td>blacklisted, moved to Switzerland</td>
</tr>
<tr class="even">
<td></td>
<td>Dalton Trumbo</td>
<td>communist associations</td>
<td>blacklisted</td>
</tr>
<tr class="odd">
<td></td>
<td>(around 100 people)</td>
<td>communist associations</td>
<td>blacklisted for a decade</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>1950s</td>
<td>Charlie Chaplin</td>
<td>communist associations</td>
<td>banned from US</td>
</tr>
<tr class="even">
<td></td>
<td>Elia Kazan</td>
<td>testifying before HUAC</td>
<td>lost some relationships in Hollywood</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>1960s</td>
<td>Jane Fonda</td>
<td>opposition to Vietnam war</td>
<td>blacklisted</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>1970s</td>
<td>Roman Polanski</td>
<td>rape of 13yo girl</td>
<td>mild disapproval from Hollywood</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>1990s</td>
<td>O J Simpson</td>
<td>murdered his wife</td>
<td>blacklisted</td>
</tr>
<tr class="odd">
<td></td>
<td>Woody Allen</td>
<td>molested 7yo daughter</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>2000s</td>
<td>Mel Gibson</td>
<td>racism &amp; anti-semitism</td>
<td>“blacklisted in Hollywood for almost a decade”</td>
</tr>
<tr class="even">
<td></td>
<td>Mira Sorvino</td>
<td>rejecting Harvey Weinstein</td>
<td>blacklisted</td>
</tr>
<tr class="odd">
<td></td>
<td>Rose McGowan</td>
<td>rejecting Harvey Weinstein</td>
<td>blacklisted</td>
</tr>
<tr class="even">
<td></td>
<td>Isaiah Washington</td>
<td>homophobic remarks</td>
<td>blacklisted</td>
</tr>
<tr class="odd">
<td></td>
<td>Michael Richards</td>
<td>racist remarks</td>
<td>blacklisted</td>
</tr>
<tr class="even">
<td></td>
<td>Kathy Griffin</td>
<td>“told Jesus to suck it”</td>
<td>banned from talk shows and TV appearances</td>
</tr>
<tr class="odd">
<td></td>
<td>Sean Penn</td>
<td>opposition to Iraq war</td>
<td>dropped from movie</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>2010s</td>
<td>Bill Cosby</td>
<td>sexual assault</td>
<td>blacklisted (&amp; later imprisoned)</td>
</tr>
<tr class="even">
<td></td>
<td>Harvey Weinstein</td>
<td>sexual assault</td>
<td>blacklisted (&amp; later imprisoned)</td>
</tr>
<tr class="odd">
<td></td>
<td>Stacy Dash</td>
<td>conservative advocacy</td>
<td>blacklisted</td>
</tr>
<tr class="even">
<td></td>
<td>Kirk Camerson</td>
<td>criticism of homosexuality</td>
<td>blacklisted</td>
</tr>
<tr class="odd">
<td></td>
<td>James Woods</td>
<td>anti-Obama tweets</td>
<td>blacklisted</td>
</tr>
<tr class="even">
<td></td>
<td>CeeLo Green</td>
<td>sexual assault</td>
<td>blacklisted</td>
</tr>
<tr class="odd">
<td></td>
<td>Louis CK</td>
<td>sexual harassment</td>
<td>blacklisted</td>
</tr>
<tr class="even">
<td></td>
<td>Kathy Griffin</td>
<td>photo with head of Trump</td>
<td>fired by CNN, lost endorsement, cancelled tour</td>
</tr>
<tr class="odd">
<td></td>
<td>T J Miller</td>
<td>substance abuse, sexual assault</td>
<td>blacklisted</td>
</tr>
<tr class="even">
<td></td>
<td>Gina Carano</td>
<td>political social media posts</td>
<td>fired from TV show</td>
</tr>
<tr class="odd">
<td></td>
<td>Kevin Spacey</td>
<td>sexual harassment</td>
<td>lost roles in films</td>
</tr>
<tr class="even">
<td></td>
<td>Jussie Smollett</td>
<td>lied about an attack</td>
<td>lost roles in TV shows</td>
</tr>
<tr class="odd">
<td></td>
<td>Neil deGrasse Tyson</td>
<td>rape, sexual harassment</td>
<td>temporarily lost roles in TV shows</td>
</tr>
<tr class="even">
<td></td>
<td>Roseanne Barr</td>
<td>racist tweet</td>
<td>lost TV show</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>2020s</td>
<td>Will Smith</td>
<td>slapping someone at Oscars</td>
<td>film projects put on hold</td>
</tr>
<tr class="odd">
<td></td>
<td>Johnny Depp</td>
<td>domestic violence</td>
<td>lost roles in films</td>
</tr>
<tr class="even">
<td></td>
<td>Amber Heard</td>
<td>involvement in trial w Johnny Depp</td>
<td>lost roles in films</td>
</tr>
<tr class="odd">
<td></td>
<td>Justin Roiland</td>
<td>sexual harassment &amp; abuse</td>
<td>lost roles in shows</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="exclusions-in-music" class="level2">
<h2 class="anchored" data-anchor-id="exclusions-in-music">Exclusions in Music</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
<col style="width: 15%">
<col style="width: 33%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th></th>
<th>alleged crime</th>
<th>result</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1940s</td>
<td>Paul Robeson</td>
<td>communist associations</td>
<td>blacklist and passport revoked</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>1950s</td>
<td>Leonard Bernstein</td>
<td>communist associations</td>
<td>brief blacklist</td>
</tr>
<tr class="even">
<td></td>
<td>Lena Horne</td>
<td>communist associations</td>
<td>blacklist</td>
</tr>
<tr class="odd">
<td></td>
<td>Pete Seeger</td>
<td>communist associations</td>
<td>blacklist</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>1960s</td>
<td>Beatles</td>
<td>saying they’re bigger than Jesus</td>
<td>consumer boycott</td>
</tr>
<tr class="even">
<td></td>
<td>Lovin Spoonful</td>
<td>cooperating with FBI</td>
<td>music industry boycott</td>
</tr>
<tr class="odd">
<td></td>
<td>Nina Simone</td>
<td>“Mississippi Goddam”</td>
<td>boycott in the South</td>
</tr>
<tr class="even">
<td></td>
<td>John Lennon</td>
<td>criticism of US and Vietnam war</td>
<td>refused entry into US</td>
</tr>
<tr class="odd">
<td></td>
<td>Eartha Kitt</td>
<td>criticism of Vietnam war</td>
<td>blacklist through LBJ and CIA</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>1970s</td>
<td>Sex Pistols</td>
<td>criticizing the Queen, swearing on TV</td>
<td>banned by the BBC, dropped by EMI</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>1980s</td>
<td>NWA</td>
<td>“Fuck the Police” &amp; similar songs</td>
<td>radio station boycott, police boycott</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>1990s</td>
<td>Bruce Springseen</td>
<td>song against police brutality</td>
<td>brief police boycott</td>
</tr>
<tr class="even">
<td></td>
<td>Marilyn Manson</td>
<td>transgressive lyrics</td>
<td>banned from performing in some states</td>
</tr>
<tr class="odd">
<td></td>
<td>Body Count</td>
<td>song “cop killer”</td>
<td>album withdrawn and reissued</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>2000s</td>
<td>Dixie Chicks</td>
<td>for opposition to Iraq war</td>
<td>blacklisting and consumer boycott</td>
</tr>
<tr class="even">
<td></td>
<td>Janet Jackson</td>
<td>showing nipple</td>
<td>VH1, MTV, &amp; Viacom radio stopped playing her music</td>
</tr>
<tr class="odd">
<td></td>
<td>R Kelly</td>
<td>sexual abuse</td>
<td>broad blacklist</td>
</tr>
<tr class="even">
<td></td>
<td>Chris Brown</td>
<td>domestic violence</td>
<td>weak boycott and blacklist</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>2010s</td>
<td>Lostprophets</td>
<td>sexual abuse</td>
<td>broad blacklist</td>
</tr>
<tr class="odd">
<td></td>
<td>Michael Jackson</td>
<td>child molestation</td>
<td>some radio stations stop playing music</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>2020s</td>
<td>Beyonce</td>
<td>song against police brutality</td>
<td>brief police boycott</td>
</tr>
<tr class="even">
<td></td>
<td>Morgan Wallen</td>
<td>using n-word</td>
<td>temporarily dropped from radio/streaming playlists</td>
</tr>
<tr class="odd">
<td></td>
<td>Kanye West</td>
<td>praise of Hitler</td>
<td>lost sponsors</td>
</tr>
</tbody>
</table>
<p><strong>Others.</strong></p>
<ul>
<li><p><strong>In radio:</strong> Father Coughlin, Rush Limbaugh, Don Imus fired from CBS for calling womens’ basketball team “nappy-headed hos”, Howard Stern fired from various radio shows for comments.</p></li>
<li><p><strong>In sport.</strong> Colin Kapaernick blacklisted from NFL for kneeling for the anthem. Pete Rose banned from MLB for gambling.</p></li>
<li><p><strong>Nazi sympathisers/collaborators.</strong> Charles Lindbergh, Henry Ford, Charles Coughlin, PG Wodehouse, Ezra Pound.</p></li>
<li><p><strong>Writers:</strong> DH Lawrence, Henry Miller, Salman Rushdie (Nicole Bonoff).</p></li>
<li><p><strong>Journalists.</strong> Jeffrey Toobin (New Yorker writer masturbated on zoom call),</p></li>
<li><p><a href="https://twitter.com/JuraWho/status/1659214427818962944">Note on R Kelly disappearing from radio</a></p></li>
</ul>
</section>
</section>
<section id="appendix-alternative-visualization" class="level1 page-columns page-full">
<h1>Appendix: Alternative Visualization</h1>
<div class="cell page-columns page-full" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-29_e06b93c3fcdf5754f98d423af1ba1401">
<div class="cell-output-display page-columns page-full">
<div class="page-columns page-full">
<figure class="figure page-columns page-full">
<p class="page-columns page-full"><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-29-1.png" class="img-fluid figure-img column-page" width="720"></p>
</figure>
</div>
</div>
</div>


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{cunningham2023,
  author = {Cunningham, Tom},
  title = {Social {Media} {Suspensions} of {Prominent} {Accounts}},
  date = {2023-01-31},
  url = {tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data.html},
  langid = {en}
}
</code></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-cunningham2023" class="csl-entry quarto-appendix-citeas">
Cunningham, Tom. 2023. <span>“Social Media Suspensions of Prominent
Accounts.”</span> January 31, 2023. <a href="https://tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data.html">tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data.html</a>.
</div></div></section></div> ]]></description>
  <guid>tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data.html</guid>
  <pubDate>Tue, 31 Jan 2023 08:00:00 GMT</pubDate>
</item>
<item>
  <title>Optimal Coronavirus Policy Should be Front-Loaded</title>
  <link>tecunningham.github.io/posts/2020-04-05-front-loading-restrictions.html</link>
  <description><![CDATA[ 





<p><strong>Q: How should you sequence policies over time?</strong> E.g. suppose you want to manage the epidemic until a vaccine arrives and you have policies (lockdowns, distancing, masks) each of which is associated with a certain effect on the growth-rate of cases, but each also has some fixed social cost per day. How should you apply the policies over time?</p>
<p><strong>A: The severity of the policies should be gradually <em>decreasing</em>,</strong> i.e.&nbsp;they should gradually become less severe, as you approach the availability of a vaccine. There should not be zig-zagging between policies in this setup.</p>
<p>Any justification for zig-zagging must come from some additional consideration like (a) non-separabilities in the costs, e.g.&nbsp;psychological/economic need for occasional respite, (b) uncertainty about the end-date, (c) uncertainty about the effect of the policies, such that there is informational-value from varying policies, or (d) desire to maintain a steady flow of cases, in order to reach herd immunity (the “mitigation” strategy).</p>
<section id="corollary-you-should-never-expect-policy-to-get-stricter" class="level2">
<h2 class="anchored" data-anchor-id="corollary-you-should-never-expect-policy-to-get-stricter">Corollary: you should never <em>expect</em> policy to get stricter</h2>
<p>You should never find yourself in the situation where you expect policy to get stricter in the future. If you anticipate that a stricter policy will be appropriate next week then that strict policy is appropriate <em>this</em> week!</p>
<p>Countries in early stages of the epidemic should be doing as much or more as countries in later stages.</p>
</section>
<section id="intuition" class="level2">
<h2 class="anchored" data-anchor-id="intuition">Intuition</h2>
<p>Suppose that there’s some tradeoff across policiers between the growth-rate and the social cost.</p>
<p>Then given any fixed time-path of policies: e.g., (A,A,B,C), if it is not monotonically decreasing in severity from high-cost to low-cost, then you can do strictly better by rearranging the path of policies to be monotonically decreasing. The social cost will be identical, because the set of policies will be the same, but the number of cases will be lower at every point in time, since at any given point the cumulative growth rates, up to that point, will be lower. Thus the final cumulative number of cases will be lower.</p>
</section>
<section id="additional-reason-to-front-load-extinction" class="level2">
<h2 class="anchored" data-anchor-id="additional-reason-to-front-load-extinction">Additional Reason to Front-Load: Extinction</h2>
<p>All of this is treating the number of cases as a continuous variable which means you can never completely extinguish the disease. However if that’s a possibility that’s within sight (e.g.&nbsp;as in NZ), then that’s a significantly <em>stronger</em> case for starting with very severe policies, to try to kill the disease entirely, and then you can go back to the garden of Eden.</p>
</section>
<section id="prior-discussion" class="level2">
<h2 class="anchored" data-anchor-id="prior-discussion">Prior Discussion</h2>
<p>There’s been some discussion of zig-zagging by the Imperial group (<a href="[URL](https://www.imperial.ac.uk/media/imperial-college/medicine/sph/ide/gida-fellowships/Imperial-College-COVID19-NPI-modelling-16-03-2020.pdf)">paper</a>) and by Timothy Gowers (<a href="https://twitter.com/wtgowers/status/1243973167879794691">twitter &amp; post</a>)</p>
<p>Gowers says the optimal policy is very short zig-zags (changing policy every other day), however I think this is misleading. It comes from fixing the lower-threshold and optimizing the upper-threshold. If instead you fixed the upper-threshold and optimized the lower-threshold, then the optimal cycle-length will be long.</p>
<p>If you choose both the upper and lower threshold (both T and S) then he notes that they’ll both be arbitarily low. However this ignores the cost of <em>getting</em> to zero given current cases.</p>
<p>Instead a well-defined problem is to choose an optimal time-path of policy given some start-point and end-point. In that case it’ll be a path of gradually decreasing strictness (without zig-zags).</p>
<p>You can see the intuition in the diagram below: the total infections is approximately the area under the zig-zag (not quite: because the y-axis is ln(cases), but this won’t matter for the argument). Thus you can reduce the area under the line by lowering the upper threshold. However if you instead take the <em>upper</em> threshold as fixed, then it’s optimal to choose a lower threshold that is as low as possible, i.e.&nbsp;you want <em>long</em> cycles, not short cycles.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.dropbox.com/s/jj93tz0k0kuau49/zigzag.png?raw=1" class="img-fluid figure-img"></p>
<figcaption>abc</figcaption>
</figure>
</div>


</section>

 ]]></description>
  <guid>tecunningham.github.io/posts/2020-04-05-front-loading-restrictions.html</guid>
  <pubDate>Sun, 05 Apr 2020 07:00:00 GMT</pubDate>
</item>
<item>
  <title>On Unconscious Influences (Part 1)</title>
  <link>tecunningham.github.io/posts/2017-12-10-unconscious-influences.html</link>
  <description><![CDATA[ 





<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Over a couple of years I spent a lot of time in offices looking out the window, thinking about decision-making &amp; the unconscious, scribbling little bits &amp; pieces in a notebook.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.dropbox.com/s/be1p2urgqwri2hp/nber_snow.png?raw=1" class="img-fluid figure-img"></p>
<figcaption>NBER</figcaption>
</figure>
</div>
<p>I ended up writing two papers - “<a href="https://dl.dropboxusercontent.com/u/13046545/paper_heuristics.pdf">Hierarchical Aggregation of Information and Decision-Making</a>” by myself and “<a href="http://bit.ly/paper_implicit">Implicit Preferences Inferred from Choice</a>” with Jon de Quidt. The papers are fairly technical, and this post is going to be a layperson’s guide to the background, what’s known about unconscious knowledge, and a tiny bit about the ideas in those papers.</p>
<p>Here is the argument in a nutshell:</p>
<ol type="1">
<li>There are plenty of reasons to think that unconscious influences are strong – in other words, that people have limited insight into what factors influence their decisions.</li>
<li>The idea of unconscious influences has been in and out of the mainstream of psychology for the last 200 years, but always hounded by arguments over what it means, i.e.&nbsp;over what evidence would be sufficient to show that a decision was influenced by an unconscious factor. The battle has had many reversals: a new types of evidence has been proposed which is thought to reveal unconscious influences, and then later the technique or interpretation is shown to have substantial flaws and the line of inquiry fizzles out. A couple of decades pass and a different approach becomes popular.</li>
<li>Two broad classes of evidence are the following: (A) people reveal their unconscious preoccupations in their involuntary responses – in how their pupils dilate, how quickly they respond to a stimulus, in their word associations, dreams, slips of the tongue; (B) people reveal unconscious influences in discrepancies between how they act and how they explain their behaviour. Both sources of evidence have got tangled in debates about interpretation, and there are substantial camps on either side with not much agreement on what constitutes sufficient evidence for unconscious influences.</li>
<li>A third type of evidence is less common but, I think, more powerful: evidence from inconsistencies in decision-making. The idea being that unconscious factors are by their nature <em>isolated</em> from conscious factors, i.e.&nbsp;they don’t interact with conscious beliefs and desires, and this isolation will cause certain characteristic inconsistencies among decisions.</li>
<li>This can be made precise with an analogy: the relationship between the conscious and unconscious brain is like the relationship between a blind man and his guide dog. The blind man makes decisions based, in part, on which direction the guide dog is pulling towards, so the guide dog’s beliefs and desires influence the man’s decisions, but without the man knowing exactly what those beliefs and desires are, and so he couldn’t tell you how much any particular factor contributed to his decision. Testing for unconscious influences in behaviour is just testing the degree to which your brain is being led by a guide dog.</li>
<li>The internal-consistency definition of unconscious influences implies two ways of looking for them: (1) testing whether people can accurately answer <em>hypothetical</em> questions about decisions they would make if factors changed - i.e.&nbsp;navigating without your guide dog; and (2) testing whether people make consistent judgments when judging two outcomes at a time.</li>
<li>First, hypothetical questions. We can ask people, how would your judgment change if this factor changed? Would you still like this painting if the name of the artist was different? Would this drawing look more like your cousin if the nostrils were bigger? Unconscious influences imply that people will not be able to give accurate answers to these hypothetical questions because if the description of the situation is abstract then their unconscious brain won’t be able to evaluate it (AKA, they don’t know which direction they would go in without knowing what their guide dog will say).</li>
<li>The second way of testing for unconscious influences is what my paper with Jon is about: unconscious influences particularly leave their mark in <em>comparisons</em>, where you evaluate two outcomes simultaneously or consecutively, or when you choose between two outcomes. When confronted with two outcomes you surface two unconscious judgments and that gives you some insight into what is affecting those judgments, which in turn will affect your conscious decision.</li>
<li>Suppose you had an unconscious preference for men over women, but a conscious preference to be indifferent, this will manifest in the following: (A) when you see two CVs which are identical, except that one is a man and one is a woman, then you’re indifferent between them; (B) when you see two CVs which differ in some other respect (e.g.&nbsp;one has a PhD, the other has an MBA), then you consistently have a preference for the CV belonging to the man. Your guide dog has a bias towards men, which you’re not aware of: the bias will only sway your decision in the second case because, in the second case, when your guide dog pulls you towards the man with a PhD, you cannot figure out how much of that pull is due to his being a man, and how much is due to his PhD.</li>
<li>In the end I think that our brains <em>are</em> full of guide dogs all pulling in different directions. If we had the stomach for it we could plot out our decisions all on a map – measure how each factor influences our judgment – and we would be able to see both the surface influences and the deeper latent influences.</li>
</ol>
<section id="contents" class="level2">
<h2 class="anchored" data-anchor-id="contents">Contents</h2>
<ol type="1">
<li>Motivating examples</li>
<li>Some definitions &amp; theory</li>
<li>Ways of measuring implicit preferences</li>
<li>The proposal</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.dropbox.com/s/lt96t4u3qoh7ywp/harvard_office.png?raw=1" class="img-fluid figure-img"></p>
<figcaption>Littauer</figcaption>
</figure>
</div>
</section>
</section>
<section id="motivating-examples" class="level1">
<h1>Motivating Examples</h1>
<p>To help with being clear on what I’m talking about here are some examples of unconscious influences. I’ll try to return to these at the end.</p>
<ol type="1">
<li><p>People judging a presidential candidate’s actions differently depending on whether it’s a man or a woman; judging an ethical lapse differently depending on whether committed by a Democrat or Republican; judging a study differently depending on whether it supports their own theory. In each case not being aware of that influence.</p></li>
<li><p>The little things that a family does to spite each other: someone says that Thursday doesn’t suit them, in part because they know that Thursday suits you best (perhaps consciously, perhaps unconsciously).</p></li>
<li><p>Freudian conjectures about trauma causing later responses: e.g.&nbsp;Freud thought Anna O’s aversion to water was because of a memory of seeing a dog drinking from a glass of water. More generally, Freud’s early seduction theory, that psychosomatic illness is caused by repressed memories of childhood sexual abuse.</p></li>
<li><p>Your preferences changing with the circumstances: saying yes to a friend, a hairdresser, bartender, waiter, or salesperson, because it seemed like a good idea at the time, and later not understanding why you didn’t say no.</p></li>
<li><p>Knowledge used in your judgments, but not consciously accessible: knowledge of grammar implicit in your judgements of whether a sentence sounds right; knowledge of the physical world implicit in your judgements of distance and velocity of the things you see; knowledge of faces implicit in recognizing your cousins.</p></li>
</ol>
<p>My examples include both unconscious knowledge and unconscious desires. I’m not going to talk about unconscious <em>sensations</em> – e.g.&nbsp;flashes of advertisement that don’t register consciously but might persuade you.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.dropbox.com/s/bczy68n3k08s6yl/tel_aviv_office.png?raw=1" class="img-fluid figure-img"></p>
<figcaption>tel aviv</figcaption>
</figure>
</div>
</section>
<section id="ways-of-measuring-unconscious-influences" class="level1 page-columns page-full">
<h1>Ways of Measuring Unconscious Influences</h1>
<p>Here are three ways of identifying whether an influence is conscious:</p>
<ol type="1">
<li><strong>Involuntary responses.</strong> In the dilation of your pupils, in your response times, in slips of the tongue, what you see in Rorsach blots, in your word associations, in your dreams.</li>
<li><strong>Ability to describe the influence.</strong> Ask people what they think influences their decisions, and compare that to what actually influences their decisions.</li>
<li><strong>Integration with other influences.</strong> Finally we can say that a factor is unconscious if its influence seems to be partitioned off from other influences. The most simple example is if a factor continues to influence you even in cases where you have reason to ignore it.</li>
</ol>
<section id="involuntary-responses" class="level2">
<h2 class="anchored" data-anchor-id="involuntary-responses">Involuntary responses</h2>
<p>Freud is the most famous theorist of extracting unconscious factors from involuntary responses – he wrote three books on different methods: one on dreams, one on jokes, one on mistakes (mis-reading, mis-hearing, mis-speaking). An example from the last book: “A woman who is very anxious to get children always reads ‘storks’ instead of ‘stocks’.” Most of Freud’s examples of unconscious influences are much more complex than this one, and more often the hidden factor influencing behaviour is something unpleasant or shameful.</p>
<p>Another way of measuring unconscious cognition is through measuring arousal. Most famous is the “Iowa card task” from Bechara et al.&nbsp;(1996). They had their subjects choose among playing cards, and receive rewards if they chose certain cards. They found that people gradually learned which types of cards were rewarded, but they also found that the subjects’ automatic responses (measured by skin conductance, i.e.&nbsp;sweating) would show an awareness of the pattern more quickly than the subjects’ choices would: after a while, when the subject’s hand hovered over one of the cards which was rewarded, the subject would sweat a little more, even though the subject wasn’t any more likely to choose that card. They said that this showed that unconscious learning was outpacing conscious learning. Antonio Damasio, one of the authors of this study, went on to write <em>Descartes’ Error</em> which accused Descartes’ of starting the great misapprehension that emotions and reason are in competition – Damasio said that his experiments show how emotions inform reason and improve decision-making. A lot of subsequent papers tried to show that snap decisions, which avoid conscious processing, can produce better outcomes than slow considered decisions.</p>
<p>Even more famous is the “Implicit Association Test” (IAT) (Greenwald, McGhee and Schwartz (1998)). Subjects are told to press a button whenever they see something from either of two different categories of stimuli, e.g.&nbsp;press the button if you see either a black face or a word with a positive association. Their finding, much-replicated, was that people are relatively quicker at tasks (meaning they have shorter response times) when they are asked to identify a set such as “black face or negative word” or “white face or positive word” than to identify a set like “black face or positive word” or “white face or negative word.” They find that this occurs even among people who report no conscious negative feelings towards black people, and they interpret this as revealing an unconscious association between black people and negative feelings, and they argue that this association could affect your decision-making without you being aware of it.</p>
<p>Many other measures of automatic responses have been popular at different times: hypnosis and word association (Freud used both of these before moving to talking therapy); Rorsach blots (AKA inkblot tests); thematic apperception test (interpret an ambiguous drawing, still widely used); lie detectors AKA polygraphs (they measure autonomic responses - blood pressure, pulse, respiration, and skin conductivity - as you are asked different questions).</p>
<p>Unfortunately a great deal of this research turns out to be both hard to replicate, and reliant on strong assumptions in order to interpret as surfacing unconscious associations. Newell and Shanks (2014) give strong arguments for both of these points, covering many of the methods I mentioned here.</p>
<p>It is worth mentioning that, although Freud’s more elaborate theories died off, his idea that psychosomatic illness is an indirect expression of a psychological stress, especially about something shameful, I believe remains one of the standard theories of modern neurology (O’Sullivan, 2015).</p>
<p>However even if we had solid evidence for unconscious influences on involuntary responses, this still stops short of unconscious influences on decision-making. It’s possible that our associations show up in sweating, response time, and dreams, but have little effect on decision-making, and if that’s so then unconscious associations are not terribly important for social science. Most of the authors in this literature have assumed that the unconscious factors they identify affect real decisions but have left that extrapolation untested. Blanton et al.&nbsp;(2009) say that there’s no persuasive evidence that implicit racial bias, as measured by the IAT, predicts peoples’ decision-making, once you control for measures of <em>explicit</em> racial bias, i.e.&nbsp;when you just ask people how they feel about black people. (Singal (2016) has a long discussion on this point).</p>
</section>
<section id="ability-to-describe-the-influence" class="level2">
<h2 class="anchored" data-anchor-id="ability-to-describe-the-influence">Ability to Describe the Influence</h2>
<p>A second type of evidence is to compare self-reported influences on behaviour with actual influences on behaviour. Here are some examples:</p>
<ol type="1">
<li>In the mid 20th-century behaviourists found that they could shape their subjects choices through conditioning with rewards and punishments, and the subjects seemed to remain ignorant of this shaping. For example if you say <em>‘mm-hmm’</em> whenever someone uses a plural noun, then after a while that person ends up using plural nouns more often, apparently unaware of the influence (Thorndike and Rock (1934); Greenspoon (1955)).</li>
<li>Since the 1970s social psychologists have published all sorts of experiments in which they vary an apparently irrelevant factor and find that this can affect peoples’ decision-making. Nisbett and Wilson (1977) summarize a lot of experiments and say “subjects are sometimes (a) unaware of the existence of a stimulus that importantly influenced a response, (b) unaware of the existence of the response, and (c) unaware that the stimulus has affected the response.”</li>
<li>Another paradigm from the 1970s asks people to make a judgement - e.g.&nbsp;which stock to pick - and also to rate the importance of factors which contributed to their decision. Slovic et al.&nbsp;(1972) find a low correlation (0.34) between the ratings that a stockbrokers put on factors, and the actual influence of these factors on their decisions. There is a small literature with similar findings across a variety of tasks.</li>
<li>Finally, since the 1970s a smaller group of psychologists have been running experiments in which people learn a complicated pattern, and then are asked about their insight into it. E.g. in Arthur Reber’s “artificial grammar” experiments subjects learn, through trial and error, to discriminate between two categories of words. After some time they become very good at the task, but when asked to explain how they are making decisions they often say they don’t know, or they come up with rules that do not match their actual performance.</li>
</ol>
<p>As in the previous category, a lot of this evidence is very fragile: either hard to replicate, or based on delicate interpretations of what is happening in the experiment. Newell and Shanks (2014) again give a good summary.</p>
<p>An additional problem is that these findings could reflect knowledge being difficult to articulate, without it being unconscious. And this literature is full of reversals which bear this out: when experiments are repeated it has often turned out that the subjects <em>do</em> report awareness of the pattern that they have learned if they are asked the question in a different way. Mitchell et al.&nbsp;(2009) say “[i]t is very difficult to provide a satisfactory demonstration of unaware conditioning simply by showing conditioning in the absence of awareness. This is because it is very difficult to be sure that the awareness measure and the conditioning measure are equally sensitive.”</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.dropbox.com/s/d5s41vykojhy4cv/iies.jpg?raw=1" class="img-fluid figure-img"></p>
<figcaption>IIES</figcaption>
</figure>
</div>
</section>
<section id="isolation-of-unconscious-influences" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="isolation-of-unconscious-influences">Isolation of Unconscious Influences</h2>
<p>Finally there’s a third type of evidence which is more strictly behavioural: an unconscious factor is one which is <em>isolated</em> from your other conscious beliefs and desires – i.e.&nbsp;it does not interact with conscious factors – and that isolation will be reflected in your behaviour. This isolation criterion has been given various names, but I don’t think it’s ever been explained as clearly as it could be.</p>
<p>To be precise think of the blind man (the conscious part of the brain) and the guide dog (the unconscious). The guide dog can know something – e.g.&nbsp;she knows when the crossing light is flashing – which the man does not know, and her knowledge will influence the man’s decisions through her recommendation of when to cross. However the guide dog’s knowledge is isolated from the man’s knowledge: it only influences his decisions through the narrow channel of pulling on the leash. Suppose you tell the man that the crossing lights are not working properly, and so whatever color they show is entirely at random and uninformative. The man and dog, considered as a system, has two pieces of information: (1) the light is green (i.e.&nbsp;indicating ready to cross); and (2) the color is uninformative. However the two pieces of information are known by different actors, implying that they will not be integrated, because neither the man or dog knows both. This will be reflected in the man’s behaviour: he will be influenced by the guide-dog’s recommendation, because the dog sees other things in addition to the crossing-light, such as oncoming traffic. And so the man’s behaviour will still be influenced by the color of the light, even though he <em>knows</em> that the color is irrelevant.</p>
<p>If information is separated in the brain, we ought to see characteristic patterns of that in behavior. I know of just a few cases where the isolation of knowledge has come up clearly in trying to define or measure unconscious influences.</p>
<p>Stich (1978) said that certain mental states are “inferentially unintegrated”: </p>
<blockquote class="blockquote">
<p>“[unconscious beliefs are] largely inferentially isolated from the large body of inferentially integrated beliefs to which a subject has access”</p>
</blockquote>
<p>He gives an example: suppose Noam Chomsky has a theory of grammar, and that there exists some grammatical rule which is a counterexample to that theory. If a linguist knows that rule consciously, then the linguist will immediately infer that Chomsky’s theory is false. But if the linguist only knows the rule <em>unconsciously</em>, then they won’t be able to make that inference, because the knowledge is “inferentially unintegrated” – i.e.&nbsp;the knowledge is isolated from the knowledge regarding Chomsky’s theory. <sup>1</sup></p>
<div class="no-row-height column-margin column-container"><div id="fn1"><p><sup>1</sup>&nbsp;Quine says you shouldn’t call this type of thing unconscious knowledge – your linguistic practice may <em>obey</em> some rule, but you can’t say that you unconsciously <em>know</em> that rule, because there are infinitely many different rules that would imply that pattern of behavior. But this skeptical objection is too tough: Quine would deny that a cow can have a belief about where a water trough is, &amp; instead admit only that the cow’s behavior is consistent with a particular belief among infinitely many others.</p></div></div><p>A separate place where this separation has come up is in the work of Zenon Pylyshyn and Jerry Fodor since the 1980s regarding perception being “cognitively impenetrable,” or “informationally encapsulated.” They mean that perceptual processes often make inferences without taking into account all the information that is available, i.e.&nbsp;by drawing only on a subset of information. Their principal argument was from perceptual illusions: they argue that illusions can typically be understood as rational inferences from a subset of the information available. Helmholtz had a nice example: if you close one eye and press with your finger on the edge of your eyelid then you’ll perceive a point of light, but the light will be coming from the <em>opposite</em> side of your field of vision from where your finger is. This is because the left side of your retina receives light from the right side of your visual field and vice versa. So when your retina receives some stimulation on the left-hand side your brain makes infers that light is coming from the right-hand side. This is a sensible inference given only the information that your eye has, i.e.&nbsp;just the information from the retina. In this case there is additional information - the fact your finger is pressing on your eyelid - which should give a different interpretation to the stimulation, but your visual cortex is not wired up to incorporate that information, and so it misinterpret the signals it receives.</p>
<p>The Helmholtz-Fodor-Pylyshyn model of encapsulated inference isn’t quite the same as the case of the blind man and the guide dog. In their examples the pre-conscious process have a strict <em>subset</em> of the information available to the conscious brain. In other words the man isn’t blind, it’s just a case where the dog leads in a different direction than the man would. Fodor (1983) does have a brief discussion on whether early perceptual processes have access to information not available to the conscious brain, which would imply unconscious influences, in my sense.</p>
<p>Finally the isolation argument has appeared in the literature on human “associative learning,” in testing whether or not the associations that we learn through conditioning are conscious. A typical experiment involves ringing a bell and then giving subjects a small electric shock. After a while people learn to flinch when they hear the bell. For a long time psychologists tried to map out the logic of how such associations would form, trying to figure out the rule which governed learning of associations. However in the last few decades an argument has been made that these learned associations are not in fact mechanical - there is no simple rule - instead they are more-or-less optimal responses to the environment based on the entirety of the information available, i.e.&nbsp;they are not <em>isolated</em> from other knowledge, though the argument isn’t usually put in terms of conscious vs unconscious knowledge. For example Colgan (1970) told subjects, after they learned an association, that the association is no longer valid (“from now on the bell will not signal an electric shock”) and he found that, although this didn’t entirely extinguish the flinching, it did cause it to markedly decrease. This implies the flinching is not isolated from your conscious knowledge: the association, at least to some degree, interacts with more abstract knowledge. There are many other circumstances where rule-based theories of association-learning have foundered because it turns out that peoples’ responses respond to outside considerations. De Houwer, Vandorpe and Beckers (2005) summarize the evidence against associative models (which can be interpreted as models with unconscious knowledge):</p>
<blockquote class="blockquote">
<p>The two types of models can be differentiated … by manipulating variables that influence the likelihood that people will reason in a certain manner but that should have no impact on the operation of the associative model. We have seen that such variables (e.g., instructions, secondary tasks, ceiling effects, nature of the cues and outcomes) do indeed have a huge effect. Given these results, it is justified to entertain the belief that participants are using controlled processes such as reasoning and to look for new ways to model and understand these processes.</p>
</blockquote>
<p>Mitchell says:</p>
<blockquote class="blockquote">
<p>“The results consistently show evidence for skin conductance [effects] only in participants who are aware of the [relationship] … [a]lthough there are many papers arguing for unaware conditioning, close inspection reveals, in almost all cases, that the measure of conditioning was most likely more sensitive than that of awareness.”</p>
</blockquote>
<p>In retrospect a lot of behavior that was studied in the lab, which was thought to be telling us about the wiring of the animals, actually was telling us about the world outside the animal, because it has turned out that the animals’ response is the <em>optimal</em> response to the typical circumstances it faces in the world. (See my other post <a href="http://tecunningham.github.io/2017/04/15/the-mechanical-and-the-rational/">The Repeated Failure of Laws of Behaviour</a> , and also Mitchell et al.&nbsp;(2009) section 4.3)</p>
<p>If this line of thought were entirely correct – if all information was integrated and fed into every decision – then there would be no unconscious influences in my sense. However I do think that there’s plenty of evidence that remains for a lack of integratation between cognitive processes.</p>
<p>In Part 2 of this essay I will give a more formal statement of how decisions can reveal unconscious knowledge (and unconscious motivations), and a survey what I think is the strength of the evidence.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="https://www.dropbox.com/s/hvaaahkc9eshz6z/office_caltech.jpg?raw=1" class="img-fluid figure-img"></p>
<figcaption>Caltech</figcaption>
</figure>
</div>
</section>
</section>
<section id="summary-so-far" class="level1">
<h1>Summary So Far</h1>
<ol type="1">
<li>Looking at involuntary reactions tells us <em>something</em> about your reaction to a situation, but it doesn’t tell us whether it has important impact on your decision-making. (And in addition, the evidence for influences on involuntary reactions is pretty weak).</li>
<li>Comparing peoples’ decisions to what they say about their decisions is also imperfect evidence for unconscious associations, because people can be inarticulate about their reasons, without being unconscious of them. (And in addition, this evidence is also pretty weak).</li>
<li>Finally, internal coherence of decision-making seems a much more solid way of identifying unconscious influences.</li>
</ol>
</section>
<section id="in-part-ii" class="level1">
<h1>In Part II</h1>
<p>Discussion of the type of evidence needed to establish unconscious knowledge.</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<p><strong>Bechara, Tranel, Damasio and Damasio (1996) “Failure to respond autonomically to anticipated future outcomes following damage to prefrontal cortex”</strong></p>
<p><strong>Hart Blanton, James Jaccard, Jonathan Klick, Barbara Mellers, Gregory Mitchell, and Philip E Tetlock (2009). “Strong claims and weak evidence: reassessing the predictive validity of the IAT”. Journal of Applied Psychology, 94(3): 567.</strong></p>
<p><strong>Greenwald McGhee &amp; Schwartz (1998) “Measuring individual differences in implicit cognition: The Implicit Association Test.”</strong></p>
<p><strong>De Houwer, Vandorpe and Beckers (2005) On the role of controlled cognitive processes in human associative learning</strong> &gt; “In hindsight, it seems obvious that people can learn about associations by using controlled processes such as reasoning and hypothesis testing. Why then, are associative models still dominant in modern research? One reason is that the associationistic view has a long tradition in psychology (and philosophy). It is thus difficult for many people to leave behind the associationistic view that has guided their thinking and research for many years. Another important reason is that associative models do quite well in accounting for the available empirical data. The well known Rescorla-Wagner model (Rescorla &amp; Wagner, 1972), for instance, is compatible with a huge number of findings while being relatively simple. If our argument is correct that associative models do not provide an accurate account of the processes that underlie associative learning, how is it possible that they are able to account for so much of the data? We agree with Lovibond (2003, p.&nbsp;105) that”the success of these models is due to them capturing, at least in part, the operating characteristics of the inferential learning system”. What this means is that associative models (as well as probabilistic models for that matter) can be seen as (mathematical) formalisations of certain deductive reasoning processes. A system that operates on the basis of associative models does not reason, but acts very much as if it is reasoning. The associative models will thus often predict the same result as a model that is based on the assumption that humans actually generate and test hypothesis or reason in a controlled, conscious manner. The two types of models can be differentiated, however, by manipulating variables that influence the likelihood that people will reason in a certain manner but that should have no impact on the operation of the associative model. We have seen that such variables (e.g., instructions, secondary tasks, ceiling effects, nature of the cues and outcomes) do indeed have a huge effect. Given these results, it is justified to entertain the belief that participants are using controlled processes such as reasoning and to look for new ways to model and understand these processes.”</p>
<p><strong>Newell &amp; Shanks (2014, BBS) “Unconscious Influences on Decision-Making: A Critical Review”</strong></p>
<ul>
<li>“There is little convincing evidence of unconscious influences on decision making in the areas we review” – they say many of Nisbett and Wilson’s results on lack of introspection have been reinterpreted</li>
<li><ol type="1">
<li>multiple-cue judgment : classic paradigm is that you compare weights revealed by decisions to self-reported weights. They say that protocol for getting self-reported weights are usually shitty, &amp; better measurements show that people understand their own judgment better</li>
</ol></li>
<li><ol start="2" type="1">
<li>decisions are better when you don’t deliberate (all the evidence for this is shitty)</li>
</ol></li>
<li><ol start="3" type="1">
<li>awareness / Iowa gambling task (your subconscious is attracted to the best deck, but conscious is not): very shitty evidence</li>
</ol></li>
<li><ol start="4" type="1">
<li>priming: people often can report what primes were presented, if you try harder- they say that there is strong evidence for top-down influence even in perceptual tasks. [I emailed them suggesting the joint/separate randomization task, they said it sounds nice]</li>
</ol></li>
<li>They say that dispute over definitions is over: “A surprising outcome of the review is that debates and disagreements about the meaning of the terms consciousness and awareness have (with a few exceptions) played a remarkably minor role in recent research. Whereas issues about how to define and measure awareness were once highly prominent and controversial (e.g., Campion et al.&nbsp;1983; Reingold &amp; Merikle 1988), it now seems to be generally accepted that awareness should be operationally defined as reportable knowledge, and that such knowledge can only be evaluated by careful and thorough probing.”</li>
</ul>
<p><strong>Michael Polanyi (1966) “The Tacit Dimension”</strong></p>
<p><strong>Nisbett &amp; Wilson (1977) “Telling More than we can know”</strong></p>
<ul>
<li>A very influential paper (7000 citations) arguing that unconscious influences are rampant: “Subjects are sometimes (a) unaware of the existence of a stimulus that importantly influenced a response, (b) unaware of the existence of the response, and (c) unaware that the stimulus has affected the response.” However a lot of the experimental literature in this field is now under a cloud – either through being hard to replicate, or being based on arguable interpretations.</li>
</ul>
<p><strong>Quine (1972) “Methodological reflections on current linguistic theory”</strong></p>
<ul>
<li>When Quine wrote this, linguists were talking a lot about how people had unconscious knowledge of grammatical rules. Quine said that you can’t say that someone knows a rule, from observing that their behaviour conforms to that rule, because there will always be infinitely many different rules which the behaviour satisfies. So this is Quine pouring some cold water on unconscious knowledge. However, in response, I say that we can still define unconscious knowledge underneath behaviour as the inability to accurately answer hypotheticals. So we don’t need to specify <em>which</em> rule the person is using, but it’s clear that they do not have conscious access to any rule which generates their observed behaviour.</li>
</ul>
<p><strong>Stich (1976) “Beliefs &amp; Subdoxastic States”</strong></p>
<p><strong>Suzanne O’Sullivan (2015) “It’s All in Your Head: Stories from the Frontline of Psychosomatic Illness”</strong></p>
<blockquote class="blockquote">
<p>p191: “For all the shortcomings in the concepts proposed by Freud and Breuer in <em>Studies</em>, the twenty-first century has brought no great advances to a better understanding of the mechanism for this disorder. The terms dissociation and conversion are still widely in use … the general principles of modern dissociation are not very different to those of Victorian times … in day-to-day practice Janet’s and Freud’s theories are regularly used, or misused.”</p>
</blockquote>
<p><strong>Singal (2016) in New York Magazine</strong> - article on Implicit Association Test.</p>


</section>


 ]]></description>
  <guid>tecunningham.github.io/posts/2017-12-10-unconscious-influences.html</guid>
  <pubDate>Fri, 08 Dec 2017 08:00:00 GMT</pubDate>
</item>
<item>
  <title>The Work of Art in the Age of Mechanical Production</title>
  <link>tecunningham.github.io/posts/2017-09-27-work-of-art-age-mechnical-production.html</link>
  <description><![CDATA[ 





<section id="aka-machine-learning-aesthetics-the-unconscious" class="level2">
<h2 class="anchored" data-anchor-id="aka-machine-learning-aesthetics-the-unconscious">AKA machine learning, aesthetics, &amp; the unconscious</h2>
<ol type="1">
<li><p>When I heard about the neural nets that copy the styles of famous painters I thought it would be the same old junk.</p></li>
<li><p>Academics have been saying forever that they were on the verge of discovering the principles of aesthetics, and that they would soon be able to automate the production of beauty – melody, harmony, proportion, plot.</p></li>
<li><p>When I was a kid I was excited to read about this sort of thing. But they always turn out to be fatuous, catastrophically oversimplified and overconfident, written - I’m guessing - by people who are intimidated &amp; resentful of the culture around them. Our technical understanding of what makes something look good is still weak, and I don’t think it’s improving very fast. I learned to, when I come across an article about art written by a scientist, turn the page.</p></li>
<li><p>But now I think that maybe the automatic production of beauty will arrive soon. The machine learning algorithms work by extrapolating from existing examples, which means that they can produce new examples that fit some pattern (such as the pattern of beauty) without anyone involved having any explicit understanding of what the pattern is or how it can be defined.</p></li>
<li><p>This extrapolation without understanding is what happened in the study of visual perception – i.e.&nbsp;making inferences from images. Our understanding of perception is slowly moving forward, as it has been for centuries, but our ability to automate perception has shot ahead. In the 15th century Leonard da Vinci studied how the light reflected by an object is related to its distance – more distant objects tend to be bluer – these are relationships that we all know unconsciously, but which take a lot of work to dig out, such that we consciously understand them. Psychologists and computer scientists are still discovering things about the physics of light which we all know unconsciously. But computer models which incorporate our explicit knowledge of the physics of light are being thrashed by pure machine-learning models, which are fed a huge databases of pictures and simply extrapolate from what they’ve already seen.[2]</p></li>
<li><p>I think the same basic point is true of aesthetic things. We really struggle trying to explain why we like a picture or dislike a melody, because most of the work is done at an unconscious level. The progress in understanding those principles will probably continue to be slow.</p></li>
<li><p>But now it seems likely to me that, before long, machines will be able to do all these things on demand – play some brand new Mozart, make elegant little drawings of animals, write a pretty good pop song. And the programmer who implements them could be – probably will be – some bozo who has no clue why it works.</p></li>
</ol>
<p>[1] 2017-05: SIGGRAPH video with style transfer - https://www.youtube.com/watch?v=HYhzZ-Abku8</p>
<p>[2] 2017-05: Michael Elad “Deep, Deep Trouble: Deep Learning’s Impact on Image Processing, Mathematics, and Humanity” https://sinews.siam.org/Details-Page/deep-deep-trouble-4</p>


</section>

 ]]></description>
  <guid>tecunningham.github.io/posts/2017-09-27-work-of-art-age-mechnical-production.html</guid>
  <pubDate>Wed, 27 Sep 2017 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Repulsion from the Prior</title>
  <link>tecunningham.github.io/posts/2017-05-26-repulsion-from-the-prior.html</link>
  <description><![CDATA[ 





<p><strong>the shortest version:</strong> <em>contrary to recent reports, I do not think it’s possible for you to be a Bayesian and consistently exaggerate things.</em></p>
<p><img src="https://www.dropbox.com/s/sbodcv65vnzxey4/anni_albers_black_mountain_college.jpg?raw=1" class="img-fluid">{: .center-image }</p>
<section id="short-version" class="level2">
<h2 class="anchored" data-anchor-id="short-version">Short Version</h2>
<ol type="1">
<li>If we think of perception as inference, it has implications about the types of biases we would have.</li>
<li>Yet many biases and illusions seems to go in the exact <em>opposite</em> direction – sometimes called “anti-Bayesian” biases – in particular there are ubiquitous <em>contrast</em> effects, while Bayesian inference seems to imply <em>assimilation</em> effects.</li>
<li>Wei and Stocker (2015) say they can rationalize these contrast effects, under the assumption that our sensory mechanisms are tuned to the environment, such that they are relatively more sensitive to more likely signals. They say that this will imply contrast effects (that the bias is inversely proportional to the slope of the prior).</li>
<li>Yet their results contradict some simple laws of Bayesian inference – the law of iterated expectations, and law of total variance – so there is something odd going on.</li>
<li>(If this explanation doesn’t work, then why do we get repulsion? I think that Ted Adelson explained the basic reason in the 70s. Will write another post on this.)</li>
</ol>
</section>
<section id="shortish-version" class="level2">
<h2 class="anchored" data-anchor-id="shortish-version">Shortish Version</h2>
<ol type="1">
<li><p>Here’s a nice crisp problem: in what cases does inference attract <em>towards</em> the prior, and in what cases does it repulse <em>away</em> from it?</p></li>
<li><p>Given an unknown variable <img src="https://latex.codecogs.com/png.latex?x"> and a signal <img src="https://latex.codecogs.com/png.latex?s">, let’s say that there’s “attraction” at a given value of <img src="https://latex.codecogs.com/png.latex?x"> if the average inferred value of <img src="https://latex.codecogs.com/png.latex?x"> is closer to the prior than <img src="https://latex.codecogs.com/png.latex?x"> itself is –</p>
<p><img src="https://latex.codecogs.com/png.latex?%7CE%5BE%5Bx%7Cs%5D%7Cx%5D-%5Cmu%7C%3C%7Cx-%5Cmu%7C"></p></li>
<li><p>Attraction effects are typically treated as the norm. For example if <img src="https://latex.codecogs.com/png.latex?x"> is drawn from a normal distribution and if <img src="https://latex.codecogs.com/png.latex?s"> is equal to <img src="https://latex.codecogs.com/png.latex?x"> plus normal noise, then you’ll always get attraction to the prior. I.e., if <img src="https://latex.codecogs.com/png.latex?x"> is above the mean, then it’ll be, on average, estimated to be closer to the mean than it actually is.</p></li>
<li><p>However this has sometimes been treated as a puzzle in studies of perception: perception seems like inference, but we also find what look like <em>repulsion</em> effects. For example “contrast” effects, in which an object seems less dark when you put it next to another, darker, object. If we assume that the colour of the neighboring objects affects your prior about the target object, then this would imply an <em>attraction</em> effect. Yet repulsion effects seems to be the norm across all sorts of judgments (lightness, colour, volume, orientation, size), and similar contrast effects occur in time as well as in space (i.e., something seems less dark if it is preceded by something darker) – though of course there are exceptions. These types of illusion are sometimes called “anti-Bayesian.”</p></li>
<li><p>A common explanation of these contrast effects is that we ‘code for differences’ – i.e.&nbsp;that something about our neural wiring causes us to encode <em>differences</em>, rather than <em>levels</em>, and this causes us to exaggerate differences, i.e.&nbsp;get contrast effects.</p></li>
<li><p>But this assumes that we encode the difference and then forget to decode (AKA <strong>coding catastrophe</strong>, AKA the <strong>el Greco fallacy</strong>). If you write down a Bayesian model, which makes its best effort to infer the level from the difference, you typically do <em>not</em> find the desired contrast effects (Schwartz, Hsu &amp; Dayan (2007)).</p></li>
<li><p>Wei and Stocker (2015) announce that they have made a breakthrough – a fully Bayesian model which generates contrast/repulsion effects generically. They say that the key assumption is that we are more sensitive to differences in areas where signals are more likely to fall – i.e., sensitivity is proportional to the density of the prior.</p></li>
<li><p>Formally, let <img src="https://latex.codecogs.com/png.latex?x%5Csim%20f">, and <img src="https://latex.codecogs.com/png.latex?s=F(x)+%5Cvarepsilon."> This means that sensitivity is proportional to the density of the prior – and it implies that <img src="https://latex.codecogs.com/png.latex?s"> will be roughly uniformly distributed – so in some sense it’s an efficient use of signal capacity. Given this setup, and some simplifications, they find that the bias is proportional to the slope of the prior – so if the prior is symmetric &amp; single-peaked then for values above the mean, the bias will be positive, and vice versa – i.e.&nbsp;<em>repulsion</em> away from the prior everywhere.</p></li>
<li><p>In the note below I give a proof that implies that it is impossible to have repulsion effects everywhere – which seems to contradict the results of Wei &amp; Stocker.</p></li>
<li><p>I’m not sure what the source of the contradiction is – it could be either (a) Wei &amp; Stocker’s results are true locally, but do not apply at the tails of the distribution, and so things balance out that way; (b) there is a difference in the implicit assumption used when taking conditional expectations (AKA the Borel-Kolmogorov paradox); or (c) I made a mistake.</p></li>
<li><p>I also mention below a related result, that there cannot be a consistent upward or downward bias (i.e., it cannot be that <img src="https://latex.codecogs.com/png.latex?E%5B%5Chat%7Bx%7D%5C%7Cx%5D%3Ex"> for all <img src="https://latex.codecogs.com/png.latex?x">). This is relevant for Wei &amp; Stocker’s result applied to asymmetric priors – e.g.&nbsp;if the prior is everywhere decreasing – where the result seems to imply a consistent upward bias. </p></li>
</ol>
<p><img src="https://www.dropbox.com/s/hwbeyp8ldrmwysp/rabbit.jpg?raw=1" class="img-fluid">{: .center-image }</p>
</section>
<section id="summary-of-proof" class="level2">
<h2 class="anchored" data-anchor-id="summary-of-proof">Summary of proof</h2>
<ol type="1">
<li>Suppose that there is repulsion from the prior everywhere, i.e.&nbsp;for all <img src="https://latex.codecogs.com/png.latex?x">, <img src="https://latex.codecogs.com/png.latex?%5C%7CE%5B%5Chat%7Bx%7D%5C%7Cx%5D-%5Cmu%5C%7C%3E%5C%7Cx-%5Cmu%5C%7C">.</li>
<li>This implies that <img src="https://latex.codecogs.com/png.latex?Var%5B%5Chat%7Bx%7D%5D%3EVar%5Bx%5D">.</li>
<li>But this contradicts the law of total variance, which says that <img src="https://latex.codecogs.com/png.latex?Var%5BE%5BA%5C%7CB%5D%5D%5Cleq%20Var%5BA%5D">.</li>
</ol>
</section>
<section id="detail" class="level2">
<h2 class="anchored" data-anchor-id="detail">Detail:</h2>
<p>Suppose there are two random variables <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?s">, and let <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bx%7D=E%5Bx%5C%7Cs%5D">. Let <img src="https://latex.codecogs.com/png.latex?x"> be mean-zero, and let’s assume repulsion from the prior everywhere, i.e.&nbsp;for all <img src="https://latex.codecogs.com/png.latex?x">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AE%5B%5Chat%7Bx%7D%7Cx%5D%7C%3E%7Cx%7C%0A"></p>
<p>From this repulsion assumption I think it’s clear that there’s more variance in <img src="https://latex.codecogs.com/png.latex?E%5B%5Chat%7Bx%7D%5C%7Cx%5D"> than in <img src="https://latex.codecogs.com/png.latex?x">:</p>
<p><img src="https://latex.codecogs.com/png.latex?Var%5BE%5B%5Chat%7Bx%7D%7Cx%5D%5D%3EVar%5Bx%5D"></p>
<p>Now let’s apply the law of total variance:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AVar%5BA%5D=&amp;%20E%5BVar%5BA%7CB%5D%5D+Var%5BE%5BA%7CB%5D%5D%20%5C%5C%5C%5C%0AVar%5B%5Chat%7Bx%7D%5D=&amp;%20E%5BVar%5B%5Chat%7Bx%7D%7Cx%5D%5D+Var%5BE%5B%5Chat%7Bx%7D%7Cx%5D%5D%0A%5Cend%7Baligned%7D%0A"></p>
<p>Thus implying that:</p>
<p><img src="https://latex.codecogs.com/png.latex?Var%5B%5Chat%7Bx%7D%5D%5Cequiv%20Var%5BE%5Bx%7Cs%5D%5D%3EVar%5Bx%5D"></p>
<p>Applying the law of total variance again we get:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0AVar%5Bx%5D=&amp;%20E%5BVar%5Bx%7Cs%5D%5D+Var%5BE%5Bx%7Cs%5D%5D%20%5C%5C%5C%5C%0A%20%20%20%20%20%20%3E&amp;%20Var%5Bx%5D%0A%5Cend%7Baligned%7D"></p>
<p>A contradiction.</p>
</section>
<section id="no-consistent-upwarddownward-bias" class="level2">
<h2 class="anchored" data-anchor-id="no-consistent-upwarddownward-bias">No consistent upward/downward bias</h2>
<p>The law of iterated expectations states that, for any <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?B">:</p>
<p><img src="https://latex.codecogs.com/png.latex?E%5BE%5BA%7CB%5D%5D=E%5BA%5D"></p>
<p>This implies that there cannot be a consistent upward or downward bias, i.e.&nbsp;it cannot be true that:</p>
<p><img src="https://latex.codecogs.com/png.latex?E%5B%5Chat%7Bx%7D%7Cx%5D%3Ex,%20%5Cforall%20x"></p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li>Schwartz, Hsu &amp; Dayan (2007, Nature Review Neuro) “Space and Time in Visual Context”</li>
<li>Wei &amp; Stocker (2015, Nature Neuroscience) <strong>“A Bayesian observer model constrained by efficient coding can explain ‘anti-Bayesian’ percepts”</strong></li>
</ul>


</section>

 ]]></description>
  <guid>tecunningham.github.io/posts/2017-05-26-repulsion-from-the-prior.html</guid>
  <pubDate>Fri, 26 May 2017 07:00:00 GMT</pubDate>
</item>
</channel>
</rss>
