<?xml version="1.0" encoding="UTF-8"?>
<rss  xmlns:atom="http://www.w3.org/2005/Atom" 
      xmlns:media="http://search.yahoo.com/mrss/" 
      xmlns:content="http://purl.org/rss/1.0/modules/content/" 
      xmlns:dc="http://purl.org/dc/elements/1.1/" 
      version="2.0">
<channel>
<title>Tom Cunningham</title>
<link>tecunningham.github.io/index.html</link>
<atom:link href="tecunningham.github.io/index.xml" rel="self" type="application/rss+xml"/>
<description>{{&lt; meta description-meta &gt;}}</description>
<generator>quarto-1.3.357</generator>
<lastBuildDate>Mon, 07 Aug 2023 07:00:00 GMT</lastBuildDate>
<item>
  <title>How Much has Social Media affected Polarization?</title>
  <dc:creator>Tom Cunningham</dc:creator>
  <link>tecunningham.github.io/posts/2023-07-27-meta-2020-elections-experiments.html</link>
  <description><![CDATA[ 




<style>
   h1 {  border-bottom: 4px solid black;}
   h2 {  border-bottom: 1px solid gray; padding-bottom: 0px; font-size: 14px; color: black; }
   dl {display: grid;grid-template-columns: max-content auto;}
   dt {grid-column-start: 1;}
   dd {grid-column-start: 2; margin-left: 2em;}
</style>
<p><strong>TL;DR: The experiments run by Meta during the 2020 elections were not big enough to test the theory that social media has made a substantial contribution to polarization in the US. Nevertheless there are other reasons to doubt it.</strong></p>
<section id="summary" class="level1 page-columns page-full">
<h1>Summary</h1>

<div class="no-row-height column-margin column-container"><div class="">
<p><img src="tecunningham.github.io/posts/images/2023-08-04-13-59-52.png" class="img-fluid"> Thanks to Dean Eckles, Solomon Messing, Jeff Allen, &amp; Brandon Silverman for discussion which led to this post. I put together the <a href="https://docs.google.com/spreadsheets/d/1_96kEzP9MFLcBFppVV0Bl7O3Cv9hQFxKArwS2zVCtXE/edit#gid=0">spreadsheet summary of results</a> with Dean and Solomon. See also a <a href="https://statmodeling.stat.columbia.edu/2023/07/27/new-research-on-social-media-during-the-2020-election-and-my-predictions/">post by Dean</a>.</p>
</div></div><div class="page-columns page-full"><p><strong>Three new experiments show that changing Facebook’s feed ranking algorithm for 1.5 months has an effect on affective polarization of less than 0.03 standard deviations.</strong> This is small compared to a growth of 1.1 standard deviations in nationwide affective polarization over the last 40 years.<sup>1</sup></p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;<span class="citation" data-cites="allcott2019trends">Allcott et al. (2019)</span>, see below for discussion of whether these standard deviations are comparable.</p></li></div></div>
<p><strong>Small effects in these experiments are consistent with large effects in aggregate.</strong> The aggregate contribution of social media to polarization will differ from these experimental estimates in a number of ways: depth, breadth, duration, timing, category, and population. My rough attempts to account for these considerations make me think the aggregate effect is likely 10 or 20 times larger than the effects that would be measured in these experiments, and so small effects in these experiments are consistent with large effects on aggregate.</p>
<p>Put simply: these experiments measure the effect of reducing exposure of an individual user (not their friends and family) to political content on Facebook by 15% for 1.5 months, and occurred in a period after Facebook had already sharply reduced the amount of partisan content circulating. Thus we should expect them to measure only a small fraction of the cumulative impact of social media, and in fact these results are consistent with social media being <em>entirely</em> responsible for the growth of polarization in the US.</p>
<p><strong>Nevertheless other evidence implies that social media has probably not made a huge contribution to US polarization.</strong> If we wish to evaluate the balance of evidence relating social media to polarization there are many other sources which are probably more informative than these experiments. I give a rough sketch below and it seems to me social media probably does not account for a majority share, mainly because (1) polarization had been growing for 20 years prior to social media’s introduction, and much of the growth since 2014 was in people without internet access; (2) a lot of partisan discourse continues to spread outside of social media, e.g.&nbsp;through cable TV and talk radio; (3) other countries do not show a similar increase in affective polarization.</p>
<p><strong>Discussion of these results has been distressingly non-quantitative.</strong> The majority of discussion of these results (in papers, editorials, on Twitter) has been about whether these changes “have an effect” or “do not have an effect.” Interpreted sympathetically these statements are compressed ways of saying “an effect larger than 0.03 standard deviations.” However I think taking this shortcut so consistently has led to far too little time thinking about what we have learned from these experiments that we didn’t already know, and what is the balance of evidence regarding the effects of social media. I give a lot of examples below.</p>
</section>
<section id="the-experiments" class="level1 page-columns page-full">
<h1>The Experiments</h1>
<div class="page-columns page-full"><p><strong>Last week’s papers reported the results of three experiments on Facebook’s News Feed.</strong> The experiments (<span class="citation" data-cites="guess2023chronological">Guess et al. (2023b)</span>, <span class="citation" data-cites="guess2023reshares">Guess et al. (2023a)</span>, <span class="citation" data-cites="nyhan2023likeminded">Nyhan et al. (2023)</span>) were run between September and December 2020, and half-way through participants were asked about their feelings towards members of their own party and the opposing party, e.g.&nbsp;<em>“how warm do you feel about Republicans on a scale of 0-100?”</em><sup>2</sup> The answers were aggregated to make an index of “affective polarization”: <img src="https://latex.codecogs.com/png.latex?%5Cxymatrix@R=0em@C=6em%7B%0A%20%20%20%20%20%20*+%5BF:%3C5pt%3E%5D%5Ctxt%7Brank%20items%20on%20News%5C%5CFeed%20chronologically%7D%20%20%5Car%5Bdr%5D%20&amp;%20%5C%5C%0A%20%20%20%20%20%20*+%5BF:%3C5pt%3E%5D%5Ctxt%7Bremove%20reshares%5C%5Con%20News%20Feed%7D%20%20%5Car%5Br%5D%20&amp;%0A%20%20%20%20%20%20%20%20%20*+%5BF:%3C5pt%3E%5D%5Ctxt%7Baffective%5C%5Cpolarization%5C%5Csurvey%7D%5C%5C%0A%20%20%20%20%20%20*+%5BF:%3C5pt%3E%5D%5Ctxt%7Bdownrank%20likeminded%5C%5Citems%20on%20News%20Feed%7D%20%20%5Car%5Bur%5D%0A%20%20%20%20%20%20%7D%0A%20%20%20"></p><div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;Although the treatments ran for 3 months (24 Sep–23 Dec 2020), the survey responses were collected during the experiment and the average survey measure was measured after around 1.5 months of treatment: see Figure S2 in the Supplementary Appendix.</p></li></div></div>
<p><strong>All three experiments found effects on polarization of less than 0.03 standard deviations (SDs).</strong> The 95% confidence intervals on affective polarization are approximately <img src="https://latex.codecogs.com/png.latex?%5Cpm"> 0.03 SDs, and the effect-sizes are all smaller than that (i.e.&nbsp;they do not estimate a significant effect). Dean Eckles, Solomon Messing, and myself put together a <a href="https://docs.google.com/spreadsheets/d/1_96kEzP9MFLcBFppVV0Bl7O3Cv9hQFxKArwS2zVCtXE/edit#gid=0">spreadsheet summary</a> of the results from all the experiments reported so far, along with other results from the literature on political effects of media.</p>
<p>They also measured effects on a number of other off-platform outcomes: removing reshares did lower news knowledge by 0.07 standard deviations, but all other outcomes (factual discernment, issue polarization, perceived legitimacy, self-reported turnout) were not significant, and had similar-sized confidence intervals.</p>
<p></p>
<p></p>
</section>
<section id="extrapolating-to-the-cumulative-effect-of-social-media" class="level1 page-columns page-full">
<h1>Extrapolating to the Cumulative Effect of Social Media</h1>
<div class="page-columns page-full"><p><strong>Many people have interpreted these results as implying that social media has not had much effect on overall polarization.</strong> E.g. one of the experimental papers says:<sup>3</sup></p><div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;<span class="citation" data-cites="guess2023chronological">Guess et al. (2023b)</span></p></li></div></div>
<blockquote class="blockquote">
<p>“these findings suggest that social media algorithms may not be the root cause of phenomena such as increasing political polarization.”</p>
</blockquote>
<p><strong>Here I try to extrapolate from these experiments to the long-run aggregate effect of social media.</strong> The comparison is between two extremes but there are a lot of other intermediate estimands that we could alternatively use, e.g.&nbsp;the effect of permanantly disabling just Facebook for everybody, or the effect of temporarily disabling all social networks for an individual user.</p>
<p>These are difficult judgment calls. I have tried my best to be neutral and discuss evidence on either side but it’s likely I’m forgetting some important considerations.</p>
<p><strong>I work through five ways in which the experimental results will differ from the aggregate impact on social media:</strong></p>
<ol type="1">
<li><strong>Depth.</strong> Whether changing one feature or disabling the app entirely.</li>
<li><strong>Breadth.</strong> Whether changing the experience for one user or for all users.</li>
<li><strong>Duration.</strong> Whether changing the experience for 1.5 months or for the whole history of social media.</li>
<li><strong>Timing.</strong> Whether changing the experience in Oct 2020, or the average effect over 2004-2020.</li>
<li><strong>Category.</strong> Whether changing the experience just for Facebook or for all social media.</li>
<li><strong>Population.</strong> Whether we are estimating the effect for all US adults or just Facebook users.</li>
</ol>
<div class="page-columns page-full"><p>I try to give quantitative estimates for each of these five differences, and it makes me think that having tight confidence intervals on the effects of the experiments (plus or minus 0.03 SDs) is still consistent with the aggregate effect of social media being having an effect as large as 1 SD or more.<sup>4</sup></p><div class="no-row-height column-margin column-container"><li id="fn4"><p><sup>4</sup>&nbsp;I am considering the effect-size rather than the uncertainty, you could separately do a similar exercise to propagate up the uncertainty, and I think this would make the experiments seem even more under-powered to measure the aggregate effect.</p></li></div></div>
<p><strong>(1) <em>Depth</em>: the experiments have small effects on exposure.</strong> Each of the experiments reported have effects on overall Facebook time-spent of less than 25%, and on exposure to political material of less than 15%. Thus the effect of complete withdrawal from Facebook seems likely to be at least 2X larger than measured by any of these experiments. The most natural causal path from Facebook use to polarization is exposure to partisan or misleading political media. An additional experiment was run which deactivated peoples’ accounts but the results from that experiment are not yet public (as of August 4).</p>
<table class="table">
<thead>
<tr class="header">
<th>effects on metric<sup>5</sup></th>
<th>time-spent</th>
<th>political impression</th>
<th>cross-cutting impressions</th>
<th>untrustworthy impressions</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>baseline</td>
<td>?</td>
<td>14pp</td>
<td>21pp</td>
<td>3pp</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>- rank chronologically</td>
<td>-21%</td>
<td>+12%</td>
<td>-10%</td>
<td>+60%</td>
</tr>
<tr class="even">
<td>- remove reshares</td>
<td>-5%</td>
<td>-14%</td>
<td>-3%</td>
<td>-32%</td>
</tr>
<tr class="odd">
<td>- downrank likeminded posts</td>
<td>-1%</td>
<td>-5%</td>
<td>+7%</td>
<td>?</td>
</tr>
</tbody>
</table>
<div class="no-row-height column-margin column-container"><li id="fn5"><p><sup>5</sup>&nbsp;<a href="https://docs.google.com/spreadsheets/d/1_96kEzP9MFLcBFppVV0Bl7O3Cv9hQFxKArwS2zVCtXE/edit#gid=0">source data</a>.</p></li></div><p></p>
<p><strong>(2) <em>Breadth</em>: Experiments exclude network effects.</strong> The effects of social media on polarization likely work not just through direct exposure but also downstream via peoples’ interactions with friends and families, in both online and offline conversations. Thus it seems likely the aggregate effect could be 2X or larger than the individual effect.</p>
<div class="page-columns page-full"><p><strong>(3) <em>Duration</em>: the experiments only measure short-run effects.</strong> These experiments measured the effect of a News Feed change on polarization after around 1.5 months, while most American adults have been using Facebook for perhaps 10 years.<sup>6</sup> It is hard to judge how quickly we should expect polarization attitudes to respond to treatment, and I have not found useful academic literature. The national polarization trends documented in <span class="citation" data-cites="allcott2019trends">Allcott et al. (2019)</span> seem fairly stable despite a volatile news cycle suggesting attitudes change relatively slowly. If the half-life of adjustment was 1.5 months (which seems quite short to me) then the effects measured in these experiments would be half of the long-run effect. It seems likely that the effects of exposure do not decay at a constant rate: there is a short-run component that decays quickly (the effect of salience), and a long-run component that decays slowly.<sup>7</sup></p><div class="no-row-height column-margin column-container"><li id="fn6"><p><sup>6</sup>&nbsp;The polarization survey measures were collected in wave 3, 4, and 5. From eyeballing Figure S3 (and assuming the response rate declines over time) it appears the average response would be collected around 1.5 months after the treatment began: <img src="tecunningham.github.io/posts/images/2023-08-07-07-34-48.png" class="img-fluid">.</p></li><li id="fn7"><p><sup>7</sup>&nbsp;The only dynamic effects discussion I could find was wave-by-wave results for survey questions in the supplementary appendices. I think it would be useful to show the dynamic effects for on-platform behavior because (1) there is good reason to expect the cumulative treatment effects will dramatically change over time, (2) the experiments are sufficiently well-powered that dynamic effects should be easy to observe.</p></li></div></div>
<p><span class="citation" data-cites="guess2023chronological">Guess et al. (2023b)</span> notes this limitation:</p>
<blockquote class="blockquote">
<p>“It is possible that such downstream effects require a more sustained intervention period … although our approximately 3-month study had a much longer duration than that of most experimental research in political communication.”</p>
</blockquote>
<p>Although if I understand correctly this is slightly misleading: the outcome variables were measured after 1.5 months exposure, not 3 months.</p>
<p><strong>(4) <em>Timing</em>: Experiments were run during the lead-up to the 2020 election.</strong> The experiments ran between September and November 2020. If we compare this to the average experience on Facebook over the previous decade there are reasons to expect relatively smaller effects on polarization:</p>
<ul>
<li><p>Following the 2016 elections facebook invested very heavily in integrity systems reducing prevalence of many types of bad content by factors of between 2X and 10X, especially misinformation and hyperpartisan political content. In May 2020 Guy Rosen <a href="https://about.fb.com/news/2020/05/investments-to-fight-polarization/">claimed</a> that Facebook had made “a number of important steps to reduce the amount of content that could drive polarization on our platform” over the prior years.</p></li>
<li><p><span class="citation" data-cites="allcott2019trends">Allcott et al. (2019)</span> estimates that exposure to misinformation on Facebook (measured by data on engagement with domains known to host misinformation) grew over 2015 and 2016, roughly doubling, then fell over 2017 and 2018, roughly halving. The data ends at the end of 2018 but I believe the trend would continue downward. <img src="tecunningham.github.io/posts/images/2023-08-02-16-19-16.png" class="img-fluid"></p></li>
<li><p>Meta’s Community Standards reports show a decline in prevalence of most types of harmful content by a factor of between 2 and 5 over roughly 2017 to 2022 (see chart <a href="https://tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data.html#meta-facebook-instagram">here</a>).</p></li>
<li><p>Prior to and during the 2020 election Facebook implemented a series of extra “break the glass” measures with the effect of suppressing extreme or fringe political content.</p></li>
<li><p>During election seasons there tends to be significantly more political content circulating. This might mean that Facebook would have a relatively larger impact on polarization in this period. However if the influence of social media on polarization depends on the <em>share</em> of exposure to partisan or polarizing content (rather than the level) then the effect would be the same in election season as outside election season.</p></li>
</ul>
<p>In fact 2020 Facebook has further reduced the prevalence of political and fringe content since 2020:</p>
<ul>
<li>The share of politics on News Feed was reduced by 50%.<sup>8</sup></li>
<li>Prevalence of hate-speech fell by factor of 5 between 2020 and 2022 (from 0.1% to 0.02%). (<a href="https://transparency.fb.com/data/community-standards-enforcement/hate-speech/facebook/">ref</a>).</li>
<li>Engagement on US right-wing politics pages has fallen by factor of 4 from 2021-2022 (<a href="https://fwiwnewsletter.substack.com/p/has-facebook-dialed-down-the-conservative">ref</a>).</li>
<li>Prevalence of engagement-bait among the top 20 most-viewed posts went from 100% to 5% between 2021Q3 to 2022Q3 (<a href="https://www.wsj.com/articles/facebooks-most-popular-posts-were-trash-here-is-how-it-cleaned-up-11669140034">ref</a>).</li>
</ul>
<div class="no-row-height column-margin column-container"><li id="fn8"><p><sup>8</sup>&nbsp;<a href="https://www.wsj.com/articles/facebook-politics-controls-zuckerberg-meta-11672929976">The WSJ</a> reported that in late 2021 <em>“Mr.&nbsp;Zuckerberg and the board chose the most drastic [option], instructing the company to demote posts on “sensitive” [(politics and health)] topics as much as possible in the newsfeed that greets users when they open the app”</em>, and that in 2022 <em>“politics accounts for less than 3% of total content views in users’ newsfeed, down from 6% around the time of the 2020 election.”</em> The article reports that these experiments reduced daily visitation (daily active users) by 0.2%.</p></li></div><p><strong>(5) <em>Category</em>: The experiments affected only Facebook, but in 2020 Facebook probably accounted for around 1/4 of all partisan political content that people are exposed to on social media.</strong> If we include YouTube, TikTok, Instagram, Twitter, Snapchat, Reddit, and the long tail of niche social networks. In contrast, if we are estimating the cumulative effect (2004-2020) then Facebook would likely comprise a significantly larger share of exposure political content.</p>
<div class="page-columns page-full"><p><strong>(6) <em>Population</em>: The experiments measure outcomes only on Facebook users.</strong> I believe that the “population average treatment effects” reported in the papers are weighted to match the Facebook-using population, not the voting population. This would be a reason for the experimental effect-size to be larger than the aggregate effect. I would guess around 2/3 of the US adult population is active once/month on Facebook, and so the aggregate effect-size could be smaller by that factor.<sup>9</sup></p><div class="no-row-height column-margin column-container"><li id="fn9"><p><sup>9</sup>&nbsp;The supplement to the Science paper mentions Facebook had 231 million monthly active users.</p></li></div></div>
<p><strong>Putting it all together.</strong> If factors 1-5 each contributed a 2X amplification, as well as factor 6 contributing a 2/3 shrinkage, then the cumulative effect of social media on polarization until 2020 would be 20X larger than the experimentally-measured effect, i.e.&nbsp;effective confidence intervals would be 0.6 SDs instead of 0.03 SDs. In other words these experiments would not be sufficiently well-powered to rule out social media being responsible for the <em>entire</em> growth of polarization since 2014.</p>
<p><strong>Note: adjusting for standard deviation size.</strong> The papers all report effect sizes on affective polarization in units of standard deviations. I wasn’t sure whether these are standard-deviations of the cross-sectional variance, or the residual variance after controlling for pre-treatment values. If the latter then I would guess they are perhaps half the size of the cross-section SD, based on my professional experience (and Supplementary Appendix page S-140). If correct this would halve the estimated effect-size when expressed in cross-sectional standard deviations, i.e.&nbsp;it would close the gap by a factor of 2.</p>
</section>
<section id="other-evidence-on-media-and-polarization" class="level1">
<h1>Other Evidence on Media and Polarization</h1>
<p>Here is a rough sketch of the evidence of related to affective polarization. I do not consider myself an expert on this literature and I would love corrections or additions. On balance this evidence seems to imply that social media hasn’t been the primary contributor to US affective polarization, but I think a thorough analysis of this evidence would be really valuable.</p>
<dl>
<dt>News Sources</dt>
<dd>
From Pew data I would guess social media is around 25% of all exposure to political news, probably a higher share of exposure to partisan political news. Cable TV and political talk radio probably account for similar shares of overall exposure to partisan media. This seems the strongest evidence that social media is not the primary driver of affective polarization.
</dd>
<dt>Professional Opinion</dt>
<dd>
The political science literature talks about “the paradox of minimal effects” and the economics-of-media literature generally seems to have a consensus that most persuasive effects of media are <a href="https://tecunningham.github.io/posts/2023-08-02-small-effects.html">small</a>. However this might just apply to marginal effects.
</dd>
<dt>Other Experiments</dt>
<dd>
<span class="citation" data-cites="allcott2020welfare">Allcott et al. (2020)</span> is often interpreted as finding an effect on affective polarization but it does not (see below). <span class="citation" data-cites="broockman2022crosscutting">Broockman and Kalla (2022)</span> finds a null effect. I don’t know of other good experiments on affective polarization.
</dd>
<dt>National Trends</dt>
<dd>
In the US affective polarization steadily grew 1978-2020, for a total of 1.1 SD over 40 years. Other countries do not show a consistent trend, and there is no clear connection with internet access or online news consumption.
</dd>
<dt>Demographic Trends</dt>
<dd>
Over 1996-2012 affective polarization grew the most in groups who have not increased their internet access. I’m not aware of more recent data.
</dd>
</dl>
<section id="discussion" class="level2">
<h2 class="anchored" data-anchor-id="discussion">Discussion</h2>
<p><strong>Trends in affective polarization.</strong> <span class="citation" data-cites="boxell2022PolarizationTrends">Boxell et al. (2022)</span> document affective polarization across a dozen countries, 1978-2020:</p>
<p><img src="tecunningham.github.io/posts/images/2023-07-27-15-03-32.png" class="img-fluid"></p>
<ol type="1">
<li><p>In the US affective polarization index increased from around 25 to 50, <em>“an increase of 1.08 standard deviations as measured in the 1978 distribution.”</em> (I’m not sure if the SD increased).</p></li>
<li><p>Across the world there’s no clear trend: some countries increased, other countries decreased. This weakens the simple argument that polarization has increased at the same time as social media use.</p></li>
<li><p>In the US the trend seems to be almost entirely due to increasing negative feelings about the opposing party:</p>
<p><img src="tecunningham.github.io/posts/images/2023-08-03-12-51-42.png" class="img-fluid"></p></li>
</ol>
<p>The US timeseries can be seen <a href="https://electionstudies.org/data-tools/anes-guide/anes-guide.html?chart=affective_polarization_parties">online</a> from the <a href="https://electionstudies.org">ANES</a>.</p>
<p><strong>Observational data finds that much of the growth in polarization in the US was among people who were not online.</strong> <span class="citation" data-cites="boxell2017greater">Boxell et al. (2017)</span> say</p>
<blockquote class="blockquote">
<p>“the growth in polarization in recent years [1996-2012] is largest for the demographic groups least likely to use the internet and social media”</p>
</blockquote>
<p><strong>Content on Meta platforms.</strong> <span class="citation" data-cites="guess2023chronological">Guess et al. (2023b)</span> has data from the control group in their 2020 experiments:</p>
<table class="table">
<thead>
<tr class="header">
<th>Share of Impressions</th>
<th>Facebook</th>
<th>Instagram</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Political content</td>
<td>14%</td>
<td>5%</td>
</tr>
<tr class="even">
<td>Political news content</td>
<td>6%</td>
<td>-</td>
</tr>
<tr class="odd">
<td>Content from untrustworthy sources</td>
<td>3%</td>
<td>1%</td>
</tr>
<tr class="even">
<td>Uncivil content</td>
<td>3%</td>
<td>2%</td>
</tr>
</tbody>
</table>
<p><a href="https://www.pewresearch.org/journalism/fact-sheet/news-platform-fact-sheet/?tabId=tab-4ef8dece-845a-4b25-8637-ceb3114503c5">Pew 2022</a> has data on where people get their news from:</p>
<table class="table">
<thead>
<tr class="header">
<th></th>
<th>pct adults regularly get news from</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>television</td>
<td>65%</td>
</tr>
<tr class="even">
<td>news websites</td>
<td>63%</td>
</tr>
<tr class="odd">
<td>search</td>
<td>60%</td>
</tr>
<tr class="even">
<td>social media</td>
<td>50%</td>
</tr>
<tr class="odd">
<td>radio</td>
<td>47%</td>
</tr>
<tr class="even">
<td>print</td>
<td>33%</td>
</tr>
<tr class="odd">
<td>podcasts</td>
<td>23%</td>
</tr>
</tbody>
</table>
<p><strong>Radio show popularity.</strong> Around half of the top 20 most-listened radio shows in the US are conservative talk, with around 90M weekly listeners (this is double-counting overlapping users). <a href="https://en.wikipedia.org/wiki/List_of_most-listened-to_radio_programs">Data from 2021</a>.</p>
<p><strong>Television.</strong> Fox News is Cable TV’s most-watched network with around 5M regular viewers. (<a href="http://www.adweek.com/tvnewser/2016-ratings-fox-news-channel-is-cable-tvs-most-watched-network/315009">source from 2016</a>).</p>
<p><strong>Time spent on social media.</strong> <a href="https://www.statista.com/statistics/433871/daily-social-media-usage-worldwide/">Statista</a>: Average time-spent 150 minutes/day/person on social networks</p>
<p><strong>The academic literature has identified other possible causes of polarization.</strong> Some potential causes: southern realignment, 1968 changes to the primary system, the Obama presidency, the tea party movement (though each of these could be in part proximal causes). Martin &amp; Yurcoglu (2017) argue that a large part of recent growth is due to cable news: &gt; “the cable news channels can explain an increase in political polarization of similar size to that observed in the US population over [2000-2008]. … In absolute terms, however, this increase is fairly small.”</p>
<p>See also Haidt and Bail’s long document <a href="https://docs.google.com/document/d/1vVAtMCQnz8WVxtSNQev_e1cGmY9rnY96ecYuAj6C548/edit#heading=h.96bogdklzo1j">Social Media and Political Dysfunction: A Collaborative Review</a></p>
<p><strong>Does <span class="citation" data-cites="allcott2020welfare">Allcott et al. (2020)</span> find that Facebook use increases polarization?</strong> This paper reports on an experiment paying people to stop using Facebook for a month. They find an effect of -0.16 SDs (<img src="https://latex.codecogs.com/png.latex?%5Cpm"> 0.08) on a measure they describe as “political polarization,” however there are some subtleties:</p>
<p><img src="tecunningham.github.io/posts/images/2023-08-07-09-12-41.png" class="img-fluid"></p>
<ol type="1">
<li><p>Unlike the questions used in typical population surveys the questions were explicitly about their feelings during the period of the experiment, e.g.&nbsp;<em>“Thinking back over the last 4 weeks, how warm or cold did you feel towards the parties and the president on the feeling thermometer?”</em></p></li>
<li><p>Polarization is measured by a composite of different measures. By far the largest effect was on the “congenial news exposure” question: <em>“over the last 4 weeks how often did you see news that made you better understand the point of view of the Democrat (Republican) party?”</em> The score was the difference between the answer for their own party vs the other-side party. It seems to me that it’s not surprising that deactivating Facebook would affect one’s exposure to such news, but that this wouldn’t normally be called a measure of “polarization” in the literature. The paper mentions in a footnote that <em>“the effect on the political polarization index is robust to excluding each of the seven individual component variables,”</em> but it turns out that removing “congenial news exposure” halves the effect-size and shifts the p-value from 0.00 to 0.09 (i.e.&nbsp;from very significant to non-significant). I’m not sure I would describe this as a finding that is “robust”.</p></li>
<li><p>The paper finds no significant effect on their two “affective polarization” measures (-0.08 <img src="https://latex.codecogs.com/png.latex?%5Cpm"> 0.08 SD, and 0 <img src="https://latex.codecogs.com/png.latex?%5Cpm"> 0.04 SD), however the 2020 papers which cite <span class="citation" data-cites="allcott2020welfare">Allcott et al. (2020)</span> seem to treat it as finding that Facebook has a positive effect on “polarization” without noting that it has a null effect on <em>affective</em> polarization.</p></li>
</ol>
</section>
</section>
<section id="quantitative-vs-qualitative-description-of-results" class="level1 page-columns page-full">
<h1>Quantitative vs Qualitative Description of Results</h1>
<div class="page-columns page-full"><p><strong>Throughout these papers and in the public discussion the findings have been described in <em>qualitative</em> terms:</strong> i.e.&nbsp;either as “positive,” “negative,” or “neutral.” Implicitly these terms are referring to whether the results are statistically-significant (p&lt;0.05), which depends on whether the effect-size is bigger than the confidence interval. These statements only make sense given some implicit understanding of how broad the confidence intervals are, yet I do not think that implicit understanding exists: I’m fairly confident that most people reading these statements (and many people making them) do not know quantitatively what the thresholds are.<sup>10</sup></p><div class="no-row-height column-margin column-container"><li id="fn10"><p><sup>10</sup>&nbsp;It’s worth stating that all of these treaments will have <em>some</em> non-zero effect, so it’s never literally correct to say “this treatement has no effect on polarization,” it can only be understood as a roundabout way of saying “this treatment has a small effect” for some definition of “small”.</p></li></div></div>
<ol type="1">
<li><p><strong>Titles and abstracts used qualitative descriptions.</strong> The titles and abstracts all used qualitative language, e.g.&nbsp;“did not reduce” or “did not significantly affect” or “had no measurable effects.” None of the abstracts of the papers gave information on the size of the effects that were ruled out.</p></li>
<li><p><strong>Hypotheses used qualitative descriptions.</strong> The pre-analysis plans contained a series of hypotheses, e.g.:</p>
<blockquote class="blockquote">
<p>H1: Decreased exposure to content shared by like-minded friends, Pages, and groups decreases affective polarization.</p>
</blockquote>
<blockquote class="blockquote">
<p>H1: Reverse chronological feed will reduce polarization and negative perceptions of out-groups.</p>
</blockquote>
<p>The terms “decrease” and “reduce” are presumably implicitly referring to the width of the confidence intervals, but I could find no discussion of how much .</p></li>
<li><p><strong>Public discussion used qualitative descriptions.</strong> Almost all discussion in editorials and on Twitter described the results in qualitative terms, whether there was an effect or not, not in quantitative terms.</p></li>
<li><p><strong>Elicitation of priors used qualitative descriptions.</strong> I was at an SSRC conference a few days before the results were released and there was a poll taken to predict the results. For “polarization” the options were (as I recall) “no effect”, “small increase”, “substantial increase”, etc., where I believe “increase” was intended to be interpreted as “statistically significant increase.” However as I recall we were not told the width of the confidence intervals when asked to make predictions. I think this is a bad way of eliciting priors: whether something is significant depends on the width of the confidence intervals as much as the effect-size. Thus an equivalent way of phrasing the question would be “do you think these experiments are sufficiently powered?”<sup>11</sup></p></li>
<li><p><strong>Power calculations used qualitative descriptions.</strong></p>
<p><span class="citation" data-cites="guess2023chronological">Guess et al. (2023b)</span> said that there was sufficient power to detect “small” effects, without explaining why they regarded 0.03 SD as small.<sup>12</sup> The supplement and pre-analysis plan do not mention “power” or seem to discuss the quantitative interpretation of these effect-sizes.</p>
<p>They do cite a previous paper:</p>
<blockquote class="blockquote">
<p>“In all cases, we could rule out effect sizes smaller than those found in previous research [citation to Allcott 2020]</p>
</blockquote>
<p>However I beleive this is a misinterpretation: <span class="citation" data-cites="allcott2020welfare">Allcott et al. (2020)</span> does test for affective polarization but they find a non-significant effect. As discussed above that paper reports a significant effect for “polarization” but the significance is due solely to the response to asking people about “congenial news exposure” over the last 4 weeks, which I think is quite different from polarization.</p>
<p>The Supplementary Appendix to <span class="citation" data-cites="guess2023chronological">Guess et al. (2023b)</span> says the sample size was chosen to detect an effect size of 1.5 percentage points in vote choice (p.&nbsp;S-139), however it is not clear why this effect size was chosen.</p>
<p><span class="citation" data-cites="guess2023reshares">Guess et al. (2023a)</span> says explicitly that they (or someone) expected a significant effect:</p>
<blockquote class="blockquote">
<p>“Contrary to expectations, the treatment does not significantly affect political polarization or any measure of individual-level political attitudes.”</p>
</blockquote>
<p>However I did not find a discussion of why they expected an effect of that size.</p></li>
</ol>
<div class="no-row-height column-margin column-container"><li id="fn11"><p><sup>11</sup>&nbsp;A similar phenomenon occurs in forecasting: if someone asks you a question for which you miss crucial context like “what is the chance of the Grockles winning the Kaplooey cup?” then you can give a good answer but it will be based on your judgment of the person asking the question, not your judgment about the substance of the question itself.</p></li><li id="fn12"><p><sup>12</sup>&nbsp;“The large samples … allowed for adequate statistical power to detect small effects (for example, for affective polarization, we were powered to detect population average treatment effects with Cohen’s d = 0.032 or larger for both Facebook and Instagram).”</p></li></div><p><strong>Good quantitative work.</strong> Some of the authors of these 2020 papers have written other papers which I think use a much more useful approach: they use observational data, are quite focussed on <em>quantitative</em> outcomes, and they perform back-of-the-envelope calculations to reconcile evidence from different sources, e.g. <span class="citation" data-cites="boxell2022PolarizationTrends">Boxell et al. (2022)</span>, <span class="citation" data-cites="boxell2017greater">Boxell et al. (2017)</span>, <span class="citation" data-cites="allcott2017social">Allcott and Gentzkow (2017)</span>.</p>



</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-allcott2020welfare" class="csl-entry">
Allcott, H., Braghieri, L., Eichmeyer, S., Gentzkow, M., 2020. The welfare effects of social media. American Economic Review 110, 629–676.
</div>
<div id="ref-allcott2017social" class="csl-entry">
Allcott, H., Gentzkow, M., 2017. Social media and fake news in the 2016 election. Journal of economic perspectives 31, 211–236.
</div>
<div id="ref-allcott2019trends" class="csl-entry">
Allcott, H., Gentzkow, M., Yu, C., 2019. Trends in the diffusion of misinformation on social media. Research &amp; Politics 6, 2053168019848554.
</div>
<div id="ref-boxell2022PolarizationTrends" class="csl-entry">
Boxell, L., Gentzkow, M., Shapiro, J.M., 2022. <span class="nocase">Cross-Country Trends in Affective Polarization</span>. The Review of Economics and Statistics 1–60. <a href="https://doi.org/10.1162/rest_a_01160">https://doi.org/10.1162/rest_a_01160</a>
</div>
<div id="ref-boxell2017greater" class="csl-entry">
Boxell, L., Gentzkow, M., Shapiro, J.M., 2017. Greater internet use is not associated with faster growth in political polarization among US demographic groups. Proceedings of the National Academy of Sciences 114, 10612–10617.
</div>
<div id="ref-broockman2022crosscutting" class="csl-entry">
Broockman, D., Kalla, J., 2022. Consuming cross-cutting media causes learning and moderates attitudes: A field experiment with fox news viewers. <a href="https://doi.org/10.31219/osf.io/jrw26">https://doi.org/10.31219/osf.io/jrw26</a>
</div>
<div id="ref-guess2023reshares" class="csl-entry">
Guess, A.M., Malhotra, N., Pan, J., Barberá, P., Allcott, H., Brown, T., Crespo-Tenorio, A., Dimmery, D., Freelon, D., Gentzkow, M., 2023a. Reshares on social media amplify political news but do not detectably affect beliefs or opinions. Science 381, 404–408.
</div>
<div id="ref-guess2023chronological" class="csl-entry">
Guess, A.M., Malhotra, N., Pan, J., Barberá, P., Allcott, H., Brown, T., Crespo-Tenorio, A., Dimmery, D., Freelon, D., Gentzkow, M., González-Bailón, S., Kennedy, E., Kim, Y.M., Lazer, D., Moehler, D., Nyhan, B., Rivera, C.V., Settle, J., Thomas, D.R., Thorson, E., Tromble, R., Wilkins, A., Wojcieszak, M., Xiong, B., Jonge, C.K. de, Franco, A., Mason, W., Stroud, N.J., Tucker, J.A., 2023b. How do social media feed algorithms affect attitudes and behavior in an election campaign? Science 381, 398–404. <a href="https://doi.org/10.1126/science.abp9364">https://doi.org/10.1126/science.abp9364</a>
</div>
<div id="ref-nyhan2023likeminded" class="csl-entry">
Nyhan, B., Settle, J., Thorson, E., Wojcieszak, M., Barberá, P., Chen, A.Y., Allcott, H., Brown, T., Crespo-Tenorio, A., Dimmery, D., Freelon, D., Gentzkow, M., González-Bailón, S., Guess, A.M., Kennedy, E., Kim, Y.M., Lazer, D., Malhotra, N., Moehler, D., Pan, J., Thomas, D.R., Tromble, R., Rivera, C.V., Wilkins, A., Xiong, B., Jonge, C.K. de, Franco, A., Mason, W., Stroud, N.J., Tucker, J.A., 2023. Like-minded sources on facebook are prevalent but not polarizing. Nature 620, 137–144. <a href="https://doi.org/10.1038/s41586-023-06297-w">https://doi.org/10.1038/s41586-023-06297-w</a>
</div>
</div></section></div> ]]></description>
  <guid>tecunningham.github.io/posts/2023-07-27-meta-2020-elections-experiments.html</guid>
  <pubDate>Mon, 07 Aug 2023 07:00:00 GMT</pubDate>
</item>
<item>
  <title>The Paradox of Small Effects</title>
  <dc:creator>Tom Cunningham</dc:creator>
  <link>tecunningham.github.io/posts/2023-08-02-small-effects.html</link>
  <description><![CDATA[ 




<style>
    h1 {  border-bottom: 4px solid black;}
    h2 {  border-bottom: 1px solid gray; padding-bottom: 0px; font-size: 14px; color: black; }
</style>
<p>In summary:</p>
<ol type="1">
<li><strong>Attitudes are hard to change.</strong> Many fields in social science have adopted a doctrine of “small effects”: high quality studies tend to show that peoples’ attitudes are not very sensitive to exposure to media, or to their peers’ attitudes.</li>
<li><strong>Yet attitudes do change.</strong> We see very wide society-level variation in attitudes, which are hard to explain without peer or media effects.</li>
<li><strong>Resolution of the paradox: each effect is small, but there are a lot of them.</strong></li>
</ol>
<p>(see an earlier <a href="https://www.facebook.com/tom.cunningham.374549/posts/pfbid022GaqAxUuKobKyS6soWnQVZYevPxkGxQ6BxAiScmA47eZdU9RAJPpGi1NrXQip6Jyl">Facebook post</a>)</p>
<section id="attitudes-are-hard-to-change" class="level1">
<h1>(1) Attitudes are Hard to Change</h1>
<p>Many fields in social science tend to say that attitudes show little influence from either peer effects or from media exposure:</p>
<ul>
<li><p><span class="citation" data-cites="angrist2014perils">Angrist (2014)</span> says studies of peer effects <em>“have mostly uncovered little in the way of socially significant causal effects.”</em></p></li>
<li><p>Political scientists talk about “the paradox of minimal effects”, <span class="citation" data-cites="ansolabehere2006paradox">Ansolabehere (2006)</span> says that election campaigns <em>“seem to be inessential to understanding who wins and who loses.”</em></p></li>
<li><p>David Stromberg says <em>“the lesson from the last 50 years of media research is that it is very hard to manipulate voters … evidence of [supply side bias] effects is weak or non-existent”</em></p></li>
</ul>
<p>There are many studies which find large effects but they tend to be treated with extreme skepticism by the methodologists: they are overwhelmingly from lab experiments or observational data and so can be very biased.</p>
</section>
<section id="attitudes-do-change" class="level1">
<h1>(2) Attitudes do Change</h1>
<p><strong>Attitudes vary a huge amount across time and space:</strong></p>
<ul>
<li>Variation in political and religious attitudes.</li>
<li>Variation in attitudes towards other races, sexes, sexualities, religions.</li>
<li>Variation in preferences over food, e.g.&nbsp;for rice vs wheat vs corn.</li>
<li>Variation in preferences over how many children to have.</li>
</ul>
<p><strong>It is hard to explain this variation with individual economic circumstances:</strong> when someone migrates to another country they face different economic circumstances (different prices and income) but they typically maintain their attitudes for decades.</p>
<p><strong>It is hard to explain this variation with genetic variation,</strong> because attitudes vary so much over time, while genes move very slowly.</p>
<p>So it seems like peer and media effects must be substantial proximal determinants of attitudes.</p>
</section>
<section id="resolution-each-effect-is-small-but-there-are-many" class="level1 page-columns page-full">
<h1>(3) Resolution: Each Effect is Small, but There are Many</h1>
<p><strong>How can we resolve small treatment effects with big variation in outcomes?</strong> It makes sense if we’ve only been testing very small treatments. Each individual effect is small but there are millions of them, so collectively the effects are large.</p>
<div class="page-columns page-full"><p><strong>Peer effect studies tend to find small effects when looking at random assignment of peers,</strong> e.g.&nbsp;random assignment of roommates, but this may be because time with your roommate constitutes only a very small share of your overall exposure to other people and ideas.<sup>1</sup> Collectively that exposure must be hugely important in your attitudes.</p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;Kremer and Levy (2008) say <em>“Most studies do not find effects of these predetermined characteristics on the whole sample of students … conventional peer effects on academic achievement … are not estimated to be particularly important.”</em></p></li></div></div>
<p><strong>Media studies tend to find small effects from exposure to social media or to television,</strong> but in most cases the media exposure is only a single-digit percentage-point share of their lifetime exposure to media. So the aggregate effect can be far larger than that measured in any credible experiment or natural experiment (in addition, much of the effect likely propagates through peer effects).</p>
<p>An individual campaign advertisement might have very small effects on voting intention, but an individual campaign advertisement is only a tiny tiny share of your lifetime exposure to political communication. Small individual effects are consistent with peoples’ political attitudes being overwhelmingly determined by exposure and persuasion.</p>
<p><strong>More technically:</strong> we can reconcile small effects with big variations if:</p>
<ol type="1">
<li>Effects have a long half-life, e.g.&nbsp;exposure in childhood can affect your attitudes as an adult.</li>
<li>Peer effects are propagated through many weak links, instead of a few strong links: i.e.&nbsp;there are substantial influences from all of society in addition to your family.</li>
<li>Persuasion works even with indirect channels, e.g.&nbsp;your political views aren’t just affected by campaign ads, but also by the implicit attitudes to politics reflected in all the media you’re exposed to</li>
<li>Attitudes are sensitive to the <em>average</em> rather than the <em>total</em> amount of persuasive material you’re exposed to, thus marginal effects can be small while total effects are large.</li>
</ol>
<p>(As a footnote: from my time in social media companies I learned that individual peer effects are tiny, yet we also know that social media demand is <em>entirely</em> peer effects, i.e.&nbsp;people only use Facebook because other people use Facebook.)</p>
</section>
<section id="other-notes" class="level1">
<h1>Other Notes</h1>
<p><strong>The paradox of large effects.</strong> <span class="citation" data-cites="tosh2021piranha">Tosh et al. (2021)</span> discuss an opposite problem: in some fields there are many claims of large effects, but it is not possible to reconcile the aggregate variance in the data with so many large effects. E.g. they discuss a paper claiming to show that exposure to age-related words tends to lower a subject’s subsequent walking speed by 13%. If people are exposed to many such primes, and they are uncorrelated, then we should expect huge and implausible variation in peoples’ day-to-day walking speed.</p>
<p>Their problem is somewhat the opposite: they are talking about a literature which has many non-credible effects from lab experiments or observational data. Instead I’m talking about literature which has credible but small effects.</p>
</section>
<section id="references" class="level1">




</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-angrist2014perils" class="csl-entry">
Angrist, Joshua D. 2014. <span>“The Perils of Peer Effects.”</span> <em>Labour Economics</em> 30: 98–108. https://doi.org/<a href="https://doi.org/10.1016/j.labeco.2014.05.008">https://doi.org/10.1016/j.labeco.2014.05.008</a>.
</div>
<div id="ref-ansolabehere2006paradox" class="csl-entry">
Ansolabehere, Stephen. 2006. <span>“The Paradox of Minimal Effects.”</span> <em>Capturing Campaign Effects</em>, 29–44.
</div>
<div id="ref-tosh2021piranha" class="csl-entry">
Tosh, Christopher, Philip Greengard, Ben Goodrich, Andrew Gelman, Aki Vehtari, and Daniel Hsu. 2021. <span>“The Piranha Problem: Large Effects Swimming in a Small Pond.”</span> <em>arXiv Preprint arXiv:2105.13445</em>.
</div>
</div></section></div> ]]></description>
  <guid>tecunningham.github.io/posts/2023-08-02-small-effects.html</guid>
  <pubDate>Wed, 02 Aug 2023 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Ranking by Engagement</title>
  <dc:creator>Tom Cunningham</dc:creator>
  <link>tecunningham.github.io/posts/2023-04-28-ranking-by-engagement.html</link>
  <description><![CDATA[ 




<style>
   h1 {  border-bottom: 4px solid black;}
   h2 {  border-bottom: 1px solid #ccc;}
   dl { display: grid; grid-auto-flow: row;}
   dt { grid-column-start: 1; }
   dd { grid-column-start: 2; }
</style>

<div class="no-row-height column-margin column-container"><div class="">
<p> Thanks to comments from Jeff Allen, Jacquelyn Zehner, David Evan Harris, Jonathan Stray, and others. If you find this note useful for your work send me an email and tell me :).</p>
</div><div class="">
<p><img src="tecunningham.github.io/posts/images/2023-06-13-15-11-29.png" class="img-fluid"></p>
</div></div>
<p><strong>Six observations on ranking by engagement on social media platforms:</strong></p>
<ol type="1">
<li><p><strong>Platforms rank content primarily by the predicted probability of engagement.</strong> Platforms show each user the items they are most likely to click on, or reply to, or to retweet, etc.<sup>1</sup></p></li>
<li><p><strong>Platforms rank by engagement because it increases user retention.</strong> In experiments which compare engagement-ranked feeds to unranked feeds (“chronological” feeds) the users with engagement-ranked feeds consistently show substantially higher long-run retention (DAU) and time-spent. Platforms care about engagement not in itself but as a means to an end, and when faced with a tradeoff between engagement and retention would choose retention.</p></li>
<li><p><strong>Engagement is negatively related to quality.</strong> The content with the highest predicted engagement very often has low scores by various measures of objective quality: clickbait, spam, scams, misleading headlines, copied content, inauthentic content, and misinformation. Intuitively this is because engagement only measures immediate appeal, and the most appealing content can be the most disappointing. Low quality content typically <em>hurts</em> retention, and as a consequence platforms often supplement their engagement-based ranking algorithms with a range of proxies for content quality.</p></li>
<li><p><strong>Sensitive content is often both engaging and retentive.</strong> Engagement-ranked feeds often increase the prevalence of various types of “sensitive” content: nudity, bad language, abuse, hate speech, hyper-partisan politics, etc.. However unlike low-quality content, reducing the prevalence of sensitive content often hurts retention, implying sensitivity is positively correlated with retention.</p></li>
<li><p><strong>Sensitive content is often preferred by users.</strong> Platforms have tried out many experiments with asking users directly for their preferences over content. The results have been mixed, and platforms have often been disappointed to find that users express fairly positive attitudes towards content that the platform considers sensitive.</p></li>
<li><p><strong>Platforms don’t want sensitive content but don’t want to be seen to be removing it.</strong> Platform decision-makers often have principled reasons for limiting the distribution of certain types of sensitive content. Additionally there are instrumental reasons: sensitive content attracts negative attention from the media, advertisers, app stores, politicians, regulators, and investors. But platforms are also liable to get negative attention if they make substantive judgments about the sensitivity of content, especially when it has some political dimension. As a consequence platforms often target sensitive content indirectly by using proxies, and they prefer to justify their decision-making by appealing to user preferences or to user retention.</p></li>
</ol>
<div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;In this note I’m using “engagement” to refer to individual actions not user-level metrics like time-spent or DAU.</p></li></div><div class="cell page-columns page-full" data-layout-align="center" data-hash="2023-04-28-ranking-by-engagement_cache/html/unnamed-chunk-1_7fb6137b1a11fb60b80d4e3c0226543f">

<div class="no-row-height column-margin column-container"><div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-04-28-ranking-by-engagement_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="288"></p>
</figure>
</div>
</div></div></div>
<p><strong>In an appendix I formalize the argument.</strong> I show that all these observations can be expressed as covariances between different properties of content, e.g.&nbsp;between the retentiveness, predicted engagement rates, and other measures of content quality. From those covariances we can derive Pareto frontiers and visualize how platforms are trading-off between different outcomes.</p>
<p><br><br><br></p>
<section id="argument-in-detail" class="level1 page-columns page-full">
<h1>Argument in Detail</h1>
<ol type="1">
<li><p><strong>Talking about ranking is complicated.</strong> To help simplify things I bucket attributes of content into five types:</p>
<ol type="1">
<li><strong>Engagement:</strong> the predicted probability of a user clicking, commenting, retweeting, etc., on a specific piece of content.</li>
<li><strong>Retentiveness:</strong> the causal contribution of seeing the content on a specific user’s long-term retention (e.g.&nbsp;DAU). Unlike the other attributes this can never be directly observed, only inferred from experiments.</li>
<li><strong>Quality:</strong> some objective measure of quality, e.g.&nbsp;whether fact-checked, whether the headline is misleading, whether the linked website has a high ad-load, whether the source is trustworthy, etc..</li>
<li><strong>Sensitivity:</strong> whether the content could be offensive, harmful, corrosive – e.g.&nbsp;nudity, bad language, abuse, hate speech.</li>
<li><strong>Preference:</strong> the user’s response to a survey question, e.g.&nbsp;“do you want to see more of this type of content?”</li>
</ol>
<p>Note that “quality” and “sensitivity” apply to pieces of content, while the other three attributes apply to relationship between a user and a piece of content.</p></li>
<li><p><strong>Social media platforms rank their content primarily by predicted engagement.</strong> The core ranking model for most social platforms is a weighted average of predicted engagement rates.<sup>2</sup></p>
<p>However ranking functions also include hundreds of other tweaks incorporating non-engagement features, upranking or downranking content depending on, for example, the media type (photo/text/video), the relationship between the user and the author (whether you follow this person), various predictions of of objective quality (classifiers predicting whether the content is spam, offensive, adult, misinformation, etc.), or other features (network centrality, off-platform popularity, etc.). They also often have some diversity rules to prevent the content that is shown from being too similar.<sup>3</sup></p>
<p>Ranking by popularity is common for other media: we look at lists of bestsellers, most popular, highest grossing, most watched, or top charting. Attention is limited and it would be inefficient to offer people a random selection of everything that’s available.</p></li>
<li><p><strong>Predicted engagement rates are mostly historical engagement rates.</strong> In many cases the most important predictors of whether a user will engage with a piece of content are (1) this user’s historical rate of engagement on similar pieces of content (e.g.&nbsp;content from the same author, or of the same media-type); (2) other users’ rate of engagement on this piece of content. Platforms do use more complicated models (embeddings and neural nets), those models typically are most valuable for qualitatively new types of content, when you have relatively sparse historical data either on the user or on the item.</p></li>
<li><p><strong>Platforms care primarily about long-run retention, engagement is a means to that end.</strong> The outcome that leadership care about the most is long-run retention, measured with metrics like Daily Active Users (DAU).<sup>4</sup> They would generally sacrifice substantial amounts of engagement in return for DAU. They also would sacrifice substantial short-term DAU if it could be shown with confidence that it would lead to higher long-term DAU.</p>
<p>This point is often unclear because many changes to ranking (as measured in experiments) move engagement and retention in the same direction, and move short-run and long-run metrics in the same direction, meaning that we cannot easily tell which metric is decisive. Individual teams are often given targets to increase short-term engagement but that is mainly because that metric is easier to measure.</p></li>
<li><p><strong>Engagement-ranked feeds have substantially higher long-term retention and time-spent than chronologically-ranked feeds.</strong> Users who are given engagement-ranked feeds in experiments typically have higher long-term DAU by single-digit percentages (1%-9%), and higher long-term time-spent by double-digit percentages (10%-99%). Accounting for network effects makes the aggregate difference even larger.<sup>5</sup></p>
<ul>
<li><strong>Meta 2020 experiments with chronological ranking showed a 20% time-spent decline on Facebook and a 10% decline on Instagram.</strong> The experiments ran for 3 months and they reported the average effect over the whole period, it is likely that the time-spent effects continued declining. The effects on DAU were not reported. See <span class="citation" data-cites="guess2023chronological">Guess et al. (2023)</span>.</li>
<li><strong>A Facebook 2018 experiment showed a 3% decline in time-spent after 10 days.</strong> The effects on time-spent seemed to be linearly trending down at the time the analysis was posted. Engagement (MSI) declined by about 20%, and politics impression reduced by 15% (the share of politics impressions is more complicated to calculate but seems unambiguously down). The effects on DAU were not reported (<a href="https://www.bigtechnology.com/p/facebook-removed-the-news-feed-algorithm?s=09">source</a>.). The experiment was not on a purely chronological feed: they retained “diversity rules, client-side ranking, read-state logging, comment-bumping” as well as integrity ranking rules.</li>
</ul></li>
<li><p><strong>Engaging content is often low quality.</strong> Despite the positive relationship between engagement and retention, many studies have found that highly-engaging content is has lower-than-average quality:</p>
<ul>
<li><strong>Meta chronological-ranking experiments.</strong> <span class="citation" data-cites="guess2023chronological">Guess et al. (2023)</span> found that replacing feed-ranking algorithms with simple chronological ranking (i.e., the most-recent posts are shown first) for 3 months (1) the share of impressions that were classified as political increased by 15% on Facebook and by 5% on Instagram, the share that were classified as “political news” increased by 40% on Facebook.</li>
<li><strong>In summer 2016 half of Facebook’s most-seen posts related to the US election were misinformation.</strong> As <a href="https://www.buzzfeednews.com/article/craigsilverman/viral-fake-election-news-outperformed-real-news-on-facebook">reported</a> by Craig Silverman at Buzzfeed. This exceeds the <em>average</em> rate of misinformation, i.e.&nbsp;the most-engaging posts have a much lower-than-average quality.</li>
<li><strong>Facebook’s top group and pages were run by troll farms in 2019.</strong> A series of internal analyses by by Jeff Allen (<a href="https://s3.documentcloud.org/documents/21063547/oct-2019-facebook-troll-farms-report.pdf">subsequently leaked</a>) found that a substantial share of Facebook’s top pages, groups, and posts, were run by “troll farms,” whose main tactic was reposting copied content that had high engagement rates. </li>
<li><strong>On Facebook high-quality content received lower engagement.</strong> In the late 2010s Facebook maintained an internal “quality” score for content (FUSS=“Feed Unified Scoring System”). A data scientist’s analysis from 2019 (<a href="https://s3.documentcloud.org/documents/21063547/oct-2019-facebook-troll-farms-report.pdf">subsequently leaked, see p.10</a>) found that low-quality content had significantly higher predicted engagement rates.<sup>6</sup></li>
<li><strong>Facebook’s most-viewed posts remain very low quality.</strong> Since early 2021 FB has been releasing a dataset of their 20 most-viewed links and posts. An <a href="https://lookerstudio.google.com/u/0/reporting/28bc32fd-a067-4b4a-9be0-637e8c9bd917/page/0Z3mC?s=g7_EWEyFjrc">Integrity Institute analysis</a> by Jeff Allen has found that each quarter 60-80% of the posts fail some basic checks, either “the account behind it is anonymous, is posting unoriginal content, using spammy page or group networks, or if the post or link violated Facebook’s community standards.”</li>
<li><strong>Twitter’s ranking has mixed effects on political content.</strong> Huszar et al.&nbsp;(2021) compare political content of users who are randomized to ranked vs chronological feeds. They report (1) for political parties, ranked feed tends to amplify the right-wing parties somewhat more than left-wing parties (but the same does not hold for individual politicians); (2) for US media sources, ranked feed amplifies “sources that are more partisan compared to ones rated as center”.<sup>7</sup></li>
</ul></li>
<li><p><strong>Many platforms have found that increasing quality helps retention.</strong> Platforms have tried to address quality problems by defining measures of objective quality:</p>
<ul>
<li><p><strong>Facebook uses many heuristics and classifiers to identify various types of low-quality content:</strong> Facebook identifies and downranks, among other things, engagement bait, links that go to ad-farms, scraped content, titles that withhold information, and titles that exaggerate information. In each case these types of content would generate high engagement but give users a bad experience, and in most cases experiments confirmed that dowranking these types of content increases long-run user retention.</p></li>
<li><p><strong>Facebook uses some metadata features to identify low-quality content.</strong> E.g. Facebook calculates the <a href="https://www.cnbc.com/2019/04/10/facebook-click-gap-google-like-approach-to-stop-fake-news-going-viral.html">“click gap”</a> (the amount of organic traffic a website gets) and <a href="https://www.wired.com/story/how-facebook-wants-to-improve-the-quality-of-your-news-feed/">“broad trust”</a> (diversity of engagement across users).</p>
<p></p></li>
<li><p><strong>YouTube has introduced a series of quality adjustments to ranking:</strong> e.g.&nbsp;downranking <a href="https://blog.youtube/inside-youtube/on-youtubes-recommendation-system/">“sensationalistic tabloid content”</a> and upranking <a href="https://blog.youtube/inside-youtube/on-youtubes-recommendation-system/">“authoritative content”</a>.</p></li>
</ul>
<p>Some companies have also shifted engagement weights to put relatively more weight on “deeper” measures of engagement:</p>
<ul>
<li><strong>In 2012 YouTube switched from maximizing clicks to maximizing watch-time.</strong> They found it led to a short-term decrease in clicks but a long-term <a href="https://blog.youtube/inside-youtube/on-youtubes-recommendation-system/">increase in retention</a>. I believe Netflix similarly has invested a lot of time in developing “deep” measures of engagement.</li>
</ul></li>
<li><p><strong>Quality also helps the producer ecosystem.</strong> There is an additional reason for prioritizing the quality of content independent of the direct effect on user retention: because prioritizing high-quality content helps foster a long-run community of creators.</p>
<p>A central fact about social media is that it relies on a tremendous amount of uncompensated labor. Most content is created for the joy of creation, with little realistic expectation of financial return. The impulse to post clearly relies on a delicate social perception or norm, and a platform could inadvertantly break this spell. I think speaking loosely Facebook mismanaged their public-content ecosystem in this way: they alienated creators in a variety of ways, especially by allowing copied content to proliferate, and high-quality creators instead posted to Instagram, Twitter, YouTube, and TikTok. Facebook leadership tried to attract creators with various monetary incentives but they often backfired: creators who are financially-motivated are often not the creators you want.</p></li>
<li><p><strong>Engagement measures <em>immediate</em> quality, and hence is a poor proxy for the quality of factual claims.</strong> Engagement necessarily measures the immediate reaction of a user to a piece of content, and thus ranking by predicted engagement will surface content that <em>appears</em> to be good. This is fine when there is no hidden aspect to quality, e.g.&nbsp;for jokes and pictures which mostly be judged in the moment. However if we rank informational content by predicted engagement it will tend to surface the claims that are the most sensational or intriguing independent of whether they are true.</p>
<p>If apples were sold only by how they looked, and not by how they tasted, then we would be offered delicious-looking and bland-tasting apples.</p>
<p>I believe this basic mechanism explains why internet platforms typically have higher rates of exaggerated, misleading, or false content compared to traditional media (newspapers, television, etc.). Traditional media do not publish whichever headlines would maximize short-run sales because that would harm long-run sales. This also explains why platforms have found that they can substantially improve retention by building proxies for quality.</p></li>
<li><p><strong>A negative relationship between engagement and quality can be caused by unscrupulous publishers.</strong> Suppose each publisher can produce a fixed number of headlines, which will vary in (1) the headline’s propensity to be clicked, and (2) whether the headline is true. There are two types of publishers:</p>
<ol type="1">
<li><p>Honest publishers: they choose the most-engaging headlines from within the subset that are true.</p></li>
<li><p>Dishonest publishers: they choose the most-engaging headlines from the universe of all possible headlines (irrespective of truth).</p></li>
</ol>
<p>In this world the most-engaging headlines will be disproportionately false compared to the average headline. In the long run consumers will learn some skepticism, and to discount headlines in proportion to how clickable they seem, but they are unlikely to learn to discriminate perfectly. There’s always a chance that an intriguing headline will be true, and so the negative correlation would persist in equilibrium.<sup>8</sup></p></li>
<li><p><strong>Platforms have been slow in improving the quality of ranked content.</strong> I discuss above some examples of Facebook’s slowness in addressing problems with the quality of content. I think this slowness is for two mains reasons. First, predicting engagement is a well-defined technical problem with a track record of success while evaluating content-quality is much more open-ended and difficult to validate. Hard-headed engineers often argue that a user’s preferences are revealed in their engagement and that evaluating quality is paternalistic. Secondly, platforms are nervous of being opinionated about objective quality because they don’t wish to take sides on politically delicate issues. In 2016 Facebook was criticized for using human judgment in determining what topics are “trending”, and in the wake of that criticism many projects which involved human judgment were shut down and replaced with automatic systems. Then in 2020 engineers on News Feed were told to avoid using words such as “trust” or “quality” or “authority”, and to instead use language that referred only to user preferences.</p>
<p></p>
<p></p></li>
<li><p><strong>Sensitive content is often both engaging and retentive.</strong> I have defined “sensitive” content to include nudity, bad language, abuse, hate speech, hyper-partisan politics, etc.. Sensitive content often has higher-than-average engagement rates, and when content is demoted this often hurts retention, implying that sensitivity is positively correlated with retentiveness.</p>
<ul>
<li>A 2023 academic study by <a href="https://drive.google.com/file/d/1HYiBOGLNM91RBiqBlxKFvjDhJAuCxe60/view">Beknazar-Yuzbashev et al.</a> found that filtering the 7% most-toxic content on Facebook reduced overall Facebook content consumed by 20%.</li>
</ul></li>
<li><p><strong>Platforms don’t want to show sensitive content.</strong> Platforms are clearly prepared to pay a cost to reduce the prevalence of sensitive content, both in terms of retentiveness (DAU), and in the monetary cost of engineers, labelers, and computation.<sup>9</sup></p>
<p>The platforms have many reasons for avoiding sensitive content, independent of its effect on retention, but internally there is often an ambiguity about the contribution of different reasons. In part decisions are driven by a feeling of moral duty to not amplify content that is harmful. However there are also many instrumental reasons, because sensitive content often causes friction with advertisers, app stores, regulators, media, employees and investors.</p>
<p>Misinformation is a somewhat special case. From what has been discussed before, misinformation can be expected to reduce retention because it’s not true. However misinformation is very often related to sensitive issues, e.g.&nbsp;partisan politics, race relations, vaccines, and often reports falsehoods that support the viewer’s political prejudices.</p></li>
<li><p><strong>Platforms avoid directly penalizing sensitive content.</strong> Platforms are caught in a double bind: there are strong pressures to reduce the amount of sensitive content on their platform, but there is also a pressure not to be seen to be making judgments about the objective value or harm of content. They want a garden with no weeds but they also wish to have clean hands. This often causes a bifurcation between the nominal reason for a policy and the real reason. Some examples:</p>
<ul>
<li><p><strong>Platforms often downrank engagement patterns because they correlate with sensitive content.</strong> It is common to downrank posts which features a specific engagement pattern (e.g.&nbsp;certain types of sharing, certain types of downstream attributes), and the downranking is justified internally based on the correlation with measures of quality or sensitivity, e.g.&nbsp;misinformation, or hate speech, or hyperpartisan content. This is odd because it would seem to be more efficient to target the sensitive content directly, i.e.&nbsp;instead of downranking the proxy, use the proxy as a feature in a classifier, and downrank based on the classifier output. However platforms avoid this approach in part because they are nervous about the perception of being perceiving as judges of the quality of content.</p></li>
<li><p><strong>Platforms speak about sensitivity rules as if they were adopted to serve the interests of their users.</strong> Google’s Jigsaw group has an influential set of definitions of content quality, their <a href="https://support.perspectiveapi.com/s/about-the-api-attributes-and-languages?language=en_US">definition</a> of a “toxic” comment is <em>“a rude, disrespectful, or unreasonable comment that is likely to make people leave a discussion.”</em> This definition is worded to presuppose that rude comments cause lower retention. The definition thus allows a platform to talk about their toxicity classifiers as if they were solely serving the interests of the users exposed to toxic language.</p></li>
<li><p><strong>Survey questions are chosen based on their correlation with measures of sensitive content.</strong> Platforms will often try out multiple different wordings of a survey question and decide which one to use by comparing the results with their internal measures of content quality and sensitivity, leading to survey questions that are somewhat awkwardly worded (e.g.&nbsp;asking people “is this good for the world?”).</p></li>
</ul></li>
<li><p><strong>Subjective user ratings of quality have a mixed relationship with objective measures of quality.</strong> Platforms have often tried to collect explicit user feedback about quality, e.g.&nbsp;asking “was this worth your time?”, “do you want to see more of this?”, “was this informative?”. In my experience, for most such questions, responses are highly correlated with engagement, but often show a negative correlation with objective measures of quality. E.g. people often rate misinformation as “informative” and “worth my time.”</p>
<p>Nevertheless some of these initiatives have had success in raising both objective quality and retention, e.g.&nbsp;Facebook recently launched a prompt asking “would you like to see more posts like this?” The signal from this prompt apparently increases both retention and many objective measures of quality.</p>
<p></p></li>
<li><p><strong>Platforms additionally care about engagement because of network effects.</strong> I said above that platforms care about engagement primarily insofar as it’s a proxy for retention, however there is an additional reason to pay attention to engagement. When one user engages (likes, comments, retweets) this increases the value of the platform to all the other users, and so has an indirect positive effect on retention. For this reason platforms are generally willing to sacrifice some retention in return for engagement, as measured in an experiment, if the sacrifice is sufficiently small.</p></li>
</ol>
<div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;I believe this is true for almost all platforms with personalized recommendations including YouTube, Netflix, and Amazon. An excellent reference for how recommenders work, with illustrations and links, is Thorburn, Bengani, &amp; Stray (2022) <a href="https://medium.com/understanding-recommenders/how-platform-recommenders-work-15e260d9a15a">“How Platform Recommenders Work”</a>.</p></li><li id="fn3"><p><sup>3</sup>&nbsp;For simplicity the rest of the discussion treats the causal effect of content as purely separable.</p></li><li id="fn4"><p><sup>4</sup>&nbsp;Most publicly-traded platforms report in their quarterly earnings just one non-financial metric: the number of active users (DAU/MAU/mDAU, etc.). I do not know of any company that publicly reports a metric of aggregate engagement.</p></li><li id="fn5"><p><sup>5</sup>&nbsp;<span class="citation" data-cites="huszar2022twitter">Huszár et al. (2022)</span> note that since Twitter introduced a ranked timeline in 2016 they maintained an experiment with 1% of users with a chronological feed.</p><div id="ref-huszar2022twitter" class="csl-entry">
Huszár, Ferenc, Sofia Ira Ktena, Conor O’Brien, Luca Belli, Andrew Schlaikjer, and Moritz Hardt. 2022. <span>“Algorithmic Amplification of Politics on Twitter.”</span> <em>Proceedings of the National Academy of Sciences</em> 119 (1): e2025334119. <a href="https://doi.org/10.1073/pnas.2025334119">https://doi.org/10.1073/pnas.2025334119</a>.
</div></li><div id="ref-guess2023chronological" class="csl-entry">
Guess, Andrew M., Neil Malhotra, Jennifer Pan, Pablo Barberá, Hunt Allcott, Taylor Brown, Adriana Crespo-Tenorio, et al. 2023. <span>“How Do Social Media Feed Algorithms Affect Attitudes and Behavior in an Election Campaign?”</span> <em>Science</em> 381 (6656): 398–404. <a href="https://doi.org/10.1126/science.abp9364">https://doi.org/10.1126/science.abp9364</a>.
</div><li id="fn6"><p><sup>6</sup>&nbsp;These correlations can be difficult to interpret: suppose there is no correlation between engagement and quality in the pool of all available content, there will nevertheless be a negative correlation among the subset of content that is <em>seen</em> if the ranking algorithm penalizes low-quality content, meaning low-engagement low-quality content will never be shown to users.</p></li><li id="fn7"><p><sup>7</sup>&nbsp;Bakshy et al.&nbsp;(2015) found that Facebook’s feed-ranking doesn’t substantially change the share of cross-cutting (across-the-aisle) content seen.</p></li><li id="fn8"><p><sup>8</sup>&nbsp;A more formal version: each consumer sees a single headline with observed signal <img src="https://latex.codecogs.com/png.latex?s"> and chooses whether to click. The payoff from clicking is <img src="https://latex.codecogs.com/png.latex?s"> if it’s from an honest publisher, zero otherwise, and there’s some stochastic outside option so the consumer’s probability of clicking is continuously increasing in the expected payoff from clicking. Honest publishers report their signals drawn from <img src="https://latex.codecogs.com/png.latex?f_H(s)">, dishonest publishers choose any signal. In equilibrium the dishonest publishers’ signal distribution, <img src="https://latex.codecogs.com/png.latex?f_D(s)">, must be such that all signals with non-zero mass have have an equal click-through rate, meaning there is some <img src="https://latex.codecogs.com/png.latex?%5Ckappa"> such that for every <img src="https://latex.codecogs.com/png.latex?s%3E%5Ckappa">, <img src="https://latex.codecogs.com/png.latex?f_D(s)=%5Cfrac%7Bs-%5Ckappa%7D%7B%5Ckappa%7Df_H(s)">. Thus low click-through-rate headlines (<img src="https://latex.codecogs.com/png.latex?s%3C%5Ckappa">) are all true, but high click-through rate headlines (<img src="https://latex.codecogs.com/png.latex?s%3E%5Ckappa">) all have some share which are false. Qualitatively: if a headline is not very interesting, then you believe it; but if it’s interesting then you discount exactly inversely to how interesting it is. In this model we have (1) retentiveness (consumer surplus) is increasing with engagement; (2) quality (truth) is decreasing with engagement; (3) retentiveness (consumer surplus) would be higher if you rank by both engagement and quality (e.g.&nbsp;by removing false stories).</p></li><li id="fn9"><p><sup>9</sup>&nbsp;Platforms often spend around 5% of their total costs on content moderation, despite the prevalence of sensitive content and the effects on retention typically being closer to 0.1% or less.</p></li></div></section>
<section id="technical-appendix-expressed-as-a-covariance-matrix" class="level1">
<h1>Technical Appendix: Expressed as a Covariance Matrix</h1>
<p><strong>We can express most of the argument above with a covariance matrix.</strong> Given a user we can give scores to each piece of content with respect to the five attributes defined above. Then we can give a reasonable characterization of the platform ranking problem with the following covariance matrix:</p>
<table class="table">
<colgroup>
<col style="width: 25%">
<col style="width: 19%">
<col style="width: 14%">
<col style="width: 10%">
<col style="width: 16%">
<col style="width: 14%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"></th>
<th style="text-align: center;">retentiveness</th>
<th style="text-align: center;">engagement</th>
<th style="text-align: center;">quality</th>
<th style="text-align: center;">sensitivity</th>
<th style="text-align: center;">preference</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><strong>retentiveness</strong></td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">+</td>
<td style="text-align: center;">+</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">+</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>engagement</strong></td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">-</td>
<td style="text-align: center;">+</td>
<td style="text-align: center;">+</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>quality</strong></td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">0</td>
<td style="text-align: center;">0</td>
</tr>
<tr class="even">
<td style="text-align: center;"><strong>sensitivity</strong></td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">+</td>
</tr>
<tr class="odd">
<td style="text-align: center;"><strong>preference</strong></td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">&nbsp;</td>
<td style="text-align: center;">&nbsp;</td>
</tr>
</tbody>
</table>
<p><strong>Given this covariance matrix, we can draw Pareto frontiers and indifference curves.</strong> Each Pareto frontier represents the set of achievable tradeoffs between two outcomes. I explain below how elliptical Pareto frontiers and linear indifference curves can be derived from the covariance matrix if we assume that everything is distributed joint Normally.</p>
<p><strong>Retentiveness and engagement.</strong> We can draw a Pareto frontier between retention and engagement as below. We do not directly observe the retentiveness of content, but we know that ranking content by engagement (i.e.&nbsp;choosing the farthest right-hand point on the Pareto frontier) increases retention relative to an unranked feed, so we can infer that retentiveness and engagement are positively correlated, thus the Pareto ellipse must be upward-sloping.</p>
<div class="cell" data-layout-align="center" data-hash="2023-04-28-ranking-by-engagement_cache/html/unnamed-chunk-2_72d01dd2d92d83f155d995d659fe1b90">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-04-28-ranking-by-engagement_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="288"></p>
</figure>
</div>
</div>
</div>
<p><strong>Engagement and quality.</strong> As discussed above, we often see that (1) measures of content quality have zero or negative correlation with engagement, (2) downranking low-quality content (equivalently, upranking high-quality content) increases retention. This is somewhat surprising because engagement and retention have a positive correlation, meaning the three correlations are not transitive.</p>
<div class="cell" data-layout-align="center" data-hash="2023-04-28-ranking-by-engagement_cache/html/unnamed-chunk-3_d91a652143616fce6f027c965e9b4782">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-04-28-ranking-by-engagement_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid figure-img" width="336"></p>
</figure>
</div>
</div>
</div>
<p>We illustrate the relationship with retention here with three lines representing different levels of retention, effectively these are indifference curves of a platform that is trying to maximize retention.</p>
<p><strong>Engagement and sensitivity.</strong> Next consider “sensitive” attributes. We often see that more sensitive content has higher engagement rates, shown below as an upward-tilt to the Pareto frontier. In addition experiments that penalize sensitive content often have a negative effect on retention: this could be either due to a positive partial correlation between engagement and retentiveness, or a positive partial correlation between sensitivity and retentiveness. But in either case it seems that sensitive content does not have a strong negative effect on retention.</p>
<p>Despite these facts, most platforms still put substantial penalties on sensitive content, either directly or indirectly (as discussed above), and they pay a price in terms of both engagement and retention.</p>
<div class="cell" data-layout-align="center" data-hash="2023-04-28-ranking-by-engagement_cache/html/unnamed-chunk-4_3eb22aba80b71510789e4450ff460016">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-04-28-ranking-by-engagement_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="288"></p>
</figure>
</div>
</div>
</div>
<p><strong>Preference and sensitivity.</strong> Finally consider a direct measure of user preference over content, e.g.&nbsp;asking users “is this informative?” or “would you like to see more like this?” In general user preference correlates relatively well with engagement, but it also offers incremental value for predicting retentiveness, in other words adding an additional term to the ranking function to predict user preference tends to increase retention.</p>
<p>However as discussed above, projects which collect survey questions are often focussed on the sensitivity of content rather than its retentiveness, and in that respect their findings are often mixed. Below we illustrate a case in which ranking by preference increases retentiveness but does not lower the amount of sensitive content (which platforms often desire). However platforms will offer try out many different wordings of survey questions, and each question will have somewhat different correlations.</p>
<div class="cell" data-layout-align="center" data-hash="2023-04-28-ranking-by-engagement_cache/html/unnamed-chunk-5_76ef17ca710f7f5ae64474e6ea79e7f0">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-04-28-ranking-by-engagement_files/figure-html/unnamed-chunk-5-1.png" class="img-fluid figure-img" width="288"></p>
</figure>
</div>
</div>
</div>
<section id="formal-observations" class="level2">
<h2 class="anchored" data-anchor-id="formal-observations">Formal Observations</h2>
<p>Here I describe a few formal properties of a model of ranking based on a joint-normal distribution of attributes. I have a longer writeup with proofs of these results which I hope to publish soon, I am happy to share a draft on request.</p>
<ol type="1">
<li><p><strong>The covariance between item attributes will determine a Pareto frontier among outcomes.</strong> Suppose we know the joint distribution of attributes and we can choose a subset with share <img src="https://latex.codecogs.com/png.latex?p"> of the distribution (e.g.&nbsp;a fixed number of impressions given a pool of possible stories to show), and we want to calculate the average value of each attribute in the subset of content shown to the user. Then we can describe the Pareto frontier over subsets, i.e.&nbsp;the set of realized average outcomes, and it will be a function of the covariances among attributes over pieces of content. With 2 attributes the Pareto frontier will be an ellipse with shape exactly equal to an isoprobability curve from the joint density.</p>
<p>The shape of the ellipse has a simple interpretation. If two attributes are positively correlated then the Pareto frontier will be <em>tight</em> meaning there is little tradeoff, i.e.&nbsp;we will have similar aggregate outcomes independent of the relative weights put on each outcome in ranking. If instead two attributes are negatively correlated then the Pareto frontier will be <em>loose</em> meaning outcomes will vary a lot with the relative weights used in ranking.</p>
<p>Our assumption that the share <img src="https://latex.codecogs.com/png.latex?p"> is fixed is equivalent to assuming that any ranking rule will get the same number of impressions. This assumption obviously has some tension with <em>retentiveness</em> being an outcome variable: if some ranking rule has low retentiveness, then we would expect lower impressions. Accounting for this would make the Pareto frontier significantly more complicated to model, for simplicity we can interpret every attribute except retentiveness as a short-run outcome. Alternatively we could interpret them as relative instead of absolute outcomes, e.g.&nbsp;as engagement/impression or engagement/DAU.</p></li>
<li><p><strong>Improving a classifiers will stretch the Pareto frontier.</strong> As a classifier gets better the average prediction will stay the same but the variance will increase, meaning the Pareto frontier will stretch out, and given a linear indifference curve we can derive the effect on outcomes.</p></li>
<li><p><strong>The joint distribution plus utility weights will determine ranking weights.</strong> If we observe only some outcomes then we can calculate the conditional expectation for other outcomes. Typically we want to know retentiveness, and we can write the conditional expectation as follows: <img src="https://latex.codecogs.com/png.latex?E%5B%5Ctext%7Bretentiveness%7D%7C%0A%20%20%20%5Ctext%7Bengagement%7D,%5Cldots,%5Ctext%7Buser%20preference%7D%5D."> This expectation has a closed-form solution when the covariance matrix is joint normal. When we have just two signals, for example engagement and quality, we can write:</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20E%5Br%7Ce,q%5D%20&amp;=%20%5Cfrac%7B1%7D%7B1-%5Cgamma%5E2%7D(%5Crho_e-%5Cgamma%5Crho_q)e%20+%0A%20%20%20%20%20%20%20%20%20%20%20%20%20%20%20%5Cfrac%7B1%7D%7B1-%5Cgamma%5E2%7D(%5Crho_q-%5Cgamma%5Crho_e)q%5C%5C%0A%20%20%20r%20%20%20%20%20&amp;=%20%5Ctext%7Bretentiveness%7D%5C%5C%0A%20%20%20e%20%20%20%20%20&amp;=%20%5Ctext%7Bengagement%20(predicted)%7D%5C%5C%0A%20%20%20q%20%20%20%20%20&amp;=%20%5Ctext%7Bquality%20(predicted)%7D%5C%5C%0A%20%20%20%5Crho_%7Be%7D%20%20%20%20%20&amp;=%20%5Ctext%7Bcovariance%20of%20engagement%20and%20retentiveness%7D%5C%5C%0A%20%20%20%5Crho_%7Bq%7D%20%20%20%20%20&amp;=%20%5Ctext%7Bcovariance%20of%20quality%20and%20retentiveness%7D%5C%5C%0A%20%20%20%5Cgamma%20%20%20%20%20&amp;=%20%5Ctext%7Bcovariance%20of%20engagement%20and%20quality%7D%0A%5Cend%7Baligned%7D">
<p>Note that the slope of the iso-retentiveness line in <img src="https://latex.codecogs.com/png.latex?(e,q)">-space will be <img src="https://latex.codecogs.com/png.latex?-%5Cfrac%7B%5Crho_e-%5Cgamma%5Crho_q%7D%7B%5Crho_q-%5Cgamma%5Crho_e%7D">.</p></li>
<li><p><strong>Experiments which vary ranking weights tell us about covariances.</strong> We can write findings from experiments as follows. First, suppose we find that retention is higher when ranked by engagement than when unranked, this can be written:</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Cutt%7BE%5Br%7Ce%3Ee%5E*%5D%7D%7Branked%20by%7D%7Bengagement%7D%20&amp;%3E%20%5Cut%7BE%5Br%5D%7D%7Bunranked%7D%0A%20%20%20%5Cend%7Baligned%7D">
<p>Here <img src="https://latex.codecogs.com/png.latex?e%5E*"> is chosen such that <img src="https://latex.codecogs.com/png.latex?P(e%3Ee%5E*)=p"> for some <img src="https://latex.codecogs.com/png.latex?p">, representing the share of potential inventory that the user consumes. This implies that engagement must positively correlate with retentiveness, <img src="https://latex.codecogs.com/png.latex?%5Crho_e%3E0">.</p>
<p>Next we can express that retention is higher when we put some weight <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> on quality:</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%5Cutt%7BE%5Br%7Ce+%5Cbeta%20q%3E%5Ckappa%5E*%5D%7D%7Branked%20by%7D%7Bengagement%20and%20quality%7D%20&amp;%3E%20%5Cutt%7BE%5Br%7Ce%3Ee%5E*%5D%7D%7Branked%20by%7D%7Bengagement%7D%0A%5Cend%7Baligned%7D">
<p>Here <img src="https://latex.codecogs.com/png.latex?%5Ckappa%5E*"> is chosen such that <img src="https://latex.codecogs.com/png.latex?P(e+%5Cbeta%20q%20%3E%20%5Ckappa%5E*)=P(e%3Ee%5E*)=p">. If <img src="https://latex.codecogs.com/png.latex?%5Cbeta"> is fairly small then we can infer that the iso-retentiveness line is downward-sloping, implying: <img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Crho_e-%5Cgamma%5Crho_q%7D%7B%5Crho_q-%5Cgamma%5Crho_e%7D%3E0."></p>
<p>This implies that both engagement and quality have the same sign. I don’t think they both can be negative, so they both must be positive:</p>
<img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0A%20%20%20%20%20%20%5Crho_e%20-%20%5Cgamma%20%5Crho_q%20&amp;%3E%200%20%5C%5C%0A%20%20%20%20%20%20%5Crho_q%20-%20%5Cgamma%20%5Crho_e%20&amp;%3E%200.%0A%20%20%20%5Cend%7Baligned%7D"></li>
<li><p><strong>I think it’s reasonable to treat preferences as locally linear.</strong> To have a well-defined maximization problem (with an interior solution) we need either nonlinear preferences or a nonlinear Pareto frontier. It’s always easier to treat things as linear when you can, so a relevant question is which of these two is closer to linear? Internally companies often treat their preferences as nonlinear, e.g.&nbsp;setting specific goals and guardrails, but those are always flexible and often have justifications as incentive devices. Typical metric changes are small, only single-digit percentage points, over that range the Pareto frontier does show significant diminishing returns while (it seems to me) value to the company does not.</p></li>
</ol>
</section>
</section>
<section id="appendix-literature" class="level1">
<h1>Appendix: Literature</h1>
<section id="overview-of-recommender-systems" class="level2">
<h2 class="anchored" data-anchor-id="overview-of-recommender-systems">Overview of Recommender Systems</h2>
<p>Adomavicius and Tuzhilin (2005) “Toward the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible Extensions”</p>
<ul>
<li>An influential overview of recommender systems (14,000 citations!). The canonical example is recommending movies to get the highest predicted rating. They use “rating” as similar to “engagement”. A more recent survey is <a href="https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00592-5">Roy and Dutta (2022)</a>.</li>
</ul>
<p>Davidson et al.&nbsp;(2010) <a href="https://www.inf.unibz.it/~ricci/ISR/papers/p293-davidson.pdf">“The YouTube Video Recommendation System”</a> &gt; “[videos] are scored and ranked using … signals [which] can be broadly categorized into three groups corresponding to three different stages of ranking: 1) video quality, 2) user specificity and 3) diversification.””</p>
<blockquote class="blockquote">
<p>“The primary metrics we consider include click through rate (CTR), long CTR (only counting clicks that led to watches of a substantial fraction of the video), session length, time until first long watch, and recommendation coverage (the fraction of logged in users with recommendations).”</p>
</blockquote>
<p>They say recommendations are good because they have high click-through rate.</p>
<p>Gomez-Uribe and Hunt (2015) <a href="https://dl.acm.org/doi/abs/10.1145/2843948">“The Netflix Recommender System: Algorithms, Business Value, and Innovation”</a></p>
<p>Clearly states that they evalaute AB tests using engagement, but it is regarded as an imperfect proxy for retention: <em>“we have observed that improving engagement—the time that our members spend viewing Netflix content—is strongly correlated with improving retention. Accordingly, we design randomized, controlled experiments … to compare the medium-term engagement with Netflix along with member cancellation rates across algorithm variants. Algorithms that improve these A/B test metrics are considered better.”</em></p>
<p>Akos Lada, Wang, &amp; Yan (2021, FB Blog) <a href="https://tech.facebook.com/engineering/2021/1/news-feed-ranking/">How does news feed predict what you want to see?</a></p>
<p>Thorburn, Bengani, &amp; Stray (2022, Understanding Recommenders) <a href="https://medium.com/understanding-recommenders/how-platform-recommenders-work-15e260d9a15a">“How Platform Recommenders Work”</a></p>
<p>This is an excellent short article with description and illustration of the stages in building a slate of content: moderation, candidate generation, ranking, and reranking. Includes links to many posts from platforms describing their systems.</p>
<p><img src="tecunningham.github.io/posts/images/2023-07-31-15-45-00.png" class="img-fluid"></p>
<p><img src="tecunningham.github.io/posts/images/2023-07-31-16-09-47.png" class="img-fluid"></p>
<p>Arvin Narayanan (2023) <a href="https://knightcolumbia.org/content/understanding-social-media-recommendation-algorithms">“Understanding Social Media Recommendation Algorithms”</a></p>
<p>A good overview of recommendation algorithms, with in-depth discussion of Facebook’s MSI.</p>
<p>Criticisms of social media recommendation: (1) harm users because “implicit-feedback-based feeds cater to our basest impulses,” (2) harm creators because “engagement optimization … is a fickle overlord,” (3) harms society because “social media platforms are weakening institutions by undermining their quality standards and making them less trustworthy. While this has been widely observed in the case of news … my claim is that every other institution is being affected, even if not to the same degree.”</p>
<p>I think the technical part of the essay is excellent but I found some of the arguments about harm and social effects hard to follow.</p>
</section>
<section id="proposals-for-change" class="level2">
<h2 class="anchored" data-anchor-id="proposals-for-change">Proposals for Change</h2>
<p>Ovadya, &amp; Thorburn (2023). <a href="https://doi.org/10.48550/arXiv.2301.09976">Bridging Systems: Open Problems for Countering Destructive Divisiveness across Ranking, Recommenders, and Governance</a></p>
<p>Stray, Iyer, Larrauri (2023) <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4429558">“The Algorithmic Management of Polarization and Violence on Social Media”</a>.</p>
<ol type="1">
<li>Our overall goal should be to minimize “destructive conflict”.</li>
<li>The major lever used has been content moderation: changing the visibility of content based on semantic criteria (e.g.&nbsp;downranking toxic, disallowing hate speech).</li>
<li>However we should put relatively more work on system design, e.g.&nbsp;adding friction or changing the mechanics of sharing or engagement-based ranking. In part because there’s a robust correlation between content that causes destructive conflict and content that is engaging.</li>
</ol>
<p>Milli, Pierson and Garg (2023) <a href="https://arxiv.org/abs/2305.17428">Choosing the Right Weights: Balancing Value, Strategy, and Noise in Recommender Systems</a></p>
<p></p>
<p></p>
<p>Andrew Mauboussin (2022, SurgeAI) <a href="https://www.surgehq.ai/blog/what-if-social-media-optimized-for-human-values">“Moving Beyond Engagement: Optimizing Facebook’s Algorithms for Human Values”</a></p>
<ul>
<li>Says that the problem is <em>“the most engaging content is often the most toxic.”</em> They propose using human raters, e.g.&nbsp;ask people “did this post make you feel closer to your friends and family on a 1-5 scale?” They label a small set of FB posts as a proof of concept.</li>
</ul>
<p>Lubin &amp; Gilbert (2023) <a href="https://arxiv.org/abs/2306.07443">“Accountability Infrastructure: How to implement limits on platform optimization to protect population health”</a></p>
<ul>
<li><p>Very wide-ranging and loose discussion of issues related to ranking content. Makes an analogy with 19th century measures to control public health. I think the main proposal is that firms come up with metrics to measure their effect on social problems such as mental health, and regularly report on how they’re doing.</p></li>
<li><p>Suggested requirements for platforms of different sizes:</p>
<table class="table">
<colgroup>
<col style="width: 2%">
<col style="width: 97%">
</colgroup>
<tbody>
<tr class="odd">
<td>1M+</td>
<td>Submitted plan for metrics and methods for evaluation of potential structural harms</td>
</tr>
<tr class="even">
<td>10M+</td>
<td>Consistent data collection on potential structural harms</td>
</tr>
<tr class="odd">
<td>50M+</td>
<td>Quarterly, enforceable assessments on product aggregate effects on structural harms, with breakouts for key subgroups</td>
</tr>
<tr class="even">
<td>100M+</td>
<td>Monthly, enforceable assessments on product aggregate effects as well as targeted assessments of specific product rollouts for any subproduct used by at least 50 million users, with breakouts for key subgroups</td>
</tr>
</tbody>
</table></li>
</ul>
<p>Bengani, Stray, &amp; Thorburn (2022,Medium) <a href="https://medium.com/understanding-recommenders/whats-right-and-what-s-wrong-with-optimizing-for-engagement-5abaac021851">“What’s Right and What’s Wrong with Optimizing for Engagement”</a></p>
<ul>
<li>They define engagement as “a set of user behaviors, generated in the normal course of interaction with the platform, which are thought to correlate with value to the user, the platform, or other stakeholders.” Reviews evidence for good and bad effects of ranking by engagement. </li>
</ul>
</section>
</section>
<section id="appendix-taxonomy-of-metrics" class="level1">
<h1>Appendix: Taxonomy of Metrics</h1>
<p>This is meant to be a parsimonious taxonomy of metrics used in a recommender. They are organized by the the types of entity they apply to. For simplicity I omit aggregations (e.g.&nbsp;a user’s like rate is just the average over likes over user-item pairs), and I omit predictions (e.g.&nbsp;an item’s pToxic is just the prediction of whether a paid rater would rate the item as toxic).</p>
<table class="table">
<colgroup>
<col style="width: 15%">
<col style="width: 24%">
<col style="width: 60%">
</colgroup>
<thead>
<tr class="header">
<th><strong>entity</strong></th>
<th><strong>type of metric</strong></th>
<th><strong>metric</strong></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>user</td>
<td>activity</td>
<td>login (DAU/DAP)</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>time spent</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>evaluation</td>
<td>survey (“are you satisfied?”)</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>————-</td>
<td>——————–</td>
<td>————————————————–</td>
</tr>
<tr class="odd">
<td>item</td>
<td>paid rater</td>
<td>policy-violating (“does this violate policy?”)</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>quality evaluation (“does this fit quality defn?”)</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>objective features</td>
<td>recency</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>item contains author info</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td>item contains link</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>————-</td>
<td>——————–</td>
<td>————————————————–</td>
</tr>
<tr class="even">
<td>producer</td>
<td>objective features</td>
<td>off-platform popularity</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>graph statistics</td>
<td>network centrality</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>————-</td>
<td>——————–</td>
<td>————————————————–</td>
</tr>
<tr class="odd">
<td>user-item</td>
<td>social interaction</td>
<td>like (heart/fav/emoji reaction)</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>comment (reply)</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td>reshare (retweet/forward)</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>downstream interactions</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td>interest signal</td>
<td>linger (time spent watching)</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td>click (follow link, expand)</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>evaluation</td>
<td>star-rating</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>upvote (downvote)</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td>survey (“did you find this worth your time?”)</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>see-more</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td>dislike (see less)</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td></td>
<td>other</td>
<td>report</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>dislike</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>————-</td>
<td>——————–</td>
<td>————————————————–</td>
</tr>
<tr class="odd">
<td>user-producer</td>
<td>interest signal</td>
<td>follow (subscribe, friend)</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td>block</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>



</section>


 ]]></description>
  <guid>tecunningham.github.io/posts/2023-04-28-ranking-by-engagement.html</guid>
  <pubDate>Mon, 08 May 2023 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Social Media Suspensions of Prominent Accounts</title>
  <dc:creator>Tom Cunningham</dc:creator>
  <link>tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data.html</link>
  <description><![CDATA[ 




<style>
    h1 {  border-bottom: 4px solid black;}
    h2 {  border-bottom: 1px solid #ccc;}
</style>
<div class="page-columns page-full"><p><strong>Tom Cunningham.</strong> (<a href="https://twitter.com/testingham"><span class="citation" data-cites="testingham">@testingham</span></a>) First version Jan 31 2023, last updated April 12 2023.<sup>1</sup></p><div class="no-row-height column-margin column-container"><li id="fn1"><p><sup>1</sup>&nbsp;Thanks to comments from Sahar Massachi, Katie Harbath, Nichole Sessego, and many others.</p></li></div></div>
<p><strong>This note describes the suspension practices of the major social media platforms.</strong> I have collected a dataset of around 200 suspensions of prominent people across 12 platforms, stored in a <a href="https://docs.google.com/spreadsheets/d/1-lch6CvywoV4iAz0Yb61UsC9U6pgXDyYI2qb8jN5wmI/edit#gid=0">google spreadsheet</a>. The chart below summarizes the full dataset:</p>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-1_cfd31c44aaedec9ef1a202f494e0608c">
<div class="cell-output-display">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid" width="720"></p>
</div>
</div>
<p><strong>The data helps illuminate what platforms are doing.</strong> It is very difficult for an outside observer to see how a platform moderates their content. The advantages of studying the suspension of prominent users are that (1) the data is public and (2) the outcomes are comparable across platforms.</p>
<p><strong>Key findings.</strong></p>
<ol type="1">
<li><p><strong>The rate of suspensions has grown over time.</strong> The increase seems to be primarily due to a progressive adoption of new policies rather than changes in user behaviour or changes in enforcement.</p></li>
<li><p><strong>Suspension practices are fairly similar across the major platforms.</strong> Meta, Twitter, and YouTube all have broadly similar policies: they each suspend users for hate speech, election misinformation, COVID misinformation, and incitement.</p></li>
<li><p><strong>The most common reasons for suspension were hate speech (15%) and COVID misinformation (12%).</strong> Platforms typically do not publicly state the reason why they suspend an account, however I was able to code the majority of cases either because the reason was clear from context, or was reported by the user, or because the reason was given by the platform to a journalist. In 19% of cases I could find no reason given. For cases with a reason there were quite a wide variety, the top reasons were hate speech (15%), covid misinfo (12%), incitement (7%), and personal information (6%).</p></li>
<li><p><strong>Twitter suspended more people than other platforms.</strong> From examining cases it seems this was primarily due to differences in the type of content that is posted on Twitter compared to other platforms, rather than differences in policies or differences in enforcement.</p></li>
<li><p><strong>US politicians were suspended at a much higher rate than non-US politicians.</strong> This seems to be a mixture of US politicians being more active on Twitter, being more likely to make policy-violating statements, and being under more scrutinty from Twitter.</p></li>
<li><p><strong>Among US Federal politicians suspended, 8 were Republicans, none were Democrats.</strong> The Republicans were suspended for a variety of different reasons and on a variety of platforms. The asymmetry does not seem to be primarily a difference in enforcement, but a higher propensity for Republicans to say or do policy-violating things.</p></li>
</ol>
<p><strong>I am working on a separate essay about <em>why</em> platforms suspend users.</strong> It is difficult to give clear reasons <em>why</em> platforms suspend users. In a separate essay I try to break down how much their action can be attributed to influence from owners, from employees, from users, from advertisers, or from governments. Having this dataset of suspensions is very useful to be able to make generalizations about platform behavior.</p>
<section id="dataset" class="level1">
<h1>Dataset</h1>
<p><strong>The figure below shows the entire dataset.</strong> Each line represents a suspension, with a given start and end date, colored according to the platform that implemented the suspension. I have highlighted the earliest suspension I observe: Courtney’s Love suspension from Twitter in 2011 for defamation. I have also highlighted all the suspensions of Donald Trump: in 2017 a rogue employee suspended him from Twitter, in January 2021 he was suspended from multiple platforms. Twitter and Meta both gave him temporary suspensions, then a number of platforms gave him indefinite suspensions. Twitter, Meta and YouTube all lifted their Trump suspensions in late 2022 or early 2023.</p>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-2_2d70775b84c329e29d4c56424a4dae31">
<div class="cell-output-display">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid" width="720"></p>
</div>
</div>
<p><strong>I am including only <em>newsworthy</em> suspensions.</strong> My basic criterion is to include any suspension which merits a mainstream news story. My process of collection has been collecting whatever caught my attention and supplementing that with data from Wikipedia, Ballotpedia, and Google searches (see Appendix for more discussion of sources). I believe that the data probably covers the majority of the newsworthy suspensions in the US since 2016, however for suspensions outside the US the fraction is surely lower. There is more detail on data sources in an Appendix. I have a list of other datasources for suspensions at the bottom of this document. Particular blind spots are (1) countries outside the US; (2) platforms like Twitch; (3) celebrities, especially adult performers who have high rates of being suspended, and who often have many followers.</p>
<p><strong>I have a rough database of platform policy changes.</strong> In an Appendix I have a rough history of platform policy changes, and references to other similar databases online. One thing that I hope to work on as followup is a database of policies that would allow apples-to-apples compare what content is banned across platforms and across time. I also include in the Appendix a table showing general trends of famous people being excluded from mainstream media (film, television, music).</p>
</section>
<section id="aggregate-patterns" class="level1 page-columns page-full">
<h1>Aggregate Patterns</h1>
<p><strong>The rate of suspensions has substantially increased over time.</strong> The data show a substantial increase in suspensions, from an average of 1-2 per year before mid-2017, to an average of 10-20 per year since then. The observed increase is probably partly due to growth in overall activity on the platforms and partly due to my observations being biased towards more-recent suspensions, but I think the primary cause is an expansion of the rules which would justify a ban, discussed further below.</p>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-3_8b5d72c69d8dedcf21564ae2ff8c03eb">
<div class="cell-output-display">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-3-1.png" class="img-fluid" width="720"></p>
</div>
</div>
<p><strong>There are three prominent peaks in the history of suspensions:</strong></p>
<ul>
<li><strong>June 2020:</strong> a variety of American and British political personalities on the right were suspended for hate speech or similar reasons by Twitter and YouTube. On Twitter: Katie Hopkins, David Duke, American Renaissance (Jared Taylor). On YouTube: Stefan Molyneux, Gavin McInnes, American Renaissance (Jared Taylor), Richard Spencer, National Policy Institute.</li>
<li><strong>January 2021:</strong> many US politicians and political commentators were suspended after the capitol riot.</li>
<li><strong>November-December 2022:</strong> Twitter suspended many prominent figures for a variety of reasons under new policies introduced by Elon Musk.</li>
</ul>
<p><strong>The most suspensions are from Twitter, then Meta, then YouTube.</strong> We can see that Twitter suspension have been growing continuously since 2015, with a spike in 2021 due to the Capitol riot. Meta and YouTube also show an increase since 2017, but less-clear pattern of continuous growth.</p>
<p>In the Appendix I compare data from Twitter’s transparency reports which report <em>total</em> account suspensions on Twitter (i.e.&nbsp;suspensions of all accounts, not just prominent accounts) between 2018 and 2021. That data shows slower growth: suspensions for abuse grew by a factor of around 2, and suspensions for hateful conduct grew from 2018 to 2019 then decreased.</p>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-4_d8a14b6f7334069b8971d6412af5959a">
<div class="cell-output-display">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid" width="720"></p>
</div>
</div>
<p><strong>Suspension policies have become stricter.</strong> Several new policies have been added by platforms over the last 10 years which have tightened the limits on acceptable speech. Some examples (see Appendix for more details). Note that policies usually describe the type of content that will be removed, violations are only sometimes punished with an account suspension, and often platforms have a “strike” system for account suspension.</p>
<ol type="1">
<li>Misgendering/deadnaming: Twitter (2018), TikTok (2022).</li>
<li>Group superiority/inferiority: Meta (2019), YouTube (2019).</li>
<li>Holocaust denial: YouTube (2019), Twitter (2020), Meta (2020), Reddit (2020).</li>
<li>Harmful medical misinformation: Meta (2020), YouTube (2021).</li>
<li>False allegations of election fraud: Meta (2021), YouTube (2021).</li>
</ol>
<p>Broadly speaking it appears that the growth in suspension is attributable more to a change in policies than due to a change in behaviour or due to a change in application of policies. This is because it appears that a majority of suspensions over the last 5 years were taken under policies that did not exist prior to 2018.</p>
<p>Two important policies were introduced in response to external events: COVID misinformation and election deligitimization. Many accounts have been suspended under these new policies since 2020 but it is difficult to say whether this reflects a generally increased strictness of platforms.</p>
<p><strong>Suspension policies have rarely been relaxed.</strong> There are two substantial instances of relaxation of moderation policy I’m aware of:</p>
<ol type="1">
<li>A “newsworthiness” exemption for bans, i.e.&nbsp;effectively a relaxation of those bans. Newsworthiness exemption were officially introduced by Meta in 2016, Twitter in 2017, and Youtube in 2019.</li>
<li>Twitter’s relaxation of policies under Elon Musk in late-2022. Twitter unsuspended many prominent accounts, however they also suspended many accounts for new reasons, and there has been little official communication of new policies.</li>
</ol>
<p><strong>Reasons for suspension.</strong> Platforms typically do not publicly state the reason why they suspend an account, however I was able to code the majority of cases either because the reason was clear from context, or was reported by the user, or because the reason was given by the platform to a journalist. In 19% of cases I could find no reason given. For cases with a reason there were quite a wide variety:</p>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-5_ff7a82dba2e985426d66949f5f51e112">
<div class="cell-output-display">

<div id="czfldrgwuz" style="padding-left:0px;padding-right:0px;padding-top:10px;padding-bottom:10px;overflow-x:auto;overflow-y:auto;width:auto;height:auto;">
<style>#czfldrgwuz table {
  font-family: system-ui, 'Segoe UI', Roboto, Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol', 'Noto Color Emoji';
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}

#czfldrgwuz thead, #czfldrgwuz tbody, #czfldrgwuz tfoot, #czfldrgwuz tr, #czfldrgwuz td, #czfldrgwuz th {
  border-style: none;
}

#czfldrgwuz p {
  margin: 0;
  padding: 0;
}

#czfldrgwuz .gt_table {
  display: table;
  border-collapse: collapse;
  line-height: normal;
  margin-left: auto;
  margin-right: auto;
  color: #333333;
  font-size: 16px;
  font-weight: normal;
  font-style: normal;
  background-color: #FFFFFF;
  width: auto;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #A8A8A8;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #A8A8A8;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
}

#czfldrgwuz .gt_caption {
  padding-top: 4px;
  padding-bottom: 4px;
}

#czfldrgwuz .gt_title {
  color: #333333;
  font-size: 125%;
  font-weight: initial;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-color: #FFFFFF;
  border-bottom-width: 0;
}

#czfldrgwuz .gt_subtitle {
  color: #333333;
  font-size: 85%;
  font-weight: initial;
  padding-top: 3px;
  padding-bottom: 5px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-color: #FFFFFF;
  border-top-width: 0;
}

#czfldrgwuz .gt_heading {
  background-color: #FFFFFF;
  text-align: center;
  border-bottom-color: #FFFFFF;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#czfldrgwuz .gt_bottom_border {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#czfldrgwuz .gt_col_headings {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
}

#czfldrgwuz .gt_col_heading {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 6px;
  padding-left: 5px;
  padding-right: 5px;
  overflow-x: hidden;
}

#czfldrgwuz .gt_column_spanner_outer {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: normal;
  text-transform: inherit;
  padding-top: 0;
  padding-bottom: 0;
  padding-left: 4px;
  padding-right: 4px;
}

#czfldrgwuz .gt_column_spanner_outer:first-child {
  padding-left: 0;
}

#czfldrgwuz .gt_column_spanner_outer:last-child {
  padding-right: 0;
}

#czfldrgwuz .gt_column_spanner {
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: bottom;
  padding-top: 5px;
  padding-bottom: 5px;
  overflow-x: hidden;
  display: inline-block;
  width: 100%;
}

#czfldrgwuz .gt_spanner_row {
  border-bottom-style: hidden;
}

#czfldrgwuz .gt_group_heading {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  text-align: left;
}

#czfldrgwuz .gt_empty_group_heading {
  padding: 0.5px;
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  vertical-align: middle;
}

#czfldrgwuz .gt_from_md > :first-child {
  margin-top: 0;
}

#czfldrgwuz .gt_from_md > :last-child {
  margin-bottom: 0;
}

#czfldrgwuz .gt_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  margin: 10px;
  border-top-style: solid;
  border-top-width: 1px;
  border-top-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 1px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 1px;
  border-right-color: #D3D3D3;
  vertical-align: middle;
  overflow-x: hidden;
}

#czfldrgwuz .gt_stub {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
}

#czfldrgwuz .gt_stub_row_group {
  color: #333333;
  background-color: #FFFFFF;
  font-size: 100%;
  font-weight: initial;
  text-transform: inherit;
  border-right-style: solid;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
  padding-left: 5px;
  padding-right: 5px;
  vertical-align: top;
}

#czfldrgwuz .gt_row_group_first td {
  border-top-width: 2px;
}

#czfldrgwuz .gt_row_group_first th {
  border-top-width: 2px;
}

#czfldrgwuz .gt_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#czfldrgwuz .gt_first_summary_row {
  border-top-style: solid;
  border-top-color: #D3D3D3;
}

#czfldrgwuz .gt_first_summary_row.thick {
  border-top-width: 2px;
}

#czfldrgwuz .gt_last_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#czfldrgwuz .gt_grand_summary_row {
  color: #333333;
  background-color: #FFFFFF;
  text-transform: inherit;
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
}

#czfldrgwuz .gt_first_grand_summary_row {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-top-style: double;
  border-top-width: 6px;
  border-top-color: #D3D3D3;
}

#czfldrgwuz .gt_last_grand_summary_row_top {
  padding-top: 8px;
  padding-bottom: 8px;
  padding-left: 5px;
  padding-right: 5px;
  border-bottom-style: double;
  border-bottom-width: 6px;
  border-bottom-color: #D3D3D3;
}

#czfldrgwuz .gt_striped {
  background-color: rgba(128, 128, 128, 0.05);
}

#czfldrgwuz .gt_table_body {
  border-top-style: solid;
  border-top-width: 2px;
  border-top-color: #D3D3D3;
  border-bottom-style: solid;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
}

#czfldrgwuz .gt_footnotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#czfldrgwuz .gt_footnote {
  margin: 0px;
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#czfldrgwuz .gt_sourcenotes {
  color: #333333;
  background-color: #FFFFFF;
  border-bottom-style: none;
  border-bottom-width: 2px;
  border-bottom-color: #D3D3D3;
  border-left-style: none;
  border-left-width: 2px;
  border-left-color: #D3D3D3;
  border-right-style: none;
  border-right-width: 2px;
  border-right-color: #D3D3D3;
}

#czfldrgwuz .gt_sourcenote {
  font-size: 90%;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 5px;
  padding-right: 5px;
}

#czfldrgwuz .gt_left {
  text-align: left;
}

#czfldrgwuz .gt_center {
  text-align: center;
}

#czfldrgwuz .gt_right {
  text-align: right;
  font-variant-numeric: tabular-nums;
}

#czfldrgwuz .gt_font_normal {
  font-weight: normal;
}

#czfldrgwuz .gt_font_bold {
  font-weight: bold;
}

#czfldrgwuz .gt_font_italic {
  font-style: italic;
}

#czfldrgwuz .gt_super {
  font-size: 65%;
}

#czfldrgwuz .gt_footnote_marks {
  font-size: 75%;
  vertical-align: 0.4em;
  position: initial;
}

#czfldrgwuz .gt_asterisk {
  font-size: 100%;
  vertical-align: 0;
}

#czfldrgwuz .gt_indent_1 {
  text-indent: 5px;
}

#czfldrgwuz .gt_indent_2 {
  text-indent: 10px;
}

#czfldrgwuz .gt_indent_3 {
  text-indent: 15px;
}

#czfldrgwuz .gt_indent_4 {
  text-indent: 20px;
}

#czfldrgwuz .gt_indent_5 {
  text-indent: 25px;
}
</style>
<table class="gt_table" data-quarto-disable-processing="false" data-quarto-bootstrap="false">
  <thead>
    
    <tr class="gt_col_headings">
      <th class="gt_col_heading gt_columns_bottom_border gt_left" rowspan="1" colspan="1" scope="col" id="reason">reason</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1" scope="col" id="n">n</th>
      <th class="gt_col_heading gt_columns_bottom_border gt_right" rowspan="1" colspan="1" scope="col" id="pct">pct</th>
    </tr>
  </thead>
  <tbody class="gt_table_body">
    <tr><td headers="reason" class="gt_row gt_left">no reason given</td>
<td headers="n" class="gt_row gt_right">43</td>
<td headers="pct" class="gt_row gt_right">19</td></tr>
    <tr><td headers="reason" class="gt_row gt_left">other</td>
<td headers="n" class="gt_row gt_right">39</td>
<td headers="pct" class="gt_row gt_right">17</td></tr>
    <tr><td headers="reason" class="gt_row gt_left">hate speech</td>
<td headers="n" class="gt_row gt_right">36</td>
<td headers="pct" class="gt_row gt_right">16</td></tr>
    <tr><td headers="reason" class="gt_row gt_left">covid misinfo</td>
<td headers="n" class="gt_row gt_right">26</td>
<td headers="pct" class="gt_row gt_right">11</td></tr>
    <tr><td headers="reason" class="gt_row gt_left">incitement</td>
<td headers="n" class="gt_row gt_right">16</td>
<td headers="pct" class="gt_row gt_right">7</td></tr>
    <tr><td headers="reason" class="gt_row gt_left">personal information</td>
<td headers="n" class="gt_row gt_right">13</td>
<td headers="pct" class="gt_row gt_right">6</td></tr>
    <tr><td headers="reason" class="gt_row gt_left">election misinfo</td>
<td headers="n" class="gt_row gt_right">12</td>
<td headers="pct" class="gt_row gt_right">5</td></tr>
    <tr><td headers="reason" class="gt_row gt_left">hate group</td>
<td headers="n" class="gt_row gt_right">10</td>
<td headers="pct" class="gt_row gt_right">4</td></tr>
    <tr><td headers="reason" class="gt_row gt_left">manipulation</td>
<td headers="n" class="gt_row gt_right">10</td>
<td headers="pct" class="gt_row gt_right">4</td></tr>
    <tr><td headers="reason" class="gt_row gt_left">terrorist group</td>
<td headers="n" class="gt_row gt_right">9</td>
<td headers="pct" class="gt_row gt_right">4</td></tr>
    <tr><td headers="reason" class="gt_row gt_left">court order</td>
<td headers="n" class="gt_row gt_right">6</td>
<td headers="pct" class="gt_row gt_right">3</td></tr>
    <tr><td headers="reason" class="gt_row gt_left">threat</td>
<td headers="n" class="gt_row gt_right">5</td>
<td headers="pct" class="gt_row gt_right">2</td></tr>
    <tr><td headers="reason" class="gt_row gt_left">accident</td>
<td headers="n" class="gt_row gt_right">4</td>
<td headers="pct" class="gt_row gt_right">2</td></tr>
  </tbody>
  
  
</table>
</div>
</div>
</div>
<div class="cell page-columns page-full" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-6_9adbb12935db39ecf3ceef2e1985821b">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-6-1.png" class="img-fluid figure-img" width="720"></p>
<figcaption class="figure-caption margin-caption">Reasons for suspension over time</figcaption>
</figure>
</div>
</div>
</div>
<p><strong>Suspension practices are fairly similar across the major platforms.</strong> We can see that Meta, Twitter, and YouTube all have broadly similar policies: they will suspend users for all of the following reasons:</p>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-7_026509960857f8ece1994dc66c0489f5">
<div class="cell-output-display">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-7-1.png" class="img-fluid" width="720"></p>
</div>
</div>
<div class="page-columns page-full"><p>It is clear that Twitter has suspended more people than Meta and YouTube, but the majority of suspensions fall in categories which are also enforced by Meta and YouTube.<sup>2</sup></p><div class="no-row-height column-margin column-container"><li id="fn2"><p><sup>2</sup>&nbsp;One important exception is Dec 2022 spike in suspensions after Twitter adopted a new policy, suspending users for posting or alluding to already-public location information. This policy is notably stronger than Meta or YouTube.</p></li></div></div>
</section>
<section id="by-type-of-target" class="level1">
<h1>By Type of Target</h1>
<section id="politicians" class="level2">
<h2 class="anchored" data-anchor-id="politicians">Politicians</h2>
<p><strong>Among US Federal politicians only Republicans have been suspended.</strong> In the US 8 Republicans have had one or more suspension, but no Democrats. Among the Republicans the suspensions were for a variety of reasons: related to the Jan 6 riots (Trump, Barry Moore, MTG), related to COVID (Ron Johnson, Rand Paul, MTG), for misgendering (Jim Banks), for tweeting a threat (Briscoe Cain), for animal blood on a profile photo (Steve Daines), one by a rogue employee (Trump).</p>
<p>It seems to me that the asymmetry in suspensions is primarily due to Republicans being more likely to violate the policies, rather than asymmetric enforcement of existing policies. I am not aware of any cases where a Democratic politician violated one of these policies but was not suspended.</p>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-8_4b39fa579b0014b4e13751b72a03cb72">
<div class="cell-output-display">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-8-1.png" class="img-fluid" width="720"></p>
</div>
</div>
<p><strong>Suspension of national politicians outside the US has been relatively rare.</strong> My dataset contains 13 national politicians who were suspended in the world outside the US, compared to 8 in the US. This is a big asymmetry, and something of a puzzle. I have discussed this with a number of people who worked in enforcement and they attribute to a mixture of (1) less policy-violating behaviour from non-US politicians; (2) looser enforcement against non-US politicians; (3) lower overall social media usage outside the US; and (4) lower coverage of non-US politicians in my dataset.</p>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-9_82e4c14aec1e2b6dc315106cff761eb1">
<div class="cell-output-display">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid" width="720"></p>
</div>
</div>
</section>
<section id="us-prominent-figures" class="level2">
<h2 class="anchored" data-anchor-id="us-prominent-figures">US Prominent Figures</h2>
<p>This shows all suspensions of US “notable people”:</p>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-10_540d531ff21c87552c7eff94546ac03d">
<div class="cell-output-display">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid" width="720"></p>
</div>
</div>
<p><strong>Between 2015 and 2017 there were a series of alt-right personalities suspended from Twitter.</strong> The suspensions were often not for their views but their behaviour:</p>
<ul>
<li>2015: Charles Johnson from Twitter for a threat.</li>
<li>2016: Milo Yiannopoulos from Twitter for harassment, Richard Spencer from Twitter for manipulation.</li>
<li>2017: Roger Stone from Twitter for abuse.</li>
<li>2018: Alex Jones from Twitter for incitement and abuse.</li>
</ul>
<p><strong>Beginning in late 2017 more alt-right accounts were suspended.</strong> Either for hate speech, for offline behaviour, or without any public reason given:</p>
<ul>
<li>Late 2017: Baked Alaska from Twitter for hate speech.</li>
<li>2018: Owen Benjamin from Twit with no reason given, Alex Jones from FB and YouTube for hate speech.</li>
<li>2019: Nick Fuentes from Meta with no reason given.</li>
</ul>
<p><strong>Between November 2020 and January 2021 a large set of prominent figures were suspended for election-related reasons.</strong> The most suspensions were on Twitter but there were also from other platforms.</p>
<ol type="1">
<li>Since November 2022 Twitter has unsuspended a large fraction of the suspended users that I track, probably around 1/2.</li>
<li>Some people have been suspended simultaneously across multiple platforms (e.g.&nbsp;Alex Jones)</li>
</ol>
</section>
</section>
<section id="by-platform" class="level1">
<h1>By Platform</h1>
<section id="meta" class="level2">
<h2 class="anchored" data-anchor-id="meta">Meta</h2>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-11_eb80943af807be235d172b5d521a9003">
<div class="cell-output-display">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-11-1.png" class="img-fluid" width="720"></p>
</div>
</div>
</section>
<section id="twitter" class="level2">
<h2 class="anchored" data-anchor-id="twitter">Twitter</h2>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-12_3e00afcd1ca8f01e55a23cb64471052f">
<div class="cell-output-display">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid" width="720"></p>
</div>
</div>
<p>The following chart shows just accounts that were un-suspended under Musk, i.e.&nbsp;people with Twitter suspension that started before Oct 27 2022 and ended after that date. See below for a more fine-grained dataset of accounts unsuspended under Musk.</p>
<p>You can see that the primary original reasons for suspension were hate speech COVID misinformation. Kanye West and Nick Fuentes were re-suspended under Musk.</p>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-13_a872a3eba51490eddb3b2b48de691a94">
<div class="cell-output-display">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-13-1.png" class="img-fluid" width="720"></p>
</div>
</div>
</section>
<section id="youtube" class="level2">
<h2 class="anchored" data-anchor-id="youtube">YouTube</h2>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-14_e58a255f4894d94b3aa8c1fe08645b71">
<div class="cell-output-display">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-14-1.png" class="img-fluid" width="720"></p>
</div>
</div>
</section>
<section id="tik-tok" class="level2">
<h2 class="anchored" data-anchor-id="tik-tok">Tik Tok</h2>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-15_10a2c304a47181c259ef42df0c03ed44">
<div class="cell-output-display">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-15-1.png" class="img-fluid" width="720"></p>
</div>
</div>
</section>
<section id="other-platforms" class="level2">
<h2 class="anchored" data-anchor-id="other-platforms">Other Platforms</h2>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-16_3e7afd182db5d2908b5172fb0129ec8d">
<div class="cell-output-display">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-16-1.png" class="img-fluid" width="720"></p>
</div>
</div>
</section>
</section>
<section id="by-policy" class="level1">
<h1>By Policy</h1>
<section id="hate-speech" class="level2">
<h2 class="anchored" data-anchor-id="hate-speech">Hate Speech</h2>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-17_a308a4f9cba6e8d6b0196bdeaa6f4d29">
<div class="cell-output-display">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-17-1.png" class="img-fluid" width="720"></p>
</div>
</div>
</section>
</section>
<section id="appendix-data-sources" class="level1 page-columns page-full">
<h1>Appendix: Data Sources</h1>
<p>In this Appendix I give a fairly lengthy discussion of what data there is available regarding suspensions on each platform, as well as data on other types of content moderation and data on the history of policy announcements by platforms.</p>
<section id="twitter-1" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="twitter-1">Twitter</h2>
<p><strong><a href="https://en.wikipedia.org/wiki/Twitter_suspensions">Wikipedia page on Twitter Suspensions.</a></strong> Wikipedia has a list of around 400 Twitter suspensions. I chose not to create my own database (partly drawing from Wikipedia) for a few reasons: (1) I would want to add a lot of annotations to the Wikipedia data, e.g.&nbsp;about reasons for suspension or types of suspension. (2) Parsing the data is nontrivial: date ranges are given in various formats and would require some work on a regex to parse consistently. (3) There is some missing and inconsistent data, e.g.&nbsp;it has Trump’s suspension start-date but not end-date, and the names of people are not consistent (e.g.&nbsp;sometimes “Donald Trump”, sometimes “Donald J Trump”).</p>
<p>The Wikipedia dataset shows a similar basic pattern to what I document above: a dramatic increase in the rate of suspensions around mid-2017</p>
<div class="cell page-columns page-full" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-18_1bf3413098221e085d48d034d037542c">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-18-1.png" class="img-fluid figure-img" width="900"></p>
<figcaption class="figure-caption margin-caption">Wikipedia-reported Twitter suspension by year</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell page-columns page-full" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-19_c5f74451b453bf7e848610f27526755f">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-19-1.png" class="img-fluid figure-img" width="1080"></p>
<figcaption class="figure-caption margin-caption">All Wikipedia-reported Twitter suspension, highlighting accounts with more than 1M followers (not all suspensions list the number of followers).</figcaption>
</figure>
</div>
</div>
</div>
<p><strong><a href="https://github.com/travisbrown/twitter-watch">Travis Brown: Twitter Watch</a></strong> This project appears to have data on almost all suspensions on Twitter since Feb 2022, and also tracks whether the suspension have been reversed. It does <em>not</em> include any suspensions which started prior to Feb 2022. There is a giant CSV file with 600K rows, <a href="https://github.com/travisbrown/twitter-watch/blob/main/data/suspensions.csv">suspensions.csv</a>. Some visualizations:</p>
<div class="cell page-columns page-full" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-21_9054861682c9376a5d55f46cee01c32b">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-21-1.png" class="img-fluid figure-img" width="720"></p>
<figcaption class="figure-caption margin-caption">Observations by date of suspension</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell page-columns page-full" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-22_e848dbe17f1db9aa7664f3a523f905f6">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-22-1.png" class="img-fluid figure-img" width="720"></p>
<figcaption class="figure-caption margin-caption">Observation by date of unsuspension</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell page-columns page-full" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-23_42058d5236b1617eb5634b37d9e5d31b">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-23-1.png" class="img-fluid figure-img" width="720"></p>
<figcaption class="figure-caption margin-caption">Observation by date of account creation</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell page-columns page-full" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-24_55268f72348608ea351ec913903bf6c5">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-24-1.png" class="img-fluid figure-img" width="720"></p>
<figcaption class="figure-caption margin-caption">Suspensions for accounts with &gt;1M followers</figcaption>
</figure>
</div>
</div>
</div>
<p><strong><a href="https://github.com/travisbrown/unsuspensions">Travis Brown: Twitter Unsuspensions</a>.</strong> This is a collection of users who Twitter has un-suspended since Oct 27 2022 (when Musk took over). For some accounts there is a date of suspension but some have missing dates, I think suspension-date is only observed if after Feb 2022. (The content of this dataset is neither a subset nor a superset of the previous daatset). Unfortunately the dataset doesn’t have follower-count or twitter handle, so it’s not easy to join with other datasets or find the most prominent accounts.</p>
<div class="cell page-columns page-full" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-25_dd7d0641c81a59a2fbf91706339566ef">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-25-1.png" class="img-fluid figure-img" width="720"></p>
<figcaption class="figure-caption margin-caption">Observations by date of suspension</figcaption>
</figure>
</div>
</div>
</div>
<div class="cell page-columns page-full" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-26_02a275045ed7c275a3263524fdc975e0">
<div class="cell-output-display page-columns page-full">
<div class="quarto-figure quarto-figure-center page-columns page-full">
<figure class="figure page-columns page-full">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-26-1.png" class="img-fluid figure-img" width="720"></p>
<figcaption class="figure-caption margin-caption">Observations by date of unsuspension</figcaption>
</figure>
</div>
</div>
</div>
<p><strong><a href="https://github.com/travisbrown/deleted-tweets/blob/main/data/suspended/README.md">Travis Brown: Deleted Tweets / Suspended Accounts</a>.</strong> This project scrapes profiles from the Wayback Machine, and seems to have a large set of accounts that were suspended with fairly long retention, I have not yet investigated further.</p>
<p><strong><a href="https://transparency.twitter.com/en/reports/rules-enforcement.html#2021-jul-dec">Twitter Transparency Reports</a>.</strong> This has data on the aggregate number of suspensions per half between July 2018 and Dec 2021. Note that the website is down but the CSV files can still be downloaded. </p>
<div class="cell" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-27_5fac0f31901c654db7f37a6c8624dcbb">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-27-1.png" class="img-fluid figure-img" width="720"></p>
<figcaption class="figure-caption">Total Accounts Suspended on Twitter by Reason, 2018H2-2021H2</figcaption>
</figure>
</div>
</div>
</div>
<p><strong><a href="https://counterhate.com/research/toxic-twitter/">CounterHate list of unsuspensions</a>.</strong> The organization CounterHate has a list of 10 large accounts reinstated by Twitter since Musk’s takeover. Note I believe they incorrectly listed Rizza Islam as an account re-activated by Twitter: I can find no evidence that the acccount <span class="citation" data-cites="RizzaIslam">@RizzaIslam</span> was ever suspended, it seems to have been continuously tweeting from November 2022 through Feb 2023. I have added all 10 accounts to my database, and checked activity across all platforms.</p>
</section>
<section id="youtube-1" class="level2">
<h2 class="anchored" data-anchor-id="youtube-1">YouTube</h2>
<p><strong><a href="https://en.wikipedia.org/wiki/YouTube_suspensions">Wikipedia page on YouTube suspensions</a>.</strong> See above for reasons why I chose not to use this dataset as the primary source.</p>
<p><strong><a href="https://youtube.fandom.com/wiki/Terminated_YouTubers">Wikitubia: Terminated YouTubers</a>.</strong> A list of around 2300 YouTubers that have been permanently banned, including date of ban, subscribers, reason for ban, and citation. They don’t have a date when <em>unbanned</em>.</p>
</section>
<section id="meta-facebook-instagram" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="meta-facebook-instagram">Meta / Facebook / Instagram</h2>
<p><strong>There is no Wikipedia page of suspensions on Facebook, Instagram or WhatsApp.</strong></p>
<p><strong>Meta’s <a href="https://transparency.fb.com/data/community-standards-enforcement/">“Community Standards Enforcement Report”</a> is shown below.</strong> Meta’s data does not include any data on account suspensions, however there are a few other patterns of interest.</p>
<ol type="1">
<li><p><strong>Content actioned is relatively stable.</strong> There are fairly few notable upward or downward trends across the different types of content actioned: terrorism content actioned has increased significantly on both platforms, hate speech actions increased up to the end of 2020, then declined.</p></li>
<li><p><strong>The proactive detection rate is close to 100% for most categories.</strong> there were dramatic improvements for bullying and for hate speech over 2017-2021. Note that the proactive detection rate is the share of <em>actioned</em> content that is automatically detected, the share of <em>true</em> positives that are automatically detected is surely much lower.</p></li>
<li><p><strong>The prevalence of volations has fallen significantly.</strong> The log axis diminishes the magnitude of the decline: prevalence has fallen by a factor of 2-5 for nudity, bullying, hate speech, and graphic content. (I only show the prevalence upper bound, but the lower bound generally tracks the same course).</p></li>
</ol>
<div class="cell page-columns page-full" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-28_22f0e5bcb4c81b7617103fa0aa729687">
<div class="cell-output-display column-page">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-28-1.png" class="img-fluid" width="1440"></p>
</div>
</div>
<p><strong><a href="https://theintercept.com/document/2021/10/12/facebook-dangerous-individuals-and-organizations-list-reproduced-snapshot/">Facebook’s dangerous organizations list</a>.</strong> This list was leaked in 2021 by the Intercept. Unfortunately it does not include the dates of when each organization was added. The list is organized into the following categories:</p>
<ul>
<li>Terror Organizations (e.g.&nbsp;Islamic State)</li>
<li>Crime Organizations (e.g.&nbsp;Bloods, Crips)</li>
<li>Hate Organizations (e.g.&nbsp;Aryan Nation, includes bands and websites)</li>
<li>Militarized Social Movements (e.g.&nbsp;United States Patrio Defense Force)</li>
<li>Violent Non-State Actors (e.g.&nbsp;Free Syrian Army)</li>
<li>Hate (e.g.&nbsp;David Duke)</li>
<li>Individuals: Crime (e.g.&nbsp;Denton Suggs, Gangster Disciples)</li>
<li>Individuals: Terror (e.g.&nbsp;Osama bin Laden)</li>
</ul>
</section>
<section id="tiktok" class="level2">
<h2 class="anchored" data-anchor-id="tiktok">TikTok</h2>
<p><strong><a href="https://www.tiktok.com/transparency/en/community-guidelines-enforcement-2022-3/">Community Standards Report</a>.</strong> Shows an increase in suspensions from around 1M accounts/quarter per 2020 to 6M accounts/quarter in 2023.</p>
<p><img src="tecunningham.github.io/posts/images/2023-02-27-16-29-44.png" class="img-fluid"></p>
</section>
<section id="twitch" class="level2">
<h2 class="anchored" data-anchor-id="twitch">Twitch</h2>
<p><strong><a href="https://streamerbans.com">StreamerBans</a>.</strong> They seem to have a pretty comprehensive database of bans on Twitch.</p>
</section>
<section id="other-platforms-1" class="level2">
<h2 class="anchored" data-anchor-id="other-platforms-1">Other Platforms</h2>
<ul>
<li><p><strong>Spotify.</strong> The only unambiguous suspension from Spotify I found was Alex Jones’ podcast. Spotify removed some episodes of Joe Rogan’s podcast, and <a href="https://www.nytimes.com/2018/05/10/arts/music/rkelly-spotify-accusations-xxxtentacion.html">removed</a> R Kelly and XXXtentacion’s music from playlists. They <a href="https://www.thefader.com/2022/09/27/new-study-finds-spotify-slow-to-take-down-white-supremacist-music">remove</a> some white-supremacist artists and music. They <a href="https://themusicnetwork.com/spotify-took-the-moral-high-ground-on-r-kelly-xxxtentacion-dont-be-shocked-this-has-happened-before/">removed</a> all music from the band LostProphets after their lead singer was convicted of child sexual abuse.</p></li>
<li><p><strong>Substack.</strong> I’m not aware of anybody who’s been kicked off Substack, they present themselves as very pro-free-speech.</p></li>
<li><p><strong>Reddit.</strong> I’m not aware of any data on reddit account suspensions.</p></li>
<li><p><strong>Rumble.</strong> The Rumble video-hosting platform has become quite large (they claim 70M MAU, and have a market cap of ). Their <a href="https://rumble.com/s/terms">terms of service</a> restrict content that is “abusive, inciting violence, harassing, harmful, hateful, anti-semitic, racist or threatening.” However I have not yet found a single example of a prominent user who has been suspended from Rumble.</p></li>
</ul>
</section>
<section id="other-data-sources" class="level2">
<h2 class="anchored" data-anchor-id="other-data-sources">Other Data Sources</h2>
<p><strong>Can find more suspensions by searching Wikipedia for “suspended from XXX”.</strong> E.g. <code>site:wikipedia.org "suspended from facebook"</code>. Possibly worth doing the same search for Google News.</p>
<p><strong><a href="https://socialblade.com/twitter/user/dbongino/monthly">SocialBlade</a></strong> has data on number of followers by month since 2018, across Twitter, FB, YouTube. I’m not sure how easy it would be to scrape this data. They have a paid API, they say “up to 3 years of Historical statistics on creators.” However the website seems to have data back to at least April 2018.</p>
<p><strong><a href="https://ballotpedia.org/Elected_officials_suspended_or_banned_from_social_media_platforms">Ballotpedia list of elected officials suspended from social media.</a></strong> It is an excellent resource, appears comprehensive and cites original reporting. I have added all of their data to the database as of January 2023.</p>
<p><strong>Global Internet Forum to Counter Terrorism (GIFCT).</strong> They mainly work on sharing hashes of terrorist content between platforms. They have some dicussion papers about “terror designation lists” but I don’t think they maintain any lists themselves.</p>
<p><strong><a href="https://en.wikipedia.org/wiki/Specially_Designated_Global_Terrorist">Specially Designated National / Global Terrorist (SDN/SDGT)</a>.</strong> This is a public list maintained by the US government, and consumed by a number of tech companies. The <a href="https://home.treasury.gov/policy-issues/financial-sanctions/specially-designated-nationals-and-blocked-persons-list-sdn-human-readable-lists">full history</a> is available, but it would be extremely difficult to parse.</p>
<p><strong><a href="https://www.lumendatabase.org/">Lumen</a>.</strong> This has an international database of government takedown requests. They also seem to include whether the request was honored.</p>
<p><strong>CCDH Disinformation Dozen.</strong> This is a list from March 2021 of prominent accounts who were spreading anti-vax information on social media: <a href="https://counterhate.com/research/the-disinformation-dozen/">original report</a>, <a href="https://f4d9b9d3-3d32-4f3a-afa6-49f8bf05279a.usrfiles.com/ugd/f4d9b9_7a52dea3ce19442a92a0aa62ece7c045.pdf">followup report from April 2021</a>). They also have a <a href="https://counterhate.com/research/the-toxic-ten/">“toxic ten”</a> report. It’s probably worth adding both lists to the database.</p>
</section>
</section>
<section id="appendix-codebook" class="level1">
<h1>Appendix: Codebook</h1>
<p><strong>What suspensions to include.</strong> I aim to include any suspension which merits a national news story. In the future it would be nice to also include any suspensions of accounts with more than a certain number of followers.</p>
<p><strong>Definition of a suspension.</strong> It is a suspension if any of the following are true:</p>
<ul>
<li>Your existing content is available but you cannot post new content.</li>
<li>Users cannot see your existing content anywhere on the site.</li>
<li>Users cannot post links to your domain (e.g.&nbsp;cannot link to <code>piratebay.se</code>).</li>
<li>Users cannot search for your hashtag (this applies to social movements, e.g.&nbsp;<code>#StopTheSteal</code>).</li>
<li>Your content is blocked in the region where the majority of your followers are.</li>
</ul>
<p>On Twitter users often can end their suspension by deleting a specific tweet, I count these as suspensions. The following do not count as suspensions:</p>
<ul>
<li>Your content is demonetized but still visible.</li>
<li>Your content is removed from some surfaces but still available on your profile.</li>
<li>Your content is blocked in a region where a minority of your followers are.</li>
<li>Your content is blocked only for a secondary type of sharing (e.g.&nbsp;when Instagram <a href="https://www.thethings.com/why-madonna-banned-from-instagram-live-controversial-photos/">disabled</a> Madonna’s ability to stream to Instagram Live, but not to post regular photos; e.g.&nbsp;in 2010 50 cent was <a href="https://metro.co.uk/2010/08/30/50-cent-in-twitter-war-after-x-rated-photos-see-him-banned-3436802/">suspended from Twitpic</a> but not Twitter).</li>
</ul>
<p><strong>Coding of platform.</strong> I categorize a suspension from either FB or Instagram as a suspension from “Meta”. There are some cases where a person has been suspended from one but not the other, in those cases I leave an annotation in the spreadsheet.</p>
<p><strong>Coding of reason for suspension.</strong> Most platforms do not give public statements on the reason for a suspension, however it appears that they commonly give information to journalists which is then reflected in news stories. When there is some quotation I include it in the “long reason.” The reasons are as stated by platform, I am obviously not endorsing their judgments of whether they are making correct determinations of whether the policy applies, or if the stated reason is the true reason. I have tried to compress the “short” reasons into a small number of distinct reasons using my judgment. In some cases there is no company statement but the reason is obvious (e.g.&nbsp;I coded Kanye West’s “I’m going death con 3 on jewish people” statement as “hate speech”).</p>
<p><strong>We record suspensions of <em>entities</em> not <em>accounts</em>.</strong> I try to record the suspension of the <em>person</em> or <em>group</em> rather than the account. E.g. Courtney Love’s original Twitter account (<a href="https://twitter.com/CourtneyLoveUK"><span class="citation" data-cites="CourtneyLoveUK">@CourtneyLoveUK</span></a>) was suspended in 2011 and is still suspended, but Courtney Love seems to have been using another account since at least April 2012, without any suspensions for ban-evasion, and so I count the end-date as April 2012.</p>
</section>
<section id="appendix-history-of-policy-changes" class="level1">
<h1>Appendix: History of Policy Changes</h1>
<p><strong>Here we collect platform statements of policy changes.</strong> I try to restrict to explicit statements of changes regarding what is banned or not. For each statement I give a summary, if a single statement contains multiple significant changes I list it multiple times. The list is very incomplete, I would guess it currently contains only around 1/2 of the relevant policy changes from the large platforms. Additionally there are probably many significant policy changes that are never explicitly announced.</p>
<ul>
<li><strong>Meta.</strong>
<ul>
<li>2013-05-01: <a href="https://www.theguardian.com/technology/2013/oct/23/facebook-removes-beheading-video">ban graphic violence (beheading)</a></li>
<li>2013-10-23: <a href="https://www.theguardian.com/technology/2013/oct/23/facebook-removes-beheading-video">ban graphic violence (beheading) (again)</a>: <em>“Facebook had introduced a temporary ban on such videos in May but later decided to remove the block on the grounds that the site is used to share information about world events.”</em> Then ban was reintroduced after outcry.</li>
<li>2014-06-13: <a href="https://time.com/2869849/facebook-breastfeeding-nipples/">exemption for nipples if breastfeeding</a></li>
<li>2015-12-01: <a href="https://www.washingtonpost.com/technology/2020/06/28/facebook-zuckerberg-trump-hate/">exemption for newsworthy (not announced, applied to Trump video)</a></li>
<li>2016-09-01: <a href="https://www.facebook.com/josofsky/posts/10157347245570231">exemption for nudity if historically important (“terror of war”/napalm girl)</a></li>
<li>2016-10-21: <a href="https://about.fb.com/news/2016/10/input-from-community-and-partners-on-our-community-standards/">exemption for newsworthy (publicly announced)</a></li>
<li>2019-03-27: <a href="https://about.fb.com/news/2019/03/standing-against-hate/">ban on white separatism</a> <em>“praise, support and representation of white nationalism and white separatism”</em></li>
<li>2019-09-24: <a href="https://about.fb.com/news/2019/09/elections-and-political-speech/">exemption for speech from politicians</a>, <em>“we will treat speech from politicians as newsworthy content that should, as a general rule, be seen and heard.”</em></li>
<li>2019-12-19: <a href="https://reclaimthenet.org/facebook-bans-denying-existence-gender-identity">ban on “denying existence” of someone’s gender identity</a>. The statement of the policy is not super clear but I believe this bans general statements of the form “trans women are men,” but it does not ban individual misgendering or deadnaming.</li>
<li>2020-06-04: <a href="https://about.fb.com/news/2021/06/facebook-response-to-oversight-board-recommendations-trump/">revoke exemption on speech from politicians (discretionary)</a> <em>“we will not treat content posted by politicians any differently from content posted by anyone else”</em></li>
<li>2020-08-27: <a href="https://transparency.fb.com/policies/community-standards/dangerous-individuals-organizations/">ban on support for hate events, mass murders or attempted mass murders, serial murders</a>. Replaces ban on support for “mass shooting”. This was two days after the Rittenhouse shooting on August 25.</li>
<li>2020-10-12: <a href="https://about.fb.com/news/2020/10/removing-holocaust-denial-content/">ban on holocaust denial</a>: “prohibit any content that denies or distorts the Holocaust.”</li>
<li>2020-10-26: <a href="https://www.facebook.com/business/news/facebook-ads-restriction-2020-us-election">temporary ban on political ads in US</a></li>
<li>2020-12-03: <a href="https://about.fb.com/news/2020/12/coronavirus/">ban on covid-19 vaccine misinfo</a> <em>“claims about these vaccines that have been debunked by public health experts”</em></li>
<li>2021-02-08: <a href="https://about.fb.com/news/2020/04/covid-19-misinfo-update/#removing-more-false-claims">ban on covid-19 misinfo</a> <em>“COVID-19 is man-made or manufactured”</em></li>
<li>2021-02-08: <a href="https://about.fb.com/news/2020/04/covid-19-misinfo-update/#removing-more-false-claims">ban on vaccine misinfo</a> <em>“claims such as: Vaccines are not effective at preventing the disease … It’s safer to get the disease than to get the vaccine … Vaccines are toxic, dangerous or cause autism.”</em></li>
<li>2021-05-26: <a href="https://about.fb.com/news/2020/04/covid-19-misinfo-update/#removing-more-false-claims">lift ban on covid-19 man-made claim</a> <em>“we will no longer remove the claim that COVID-19 is man-made or manufactured”</em></li>
<li>2021-06-23: <a href="https://www.brennancenter.org/our-work/analysis-opinion/facebooks-new-dangerous-individuals-and-organizations-policy-brings-more">three tiers of dangerous organizations</a></li>
</ul></li>
<li><strong>YouTube</strong>
<ul>
<li>2019-06-05: <a href="https://blog.youtube/news-and-events/our-ongoing-work-to-tackle-hate">ban on group superiority, holocaust denial, &amp; massacre denial</a>: <em>“prohibiting videos alleging that a group is superior in order to justify discrimination, segregation or exclusion based on qualities like age, gender, race, caste, religion, sexual orientation or veteran status. … we will remove content denying that well-documented violent events, like the Holocaust or the shooting at Sandy Hook Elementary, took place.”</em></li>
<li>2019-09-26: <a href="https://www.vox.com/recode/2019/9/26/20885783/facebook-twitter-youtube-policies-political-content">exemption for newsworthy</a> <em>“exemption for content with”educational, documentary/news, scientific or artistic value”</em></li>
<li>2020-09-25: <a href="https://www.npr.org/2020/09/25/916957090/google-to-halt-political-ads-after-polls-close-amid-worries-over-delayed-results">temporary ban on political ads after election</a></li>
<li>2020-10-15: <a href="https://blog.youtube/news-and-events/harmful-conspiracy-theories-youtube">ban on conspiracy theories - Qanon and pizzagate</a><em>“prohibit content that targets an individual or group with conspiracy theories that have been used to justify real-world violence.”</em></li>
<li>2021-01-07: <a href="https://twitter.com/YouTubeInsider/status/1347231471212371970">ban on false election claims</a> – accounts will get a strike if they make false election claim.</li>
<li>2021-09-19: <a href="https://blog.youtube/news-and-events/managing-harmful-vaccine-content-youtube/">ban on vaccine misinformation</a> <em>“content that falsely alleges that approved vaccines are dangerous and cause chronic health effects, claims that vaccines do not reduce transmission or contraction of disease, or contains misinformation on the substances contained in vaccines”</em></li>
</ul></li>
<li><strong>Twitter:</strong>
<ul>
<li>2009-01-18: <a href="https://web.archive.org/web/20090118211301/http://twitter.zendesk.com/forums/26257/entries/18311">first rules</a> – bans impersonation, private information, threats of violence, copyright infringement, spam, pornography in profile picture.</li>
<li>2013-08-07: <a href="https://www.npr.org/sections/alltechconsidered/2013/08/07/209602106/as-twitter-expands-reach-abuse-policy-gets-added-scrutiny">ban on targeted abuse</a>: <em>“Unlike Facebook, Twitter doesn’t ban bullying … targeted abuse and real threats of violence are verboten, but the page is kind of buried. So on Saturday, Twitter amended its rules, adding language on targeted abuse and harassment more prominently.”</em>. <a href="https://www.vice.com/en/article/z43xw3/the-history-of-twitters-rules">Vice article</a> says this was in response to abuse of UK women over the new pound notes.</li>
<li>2015-01-01: <a href="https://www.vice.com/en/article/z43xw3/the-history-of-twitters-rules">ban on “excessively violent media”</a></li>
<li>2015-04-01: <a href="https://www.vice.com/en/article/z43xw3/the-history-of-twitters-rules">ban on “threatening or promoting terrorism”</a></li>
<li>2015-04-01: <a href="https://www.vice.com/en/article/z43xw3/the-history-of-twitters-rules">ban on “promot[ing] violence against others… on the basis of race, ethnicity, national origin, religion, sexual orientation, gender, gender identity, age, or disability.”</a>. <em>“Twitter spokesperson …[said] the company does not prohibit hate speech.”‘Hateful conduct’ differs from ‘hate speech’ in that the latter focuses on words. It’s the incitement to violence that we’re prohibiting. Offensive content and controversial viewpoints are still permitted on Twitter.”</em></li>
<li>2015-08-02: <a href="https://web.archive.org/web/20150802125743/https://support.twitter.com/articles/20169997">ban on indirect threats of violence</a>. Vice says <em>“the original rules set out in 2009, which had explicitly limited the prohibition on threats to”direct” and “specific” threats.”</em> They say <em>“[prohibited] .. also the”incitement” of harassment – speech that wasn’t a threat per se, but was intended to result in threats regardless.”</em></li>
<li>2015-03-12: <a href="https://www.wired.com/2015/03/twitter-bans-revenge-porn/">ban on revenge porn</a>.</li>
<li>2017-09-01: <a href="https://www.washingtonpost.com/news/the-switch/wp/2017/09/26/twitter-explains-why-it-wont-take-down-trumps-north-korea-tweet/">newsworthiness exemption</a></li>
<li>2019-10-30: <a href="https://www.nytimes.com/2019/10/30/technology/twitter-political-ads-ban.html">ban on political ads</a></li>
<li>2020-10-14: <a href="https://www.bloomberg.com/news/articles/2020-10-14/twitter-like-facebook-to-remove-posts-denying-the-holocaust">ban on holocaust denial</a></li>
<li>2022-11-29: <a href="https://www.cnn.com/2022/12/19/tech/twitter-elon-musk-deletes-policy/index.html">ban sharing links to other platforms (lasted 1 day)</a></li>
<li>2022-12-14: <a href="https://twitter.com/TwitterSafety/status/1603165959669354496">ban sharing live location</a></li>
<li>2022-12-16: <a href="https://twitter.com/TwitterSafety/status/1619125418371723264">general reinstatement of suspended accounts</a> <em>“We did not reinstate accounts that engaged in illegal activity, threats of harm or violence, large-scale spam and platform manipulation, or when there was no recent appeal to have the account reinstated.”</em></li>
<li>2023-01-04: <a href="https://www.euronews.com/next/2023/01/04/twitter-advertising">lifts ban on political ads</a></li>
<li>2023-02-28: <a href="https://www.engadget.com/twitter-updates-violent-speech-policy-to-ban-wishes-of-harm-214320985.html">ban on wishes of harm</a> <em>“ban on violent threats, wishes of harm, glorification of violence, and incitement of violence”</em></li>
<li>2023-04-08: <a href="https://www.theverge.com/2023/4/18/23688192/twitter-harrass-transgender-users-policy">lifts ban on misgendering/deadnaming</a> removed <em>“targeted misgendering or deadnaming of transgender individuals.”</em>.</li>
</ul></li>
<li><strong>Tik Tok.</strong>
<ul>
<li>2019-10-02: <a href="https://newsroom.tiktok.com/en-us/understanding-our-policies-around-paid-ads">ban on political ads</a></li>
<li>2020-01-08: <a href="https://www.wired.com/story/tiktok-overhauls-community-guidelines/">ban on promoting discrimination</a>, “promoting or justifying exclusion, segregation, or discrimination”</li>
<li>2020-01-08: <a href="https://www.wired.com/story/tiktok-overhauls-community-guidelines/">ban on showing guns</a>, “the depiction, trade, or promotion of firearms,” with some exceptions.</li>
<li>2020-01-08: <a href="https://www.mediamatters.org/tiktok/tiktok-full-boogaloo-videos-even-though-it-prohibits-content-dangerous-individuals-and">ban on dangerous individuals and organizations</a></li>
<li>2020-01-08: <a href="https://www.mediamatters.org/fake-news/hoax-about-national-coronavirus-quarantine-spread-tiktok-despite-platforms-anti">ban on harmful misinformation</a> “misinformation that could cause harm to an individual’s health or wider public safety.”</li>
<li>2020-10-20: <a href="https://newsroom.tiktok.com/en-gb/countering-hate-on-tiktok-gb">ban on white nationalism, anti-semitic ideas, conversion therapy</a> <em>“white nationalism, white genocide theory, as well as statements that have their origin in these ideologies, and movements such as Identitarianism and male supremacy … misinformation about notable Jewish individuals and families who are used as proxies to spread antisemitism. We’re also removing content that is hurtful to the LGBTQ+ community by removing hateful ideas, including content that promotes conversion therapy and the idea that no one is born LGBTQ+.”</em></li>
<li>2022-02-09: <a href="https://www.npr.org/2022/02/09/1079643611/tiktok-bans-deadnaming-misgendering">ban on misgendering and deadnaming</a></li>
<li>2022-09-21: <a href="https://newsroom.tiktok.com/en-us/updating-our-policies-for-political-accounts">ban on fundraising by politicians</a></li>
</ul></li>
<li><strong>Reddit.</strong>
<ul>
<li>2015-07-16: <a href="https://www.buzzfeednews.com/article/charliewarzel/nothing-changes-at-reddit#.rdaWGZrbp">2015-07, new Reddit policy</a></li>
<li>2015-02-24: <a href="https://arstechnica.com/tech-policy/2015/02/reddit-bans-nude-images-posted-without-consent/">ban on non-consensual nudity</a></li>
<li>2015-08-05: <a href="https://boingboing.net/2015/08/05/reddits-new-content-policy-g.html">new policy</a>: removed “/r/CoonTown, /r/WatchNiggersDie, /r/bestofcoontown, /r/koontown, /r/CoonTownMods, /r/CoonTownMeta”.</li>
<li>2020-06-20: <a href="https://www.reddit.com/r/announcements/comments/hi3oht/update_to_our_content_policy/">updated hate speech policy</a>. <em>“communities and users that promote hate based on identity or vulnerability will be banned.”</em> They don’t explicitly mention holocaust denial in the post, but reporting indicates that they’re interpreting this to mean that holocaust denial is banned.</li>
</ul></li>
<li><strong>4chan.</strong>
<ul>
<li>2013-09-19: <a href="https://gigaom.com/2013/09/19/4chan-has-rules-now-apparently/">ban on doxxing</a></li>
<li>2014-09-03: <a href="https://arstechnica.com/tech-policy/2014/09/4chan-adopts-dmca-policy-after-nude-celebrity-photo-postings/">ban on DMCA/copyrighted content</a></li>
</ul></li>
</ul>
<section id="third-party-sources-on-platform-policies" class="level2">
<h2 class="anchored" data-anchor-id="third-party-sources-on-platform-policies">Third-Party Sources on Platform Policies</h2>
<p>There are a variety of third-party resources comparing policies across platforms, however none seem to have data comparable to the list above, i.e.&nbsp;a summary of specific content policy changes over time.</p>
<p><strong>Comparisons at a single point in time.</strong></p>
<ul>
<li><p>DNC (2020) <strong><a href="https://democrats.org/who-we-are/what-we-do/disinfo/comparative-social-media-policy-analysis/">Comparison of Misinformation Policies, 2020</a></strong></p></li>
<li><p>Consumer Reports (Aug 13 2020) <strong><a href="https://www.consumerreports.org/social-media/social-media-misinformation-policies/">Comparison of Misinformation Policies, 2020</a></strong>. Data as of 2020, with three levels: “allowed”, “sometimes”, and “prohibited”.</p></li>
<li><p>UNC CITAP (May 22 2020) <strong><a href="https://citapdigitalpolitics.com/?page_id=2508">Comparison of Misinformation Policies, 2020</a></strong>. Has tables comparing misinfo policies as of 2020, three levels: “prohibited”, “flagged”, “allowed.”</p></li>
<li><p>Election Integrity Partnership (Oct 28 2020) <strong><a href="https://www.eipartnership.net/policy-analysis/platform-policies">Comparison of Election Policies, 2020</a></strong>.</p></li>
<li><p>Carnegie Endowment (April 1 2021) <strong><a href="https://carnegieendowment.org/2021/04/01/how-social-media-platforms-community-standards-address-influence-operations-pub-84201">Existence of Policies, 2021</a></strong>. Table just marks <em>whether</em> a platform has a policy on some type of content, not nature of policy. They also they have a database of platform policies but it seems to only have data from February 2021.</p></li>
<li><p>Virality Project <strong><a href="https://www.viralityproject.org/policy-analysis/evaluating-covid-19-vaccine-policies-on-social-media-platforms">Comparison of COVID Vaccine Policies in 2021</a>.</strong></p></li>
</ul>
<p><strong>Policies tracked over time.</strong></p>
<ul>
<li><p>Katie Harbath and Collier Fernenkes (August 2022) <strong><a href="https://bipartisanpolicy.org/blog/tech-platforms-election-database/">Election Policy Announcements, 2003-2022</a></strong>. Google spreadsheet with links to around 600 policy announcements, organized by platform, author, date, product-type, and country. Focussed on election-related policies, and they don’t include summaries of the policy announcement. They also wrote up analyses: (1) <a href="https://bipartisanpolicy.org/report/history-tech-elections/">“A Brief History of Tech and Elections”</a>; (2) <a href="https://bipartisanpolicy.org/blog/tech-midterm-election-announcements/">2022 election announcements</a>.</p></li>
<li><p>Ranking Digital Rights Index, <strong><a href="https://rankingdigitalrights.org/bts22/explore-indicators">Comparison of Privacy and Transparency Policies, 2017-2022</a></strong>. They collect perhaps 100 different indicators across around 15 tech companies, mostly related to privacy and transparency, earliest data from 2017. All the data is available.</p></li>
<li><p>GLAAD <strong><a href="https://sites.google.com/glaad.org/smsi/platform-scores?authuser=0">Comparison of LGBTQ user safety, 2021-2022</a></strong></p></li>
<li><p>CELE, <strong><a href="https://letrachica.digital">Letra Chica</a>.</strong> Tracks all public policy changes on Meta, YouTube, and Twitter. Most data from May 2020, but they go back to 2019 for Facebook by using FB’s Transparency Center. Each policy update includes a short summary of what’s changed. Tracks both Spanish and English versions. Data stored on coda.io, I think it’s queryable.</p></li>
<li><p>Linterna Verdes, <strong><a href="https://www.circuito.digital">Circuito</a></strong>. Has about 15 in-depth case studies of platform moderation decisions.</p></li>
<li><p>Humboldt Institute, <strong><a href="https://pga.hiig.de">Platform Governance Archive</a>.</strong> Comprehensive archive of ToS, Privacy Policy, and Community Guidlines, from 2004 until late 2021, for FB, IG, Twitter, and YouTube. The data will not be updated.</p></li>
<li><p><strong><a href="https://opentermsarchive.org">Open Terms Archive</a></strong>. Started by the French Ambassador for Digital Affairs, but now a collaboration. Tracks terms for many different online services in a <a href="https://github.com/OpenTermsArchive/">github repo</a>. The Platform Governance Archive has moved to be part of this project, <a href="https://github.com/OpenTermsArchive/pga-snapshots">here</a>.</p></li>
<li><p>EFF, <strong><a href="https://tosback.org">TOSback</a></strong>. Database of historical ToS documents from different services, with cross-platform comparisons. The most recent updates seem to be from May 2021, possibly was succeeded by Open Terms Archive.</p></li>
<li><p>European Commission, <strong><a href="https://zenodo.org/record/6461568#.ZD7KCi-21pS">Copyright Content Moderation and Removal</a>.</strong> This PDF report includes a lot of work which maps the copyright policies of major platforms.</p></li>
</ul>
<p><strong>Narrative histories:</strong></p>
<ul>
<li>Catherine Buni and Soraya Chemaly (2016, the Verge) <strong><a href="https://www.theverge.com/2016/4/13/11387934/internet-moderator-history-youtube-facebook-reddit-censorship-free-speech">History of Moderation</a></strong>.</li>
<li>Sarah Jeong (2016, Vice) <strong><a href="https://www.vice.com/en/article/z43xw3/the-history-of-twitters-rules">The History of Twitter’s Rules</a></strong></li>
<li>Bergen (2022) <strong><a href="https://www.penguinrandomhouse.com/books/653248/like-comment-subscribe-by-mark-bergen/">Like, Comment, Subscribe</a>.</strong> A book on the history of YouTube, it has a lot of detail on policy changes.</li>
</ul>
</section>
</section>
<section id="appendix-exclusion-of-prominent-people-from-mass-media" class="level1 page-columns page-full">
<h1>Appendix: Exclusion of Prominent People from Mass Media</h1>
<p><strong>This is a very crude history of people who’ve been “excluded” from mass media in the US.</strong></p>
<p><strong>Definition of exclusion:</strong> any impediment to work in entertainment due to your behaviour, independent of the quality of your work, either through explicit blacklisting or general sentiment. I have not taken the time to include references. In some cases there are conflicting claims about whether there was an exclusion, I generally take the person’s own judgment at face value.</p>
<div class="page-columns page-full"><p><strong>More important than exclusions of individuals were exclusions of entire classes.</strong> For much of the 20th century african americans, asians, women, &amp; openly gay people, had a pronounced difficulty getting roles in entertainment. This would be useful to document quantitatively but I think it would be more work.<sup>3</sup></p><div class="no-row-height column-margin column-container"><li id="fn3"><p><sup>3</sup>&nbsp;There’s a nice visualization <a href="https://www.scmp.com/infographics/article/1917641/infographic-what-color-oscar">here</a> showing the history of oscar nominations: between 1927 and 1948 there were approximately 400 nominations for best actor or director, the only black nominee was Hattie McDaniel in 1939 for best supporting actress in Gone with the Wind.</p></li></div></div>
<p><strong>There are three basic reasons for exclusion: politics, art, or personal life.</strong></p>
<p><strong>Exclusion of people for their politics.</strong> By far the biggest exclusion was the anti-communist blacklisting of the 1940s and 1950s. Many of the most prominent people in Hollywood were out of work for a decade or more, often based on fairly weak evidence of communist associations.</p>
<p>During wars celebrities have sometimes been excluded for being disloyal: during the Vietnam war (Jane Fonda, John Lennon, Eartha Kitt), and during the Iraq War (Sean Penn, Dixie Chicks).</p>
<p>In the last 20 years a number of people have been excluded for anti-homosexual statements (Isaiah Washington, Kirk Cameron) or generally conservative statements (Gina Carano, James Woods, Stacy Dash).</p>
<p><strong>Exclusion of people for their art.</strong> These seem relatively rare. During the 1970s-1990s a few musicians were excluded for shocking or transgressive art, e.g.&nbsp;the Sex Pistols, Marilyn Manson, Body Count. Kathy Griffin has had a couple of episodes of exclusion for different statements intended to shock. In the 1980s and 1990s a number of radio hosts said shocking things and were fined or fired (e.g.&nbsp;Howard Stern).</p>
<p><strong>Exclusion of people for their personal life:</strong> Exclusions prior to the mid-2000s were mostly for more severe accusations (Roman Polanski, Woody Allen, OJ Simpson, Mel Gibson), and the exclusions were sometimes relatively mild. In the last 20 years these exclusions have become much more common, especially for sex-related accusations against men.</p>
<p><strong>Exclusion for other reasons.</strong> (1) some have been boycotted by their peers for informing on fellow artists (Eliza Kazan, Lovin Spoonful); (2) the police have temporarily boycotted a number of musicians for songs criticizing the police (NWA, Bruce Springsteen, Beyonce).</p>
<section id="exclusions-in-film-and-television" class="level2">
<h2 class="anchored" data-anchor-id="exclusions-in-film-and-television">Exclusions in Film and Television</h2>
<table class="table">
<colgroup>
<col style="width: 4%">
<col style="width: 18%">
<col style="width: 32%">
<col style="width: 44%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th></th>
<th>alleged crime</th>
<th>result</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1920s</td>
<td>Fatty Arbuckle</td>
<td>rumors of immorality</td>
<td>film industry blacklisted</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>1940s</td>
<td>Orson Welles</td>
<td>communist associations</td>
<td>blacklisted, moved to Switzerland</td>
</tr>
<tr class="even">
<td></td>
<td>Dalton Trumbo</td>
<td>communist associations</td>
<td>blacklisted</td>
</tr>
<tr class="odd">
<td></td>
<td>(around 100 people)</td>
<td>communist associations</td>
<td>blacklisted for a decade</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>1950s</td>
<td>Charlie Chaplin</td>
<td>communist associations</td>
<td>banned from US</td>
</tr>
<tr class="even">
<td></td>
<td>Elia Kazan</td>
<td>testifying before HUAC</td>
<td>lost some relationships in Hollywood</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>1960s</td>
<td>Jane Fonda</td>
<td>opposition to Vietnam war</td>
<td>blacklisted</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>1970s</td>
<td>Roman Polanski</td>
<td>rape of 13yo girl</td>
<td>mild disapproval from Hollywood</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>1990s</td>
<td>O J Simpson</td>
<td>murdered his wife</td>
<td>blacklisted</td>
</tr>
<tr class="odd">
<td></td>
<td>Woody Allen</td>
<td>molested 7yo daughter</td>
<td></td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>2000s</td>
<td>Mel Gibson</td>
<td>racism &amp; anti-semitism</td>
<td>“blacklisted in Hollywood for almost a decade”</td>
</tr>
<tr class="even">
<td></td>
<td>Mira Sorvino</td>
<td>rejecting Harvey Weinstein</td>
<td>blacklisted</td>
</tr>
<tr class="odd">
<td></td>
<td>Rose McGowan</td>
<td>rejecting Harvey Weinstein</td>
<td>blacklisted</td>
</tr>
<tr class="even">
<td></td>
<td>Isaiah Washington</td>
<td>homophobic remarks</td>
<td>blacklisted</td>
</tr>
<tr class="odd">
<td></td>
<td>Michael Richards</td>
<td>racist remarks</td>
<td>blacklisted</td>
</tr>
<tr class="even">
<td></td>
<td>Kathy Griffin</td>
<td>“told Jesus to suck it”</td>
<td>banned from talk shows and TV appearances</td>
</tr>
<tr class="odd">
<td></td>
<td>Sean Penn</td>
<td>opposition to Iraq war</td>
<td>dropped from movie</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>2010s</td>
<td>Bill Cosby</td>
<td>sexual assault</td>
<td>blacklisted (&amp; later imprisoned)</td>
</tr>
<tr class="even">
<td></td>
<td>Harvey Weinstein</td>
<td>sexual assault</td>
<td>blacklisted (&amp; later imprisoned)</td>
</tr>
<tr class="odd">
<td></td>
<td>Stacy Dash</td>
<td>conservative advocacy</td>
<td>blacklisted</td>
</tr>
<tr class="even">
<td></td>
<td>Kirk Camerson</td>
<td>criticism of homosexuality</td>
<td>blacklisted</td>
</tr>
<tr class="odd">
<td></td>
<td>James Woods</td>
<td>anti-Obama tweets</td>
<td>blacklisted</td>
</tr>
<tr class="even">
<td></td>
<td>CeeLo Green</td>
<td>sexual assault</td>
<td>blacklisted</td>
</tr>
<tr class="odd">
<td></td>
<td>Louis CK</td>
<td>sexual harassment</td>
<td>blacklisted</td>
</tr>
<tr class="even">
<td></td>
<td>Kathy Griffin</td>
<td>photo with head of Trump</td>
<td>fired by CNN, lost endorsement, cancelled tour</td>
</tr>
<tr class="odd">
<td></td>
<td>T J Miller</td>
<td>substance abuse, sexual assault</td>
<td>blacklisted</td>
</tr>
<tr class="even">
<td></td>
<td>Gina Carano</td>
<td>political social media posts</td>
<td>fired from TV show</td>
</tr>
<tr class="odd">
<td></td>
<td>Kevin Spacey</td>
<td>sexual harassment</td>
<td>lost roles in films</td>
</tr>
<tr class="even">
<td></td>
<td>Jussie Smollett</td>
<td>lied about an attack</td>
<td>lost roles in TV shows</td>
</tr>
<tr class="odd">
<td></td>
<td>Neil deGrasse Tyson</td>
<td>rape, sexual harassment</td>
<td>temporarily lost roles in TV shows</td>
</tr>
<tr class="even">
<td></td>
<td>Roseanne Barr</td>
<td>racist tweet</td>
<td>lost TV show</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>2020s</td>
<td>Will Smith</td>
<td>slapping someone at Oscars</td>
<td>film projects put on hold</td>
</tr>
<tr class="odd">
<td></td>
<td>Johnny Depp</td>
<td>domestic violence</td>
<td>lost roles in films</td>
</tr>
<tr class="even">
<td></td>
<td>Amber Heard</td>
<td>involvement in trial w Johnny Depp</td>
<td>lost roles in films</td>
</tr>
<tr class="odd">
<td></td>
<td>Justin Roiland</td>
<td>sexual harassment &amp; abuse</td>
<td>lost roles in shows</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
</tbody>
</table>
</section>
<section id="exclusions-in-music" class="level2">
<h2 class="anchored" data-anchor-id="exclusions-in-music">Exclusions in Music</h2>
<table class="table">
<colgroup>
<col style="width: 4%">
<col style="width: 15%">
<col style="width: 33%">
<col style="width: 45%">
</colgroup>
<thead>
<tr class="header">
<th></th>
<th></th>
<th>alleged crime</th>
<th>result</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1940s</td>
<td>Paul Robeson</td>
<td>communist associations</td>
<td>blacklist and passport revoked</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>1950s</td>
<td>Leonard Bernstein</td>
<td>communist associations</td>
<td>brief blacklist</td>
</tr>
<tr class="even">
<td></td>
<td>Lena Horne</td>
<td>communist associations</td>
<td>blacklist</td>
</tr>
<tr class="odd">
<td></td>
<td>Pete Seeger</td>
<td>communist associations</td>
<td>blacklist</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>1960s</td>
<td>Beatles</td>
<td>saying they’re bigger than Jesus</td>
<td>consumer boycott</td>
</tr>
<tr class="even">
<td></td>
<td>Lovin Spoonful</td>
<td>cooperating with FBI</td>
<td>music industry boycott</td>
</tr>
<tr class="odd">
<td></td>
<td>Nina Simone</td>
<td>“Mississippi Goddam”</td>
<td>boycott in the South</td>
</tr>
<tr class="even">
<td></td>
<td>John Lennon</td>
<td>criticism of US and Vietnam war</td>
<td>refused entry into US</td>
</tr>
<tr class="odd">
<td></td>
<td>Eartha Kitt</td>
<td>criticism of Vietnam war</td>
<td>blacklist through LBJ and CIA</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>1970s</td>
<td>Sex Pistols</td>
<td>criticizing the Queen, swearing on TV</td>
<td>banned by the BBC, dropped by EMI</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>1980s</td>
<td>NWA</td>
<td>“Fuck the Police” &amp; similar songs</td>
<td>radio station boycott, police boycott</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>1990s</td>
<td>Bruce Springseen</td>
<td>song against police brutality</td>
<td>brief police boycott</td>
</tr>
<tr class="even">
<td></td>
<td>Marilyn Manson</td>
<td>transgressive lyrics</td>
<td>banned from performing in some states</td>
</tr>
<tr class="odd">
<td></td>
<td>Body Count</td>
<td>song “cop killer”</td>
<td>album withdrawn and reissued</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>2000s</td>
<td>Dixie Chicks</td>
<td>for opposition to Iraq war</td>
<td>blacklisting and consumer boycott</td>
</tr>
<tr class="even">
<td></td>
<td>Janet Jackson</td>
<td>showing nipple</td>
<td>VH1, MTV, &amp; Viacom radio stopped playing her music</td>
</tr>
<tr class="odd">
<td></td>
<td>R Kelly</td>
<td>sexual abuse</td>
<td>broad blacklist</td>
</tr>
<tr class="even">
<td></td>
<td>Chris Brown</td>
<td>domestic violence</td>
<td>weak boycott and blacklist</td>
</tr>
<tr class="odd">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="even">
<td>2010s</td>
<td>Lostprophets</td>
<td>sexual abuse</td>
<td>broad blacklist</td>
</tr>
<tr class="odd">
<td></td>
<td>Michael Jackson</td>
<td>child molestation</td>
<td>some radio stations stop playing music</td>
</tr>
<tr class="even">
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr class="odd">
<td>2020s</td>
<td>Beyonce</td>
<td>song against police brutality</td>
<td>brief police boycott</td>
</tr>
<tr class="even">
<td></td>
<td>Morgan Wallen</td>
<td>using n-word</td>
<td>temporarily dropped from radio/streaming playlists</td>
</tr>
<tr class="odd">
<td></td>
<td>Kanye West</td>
<td>praise of Hitler</td>
<td>lost sponsors</td>
</tr>
</tbody>
</table>
<p><strong>Others.</strong></p>
<ul>
<li><p><strong>In radio:</strong> Father Coughlin, Rush Limbaugh, Don Imus fired from CBS for calling womens’ basketball team “nappy-headed hos”, Howard Stern fired from various radio shows for comments.</p></li>
<li><p><strong>In sport.</strong> Colin Kapaernick blacklisted from NFL for kneeling for the anthem. Pete Rose banned from MLB for gambling.</p></li>
<li><p><strong>Nazi sympathisers/collaborators.</strong> Charles Lindbergh, Henry Ford, Charles Coughlin, PG Wodehouse, Ezra Pound.</p></li>
<li><p><strong>Writers:</strong> DH Lawrence, Henry Miller, Salman Rushdie (Nicole Bonoff).</p></li>
<li><p><strong>Journalists.</strong> Jeffrey Toobin (New Yorker writer masturbated on zoom call),</p></li>
<li><p><a href="https://twitter.com/JuraWho/status/1659214427818962944">Note on R Kelly disappearing from radio</a></p></li>
</ul>
</section>
</section>
<section id="appendix-alternative-visualization" class="level1 page-columns page-full">
<h1>Appendix: Alternative Visualization</h1>
<div class="cell page-columns page-full" data-hash="2023-01-31-social-media-suspensions-data_cache/html/unnamed-chunk-29_e06b93c3fcdf5754f98d423af1ba1401">
<div class="cell-output-display column-page">
<p><img src="tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data_files/figure-html/unnamed-chunk-29-1.png" class="img-fluid" width="720"></p>
</div>
</div>


</section>


 ]]></description>
  <guid>tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data.html</guid>
  <pubDate>Tue, 31 Jan 2023 08:00:00 GMT</pubDate>
</item>
<item>
  <title>Optimal Coronavirus Policy Should be Front-Loaded</title>
  <link>tecunningham.github.io/posts/2020-04-05-front-loading-restrictions.html</link>
  <description><![CDATA[ 




<p><strong>Q: How should you sequence policies over time?</strong> E.g. suppose you want to manage the epidemic until a vaccine arrives and you have policies (lockdowns, distancing, masks) each of which is associated with a certain effect on the growth-rate of cases, but each also has some fixed social cost per day. How should you apply the policies over time?</p>
<p><strong>A: The severity of the policies should be gradually <em>decreasing</em>,</strong> i.e.&nbsp;they should gradually become less severe, as you approach the availability of a vaccine. There should not be zig-zagging between policies in this setup.</p>
<p>Any justification for zig-zagging must come from some additional consideration like (a) non-separabilities in the costs, e.g.&nbsp;psychological/economic need for occasional respite, (b) uncertainty about the end-date, (c) uncertainty about the effect of the policies, such that there is informational-value from varying policies, or (d) desire to maintain a steady flow of cases, in order to reach herd immunity (the “mitigation” strategy).</p>
<section id="corollary-you-should-never-expect-policy-to-get-stricter" class="level2">
<h2 class="anchored" data-anchor-id="corollary-you-should-never-expect-policy-to-get-stricter">Corollary: you should never <em>expect</em> policy to get stricter</h2>
<p>You should never find yourself in the situation where you expect policy to get stricter in the future. If you anticipate that a stricter policy will be appropriate next week then that strict policy is appropriate <em>this</em> week!</p>
<p>Countries in early stages of the epidemic should be doing as much or more as countries in later stages.</p>
</section>
<section id="intuition" class="level2">
<h2 class="anchored" data-anchor-id="intuition">Intuition</h2>
<p>Suppose that there’s some tradeoff across policiers between the growth-rate and the social cost.</p>
<p>Then given any fixed time-path of policies: e.g., (A,A,B,C), if it is not monotonically decreasing in severity from high-cost to low-cost, then you can do strictly better by rearranging the path of policies to be monotonically decreasing. The social cost will be identical, because the set of policies will be the same, but the number of cases will be lower at every point in time, since at any given point the cumulative growth rates, up to that point, will be lower. Thus the final cumulative number of cases will be lower.</p>
</section>
<section id="additional-reason-to-front-load-extinction" class="level2">
<h2 class="anchored" data-anchor-id="additional-reason-to-front-load-extinction">Additional Reason to Front-Load: Extinction</h2>
<p>All of this is treating the number of cases as a continuous variable which means you can never completely extinguish the disease. However if that’s a possibility that’s within sight (e.g.&nbsp;as in NZ), then that’s a significantly <em>stronger</em> case for starting with very severe policies, to try to kill the disease entirely, and then you can go back to the garden of Eden.</p>
</section>
<section id="prior-discussion" class="level2">
<h2 class="anchored" data-anchor-id="prior-discussion">Prior Discussion</h2>
<p>There’s been some discussion of zig-zagging by the Imperial group (<a href="[URL](https://www.imperial.ac.uk/media/imperial-college/medicine/sph/ide/gida-fellowships/Imperial-College-COVID19-NPI-modelling-16-03-2020.pdf)">paper</a>) and by Timothy Gowers (<a href="https://twitter.com/wtgowers/status/1243973167879794691">twitter &amp; post</a>)</p>
<p>Gowers says the optimal policy is very short zig-zags (changing policy every other day), however I think this is misleading. It comes from fixing the lower-threshold and optimizing the upper-threshold. If instead you fixed the upper-threshold and optimized the lower-threshold, then the optimal cycle-length will be long.</p>
<p>If you choose both the upper and lower threshold (both T and S) then he notes that they’ll both be arbitarily low. However this ignores the cost of <em>getting</em> to zero given current cases.</p>
<p>Instead a well-defined problem is to choose an optimal time-path of policy given some start-point and end-point. In that case it’ll be a path of gradually decreasing strictness (without zig-zags).</p>
<p>You can see the intuition in the diagram below: the total infections is approximately the area under the zig-zag (not quite: because the y-axis is ln(cases), but this won’t matter for the argument). Thus you can reduce the area under the line by lowering the upper threshold. However if you instead take the <em>upper</em> threshold as fixed, then it’s optimal to choose a lower threshold that is as low as possible, i.e.&nbsp;you want <em>long</em> cycles, not short cycles.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/https:/www.dropbox.com/s/jj93tz0k0kuau49/zigzag.png?raw=1" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">abc</figcaption>
</figure>
</div>


</section>

 ]]></description>
  <guid>tecunningham.github.io/posts/2020-04-05-front-loading-restrictions.html</guid>
  <pubDate>Sun, 05 Apr 2020 07:00:00 GMT</pubDate>
</item>
<item>
  <title>On Unconscious Influences (Part 1)</title>
  <link>tecunningham.github.io/posts/2017-12-10-unconscious-influences.html</link>
  <description><![CDATA[ 




<section id="introduction" class="level1">
<h1>Introduction</h1>
<p>Over a couple of years I spent a lot of time in offices looking out the window, thinking about decision-making &amp; the unconscious, scribbling little bits &amp; pieces in a notebook.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/https:/www.dropbox.com/s/be1p2urgqwri2hp/nber_snow.png?raw=1" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">NBER</figcaption>
</figure>
</div>
<p>I ended up writing two papers - “<a href="https://dl.dropboxusercontent.com/u/13046545/paper_heuristics.pdf">Hierarchical Aggregation of Information and Decision-Making</a>” by myself and “<a href="http://bit.ly/paper_implicit">Implicit Preferences Inferred from Choice</a>” with Jon de Quidt. The papers are fairly technical, and this post is going to be a layperson’s guide to the background, what’s known about unconscious knowledge, and a tiny bit about the ideas in those papers.</p>
<p>Here is the argument in a nutshell:</p>
<ol type="1">
<li>There are plenty of reasons to think that unconscious influences are strong – in other words, that people have limited insight into what factors influence their decisions.</li>
<li>The idea of unconscious influences has been in and out of the mainstream of psychology for the last 200 years, but always hounded by arguments over what it means, i.e.&nbsp;over what evidence would be sufficient to show that a decision was influenced by an unconscious factor. The battle has had many reversals: a new types of evidence has been proposed which is thought to reveal unconscious influences, and then later the technique or interpretation is shown to have substantial flaws and the line of inquiry fizzles out. A couple of decades pass and a different approach becomes popular.</li>
<li>Two broad classes of evidence are the following: (A) people reveal their unconscious preoccupations in their involuntary responses – in how their pupils dilate, how quickly they respond to a stimulus, in their word associations, dreams, slips of the tongue; (B) people reveal unconscious influences in discrepancies between how they act and how they explain their behaviour. Both sources of evidence have got tangled in debates about interpretation, and there are substantial camps on either side with not much agreement on what constitutes sufficient evidence for unconscious influences.</li>
<li>A third type of evidence is less common but, I think, more powerful: evidence from inconsistencies in decision-making. The idea being that unconscious factors are by their nature <em>isolated</em> from conscious factors, i.e.&nbsp;they don’t interact with conscious beliefs and desires, and this isolation will cause certain characteristic inconsistencies among decisions.</li>
<li>This can be made precise with an analogy: the relationship between the conscious and unconscious brain is like the relationship between a blind man and his guide dog. The blind man makes decisions based, in part, on which direction the guide dog is pulling towards, so the guide dog’s beliefs and desires influence the man’s decisions, but without the man knowing exactly what those beliefs and desires are, and so he couldn’t tell you how much any particular factor contributed to his decision. Testing for unconscious influences in behaviour is just testing the degree to which your brain is being led by a guide dog.</li>
<li>The internal-consistency definition of unconscious influences implies two ways of looking for them: (1) testing whether people can accurately answer <em>hypothetical</em> questions about decisions they would make if factors changed - i.e.&nbsp;navigating without your guide dog; and (2) testing whether people make consistent judgments when judging two outcomes at a time.</li>
<li>First, hypothetical questions. We can ask people, how would your judgment change if this factor changed? Would you still like this painting if the name of the artist was different? Would this drawing look more like your cousin if the nostrils were bigger? Unconscious influences imply that people will not be able to give accurate answers to these hypothetical questions because if the description of the situation is abstract then their unconscious brain won’t be able to evaluate it (AKA, they don’t know which direction they would go in without knowing what their guide dog will say).</li>
<li>The second way of testing for unconscious influences is what my paper with Jon is about: unconscious influences particularly leave their mark in <em>comparisons</em>, where you evaluate two outcomes simultaneously or consecutively, or when you choose between two outcomes. When confronted with two outcomes you surface two unconscious judgments and that gives you some insight into what is affecting those judgments, which in turn will affect your conscious decision.</li>
<li>Suppose you had an unconscious preference for men over women, but a conscious preference to be indifferent, this will manifest in the following: (A) when you see two CVs which are identical, except that one is a man and one is a woman, then you’re indifferent between them; (B) when you see two CVs which differ in some other respect (e.g.&nbsp;one has a PhD, the other has an MBA), then you consistently have a preference for the CV belonging to the man. Your guide dog has a bias towards men, which you’re not aware of: the bias will only sway your decision in the second case because, in the second case, when your guide dog pulls you towards the man with a PhD, you cannot figure out how much of that pull is due to his being a man, and how much is due to his PhD.</li>
<li>In the end I think that our brains <em>are</em> full of guide dogs all pulling in different directions. If we had the stomach for it we could plot out our decisions all on a map – measure how each factor influences our judgment – and we would be able to see both the surface influences and the deeper latent influences.</li>
</ol>
<section id="contents" class="level2">
<h2 class="anchored" data-anchor-id="contents">Contents</h2>
<ol type="1">
<li>Motivating examples</li>
<li>Some definitions &amp; theory</li>
<li>Ways of measuring implicit preferences</li>
<li>The proposal</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/https:/www.dropbox.com/s/lt96t4u3qoh7ywp/harvard_office.png?raw=1" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Littauer</figcaption>
</figure>
</div>
</section>
</section>
<section id="motivating-examples" class="level1">
<h1>Motivating Examples</h1>
<p>To help with being clear on what I’m talking about here are some examples of unconscious influences. I’ll try to return to these at the end.</p>
<ol type="1">
<li><p>People judging a presidential candidate’s actions differently depending on whether it’s a man or a woman; judging an ethical lapse differently depending on whether committed by a Democrat or Republican; judging a study differently depending on whether it supports their own theory. In each case not being aware of that influence.</p></li>
<li><p>The little things that a family does to spite each other: someone says that Thursday doesn’t suit them, in part because they know that Thursday suits you best (perhaps consciously, perhaps unconsciously).</p></li>
<li><p>Freudian conjectures about trauma causing later responses: e.g.&nbsp;Freud thought Anna O’s aversion to water was because of a memory of seeing a dog drinking from a glass of water. More generally, Freud’s early seduction theory, that psychosomatic illness is caused by repressed memories of childhood sexual abuse.</p></li>
<li><p>Your preferences changing with the circumstances: saying yes to a friend, a hairdresser, bartender, waiter, or salesperson, because it seemed like a good idea at the time, and later not understanding why you didn’t say no.</p></li>
<li><p>Knowledge used in your judgments, but not consciously accessible: knowledge of grammar implicit in your judgements of whether a sentence sounds right; knowledge of the physical world implicit in your judgements of distance and velocity of the things you see; knowledge of faces implicit in recognizing your cousins.</p></li>
</ol>
<p>My examples include both unconscious knowledge and unconscious desires. I’m not going to talk about unconscious <em>sensations</em> – e.g.&nbsp;flashes of advertisement that don’t register consciously but might persuade you.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/https:/www.dropbox.com/s/bczy68n3k08s6yl/tel_aviv_office.png?raw=1" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">tel aviv</figcaption>
</figure>
</div>
</section>
<section id="ways-of-measuring-unconscious-influences" class="level1">
<h1>Ways of Measuring Unconscious Influences</h1>
<p>Here are three ways of identifying whether an influence is conscious:</p>
<ol type="1">
<li><strong>Involuntary responses.</strong> In the dilation of your pupils, in your response times, in slips of the tongue, what you see in Rorsach blots, in your word associations, in your dreams.</li>
<li><strong>Ability to describe the influence.</strong> Ask people what they think influences their decisions, and compare that to what actually influences their decisions.</li>
<li><strong>Integration with other influences.</strong> Finally we can say that a factor is unconscious if its influence seems to be partitioned off from other influences. The most simple example is if a factor continues to influence you even in cases where you have reason to ignore it.</li>
</ol>
<section id="involuntary-responses" class="level2">
<h2 class="anchored" data-anchor-id="involuntary-responses">Involuntary responses</h2>
<p>Freud is the most famous theorist of extracting unconscious factors from involuntary responses – he wrote three books on different methods: one on dreams, one on jokes, one on mistakes (mis-reading, mis-hearing, mis-speaking). An example from the last book: “A woman who is very anxious to get children always reads ‘storks’ instead of ‘stocks’.” Most of Freud’s examples of unconscious influences are much more complex than this one, and more often the hidden factor influencing behaviour is something unpleasant or shameful.</p>
<p>Another way of measuring unconscious cognition is through measuring arousal. Most famous is the “Iowa card task” from Bechara et al.&nbsp;(1996). They had their subjects choose among playing cards, and receive rewards if they chose certain cards. They found that people gradually learned which types of cards were rewarded, but they also found that the subjects’ automatic responses (measured by skin conductance, i.e.&nbsp;sweating) would show an awareness of the pattern more quickly than the subjects’ choices would: after a while, when the subject’s hand hovered over one of the cards which was rewarded, the subject would sweat a little more, even though the subject wasn’t any more likely to choose that card. They said that this showed that unconscious learning was outpacing conscious learning. Antonio Damasio, one of the authors of this study, went on to write <em>Descartes’ Error</em> which accused Descartes’ of starting the great misapprehension that emotions and reason are in competition – Damasio said that his experiments show how emotions inform reason and improve decision-making. A lot of subsequent papers tried to show that snap decisions, which avoid conscious processing, can produce better outcomes than slow considered decisions.</p>
<p>Even more famous is the “Implicit Association Test” (IAT) (Greenwald, McGhee and Schwartz (1998)). Subjects are told to press a button whenever they see something from either of two different categories of stimuli, e.g.&nbsp;press the button if you see either a black face or a word with a positive association. Their finding, much-replicated, was that people are relatively quicker at tasks (meaning they have shorter response times) when they are asked to identify a set such as “black face or negative word” or “white face or positive word” than to identify a set like “black face or positive word” or “white face or negative word.” They find that this occurs even among people who report no conscious negative feelings towards black people, and they interpret this as revealing an unconscious association between black people and negative feelings, and they argue that this association could affect your decision-making without you being aware of it.</p>
<p>Many other measures of automatic responses have been popular at different times: hypnosis and word association (Freud used both of these before moving to talking therapy); Rorsach blots (AKA inkblot tests); thematic apperception test (interpret an ambiguous drawing, still widely used); lie detectors AKA polygraphs (they measure autonomic responses - blood pressure, pulse, respiration, and skin conductivity - as you are asked different questions).</p>
<p>Unfortunately a great deal of this research turns out to be both hard to replicate, and reliant on strong assumptions in order to interpret as surfacing unconscious associations. Newell and Shanks (2014) give strong arguments for both of these points, covering many of the methods I mentioned here.</p>
<p>It is worth mentioning that, although Freud’s more elaborate theories died off, his idea that psychosomatic illness is an indirect expression of a psychological stress, especially about something shameful, I believe remains one of the standard theories of modern neurology (O’Sullivan, 2015).</p>
<p>However even if we had solid evidence for unconscious influences on involuntary responses, this still stops short of unconscious influences on decision-making. It’s possible that our associations show up in sweating, response time, and dreams, but have little effect on decision-making, and if that’s so then unconscious associations are not terribly important for social science. Most of the authors in this literature have assumed that the unconscious factors they identify affect real decisions but have left that extrapolation untested. Blanton et al.&nbsp;(2009) say that there’s no persuasive evidence that implicit racial bias, as measured by the IAT, predicts peoples’ decision-making, once you control for measures of <em>explicit</em> racial bias, i.e.&nbsp;when you just ask people how they feel about black people. (Singal (2016) has a long discussion on this point).</p>
</section>
<section id="ability-to-describe-the-influence" class="level2">
<h2 class="anchored" data-anchor-id="ability-to-describe-the-influence">Ability to Describe the Influence</h2>
<p>A second type of evidence is to compare self-reported influences on behaviour with actual influences on behaviour. Here are some examples:</p>
<ol type="1">
<li>In the mid 20th-century behaviourists found that they could shape their subjects choices through conditioning with rewards and punishments, and the subjects seemed to remain ignorant of this shaping. For example if you say <em>‘mm-hmm’</em> whenever someone uses a plural noun, then after a while that person ends up using plural nouns more often, apparently unaware of the influence (Thorndike and Rock (1934); Greenspoon (1955)).</li>
<li>Since the 1970s social psychologists have published all sorts of experiments in which they vary an apparently irrelevant factor and find that this can affect peoples’ decision-making. Nisbett and Wilson (1977) summarize a lot of experiments and say “subjects are sometimes (a) unaware of the existence of a stimulus that importantly influenced a response, (b) unaware of the existence of the response, and (c) unaware that the stimulus has affected the response.”</li>
<li>Another paradigm from the 1970s asks people to make a judgement - e.g.&nbsp;which stock to pick - and also to rate the importance of factors which contributed to their decision. Slovic et al.&nbsp;(1972) find a low correlation (0.34) between the ratings that a stockbrokers put on factors, and the actual influence of these factors on their decisions. There is a small literature with similar findings across a variety of tasks.</li>
<li>Finally, since the 1970s a smaller group of psychologists have been running experiments in which people learn a complicated pattern, and then are asked about their insight into it. E.g. in Arthur Reber’s “artificial grammar” experiments subjects learn, through trial and error, to discriminate between two categories of words. After some time they become very good at the task, but when asked to explain how they are making decisions they often say they don’t know, or they come up with rules that do not match their actual performance.</li>
</ol>
<p>As in the previous category, a lot of this evidence is very fragile: either hard to replicate, or based on delicate interpretations of what is happening in the experiment. Newell and Shanks (2014) again give a good summary.</p>
<p>An additional problem is that these findings could reflect knowledge being difficult to articulate, without it being unconscious. And this literature is full of reversals which bear this out: when experiments are repeated it has often turned out that the subjects <em>do</em> report awareness of the pattern that they have learned if they are asked the question in a different way. Mitchell et al.&nbsp;(2009) say “[i]t is very difficult to provide a satisfactory demonstration of unaware conditioning simply by showing conditioning in the absence of awareness. This is because it is very difficult to be sure that the awareness measure and the conditioning measure are equally sensitive.”</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/https:/www.dropbox.com/s/d5s41vykojhy4cv/iies.jpg?raw=1" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">IIES</figcaption>
</figure>
</div>
</section>
<section id="isolation-of-unconscious-influences" class="level2">
<h2 class="anchored" data-anchor-id="isolation-of-unconscious-influences">Isolation of Unconscious Influences</h2>
<p>Finally there’s a third type of evidence which is more strictly behavioural: an unconscious factor is one which is <em>isolated</em> from your other conscious beliefs and desires – i.e.&nbsp;it does not interact with conscious factors – and that isolation will be reflected in your behaviour. This isolation criterion has been given various names, but I don’t think it’s ever been explained as clearly as it could be.</p>
<p>To be precise think of the blind man (the conscious part of the brain) and the guide dog (the unconscious). The guide dog can know something – e.g.&nbsp;she knows when the crossing light is flashing – which the man does not know, and her knowledge will influence the man’s decisions through her recommendation of when to cross. However the guide dog’s knowledge is isolated from the man’s knowledge: it only influences his decisions through the narrow channel of pulling on the leash. Suppose you tell the man that the crossing lights are not working properly, and so whatever color they show is entirely at random and uninformative. The man and dog, considered as a system, has two pieces of information: (1) the light is green (i.e.&nbsp;indicating ready to cross); and (2) the color is uninformative. However the two pieces of information are known by different actors, implying that they will not be integrated, because neither the man or dog knows both. This will be reflected in the man’s behaviour: he will be influenced by the guide-dog’s recommendation, because the dog sees other things in addition to the crossing-light, such as oncoming traffic. And so the man’s behaviour will still be influenced by the color of the light, even though he <em>knows</em> that the color is irrelevant.</p>
<p>If information is separated in the brain, we ought to see characteristic patterns of that in behavior. I know of just a few cases where the isolation of knowledge has come up clearly in trying to define or measure unconscious influences.</p>
<p>Stich (1978) said that certain mental states are “inferentially unintegrated”: </p>
<blockquote class="blockquote">
<p>“[unconscious beliefs are] largely inferentially isolated from the large body of inferentially integrated beliefs to which a subject has access”</p>
</blockquote>
<p>He gives an example: suppose Noam Chomsky has a theory of grammar, and that there exists some grammatical rule which is a counterexample to that theory. If a linguist knows that rule consciously, then the linguist will immediately infer that Chomsky’s theory is false. But if the linguist only knows the rule <em>unconsciously</em>, then they won’t be able to make that inference, because the knowledge is “inferentially unintegrated” – i.e.&nbsp;the knowledge is isolated from the knowledge regarding Chomsky’s theory. <sup>1</sup></p>
<p>A separate place where this separation has come up is in the work of Zenon Pylyshyn and Jerry Fodor since the 1980s regarding perception being “cognitively impenetrable,” or “informationally encapsulated.” They mean that perceptual processes often make inferences without taking into account all the information that is available, i.e.&nbsp;by drawing only on a subset of information. Their principal argument was from perceptual illusions: they argue that illusions can typically be understood as rational inferences from a subset of the information available. Helmholtz had a nice example: if you close one eye and press with your finger on the edge of your eyelid then you’ll perceive a point of light, but the light will be coming from the <em>opposite</em> side of your field of vision from where your finger is. This is because the left side of your retina receives light from the right side of your visual field and vice versa. So when your retina receives some stimulation on the left-hand side your brain makes infers that light is coming from the right-hand side. This is a sensible inference given only the information that your eye has, i.e.&nbsp;just the information from the retina. In this case there is additional information - the fact your finger is pressing on your eyelid - which should give a different interpretation to the stimulation, but your visual cortex is not wired up to incorporate that information, and so it misinterpret the signals it receives.</p>
<p>The Helmholtz-Fodor-Pylyshyn model of encapsulated inference isn’t quite the same as the case of the blind man and the guide dog. In their examples the pre-conscious process have a strict <em>subset</em> of the information available to the conscious brain. In other words the man isn’t blind, it’s just a case where the dog leads in a different direction than the man would. Fodor (1983) does have a brief discussion on whether early perceptual processes have access to information not available to the conscious brain, which would imply unconscious influences, in my sense.</p>
<p>Finally the isolation argument has appeared in the literature on human “associative learning,” in testing whether or not the associations that we learn through conditioning are conscious. A typical experiment involves ringing a bell and then giving subjects a small electric shock. After a while people learn to flinch when they hear the bell. For a long time psychologists tried to map out the logic of how such associations would form, trying to figure out the rule which governed learning of associations. However in the last few decades an argument has been made that these learned associations are not in fact mechanical - there is no simple rule - instead they are more-or-less optimal responses to the environment based on the entirety of the information available, i.e.&nbsp;they are not <em>isolated</em> from other knowledge, though the argument isn’t usually put in terms of conscious vs unconscious knowledge. For example Colgan (1970) told subjects, after they learned an association, that the association is no longer valid (“from now on the bell will not signal an electric shock”) and he found that, although this didn’t entirely extinguish the flinching, it did cause it to markedly decrease. This implies the flinching is not isolated from your conscious knowledge: the association, at least to some degree, interacts with more abstract knowledge. There are many other circumstances where rule-based theories of association-learning have foundered because it turns out that peoples’ responses respond to outside considerations. De Houwer, Vandorpe and Beckers (2005) summarize the evidence against associative models (which can be interpreted as models with unconscious knowledge):</p>
<blockquote class="blockquote">
<p>The two types of models can be differentiated … by manipulating variables that influence the likelihood that people will reason in a certain manner but that should have no impact on the operation of the associative model. We have seen that such variables (e.g., instructions, secondary tasks, ceiling effects, nature of the cues and outcomes) do indeed have a huge effect. Given these results, it is justified to entertain the belief that participants are using controlled processes such as reasoning and to look for new ways to model and understand these processes.</p>
</blockquote>
<p>Mitchell says:</p>
<blockquote class="blockquote">
<p>“The results consistently show evidence for skin conductance [effects] only in participants who are aware of the [relationship] … [a]lthough there are many papers arguing for unaware conditioning, close inspection reveals, in almost all cases, that the measure of conditioning was most likely more sensitive than that of awareness.”</p>
</blockquote>
<p>In retrospect a lot of behavior that was studied in the lab, which was thought to be telling us about the wiring of the animals, actually was telling us about the world outside the animal, because it has turned out that the animals’ response is the <em>optimal</em> response to the typical circumstances it faces in the world. (See my other post <a href="http://tecunningham.github.io/2017/04/15/the-mechanical-and-the-rational/">The Repeated Failure of Laws of Behaviour</a> , and also Mitchell et al.&nbsp;(2009) section 4.3)</p>
<p>If this line of thought were entirely correct – if all information was integrated and fed into every decision – then there would be no unconscious influences in my sense. However I do think that there’s plenty of evidence that remains for a lack of integratation between cognitive processes.</p>
<p>In Part 2 of this essay I will give a more formal statement of how decisions can reveal unconscious knowledge (and unconscious motivations), and a survey what I think is the strength of the evidence.</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/https:/www.dropbox.com/s/hvaaahkc9eshz6z/office_caltech.jpg?raw=1" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Caltech</figcaption>
</figure>
</div>
</section>
</section>
<section id="summary-so-far" class="level1">
<h1>Summary So Far</h1>
<ol type="1">
<li>Looking at involuntary reactions tells us <em>something</em> about your reaction to a situation, but it doesn’t tell us whether it has important impact on your decision-making. (And in addition, the evidence for influences on involuntary reactions is pretty weak).</li>
<li>Comparing peoples’ decisions to what they say about their decisions is also imperfect evidence for unconscious associations, because people can be inarticulate about their reasons, without being unconscious of them. (And in addition, this evidence is also pretty weak).</li>
<li>Finally, internal coherence of decision-making seems a much more solid way of identifying unconscious influences.</li>
</ol>
</section>
<section id="in-part-ii" class="level1">
<h1>In Part II</h1>
<p>Discussion of the type of evidence needed to establish unconscious knowledge.</p>
</section>
<section id="references" class="level1">
<h1>References</h1>
<p><strong>Bechara, Tranel, Damasio and Damasio (1996) “Failure to respond autonomically to anticipated future outcomes following damage to prefrontal cortex”</strong></p>
<p><strong>Hart Blanton, James Jaccard, Jonathan Klick, Barbara Mellers, Gregory Mitchell, and Philip E Tetlock (2009). “Strong claims and weak evidence: reassessing the predictive validity of the IAT”. Journal of Applied Psychology, 94(3): 567.</strong></p>
<p><strong>Greenwald McGhee &amp; Schwartz (1998) “Measuring individual differences in implicit cognition: The Implicit Association Test.”</strong></p>
<p><strong>De Houwer, Vandorpe and Beckers (2005) On the role of controlled cognitive processes in human associative learning</strong> &gt; “In hindsight, it seems obvious that people can learn about associations by using controlled processes such as reasoning and hypothesis testing. Why then, are associative models still dominant in modern research? One reason is that the associationistic view has a long tradition in psychology (and philosophy). It is thus difficult for many people to leave behind the associationistic view that has guided their thinking and research for many years. Another important reason is that associative models do quite well in accounting for the available empirical data. The well known Rescorla-Wagner model (Rescorla &amp; Wagner, 1972), for instance, is compatible with a huge number of findings while being relatively simple. If our argument is correct that associative models do not provide an accurate account of the processes that underlie associative learning, how is it possible that they are able to account for so much of the data? We agree with Lovibond (2003, p.&nbsp;105) that”the success of these models is due to them capturing, at least in part, the operating characteristics of the inferential learning system”. What this means is that associative models (as well as probabilistic models for that matter) can be seen as (mathematical) formalisations of certain deductive reasoning processes. A system that operates on the basis of associative models does not reason, but acts very much as if it is reasoning. The associative models will thus often predict the same result as a model that is based on the assumption that humans actually generate and test hypothesis or reason in a controlled, conscious manner. The two types of models can be differentiated, however, by manipulating variables that influence the likelihood that people will reason in a certain manner but that should have no impact on the operation of the associative model. We have seen that such variables (e.g., instructions, secondary tasks, ceiling effects, nature of the cues and outcomes) do indeed have a huge effect. Given these results, it is justified to entertain the belief that participants are using controlled processes such as reasoning and to look for new ways to model and understand these processes.”</p>
<p><strong>Newell &amp; Shanks (2014, BBS) “Unconscious Influences on Decision-Making: A Critical Review”</strong></p>
<ul>
<li>“There is little convincing evidence of unconscious influences on decision making in the areas we review” – they say many of Nisbett and Wilson’s results on lack of introspection have been reinterpreted</li>
<li><ol type="1">
<li>multiple-cue judgment : classic paradigm is that you compare weights revealed by decisions to self-reported weights. They say that protocol for getting self-reported weights are usually shitty, &amp; better measurements show that people understand their own judgment better</li>
</ol></li>
<li><ol start="2" type="1">
<li>decisions are better when you don’t deliberate (all the evidence for this is shitty)</li>
</ol></li>
<li><ol start="3" type="1">
<li>awareness / Iowa gambling task (your subconscious is attracted to the best deck, but conscious is not): very shitty evidence</li>
</ol></li>
<li><ol start="4" type="1">
<li>priming: people often can report what primes were presented, if you try harder- they say that there is strong evidence for top-down influence even in perceptual tasks. [I emailed them suggesting the joint/separate randomization task, they said it sounds nice]</li>
</ol></li>
<li>They say that dispute over definitions is over: “A surprising outcome of the review is that debates and disagreements about the meaning of the terms consciousness and awareness have (with a few exceptions) played a remarkably minor role in recent research. Whereas issues about how to define and measure awareness were once highly prominent and controversial (e.g., Campion et al.&nbsp;1983; Reingold &amp; Merikle 1988), it now seems to be generally accepted that awareness should be operationally defined as reportable knowledge, and that such knowledge can only be evaluated by careful and thorough probing.”</li>
</ul>
<p><strong>Michael Polanyi (1966) “The Tacit Dimension”</strong></p>
<p><strong>Nisbett &amp; Wilson (1977) “Telling More than we can know”</strong></p>
<ul>
<li>A very influential paper (7000 citations) arguing that unconscious influences are rampant: “Subjects are sometimes (a) unaware of the existence of a stimulus that importantly influenced a response, (b) unaware of the existence of the response, and (c) unaware that the stimulus has affected the response.” However a lot of the experimental literature in this field is now under a cloud – either through being hard to replicate, or being based on arguable interpretations.</li>
</ul>
<p><strong>Quine (1972) “Methodological reflections on current linguistic theory”</strong></p>
<ul>
<li>When Quine wrote this, linguists were talking a lot about how people had unconscious knowledge of grammatical rules. Quine said that you can’t say that someone knows a rule, from observing that their behaviour conforms to that rule, because there will always be infinitely many different rules which the behaviour satisfies. So this is Quine pouring some cold water on unconscious knowledge. However, in response, I say that we can still define unconscious knowledge underneath behaviour as the inability to accurately answer hypotheticals. So we don’t need to specify <em>which</em> rule the person is using, but it’s clear that they do not have conscious access to any rule which generates their observed behaviour.</li>
</ul>
<p><strong>Stich (1976) “Beliefs &amp; Subdoxastic States”</strong></p>
<p><strong>Suzanne O’Sullivan (2015) “It’s All in Your Head: Stories from the Frontline of Psychosomatic Illness”</strong></p>
<blockquote class="blockquote">
<p>p191: “For all the shortcomings in the concepts proposed by Freud and Breuer in <em>Studies</em>, the twenty-first century has brought no great advances to a better understanding of the mechanism for this disorder. The terms dissociation and conversion are still widely in use … the general principles of modern dissociation are not very different to those of Victorian times … in day-to-day practice Janet’s and Freud’s theories are regularly used, or misused.”</p>
</blockquote>
<p><strong>Singal (2016) in New York Magazine</strong> - article on Implicit Association Test.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Quine says you shouldn’t call this type of thing unconscious knowledge – your linguistic practice may <em>obey</em> some rule, but you can’t say that you unconsciously <em>know</em> that rule, because there are infinitely many different rules that would imply that pattern of behavior. But this skeptical objection is too tough: Quine would deny that a cow can have a belief about where a water trough is, &amp; instead admit only that the cow’s behavior is consistent with a particular belief among infinitely many others.↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>tecunningham.github.io/posts/2017-12-10-unconscious-influences.html</guid>
  <pubDate>Fri, 08 Dec 2017 08:00:00 GMT</pubDate>
</item>
<item>
  <title>The Work of Art in the Age of Mechanical Production</title>
  <link>tecunningham.github.io/posts/2017-09-27-work-of-art-age-mechnical-production.html</link>
  <description><![CDATA[ 




<section id="aka-machine-learning-aesthetics-the-unconscious" class="level2">
<h2 class="anchored" data-anchor-id="aka-machine-learning-aesthetics-the-unconscious">AKA machine learning, aesthetics, &amp; the unconscious</h2>
<ol type="1">
<li><p>When I heard about the neural nets that copy the styles of famous painters I thought it would be the same old junk.</p></li>
<li><p>Academics have been saying forever that they were on the verge of discovering the principles of aesthetics, and that they would soon be able to automate the production of beauty – melody, harmony, proportion, plot.</p></li>
<li><p>When I was a kid I was excited to read about this sort of thing. But they always turn out to be fatuous, catastrophically oversimplified and overconfident, written - I’m guessing - by people who are intimidated &amp; resentful of the culture around them. Our technical understanding of what makes something look good is still weak, and I don’t think it’s improving very fast. I learned to, when I come across an article about art written by a scientist, turn the page.</p></li>
<li><p>But now I think that maybe the automatic production of beauty will arrive soon. The machine learning algorithms work by extrapolating from existing examples, which means that they can produce new examples that fit some pattern (such as the pattern of beauty) without anyone involved having any explicit understanding of what the pattern is or how it can be defined.</p></li>
<li><p>This extrapolation without understanding is what happened in the study of visual perception – i.e.&nbsp;making inferences from images. Our understanding of perception is slowly moving forward, as it has been for centuries, but our ability to automate perception has shot ahead. In the 15th century Leonard da Vinci studied how the light reflected by an object is related to its distance – more distant objects tend to be bluer – these are relationships that we all know unconsciously, but which take a lot of work to dig out, such that we consciously understand them. Psychologists and computer scientists are still discovering things about the physics of light which we all know unconsciously. But computer models which incorporate our explicit knowledge of the physics of light are being thrashed by pure machine-learning models, which are fed a huge databases of pictures and simply extrapolate from what they’ve already seen.[2]</p></li>
<li><p>I think the same basic point is true of aesthetic things. We really struggle trying to explain why we like a picture or dislike a melody, because most of the work is done at an unconscious level. The progress in understanding those principles will probably continue to be slow.</p></li>
<li><p>But now it seems likely to me that, before long, machines will be able to do all these things on demand – play some brand new Mozart, make elegant little drawings of animals, write a pretty good pop song. And the programmer who implements them could be – probably will be – some bozo who has no clue why it works.</p></li>
</ol>
<p>[1] 2017-05: SIGGRAPH video with style transfer - https://www.youtube.com/watch?v=HYhzZ-Abku8</p>
<p>[2] 2017-05: Michael Elad “Deep, Deep Trouble: Deep Learning’s Impact on Image Processing, Mathematics, and Humanity” https://sinews.siam.org/Details-Page/deep-deep-trouble-4</p>


</section>

 ]]></description>
  <guid>tecunningham.github.io/posts/2017-09-27-work-of-art-age-mechnical-production.html</guid>
  <pubDate>Wed, 27 Sep 2017 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Repulsion from the Prior</title>
  <link>tecunningham.github.io/posts/2017-05-26-repulsion-from-the-prior.html</link>
  <description><![CDATA[ 




<p><strong>the shortest version:</strong> <em>contrary to recent reports, I do not think it’s possible for you to be a Bayesian and consistently exaggerate things.</em></p>
<p><img src="tecunningham.github.io/posts/https:/www.dropbox.com/s/sbodcv65vnzxey4/anni_albers_black_mountain_college.jpg?raw=1" class="img-fluid">{: .center-image }</p>
<section id="short-version" class="level2">
<h2 class="anchored" data-anchor-id="short-version">Short Version</h2>
<ol type="1">
<li>If we think of perception as inference, it has implications about the types of biases we would have.</li>
<li>Yet many biases and illusions seems to go in the exact <em>opposite</em> direction – sometimes called “anti-Bayesian” biases – in particular there are ubiquitous <em>contrast</em> effects, while Bayesian inference seems to imply <em>assimilation</em> effects.</li>
<li>Wei and Stocker (2015) say they can rationalize these contrast effects, under the assumption that our sensory mechanisms are tuned to the environment, such that they are relatively more sensitive to more likely signals. They say that this will imply contrast effects (that the bias is inversely proportional to the slope of the prior).</li>
<li>Yet their results contradict some simple laws of Bayesian inference – the law of iterated expectations, and law of total variance – so there is something odd going on.</li>
<li>(If this explanation doesn’t work, then why do we get repulsion? I think that Ted Adelson explained the basic reason in the 70s. Will write another post on this.)</li>
</ol>
</section>
<section id="shortish-version" class="level2">
<h2 class="anchored" data-anchor-id="shortish-version">Shortish Version</h2>
<ol type="1">
<li><p>Here’s a nice crisp problem: in what cases does inference attract <em>towards</em> the prior, and in what cases does it repulse <em>away</em> from it?</p></li>
<li><p>Given an unknown variable <img src="https://latex.codecogs.com/png.latex?x"> and a signal <img src="https://latex.codecogs.com/png.latex?s">, let’s say that there’s “attraction” at a given value of <img src="https://latex.codecogs.com/png.latex?x"> if the average inferred value of <img src="https://latex.codecogs.com/png.latex?x"> is closer to the prior than <img src="https://latex.codecogs.com/png.latex?x"> itself is –</p>
<p><img src="https://latex.codecogs.com/png.latex?%7CE%5BE%5Bx%7Cs%5D%7Cx%5D-%5Cmu%7C%3C%7Cx-%5Cmu%7C"></p></li>
<li><p>Attraction effects are typically treated as the norm. For example if <img src="https://latex.codecogs.com/png.latex?x"> is drawn from a normal distribution and if <img src="https://latex.codecogs.com/png.latex?s"> is equal to <img src="https://latex.codecogs.com/png.latex?x"> plus normal noise, then you’ll always get attraction to the prior. I.e., if <img src="https://latex.codecogs.com/png.latex?x"> is above the mean, then it’ll be, on average, estimated to be closer to the mean than it actually is.</p></li>
<li><p>However this has sometimes been treated as a puzzle in studies of perception: perception seems like inference, but we also find what look like <em>repulsion</em> effects. For example “contrast” effects, in which an object seems less dark when you put it next to another, darker, object. If we assume that the colour of the neighboring objects affects your prior about the target object, then this would imply an <em>attraction</em> effect. Yet repulsion effects seems to be the norm across all sorts of judgments (lightness, colour, volume, orientation, size), and similar contrast effects occur in time as well as in space (i.e., something seems less dark if it is preceded by something darker) – though of course there are exceptions. These types of illusion are sometimes called “anti-Bayesian.”</p></li>
<li><p>A common explanation of these contrast effects is that we ‘code for differences’ – i.e.&nbsp;that something about our neural wiring causes us to encode <em>differences</em>, rather than <em>levels</em>, and this causes us to exaggerate differences, i.e.&nbsp;get contrast effects.</p></li>
<li><p>But this assumes that we encode the difference and then forget to decode (AKA <strong>coding catastrophe</strong>, AKA the <strong>el Greco fallacy</strong>). If you write down a Bayesian model, which makes its best effort to infer the level from the difference, you typically do <em>not</em> find the desired contrast effects (Schwartz, Hsu &amp; Dayan (2007)).</p></li>
<li><p>Wei and Stocker (2015) announce that they have made a breakthrough – a fully Bayesian model which generates contrast/repulsion effects generically. They say that the key assumption is that we are more sensitive to differences in areas where signals are more likely to fall – i.e., sensitivity is proportional to the density of the prior.</p></li>
<li><p>Formally, let <img src="https://latex.codecogs.com/png.latex?x%5Csim%20f">, and <img src="https://latex.codecogs.com/png.latex?s=F(x)+%5Cvarepsilon."> This means that sensitivity is proportional to the density of the prior – and it implies that <img src="https://latex.codecogs.com/png.latex?s"> will be roughly uniformly distributed – so in some sense it’s an efficient use of signal capacity. Given this setup, and some simplifications, they find that the bias is proportional to the slope of the prior – so if the prior is symmetric &amp; single-peaked then for values above the mean, the bias will be positive, and vice versa – i.e.&nbsp;<em>repulsion</em> away from the prior everywhere.</p></li>
<li><p>In the note below I give a proof that implies that it is impossible to have repulsion effects everywhere – which seems to contradict the results of Wei &amp; Stocker.</p></li>
<li><p>I’m not sure what the source of the contradiction is – it could be either (a) Wei &amp; Stocker’s results are true locally, but do not apply at the tails of the distribution, and so things balance out that way; (b) there is a difference in the implicit assumption used when taking conditional expectations (AKA the Borel-Kolmogorov paradox); or (c) I made a mistake.</p></li>
<li><p>I also mention below a related result, that there cannot be a consistent upward or downward bias (i.e., it cannot be that <img src="https://latex.codecogs.com/png.latex?E%5B%5Chat%7Bx%7D%5C%7Cx%5D%3Ex"> for all <img src="https://latex.codecogs.com/png.latex?x">). This is relevant for Wei &amp; Stocker’s result applied to asymmetric priors – e.g.&nbsp;if the prior is everywhere decreasing – where the result seems to imply a consistent upward bias. </p></li>
</ol>
<p><img src="tecunningham.github.io/posts/https:/www.dropbox.com/s/hwbeyp8ldrmwysp/rabbit.jpg?raw=1" class="img-fluid">{: .center-image }</p>
</section>
<section id="summary-of-proof" class="level2">
<h2 class="anchored" data-anchor-id="summary-of-proof">Summary of proof</h2>
<ol type="1">
<li>Suppose that there is repulsion from the prior everywhere, i.e.&nbsp;for all <img src="https://latex.codecogs.com/png.latex?x">, <img src="https://latex.codecogs.com/png.latex?%5C%7CE%5B%5Chat%7Bx%7D%5C%7Cx%5D-%5Cmu%5C%7C%3E%5C%7Cx-%5Cmu%5C%7C">.</li>
<li>This implies that <img src="https://latex.codecogs.com/png.latex?Var%5B%5Chat%7Bx%7D%5D%3EVar%5Bx%5D">.</li>
<li>But this contradicts the law of total variance, which says that <img src="https://latex.codecogs.com/png.latex?Var%5BE%5BA%5C%7CB%5D%5D%5Cleq%20Var%5BA%5D">.</li>
</ol>
</section>
<section id="detail" class="level2">
<h2 class="anchored" data-anchor-id="detail">Detail:</h2>
<p>Suppose there are two random variables <img src="https://latex.codecogs.com/png.latex?x"> and <img src="https://latex.codecogs.com/png.latex?s">, and let <img src="https://latex.codecogs.com/png.latex?%5Chat%7Bx%7D=E%5Bx%5C%7Cs%5D">. Let <img src="https://latex.codecogs.com/png.latex?x"> be mean-zero, and let’s assume repulsion from the prior everywhere, i.e.&nbsp;for all <img src="https://latex.codecogs.com/png.latex?x">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AE%5B%5Chat%7Bx%7D%7Cx%5D%7C%3E%7Cx%7C%0A"></p>
<p>From this repulsion assumption I think it’s clear that there’s more variance in <img src="https://latex.codecogs.com/png.latex?E%5B%5Chat%7Bx%7D%5C%7Cx%5D"> than in <img src="https://latex.codecogs.com/png.latex?x">:</p>
<p><img src="https://latex.codecogs.com/png.latex?Var%5BE%5B%5Chat%7Bx%7D%7Cx%5D%5D%3EVar%5Bx%5D"></p>
<p>Now let’s apply the law of total variance:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0AVar%5BA%5D=&amp;%20E%5BVar%5BA%7CB%5D%5D+Var%5BE%5BA%7CB%5D%5D%20%5C%5C%5C%5C%0AVar%5B%5Chat%7Bx%7D%5D=&amp;%20E%5BVar%5B%5Chat%7Bx%7D%7Cx%5D%5D+Var%5BE%5B%5Chat%7Bx%7D%7Cx%5D%5D%0A%5Cend%7Baligned%7D%0A"></p>
<p>Thus implying that:</p>
<p><img src="https://latex.codecogs.com/png.latex?Var%5B%5Chat%7Bx%7D%5D%5Cequiv%20Var%5BE%5Bx%7Cs%5D%5D%3EVar%5Bx%5D"></p>
<p>Applying the law of total variance again we get:</p>
<p><img src="https://latex.codecogs.com/png.latex?%5Cbegin%7Baligned%7D%0AVar%5Bx%5D=&amp;%20E%5BVar%5Bx%7Cs%5D%5D+Var%5BE%5Bx%7Cs%5D%5D%20%5C%5C%5C%5C%0A%20%20%20%20%20%20%3E&amp;%20Var%5Bx%5D%0A%5Cend%7Baligned%7D"></p>
<p>A contradiction.</p>
</section>
<section id="no-consistent-upwarddownward-bias" class="level2">
<h2 class="anchored" data-anchor-id="no-consistent-upwarddownward-bias">No consistent upward/downward bias</h2>
<p>The law of iterated expectations states that, for any <img src="https://latex.codecogs.com/png.latex?A"> and <img src="https://latex.codecogs.com/png.latex?B">:</p>
<p><img src="https://latex.codecogs.com/png.latex?E%5BE%5BA%7CB%5D%5D=E%5BA%5D"></p>
<p>This implies that there cannot be a consistent upward or downward bias, i.e.&nbsp;it cannot be true that:</p>
<p><img src="https://latex.codecogs.com/png.latex?E%5B%5Chat%7Bx%7D%7Cx%5D%3Ex,%20%5Cforall%20x"></p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<ul>
<li>Schwartz, Hsu &amp; Dayan (2007, Nature Review Neuro) “Space and Time in Visual Context”</li>
<li>Wei &amp; Stocker (2015, Nature Neuroscience) <strong>“A Bayesian observer model constrained by efficient coding can explain ‘anti-Bayesian’ percepts”</strong></li>
</ul>


</section>

 ]]></description>
  <guid>tecunningham.github.io/posts/2017-05-26-repulsion-from-the-prior.html</guid>
  <pubDate>Fri, 26 May 2017 07:00:00 GMT</pubDate>
</item>
<item>
  <title>The Repeated Failure of Laws of Behaviour</title>
  <link>tecunningham.github.io/posts/2017-04-15-the-mechanical-and-the-rational.html</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/https:/www.dropbox.com/s/g5zrkpncc4mff29/krazy.jpg?raw=1" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">krazy kat</figcaption>
</figure>
</div>
<section id="nutshell" class="level1">
<h1>Nutshell</h1>
<ol type="1">
<li><p>In retrospect a lot of behaviour that was studied in the lab, which we thought was telling us about the wiring of animals, actually was telling us about the world outside the animal..</p></li>
<li><p>It has turned out, over and over again, that an animal’s response to a stimulus reflects the animal’s <em>beliefs</em> about what that stimulus represents in the world. So the “laws” of behaviour that we discovered are actually just describing, at a remove, regularities in the world.</p>
<table class="table">
<colgroup>
<col style="width: 47%">
<col style="width: 52%">
</colgroup>
<thead>
<tr class="header">
<th>“law” of behaviour</th>
<th>truth about the environment</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Animals will tend to repeat whichever action is rewarded</td>
<td>Actions which have been rewarded in the past tend to be rewarded in the future</td>
</tr>
<tr class="even">
<td>Objects appear darker to people when neighboring objects are brighter</td>
<td>Objects <em>are</em> darker when neighboring objects are brighter</td>
</tr>
<tr class="odd">
<td>Blue objects appear more distant to people</td>
<td>Blue objects <em>are</em> more distant</td>
</tr>
<tr class="even">
<td>People brake when the rate of change of the angle of an approaching object (<img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cdot%7B%5Ctheta%7D%7D%7B%5Ctheta%7D">) exceeds some threshold</td>
<td><img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cdot%7B%5Ctheta%7D%7D%7B%5Ctheta%7D"> determines the time to impact of an approaching object in typical circumstances, and so the optimal time to brake</td>
</tr>
<tr class="odd">
<td>Consumers’ expenditure increases less than proportionally with changes in income</td>
<td>Changes in income are typically temporary, and so imply a less-than-proportional response to maintain stable long-run expenditure</td>
</tr>
</tbody>
</table></li>
<li><p>Here’s the point expressed formally. Some scientist observes response <img src="https://latex.codecogs.com/png.latex?r"> and stimulus <img src="https://latex.codecogs.com/png.latex?s">, and proposes a law of behaviour, a simple function from <img src="https://latex.codecogs.com/png.latex?s"> to <img src="https://latex.codecogs.com/png.latex?r">. But for any such law, <img src="https://latex.codecogs.com/png.latex?r(s)"> there exists at least one rationalization, under which the organism has beliefs about an unobserved variable <img src="https://latex.codecogs.com/png.latex?x">, and they choose <img src="https://latex.codecogs.com/png.latex?r"> optimally given what they infer about <img src="https://latex.codecogs.com/png.latex?x"> from observing <img src="https://latex.codecogs.com/png.latex?s">, i.e.,</p>
<p><img src="https://latex.codecogs.com/png.latex?r(s)%20=%20%5Carg%20%5Cmax_%7Br%7D%20%5Cint%20u(s,r,x)%20f(x%7Cs)"></p>
<p>Given some pattern of behaviour <img src="https://latex.codecogs.com/png.latex?r(s)">, we can back out the beliefs that would justify that behaviour, <img src="https://latex.codecogs.com/png.latex?f(x%5C%7Cs)">, and we’ve seen – many times repeated – that those beliefs turn out to be <em>accurate</em> – as in the cases above, even when the scientist wasn’t aware of that truth.</p></li>
<li><p>You could reply that, sure, it’s optimal in the typical situation, but animals keep applying the same behaviours in cases where it’s not optimal, and that’s why they are laws of behaviour. There are some cases like that, but it seems to me that there are many more cases which go in the other direction: when the situation is changed, the behaviour changes, and it turns out the animal does what’s optimal, not what the law implies.</p></li>
<li><p>The biggest example of the failure of behavioural laws is the theory of <strong>conditioning and associative learning</strong>. Psychologists started with proposing a simple function that governs behaviour – the more often you are rewarded for doing something, the more often you do it – but then were forced to add a long list of qualifications and special cases (context-dependence, blocking, intermittent reinforcement, extinction, matching). It gradually became clear that the complications were not arbitrary, but made sense from the animal’s point of view: they are sensible strategies based on what an animal should expect in a typical environment. So the complex rules that we had been mapping out were not telling us about the animal’s wiring, they were instead telling us about the world that the animal lives in. Animals tend to repeat actions that are rewarded (i.e.&nbsp;obey the laws of reinforcement learning) only when they have reason to believe that rewards will be positively correlated across time. When they are in a situation where they don’t expect that correlation, then they no longer obey the rules of reinforcement learning. Mitchell et al.&nbsp;(2009) cite a lot of evidence about human associative learning and say:</p>
<blockquote class="blockquote">
<p>“we reconsider (and reject) one of the oldest and most deeply entrenched dual-system theories in the behavioral sciences, namely the traditional view of associative learning as an unconscious, automatic process that is divorced from higher-order cognition.”</p>
</blockquote>
<p>Their rejection is based on evidence that, when people learn associations, they only follow them insofar as those associations are good guides to achieving their goals.</p></li>
<li><p>There’s a very similar case in <strong>perception</strong>, where psychologists have been trying to learn the function from sensation to perception. A famous observation was of lateral inhibition: a stimulus seems less bright when the neighbouring stimulus gets brighter. In the 1950s this was thought to be due to wiring of neurons in the eye, but gradually it became clear that the effect only occurs in certain cases, and in other cases the opposite effect is observed. And then people realized that the cases in which it works are exactly the cases where it would be a reasonable inference in a typical environment. Adelson (1993, <em>Science</em>):</p>
<blockquote class="blockquote">
<p>“All of the phenomena discussed above lead to the same conclusion: Brightness judgments cannot be simply explained with low-level mechanisms. Geometrical changes that should be inconsequential for low-level mechanisms can cause dramatic changes in the brightness report. It is as if the visual system automatically estimates the reflectances of surfaces in the world[.]”</p>
</blockquote></li>
<li><p>A similar thing has happened in the study of <strong>control laws</strong>, or invariants, simple principles which map stimulus to response. For example Lee’s (1976) tau-dot model of braking: you brake when <img src="https://latex.codecogs.com/png.latex?%5Cdot%7B%5Ctau%7D"> is above some threshold, where <img src="https://latex.codecogs.com/png.latex?%5Ctau=%5Cfrac%7B%5Ctheta%7D%7B%5Cdot%7B%5Ctheta%7D%7D">, and <img src="https://latex.codecogs.com/png.latex?%5Ctheta"> is the angle of an approaching object. Many people spent a lot of time proposing control laws for different domains, and testing control laws against each other, but the field (I believe) has now mostly given up the hope of finding simple laws to model behaviour. Weber and Fajen (2014) say:</p>
<blockquote class="blockquote">
<p>“numerous studies have demonstrated that observers often rely on non-invariants and that the particular optical variables upon which they rely to guide action can change—as a consequence of practice, as a function of the range of conditions that are encountered, and as a function of the dynamics of the controlled system.”</p>
</blockquote></li>
<li><p>Slightly more of a stretch - there are some similar episodes in the study of <strong>economic decision-making</strong>. In the 1950s economists had established various laws about how expenditure related to a person’s income. Milton Friedman, in 1957, showed that many properties and puzzles of the expenditure function could be understood as byproducts of a person rationally planning to spread their income over time. Likewise Lucas (1976) argued that, in three different cases, where a statistical regularity in decisions was observed, you could explain <em>why</em> they occur if you model people as making tradeoffs given sensible beliefs about their economic prospects.</p></li>
<li><p>I’m not trying to argue that all behaviour is rational and that the brain optimally combines all available information. But looking at the track record of psychologists they have systematically underestimated the brain – they keep proposing simple behavioural rules – response as a function of stimulus – which later turn out to be only true insofar as they reflect some deeper organizing principle.</p></li>
<li><p>I think the same is true for a lot of <strong>behavioural biases.</strong> Economists sometimes treat “loss aversion”, “probability weighting”, etc., as if they are hard-wired, and scratch their heads when an experiment finds behaviour going in the opposite direction. But almost certainly these regularities are just local manifestations of some deeper - as yet unknown - principles. (An earlier <a href="../2016/04/30/relative-thinking/">post</a> makes this point about “relative thinking” effects).</p></li>
<li><p>The rest of this note gives some more detail about how this pattern played out in the history of reinforcement learning.</p></li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/https:/www.dropbox.com/s/u3pqkseoxfqvh1k/herriman-03.jpg?raw=1" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Krazy Kat</figcaption>
</figure>
</div>
</section>
<section id="reinforcement-learning" class="level1">
<h1>Reinforcement Learning</h1>
<p>For a long time I’d been confused about the status of reinforcement learning theory – the theory was massively popular in the first half of the 20th century, but these days psychologists seem to treat it as defunct, an ex-theory. And yet it’s still being used: people who train animals still talk a lot about conditioning, neuroscientists are crazy about reinforcement learning, as are people who work in artificial intelligence.</p>
<p>From having read a lot of this stuff my basic understanding is now this: The generalizations about how animals and humans learn are valid only in specific contexts, they are <em>not</em> deep and fundamental laws of behaviour, as had been once believed. However the theory remains a very useful simple model of decision-making because it’s a good model of the <em>environment</em> in which decision-makers typically work. I.e., the theory doesn’t tell us much about how the brain works, it tells us mostly about the environment which the brain was designed for.</p>
</section>
<section id="classic-examples-of-reinforcement-learning" class="level1">
<h1>Classic Examples of Reinforcement Learning</h1>
<p>In 1905 Thorndike proposed a basic principle of animal learning: “Of several responses … those which are accompanied or closely followed by satisfaction … will be more likely to recur.” The same basic idea was later described as “instrumental conditioning,” “operant conditioning”, or “reinforcement learning.” The term ‘behaviorism’ referred to a school of psychology, popular in the middle of the 20th century, based on this principle or variants.</p>
<p>Rats and pigeons in boxes will pretty quickly learn to tap a button, when tapping the button is followed by receiving a food pellet. They can also learn much more complex functions through reinforcement, like learning to tap on the left button when they see a square, and on the right button when they see a circle. (B F Skinner worked on using pigeons to guide missiles, by training them to tap on pictures of military targets.)</p>
<p>In the 1950s there were some influential studies in which human behaviour was manipulated through conditioning. In Greenspoon (1955) a subject would be asked to say aloud a sequence of words, whichever came to mind, and the experimenter would give subtle positive cues when he heard certain types of word - for example plural nouns. Gradually subjects would start saying plural nouns more often, without being aware of this. The implication drawn was that many of our everyday decisions, which we think of as conscious and deliberate, are actually just imprints of previous patterns of reinforcement.</p>
<p>I believe that lots of animal training still uses principles of conditioning: you give the animal a small reward whenever it does something you want it to, and you gradually build up more complicated behaviours. They also use other concepts from conditioning like secondary reinforcers and intermittent reinforcement.</p>
<p>In 1992 IBM built a backgammon-playing neural net that used a kind of reinforcement learning – in short the computer would be more likely to make the same move again if it had a good outcome in previous cases. The ultimate reinforcement was from winning the game, and expectations propagated back from that, to learn the value of positions in the middle of the game. The program was a great success – it was trained against itself, and quickly became good enough to beat most human players.</p>
<p>In the 1990s there was a lot of excitement when some neuroscientists discovered that levels of dopamine in the mid-brain responded to rewards in a way consistent with a reinforcement-learning model. In particular, dopamine didn’t correlate with the level of reward, but correlated with the level of <em>unexpected</em> reward: i.e., if you receive a reward in a situation where you wouldn’t normally. This is the kind of calculation which an algorithm would do if it was implementing reinforcement learning: it would update weights when a rewards is different from the expected level of reward.</p>
</section>
<section id="additional-laws" class="level1">
<h1>Additional Laws</h1>
<p>There are some interesting additional laws that were discovered about conditioning.</p>
<p><strong>blocking.</strong> The order in which associations are learned is important. Suppose a pigeon learns to press a lever whenever she hears a beep. Subsequently, the beep is always accompanied by a flash. When the flash appears by itself, the pigeon won’t have learned to peck. But if the beep and flash were paired right from the beginning, then both the beep or flash would, by themselves, be sufficient for the pigeon to peck. So learning one association can “block” another association from being learned. (“Reverse blocking” is sometimes, but not always, also observed: the pigeon learns to associate A and B with a reward, but then when she finds that B predicts the reward by itself, she subsequently ignores A.).</p>
<p><strong>intermittent reinforcement.</strong> Intermittent reinforcement is often found to create more robust associations than unvarying reinforcement. If you give a pigeon a pellet every time she pecks the button, then when you stop giving her pellets she’ll stop pecking the button. If you only give her a pellet occasionally, then the behaviour will take much longer to die out.</p>
<p><strong>matching.</strong> If you give a pigeon two different levers, each of which will release a pellet with a fixed probability, then the bird will learn to peck preferentially on the lever with the higher probability. However it will still occasionally peck on the lever with the lower probability, roughly in proportion to the ratio of probabilities. This seems to be a violation of rationality because if the pigeon had learned the probabilities, and was maximizing expected value, then it should peck constantly at the high-value lever.</p>
<p>(also: secondary reinforcement; overtraining &amp; extinction; superstition.)</p>
<p>At the height of the enthusiasm for conditioning many people thought these laws gave insight into all aspects of human behaviour - mental illness, adolescent delinquency, sexual behaviour, language.</p>
</section>
<section id="difficult-cases" class="level1">
<h1>Difficult Cases</h1>
<p>In the first few decades of reinforcement learning many confirmations of the basic theory were published but, as often happens, the published evidence became less coherent as time went on. Many of the laws of reinforcement learning turn out to apply only in a subset of situations, or the parameters varied widely, and in other situations the effects seem to reverse.</p>
<p><strong>reverse reinforcement.</strong> An old finding, regarding rats running mazes, is that when the rat finds a piece of cheese down one passage, then they were <em>less</em> likely to go down that passage the next time they were in the maze. According to reinforcement learning they should be more likely to go down that passage.</p>
<p><strong>context specificity.</strong> The speed of learning associations between stimuli and responses is very different depending on the stimulus and the response. Some associations can be learned firmly with just a single experience, for example a rat refusing to eat red pellets after getting nauseous after eating a red pellet. Others associations take far longer, e.g.&nbsp;a rat learning to associate a sound with getting nauseous.</p>
<p><strong>awareness.</strong> In humans, despite many attempts, very few cases have been found in which reinforcement can affect behaviour without people being consciously aware of it. Some of the classic findings have been reconsidered: in the study which manipulated peoples’ choice of words, it was found that the effect only occurred among subjects who were consciously aware of the association. Additionally, many learned responses can be turned on or off by simply telling the person. Colgan (1970) told subjects, after they learned an association, that the association is no longer valid (“from now on the bell will not signal an electric shock”) and he found that, although this didn’t entirely extinguish the flinching, it was much less pronounced after the instruction.</p>
</section>
<section id="putting-it-back-together" class="level1">
<h1>Putting it Back Together</h1>
<p>As I said, the laws of reinforcement learning turn out to apply only in a subset of situations.</p>
<p>One interpretation is that there do exist laws of learning, but that they are more complex. However the agreements and deviations from reinforcement-learning are not at random: in many cases they can be understood: the theory works in just those contexts where past associations tend to be a good guide to future associations; and it fails in contexts where that’s not true. E.g., a rat knows that the taste of food is a good predictor of whether it’ll make you sick, but doesn’t have reason to believe that the sound you hear when you eat food is a very good predictor.</p>
<p>Consider the rat who is less likely to run down a passage when they were previously rewarded for running down that passage. This makes sense if the rat remembers that he just ate the food down that passage, and so wants to look elsewhere. In this situation the rat expects the future payoff of an action to be <em>negatively</em> correlated with the past payoff, rather than positively correlated, and so we get the opposite effect than that predictedb by reinforcement learning.</p>
<p>The basic law of reinforcement learning can be recast in terms of beliefs: if you expect the future payoff of an action to be positively correlated with its past payoff, then it is rational to perform whichever act was rewarded in a similar situation in the past.</p>
<p>The other laws of reinforcement can also be recast in terms of beliefs. And the situations in which those laws are violated are often exactly the situations where such beliefs would not apply.</p>
<p><strong>intermittent reinforcement.</strong> Suppose an act was only occasionally reinforced. This means that on previous occasions when rewards stopped they resumed again later. So it’s not surprising that, having experienced rewards stop and resume once before, when they stop again you expect them to resume again.</p>
<p><strong>matching.</strong> There’s an obvious argument that probability matching is rational – in your usual environment the probability of reward changes over time, so it makes sense to continue monitoring each action, to see if the payoff has changed (see Estes, 1976).</p>
<p><strong>blocking.</strong> Blocking can be explained by a learning model, given a prior that <em>either</em> A or B is predictive of the reward, but not both. (See “Explaining away in Weight Space” by Dayan and Kakade, and good summary at http://www.cs.cmu.edu/~ggordon/conditioning-slides.pdf ).</p>
<p>Sometimes we observe that forward blocking occurs but not backward blocking. This can be explained by a mechnical prediction model (a Kalman filter), where you update weights only when unexpected things happen. So, you learn that A &amp; B are both associated with reward, then you are exposed to cases with just A and reward. Can this model be rationalized? It’s not clear to me. (AKA, what priors would justify a Kalman filter, given that it depends on the order of presentation?)</p>
<p>Mitchell (2009) also notes that there is <em>less</em> blocking when you introduce cognitive load, implying that it’s not an automatic or mechanical effect (subjects “showed blocking of skin conductance CRs only when blocking was a valid inference.”)</p>
<p><strong>transfer learning.</strong> There are some examples where organisms can transfer patterns they have learned across quite different stimuli, e.g.&nbsp;learning patterns of complementarity/substitutability (Mitchell (2009) p190). This would require an elaborate reinforcement learning model to rationalize, but is simple with a rational model.</p>
<p><strong>context specificity.</strong> De Houwer, Vandorpe and Beckers (2005) say (in “Why have associative models fared so well?”)<sup>1</sup></p>
<blockquote class="blockquote">
<p>The two types of models can be differentiated … by manipulating variables that influence the likelihood that people will reason in a certain manner but that should have no impact on the operation of the associative model. We have seen that such variables (e.g., instructions, secondary tasks, ceiling effects, nature of the cues and outcomes) do indeed have a huge effect. Given these results, it is justified to entertain the belief that participants are using controlled processes such as reasoning and to look for new ways to model and understand these processes.</p>
</blockquote>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>I guess there are two natural followup questions to this argument:</p>
<p><strong>(1) If you looked in the history of psychology, could you find as many examples of making the opposite mistake?</strong></p>
<p>In other words, how often have we have over-estimated the rationality of human behaviour. Yeah, maybe you’re right. It’s not hard to find economists who will insist that, whatever people do, it’s in their best interest. I just think that psychologists tend to make the other mistake.</p>
<p><strong>(2) If you’re so down on it, then why is reinforcement learning useful to dog trainers and to computer programmers?</strong></p>
</section>
<section id="misc-notes" class="level1">
<h1>Misc Notes</h1>
<hr>
<p><strong>economic applications of association-based decision-making.</strong> Gilboa &amp; Schmeidler: case-based decision-making; Camerer: experience-weighted attraction learning. The NYU guy has a paper. Erev &amp; Roth (1998) say that reinforcement learning does a good job predicting behaviour in some games, better than equilibrium play. I wouldn’t defend completely rational behaviour, but on the other hand I wouldn’t expect RL behaviour to be <em>stable</em>: probably behaviour approximates RL in some contexts, and does the opposite in others. It’s not obvious that the RL model is a very good level of abstraction to describe behaviour at. Charness &amp; Levin (2005) run an experiment where reinforcement &amp; Bayesian updating give different predictions: you choose between urns, one more sensitive to state, one less sensitive. If you draw from the less-sensitive urn, and you receive a positive outcome, then you update about the state, and the Bayesian prediction is that you should switch urns, while reinforcement learning says you’ll stay with the same urn. They find that people largely stay with the same urn.</p>
<hr>
<p><strong>the gambler’s fallacy goes in the opposite direction to reinforcement learning.</strong> The gambler’s fallacy: winning a gamble at time <img src="https://latex.codecogs.com/png.latex?t"> makes you <em>decrease</em> the expectation of winning at <img src="https://latex.codecogs.com/png.latex?t+1">, i.e.&nbsp;the <em>opposite</em> prediction of a simple reinforcement model. However there’s a heuristic rationalization similar to the rationalization of rats in a maze: caveman Ug is shaking trees to get coconuts out. If there’s no coconut at time t, then there’s an increased probability of a coconut at t+1.</p>
<p><strong>Poggio and visual perception.</strong> In 1983 Poggio found that he could reinterpret prior findings in perception as implementation of Bayesian inference: &gt; “All problems in vision and more general perception were inverse problems, going back from the image to 3-D properties of objects and scenes. They were also, as typical for inverse problems, ill-posed. We used regularization techniques to “solve” specific vision problems such as edge detection and motion computation. In the process, we found that some of the existing algorithms for shape-from-shading, optical flow, and surface interpolation were a form of regularization. Our main contribution was to recognize illposedness as the main characteristic of vision problems and regularization as the set of techniques to be used for solving them.”</p>
<p><strong>Shephard’s theory of generalization.</strong> Shepard (1987) and Tenenbaum and Griffiths (2001) give a persuasive argument that apparent laws governing generalization between stimuli are context-dependent, in a way that is consistent with Bayesian inference.</p>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Also Seligman (1970) <em>On the Generality of Laws of Learning</em>, “That all events are equally associable and obey common laws is a central assumption of general process learning theory … A review of data from the traditional learning paradigms shows that the assumption of equivalent associability is false … it is speculated that the laws of learning themselves may vary with the preparedness of the organism for the associa- tion and that different physiological and cognitive mechanisms may covary with the dimension.”↩︎</p></li>
</ol>
</section></div> ]]></description>
  <guid>tecunningham.github.io/posts/2017-04-15-the-mechanical-and-the-rational.html</guid>
  <pubDate>Sat, 15 Apr 2017 07:00:00 GMT</pubDate>
</item>
<item>
  <title>Samuelson &amp; Expected Utility</title>
  <link>tecunningham.github.io/posts/2017-02-25-samuelson-expected-utility.html</link>
  <description><![CDATA[ 




<script type="text/javascript" charset="utf-8" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML,
https://vincenttam.github.io/javascripts/MathJaxLocal.js"></script>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/https:/www.dropbox.com/s/3oikh1bx9s0ywhd/clownleopard.jpg?raw=1" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">clown</figcaption>
</figure>
</div>
<blockquote class="blockquote">
<p>“If in every event which can possibly occur the consequence of action I is not preferred to that of action II, and if in some possible event the consequence of II is preferred to that of I, then any sane preferer would prefer II to I.”</p>
</blockquote>
<p>This sentence, in a letter from Savage in 1950, finally persuaded Samuelson that rational choices must obey expected utility. He had been skeptical – thinking that expected utility was just a simple approximation, like exponential discounting or like separability. Marschack and Savage and Friedman all wrote letters trying to persuade him, and though they were right, they kept using bad arguments, and Samuelson disposed of them.</p>
<p>Savage and Friedman wrote a paper saying that, because people buy both insurance and lottery tickets, expected utility implies that the utility-of-money must be concave then convex. Samuelson saw that this was ridiculous, &amp; said “there’s as much to be learned about gambling from Dostoyevsky as from Pascal.”</p>
<p>But the sentence from the letter above finally persuaded Samuelson.</p>
<p>This is all from Ivan Moscati’s <a href="http://pubs.aeaweb.org/doi/pdfplus/10.1257/jep.30.2.219">“How Economists Came to Accept Expected Utility Theory”</a>.</p>



 ]]></description>
  <guid>tecunningham.github.io/posts/2017-02-25-samuelson-expected-utility.html</guid>
  <pubDate>Sat, 25 Feb 2017 08:00:00 GMT</pubDate>
</item>
<item>
  <title>Economist Explorers</title>
  <link>tecunningham.github.io/posts/2017-02-25-economist-explorers.html</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/https:/www.dropbox.com/s/nuzwl1edqelaeim/explorers.jpg?raw=1" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">explorers</figcaption>
</figure>
</div>
<p>Imagine a group of adventurers set out to explore a new continent, each one choosing a different valley to map. And suppose that, instead of sending back reports of what they found (“a forest, a swamp, mosquitoes”), each one wrote reports calculated to appear as exciting as possible (“a forest, abundant water, rich fauna, couldn’t disconfirm rumors of a city of gold”).</p>
<p>This is how I feel when I’m refereeing economics papers: everyone’s trying to tell an exciting story, and it takes a great deal of work to figure out what actual novel facts they have discovered. It’s frustrating because it’s such an inefficient way to explore the territory: so many people have spent so much time on this, and we have so little to show. What have we discovered about decision-making in the last 50 years after proposing thousands of different models, running tens of thousands of experiments, and regressing millions of variables? Couldn’t we have accumulated more knowledge if we’d organized things differently?</p>
<p>(Although I have to admit that many of my own papers do include some speculation about cities of gold. But I think the most useful thing a referee can do is to suggest changes to the title and abstract, to make it more transparent exactly what the paper has found.)</p>
<p><a href="https://www.facebook.com/tom.cunningham.374549/posts/10157050635460230">My original post on Facebook</a></p>
<p>–</p>
<p>Followup: what have we discovered about decision-making in the last 50 years?</p>
<p>Once, when I taught a graduate class in behavioural economics, a couple of students came from civil engineering &amp; dentistry, hoping to learn something useful that they could use in modelling decision-making - e.g.&nbsp;in modelling how people make decisions about commuting. I was embarrassed in how little I could help them. I was able to think of a lot of useful stuff about decision making that’s ~50 years or older: utility &amp; expected utility, exponential (&amp; hyperbolic) discounting, Engel curves, responses to permanent &amp; temporary income, tractable demand estimation, the offsetting income &amp; substitution effects of wages. All of this is immediately useful in quantifying decision-making. But I can think of few recent examples of quantitatively useful findings. Special mention to kahneman &amp; tversky. If you ask, say, about attitudes to uncertainty, to time, to temptation, to intertemporal complementarities, I would begin my answer with “there are various schools of thought…”.</p>



 ]]></description>
  <guid>tecunningham.github.io/posts/2017-02-25-economist-explorers.html</guid>
  <pubDate>Sat, 25 Feb 2017 08:00:00 GMT</pubDate>
</item>
<item>
  <title>Weber’s Law Doesn’t Imply Concave Representations or Concave Judgments</title>
  <link>tecunningham.github.io/posts/2017-02-25-weber-fechner-law.html</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/https:/www.dropbox.com/s/24cumup0i7uiksn/runningman.jpg?raw=1" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">runningman</figcaption>
</figure>
</div>
<p><strong>Nutshell.</strong></p>
<ol type="1">
<li>If you spend any time reading about behavioural economics you’ll come across “diminishing sensitivity” pretty soon, applied to all sorts of things in decision-making (probability weighting, value-function weighting, relative thinking), and often you’ll find a reference to “Weber’s law” from psychology.</li>
<li>Weber’s law says that if you can tell the difference between a 1-pound and a 1.2-pound weight 90% of the time, then you will also be able to tell the difference between a 10-pound and 12-pound weight about 90% of the time (and generally, the ability to discriminate between two sensations is a function of the ratio of the magnitudes).</li>
<li>This finding is often interpreted as implying that we have, in some sense, <em>concave</em> representations of value. And indeed this behaviour <em>would</em> be implied by a model in which we receive a signal which is a concave function of the underlying value, plus additive noise.</li>
<li>However Weber’s law would also be seen if we have a linear representation of value with multiplicative noise. And there are good reasons to think that perceptual noise is generally closer to multiplicative than additive (but I leave that for another time).</li>
<li>Also of interest is, given our assumptions about value and noise, how a person’s posteriors look. I.e., what is the average best-guess of the weight of an object, given it’s true weight. I also derive those below (you have to make assumptions about a person’s priors), and show that, under multiplicative noise &amp; lognormal priors, expected weight is concave function of true weight, while under additive noise &amp; uniform priors, expected weight is a linear function of true weight.</li>
<li>In general I think that Weber’s law is <em>not</em> relevant most of the anomalies we see in decision-making. This note doesn’t really tie the knot on that argument, but I think these are useful results going in that direction. (I’m sure these mathematical points must have been made elsewhere but I’ve never been able to find them.)</li>
</ol>
<section id="history." class="level1">
<h1>History.</h1>
<p>Ernst Weber (1795-1878) tested subjects’ ability to discriminate between the strength of sensations - for example two sounds, two shades of light, two intensities of touch - and found that the Just Noticeable Difference (JND) between two sensations tended to be proportional to the average magnitude of the two sensations: e.g., if you can determine which of two lights is brighter with 80% accuracy, then Weber predicts that you will also have 80% accuracy when the brightness of both lights is doubled. Gustav Fechner (1801-1887) showed that, under the assumption that each JND has an equal subjective difference, this implies a logarithmic relationship between objective and subjective magnitude. Later S. S. Stevens (1906-1973) directly asked subjects for cardinal reports of subjective intensity and found a power law relationship (i.e., the logarithmic relation is a special case of this).</p>
</section>
<section id="in-decision-making" class="level1">
<h1>In Decision-Making</h1>
<p>In the 19th century a number of economists made a connection between the Weber-Fechner law and the diminishing marginal utility of consumption. However Max Weber – a different Weber – Weber the sociologist, wrote an essay in 1908 to explain that they are fundamentally distinct phenomena (“Marginal Utility Theory and the Fundamental Law of Psychophysics”). Stigler later wrote about Max Weber’s essay and agreed that there is little meaningful connection.</p>
<p>More recently the Weber-Fechner law has been cited by a number of economists to explain diminishing sensitivity in other domains (not just in absolute consumption): Thaler (1980), Tversky &amp; Kahneman (1992), Thaler (1999), Gonzalez &amp; Wu (1999) all argue that the curvature of a gain/loss function, and of probability-weighting function, are somehow connected with the Weber-Fechner law in perception.</p>
<p>However, I show below that the Weber behaviour (the proportional JND being invariant to scale) does not imply either (A) a concave internal representation of magnitude; or (B) a concave bias in judgment of magnitudes.</p>
<p>(It’s also worth noting that a <em>literal</em> application of Weber’s law to decision-making would be the following: that subjects are equally likely to choose \$7 over \$8, as they are to choose \$70 over \$80. I’m not sure if this has ever been tested, it doesn’t seem like an especially interesting fact for economic decision-making.)</p>
</section>
<section id="webers-law-linear-vs-concave-representations-of-value" class="level1">
<h1>Weber’s Law &amp; Linear vs Concave Representations of Value</h1>
<p>First I show that Weber’s law does not imply that we must have, in some sense, a concave representation of value. I show that the law can be derived from two different setups: one in which we have a linear representation and multiplicative noise; one with a concave representation and additive noise.</p>
<p>Suppose that we can observe subjects’ judgments about which of two values (<img src="https://latex.codecogs.com/png.latex?v_%7B1%7D"> and <img src="https://latex.codecogs.com/png.latex?v_%7B2%7D">) is greater. Then we can define a just noticeable difference, <img src="https://latex.codecogs.com/png.latex?JND(v,p)">, as the difference in values, such that subjects correctly discriminate <img src="https://latex.codecogs.com/png.latex?v"> and <img src="https://latex.codecogs.com/png.latex?v+JND(v,p)"> in a proportion <img src="https://latex.codecogs.com/png.latex?p"> of trials.</p>
<section id="linear-representation-multiplicative-noise" class="level2">
<h2 class="anchored" data-anchor-id="linear-representation-multiplicative-noise">Linear Representation &amp; Multiplicative Noise</h2>
<p>Assume people get signals about underlying value with multiplicative noise, <img src="https://latex.codecogs.com/png.latex?s=v%5Ccdot%20e">, with <img src="https://latex.codecogs.com/png.latex?e"> lognormal. For conciseness let <img src="https://latex.codecogs.com/png.latex?%5Cdelta=JND(v_%7B1%7D,p)">, then <img src="https://latex.codecogs.com/png.latex?%5Cdelta"> can be implicitly defined as:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0Ap%20%20%20=&amp;%20%20P(E%5Bv_%7B1%7D+%5Cdelta%7Cs_%7B2%7D%5D%3EE%5Bv_%7B1%7D%7Cs_%7B1%7D%5D)%20%5C%5C%0A%20%20%20%20=&amp;%20%20P((v_%7B1%7D+%5Cdelta)e_%7B2%7D%3Ev_%7B1%7De_%7B1%7D)%20%5C%5C%0A%20%20%20%20=&amp;%20%20P(%5Cln(v_%7B1%7D+%5Cdelta)+%5Cln%20e_%7B2%7D%3E%5Cln%20v_%7B1%7D+%5Cln%20e_%7B1%7D)%20%5C%5C%0A%20%20%20%20=&amp;%20%20%5CPhi%5Cleft(%5Cfrac%7B%5Cln(v_%7B1%7D+%5Cdelta)-%5Cln%20v_%7B1%7D%7D%7B%5Csigma_%7Be%7D%5E%7B2%7D+%5Csigma_%7Be%7D%5E%7B2%7D%7D%5Cright)%0A%5Cend%7Baligned%7D%0A"></p>
<p>where <img src="https://latex.codecogs.com/png.latex?%5CPhi"> is the CDF of a standard normal distribution. Then,</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%5Cln(v_%7B1%7D+%5Cdelta)-%5Cln%20v_%7B1%7D%20=&amp;%20%20%202%5Csigma_%7Be%7D%5E%7B2%7D%5CPhi%5E%7B-1%7D(p)%20%5C%5C%0A%5Cfrac%7Bv_%7B1%7D+%5Cdelta%7D%7Bv_%7B1%7D%7D%20%20=&amp;%20%20%20%5Cexp(2%5Csigma_%7Be%7D%5E%7B2%7D%5CPhi%5E%7B-1%7D(p))%5C%5C%0AJND(v_%7B1%7D,p)=%5Cdelta%20%20%20%20%20%20%20%20%20=&amp;%20%20%20v_%7B1%7D%5Cleft%5B%5Cexp(2%5Csigma_%7Be%7D%5E%7B2%7D%5CPhi%5E%7B-1%7D(p))-1%5Cright%5D%0A%5Cend%7Baligned%7D%0A"></p>
<p>In other words, the just noticeable difference is proportional to the value, <img src="https://latex.codecogs.com/png.latex?v_%7B1%7D">, as found by Weber.</p>
</section>
<section id="a-concave-representation-additive-noise" class="level2">
<h2 class="anchored" data-anchor-id="a-concave-representation-additive-noise">A Concave Representation &amp; Additive Noise</h2>
<p>Suppose that the decision-maker receives a concave signal of value with additive noise, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?s=%5Cln%20v+e">, with Gaussian <img src="https://latex.codecogs.com/png.latex?e">. Then the derivation is very similar:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Baligned%7D%0A%20%20p%20=&amp;%20P(E%5Bv_%7B1%7D%7Cs_%7B2%7D%5D%3EE%5Bv_%7B1%7D%7Cs_%7B1%7D%5D)%20%5C%5C%0A%20%20%20%20=&amp;%20P(%5Cln(v_%7B1%7D+%5Cdelta)+e_%7B2%7D%3E%5Cln%20v_%7B1%7D+e_%7B1%7D).%0A%5Cend%7Baligned%7D%0A"></p>
<p>The rest of the derivation is the same: i.e., the JND in the neighborhood of <img src="https://latex.codecogs.com/png.latex?v_%7B1%7D"> will be proportional to <img src="https://latex.codecogs.com/png.latex?v_%7B1%7D">.</p>
</section>
</section>
<section id="webers-law-bias-in-judgment" class="level1">
<h1>Weber’s Law &amp; Bias in Judgment</h1>
<p>Weber’s law implies that perception is noisy. We can then ask what is the appropriate Bayesian posterior given noise. In particular, what will the relationship be between a value <img src="https://latex.codecogs.com/png.latex?v">, and the subjective best-estimate of that value given the signal <img src="https://latex.codecogs.com/png.latex?s(v)">, i.e.&nbsp;<img src="https://latex.codecogs.com/png.latex?E%5BE%5Bv%5C%7Cs%5D%5C%7Cv%5D">.</p>
<p>It has often been assumed that Weber’s law will imply or justify a concave bias in estimates of value. However I show that this is not necessarily so.</p>
<p><strong>Note on expectations:</strong> In a continuous setup, like the below, you can’t just write the conditional expectation as an integral without further notes. The conditional expectation is undefined without specifying which <em>limit</em> you’re taking (<a href="https://en.wikipedia.org/wiki/Borel%E2%80%93Kolmogorov_paradox">AKA Borel-Kolmogorov paradox</a>). Below I <em>believe</em> that the integrals follow from taking limits with respect to the noise. Intuitively, this is like assuming that you observe only that the signal is in some range <img src="https://latex.codecogs.com/png.latex?(s-%5Cvarepsilon,s+%5Cvarepsilon)">, and taking limits as <img src="https://latex.codecogs.com/png.latex?%5Cvarepsilon%5Crightarrow0">.</p>
<section id="multiplicative-noise-posteriors-are-concave-in-v" class="level2">
<h2 class="anchored" data-anchor-id="multiplicative-noise-posteriors-are-concave-in-v">Multiplicative Noise =&gt; Posteriors are Concave in <img src="https://latex.codecogs.com/png.latex?v"></h2>
<p>Suppose we have lognormal priors for both <img src="https://latex.codecogs.com/png.latex?v"> and <img src="https://latex.codecogs.com/png.latex?e">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Beqnarray*%7D%0A%5Cln%20v%20&amp;%20%5Csim%20&amp;%20N(%5Cmu_%7Bv%7D,%5Csigma_%7Bv%7D%5E%7B2%7D)%5C%5C%0A%5Cln%20e%20&amp;%20%5Csim%20&amp;%20N(%5Cmu_%7Be%7D,%5Csigma_%7Be%7D%5E%7B2%7D),%0A%5Cend%7Beqnarray*%7D%0A"></p>
<p>and <img src="https://latex.codecogs.com/png.latex?s=v%5Ccdot%20e">, then we will have posteriors like:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Beqnarray*%7D%0Af(%5Cln%20v%7Cs)%20&amp;%20%5Csim%20&amp;%20N(%5Cfrac%7B%5Csigma_%7Bv%7D%5E%7B2%7D%7D%7B%5Csigma_%7Be%7D%5E%7B2%7D+%5Csigma_%7Bv%7D%5E%7B2%7D%7D%5Cln%20s,%5Cleft(%5Csigma_%7Bv%7D%5E%7B-2%7D+%5Csigma_%7Be%7D%5E%7B-2%7D%5Cright)%5E%7B-1%7D)%5C%5C%0AE%5Bv%7Cs%5D%20&amp;%20=%20&amp;%20%5Cexp%5Cleft(%5Cfrac%7B%5Csigma_%7Bv%7D%5E%7B2%7D%7D%7B%5Csigma_%7Be%7D%5E%7B2%7D+%5Csigma_%7Bv%7D%5E%7B2%7D%7D%5Cln%20s+%5Cfrac%7B1%7D%7B2%7D%5Cleft(%5Csigma_%7Bv%7D%5E%7B-2%7D+%5Csigma_%7Be%7D%5E%7B-2%7D%5Cright)%5E%7B-1%7D%5Cright)%5C%5C%0A&amp;%20=%20&amp;%20s%5E%7B%5Cfrac%7B%5Csigma_%7Bv%7D%5E%7B2%7D%7D%7B%5Csigma_%7Be%7D%5E%7B2%7D+%5Csigma_%7Bv%7D%5E%7B2%7D%7D%7De%5E%7B%5Cfrac%7B1%7D%7B2%7D(%5Csigma_%7Bv%7D%5E%7B-2%7D+%5Csigma_%7Be%7D%5E%7B-2%7D)%5E%7B-1%7D%7D%0A%5Cend%7Beqnarray*%7D%0A"></p>
<p>This means that the expected <img src="https://latex.codecogs.com/png.latex?v"> is concave in the signal <img src="https://latex.codecogs.com/png.latex?s"> (because the exponent is less than one). Intuitively: a doubling of the value, which causes a doubling of the stimulus, will cause a <em>less</em> than doubling of the expected value conditional on that stimulus, because it will cause us to revise upwards our beliefs about both <img src="https://latex.codecogs.com/png.latex?v"> and <img src="https://latex.codecogs.com/png.latex?e">.</p>
<p>Finally, we are also interested in the <em>average</em> posterior for a given <img src="https://latex.codecogs.com/png.latex?v">. This will also be concave (abbreviating <img src="https://latex.codecogs.com/png.latex?%5Calpha=%5Cfrac%7B%5Csigma_%7Bv%7D%5E%7B2%7D%7D%7B%5Csigma_%7Be%7D%5E%7B2%7D+%5Csigma_%7Bv%7D%5E%7B2%7D%7D">, and dropping the constant coefficient in <img src="https://latex.codecogs.com/png.latex?E%5Bv%7Cs%5D">):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Beqnarray*%7D%0AE%5BE%5Bv%7Cs%5D%7Cv%5D%20&amp;%20=%20&amp;%20%5Cint(v%5Ccdot%20e)%5E%7B%5Calpha%7Df(e)de%5C%5C%0A&amp;%20=%20&amp;%20v%5E%7B%5Calpha%7D%5Cint%20e%5E%7B%5Calpha%7Df(e)de.%0A%5Cend%7Beqnarray*%7D%0A"></p>
</section>
<section id="additive-noise-posteriors-are-linear-in-v" class="level2">
<h2 class="anchored" data-anchor-id="additive-noise-posteriors-are-linear-in-v">Additive Noise =&gt; Posteriors are Linear in <img src="https://latex.codecogs.com/png.latex?v"></h2>
<p>Suppose again that the decision-maker receives a logarithmic signal with additive noise: <img src="https://latex.codecogs.com/png.latex?s=%5Cln%20v+u">, and let <img src="https://latex.codecogs.com/png.latex?u"> be Gaussian. (I changed notation from <img src="https://latex.codecogs.com/png.latex?e"> to <img src="https://latex.codecogs.com/png.latex?u"> because I use a lot of exponential functions in the derivation.) Now assume that, in addition, <img src="https://latex.codecogs.com/png.latex?v"> is drawn from an improper uniform <img src="https://latex.codecogs.com/png.latex?(0,%5Cinfty)">. Consider the expected value of <img src="https://latex.codecogs.com/png.latex?v"> given the signal <img src="https://latex.codecogs.com/png.latex?s"> (I drop the constant term from the Gaussian distribution for conciseness):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Beqnarray*%7D%0AE%5Bv%7Cs%5D%20&amp;%20=%20&amp;%20%5Cfrac%7B%5Cint_%7B0%7D%5E%7B%5Cinfty%7Dve%5E%7B-%5Cleft(s-%5Cln%20v%5Cright)%5E%7B2%7D%7Ddv%7D%7B%5Cint_%7B0%7D%5E%7B%5Cinfty%7De%5E%7B-%5Cleft(s-%5Cln%20v%5Cright)%5E%7B2%7D%7Ddv%7D.%0A%5Cend%7Beqnarray*%7D%0A"></p>
<p>Now exchange variables, so that <img src="https://latex.codecogs.com/png.latex?v=e%5E%7Bz%7D">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Beqnarray*%7D%0AE%5Bv%7Cs%5D%20&amp;%20=%20&amp;%20%5Cfrac%7B%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7De%5E%7Bz%7De%5E%7B-(s-z)%5E%7B2%7D%7De%5E%7Bz%7Ddz%7D%7B%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7De%5E%7B-(s-z)%5E%7B2%7D%7De%5E%7Bz%7Ddz%7D%5C%5C%0A&amp;%20=%20&amp;%20%5Cfrac%7B%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7De%5E%7B-s%5E%7B2%7D+2(1+s)z-z%5E%7B2%7D%7Ddz%7D%7B%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7De%5E%7Bz-(s-z)%5E%7B2%7D%7Ddz%7D%5C%5C%0A&amp;%20=%20&amp;%20%5Cfrac%7B%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7De%5E%7B-(z-1-s)%5E%7B2%7D%7De%5E%7B1+2s%7Ddz%7D%7B%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7De%5E%7Bs+%5Cfrac%7B1%7D%7B4%7D%7De%5E%7B-((s+%5Cfrac%7B1%7D%7B2%7D)-z)%5E%7B2%7D%7Ddz%7D%5C%5C%0A&amp;%20=%20&amp;%20e%5E%7Bs%7De%5E%7B3/4%7D%5Cfrac%7B%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7De%5E%7B-(z-1-s)%5E%7B2%7D%7Ddz%7D%7B%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7De%5E%7B-((s+%5Cfrac%7B1%7D%7B2%7D)-z)%5E%7B2%7D%7Ddz%7D%0A%5Cend%7Beqnarray*%7D%0A"></p>
<p>Note that both of the integrals are independent of <img src="https://latex.codecogs.com/png.latex?s"> (because the integration is between <img src="https://latex.codecogs.com/png.latex?-%5Cinfty"> and <img src="https://latex.codecogs.com/png.latex?%5Cinfty">), so there exists some <img src="https://latex.codecogs.com/png.latex?%5Ckappa"> such that:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AE%5Bx%7Cs%5D=%5Ctext%7Be%7D%5E%7Bs%7D%5Ckappa.%0A"></p>
<p>Finally we are interested in the average posterior for a given <img src="https://latex.codecogs.com/png.latex?v"> (here I’m again ignoring all constant terms):</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cbegin%7Beqnarray*%7D%0AE%5BE%5Bv%7Cs%5D%7Cv%5D%20&amp;%20=%20&amp;%20%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7DE%5Bv%7Cs=%5Cln%20v+u%5D%5Ctext%7Be%7D%5E%7B-u%5E%7B2%7D%7Ddu%5C%5C%0A&amp;%20=%20&amp;%20%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7D%5Ctext%7Be%7D%5E%7B(%5Cln%20v+u)%7D%5Ckappa%5Ctext%7Be%7D%5E%7B-u%5E%7B2%7D%7Ddu%5C%5C%0A&amp;%20=%20&amp;%20v%5Cint_%7B-%5Cinfty%7D%5E%7B%5Cinfty%7D%5Ckappa%5Ctext%7Be%7D%5E%7Bu-u%5E%7B2%7D%7Ddu.%0A%5Cend%7Beqnarray*%7D%0A"></p>
<p>I.e., despite the logarithmic internal representation, the average posterior is linear in the value.</p>


</section>
</section>

 ]]></description>
  <guid>tecunningham.github.io/posts/2017-02-25-weber-fechner-law.html</guid>
  <pubDate>Sat, 25 Feb 2017 08:00:00 GMT</pubDate>
</item>
<item>
  <title>Relative Thinking</title>
  <link>tecunningham.github.io/posts/2016-04-30-relative-thinking.html</link>
  <description><![CDATA[ 




<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/https:/www.dropbox.com/s/35e22lkdea8mcwh/peaches-wall-painting-ercolano.jpg?raw=1" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">peaches</figcaption>
</figure>
</div>
<p>There are a lot of cute thought experiments where the apparent value of something depends on what it’s compared to:</p>
<ul>
<li>A \$5 discount off a \$20 radio feels more valuable than a \$5 discount off a \$500 bed.</li>
<li>The difference between a 1% chance and a 2% chance seems more important than between a 51% chance and a 52% chance (likewise with 1 and 2 days vs 51 and 52 days).</li>
<li>Paying \$10 for a liter of ice cream seems more attractive if you see that the price of half-liter is \$8</li>
<li>(I list some more examples at the bottom).</li>
</ul>
<p>Many people have felt that there’s a common principle at work, in particular: that the sensitivity to an attribute (price, probability, square feet) depends on the set of quantities that you’re considering. But different people have proposed different principles:</p>
<ul>
<li><strong>Range</strong>: sensitivity is decreasing in the range of values observed, i.e.&nbsp;you’re less sensitive as the difference between the maximum and minimum increases: Volkmann (1951), Mellers &amp; Cooke (1994), Bushong Schwarzstein &amp; Rabin (2016).</li>
<li><strong>Negative Range</strong>: sensitivity is <em>increasing</em> in the range of values observed, the exact opposite of the above: Koszegi &amp; Szeidl (2003).</li>
<li><strong>Proportional Range</strong>: sensitivity is increasing in the <em>proportional</em> range of values (the range divided by the average): Bordalo Gennaioli &amp; Shleifer (2013).</li>
<li><strong>Frequency</strong>: sensitivity at any point is increasing in the density of values in that neighborhood: Parducci (1965), Stewart Chater &amp; Brown (2010).</li>
<li><strong>Magnitude</strong>: sensitivity is decreasing in the magnitude of the values (e.g.&nbsp;the average): my own paper: Cunningham (2015).</li>
<li>There are lots of other variants, most notably ones with a reference point, e.g.&nbsp;<strong>Loss Aversion</strong> says you’re more sensitive <em>below</em> a reference point, and <strong>Diminishing Sensitivity</strong> says you’re more sensitive in the <em>neighborhood</em> of a reference point (both are in Tversky &amp; Kahneman 1991)..</li>
</ul>
<p>All of these models can be thought about as indifference curves that change slope as you change the elements in the choice set, e.g.&nbsp;below adding option C makes the indifference curves rotate clockwise, and so makes you prefer B to A:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%20%20%20%5Cxymatrix%7B%5C,%20&amp;%20%20&amp;%20%20&amp;%20%20&amp;%20%20&amp;%20.%20&amp;%20%5C,%5C,%20&amp;%20%20&amp;%20%5C,%5C%5C%0A%20%20%20%5C,%20&amp;%20A%20&amp;%20%20&amp;%20%20&amp;%20%20&amp;%20%20&amp;%20A%5C%5C%0A%20%20%20%5C,%20&amp;%20%20&amp;%20%20&amp;%20B%20&amp;%20%5Car@%7B-%7D%5Buullll%5D%20&amp;%20%20&amp;%20%20&amp;%20%20&amp;%20B%20&amp;%20%5Car@%7B-%7D%5Buul%5D%5C%5C%0A%20%20%20%5C,%20&amp;%20%20&amp;%20%20&amp;%20%20&amp;%20%20&amp;%20%20&amp;%20%20&amp;%20%20&amp;%20%20&amp;%20%20C%20%5C%5C%0A%20%20%20%5Car%5Buuuu%5D%5Car%5Brrrr%5D%20&amp;%20%20&amp;%20%20&amp;%20%20&amp;%20%5Car@%7B-%7D%5Buullll%5D%20&amp;%20%5Car%5Buuuu%5D%5Car%5Brrrr%5D%20&amp;%20%20&amp;%20%20&amp;%20%5Car@%7B-%7D%5Buuuull%5D%20&amp;%20%5C,%0A%20%20%20%7D%0A"></p>
<p>But each theory has different assumption about how the slope of the indifference curves depends on the placement of the options.</p>
<hr>
<p>I’m going to try to make the following points:</p>
<ol type="1">
<li><p><strong>None of these laws is hardwired.</strong> It’s easy to find examples where relative-value effects go in the opposite direction to that predicted by each of the laws above. Each of the papers listed above are only able to give the impression of unifying many observed effects by selective use of the evidence, selective use of predictions, or inconsistent interpretation of the laws that they propose.</p></li>
<li><p>However <strong>each of these laws has intuitive appeal because it fits a rational inference in certain situations.</strong> For example in some subset of cases the range of values <em>is</em> a good informative cue about the value of that attribute. However this is only true inside that subset, and when we step outside, the law stops working, and instead behaviour is governed by the inference (and this explains where the counterexamples mentioned in the previous point come from). Each of these laws (range, magnitude, etc.) are specified at the wrong level: we <em>cannot</em> meaningfully predict comparison effects just from the distribution of quantities on each dimension, we need to know something about the context to see what kind of inference is at work.</p></li>
<li><p>Despite all this, <strong>relative-value effects are <em>not</em> entirely rational inferences.</strong> We know that because many of the effects still work even when people are aware that there is no ground for the inference – e.g.&nbsp;when the choice set is generated randomly. The inferences are performed <em>pre-consciously</em> – the inferences are baked into instincts, that we can’t suppress, but none of the laws listed above does a good job of summarizing those instincts.</p></li>
</ol>
<p><strong>A few other points that I’ll leave for later:</strong></p>
<ol type="1">
<li>There are some cute tricks for identifying different types of relative-thinking effects through intransitive choices.</li>
<li>The quality of the evidence we have on almost all of these choice-set effects is very low.</li>
<li>Weber’s law is not relevant to diminishing sensitivity in choice. (<a href="./2016/02/25/weber-fechner-law/">Now written up!</a>)</li>
</ol>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/https:/www.dropbox.com/s/iuzozpx81eakwyl/mosaique_5th.jpg?raw=1" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">party</figcaption>
</figure>
</div>
<section id="digression-perception" class="level2">
<h2 class="anchored" data-anchor-id="digression-perception">DIGRESSION: PERCEPTION</h2>
<p>Comparison effects have been studied in perception for a long time, and the same points I make here also apply there. At first people proposed that perceptual comparison effects were hardwired things, mechanical effects, but on further study it turned out that they were context-dependent, in a way that makes them look like sensible inferences.</p>
<p>Here’s a classic contrast effect:</p>
<p><img src="tecunningham.github.io/posts/https:/www.dropbox.com/s/xq7j30n5j822lji/illusion_contrast.gif?raw=1" class="img-fluid"></p>
<p>the same shade of grey looks darker when surrounded by white, than when surrounded by black.</p>
<p>For a long time psychology textbooks gave this as an example of a <em>hardwired</em> contrast effect – i.e.&nbsp;this is caused by the basic wiring of neurons in our eyes. (Some still do).</p>
<p>But take a look at this (White’s illusion):</p>
<p><img src="tecunningham.github.io/posts/https:/www.dropbox.com/s/pgc6oj9hxgrz1uv/illusion_white.png?raw=1" class="img-fluid"></p>
<p>here the same shade of grey looks <em>lighter</em> on the left than on the right, despite the surroundings being relatively <em>lighter</em> on the left than on the right. This is exactly the opposite of what’s predicted by a hardwired contrast effect in perception.</p>
<p>(An even cleaner example would be <em>ceteris paribus</em>, where making some part of the background lighter, all else held equal, makes the foreground appear lighter. The figure above does imply that there must exist at least one such case: imagine a third set of grey rectangles which are surrounded only by white. That third case must serve as a <em>ceteris paribus</em> case for one or other of the two cases above, probably both.)</p>
<p>The general point is this: <strong>There is not a stable relationship between the perceived-lightness of an object and the lightness of surrounding objects.</strong> There isn’t even an all-else-equal relationship. The relationship can run in either direction depending on the context.</p>
<p>But that’s not the last word. The set of contexts where it goes one way or the other way aren’t arbitrary (“contrast effects” and “assimilation effects”). Adelson (2001) shows that you can usually predict when you’ll observe one effect or the other: roughly, whether or not the surrounding lightness is a positive or negative ecological cue for illumination. I.e., in typical circumstances, is the surrounding lightness positively or negatively associated with illumination? (See an example at the bottom of this post.)</p>
</section>
<section id="digression-2" class="level2">
<h2 class="anchored" data-anchor-id="digression-2">DIGRESSION 2</h2>
<p>I would write out lists of comparison-effect examples over and over while working on my PhD. My train of thought would get detached and I’d end up asking myself questions like: What are you doing sitting in this office, a continent away from your friends and an ocean and a continent away from your family? Why do you spend your weekends in this sad building, where people stare at the carpet when they pass each other? What rock did you hit in adolescence that knocked you out of orbit, and sent you here? Are you trying to make your mother proud? Avenge your father? Do these professors you work with look like the kind of man you want to be? Did you stumble into one of those academic fields that people snigger about? Why, when you talk about your work, do the people you admire glaze over, and the people who bore you perk up? Do you think that giving your life to intellectual things makes you better than other people? Do you look down on people who don’t think so clearly? What are you doing on a Friday night at the NBER eating a tuna subway sandwich and reading reddit? If it takes you 5 years to get straight one point about relative thinking – one corner of one shelf in one cupboard – then how long is it going to take to tidy up the whole house? When an undergraduate corners you, asking earnest &amp; tedious questions, doesn’t it remind you of yourself?</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/https:/www.dropbox.com/s/ne3zsq37srpobul/nber_desk.jpg?raw=1" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">desk</figcaption>
</figure>
</div>
</section>
<section id="alternative-heuristics" class="level2">
<h2 class="anchored" data-anchor-id="alternative-heuristics">ALTERNATIVE HEURISTICS</h2>
<p>OK. Well, here’s the body of the argument: I’m going to discuss a few perfectly reasonable reasons why we might infer the value of different attributes from the choice set, and each reason will imply one of the above laws <em>in a subset of cases</em>. However I also show that the same reason can imply the exact opposite of that law, for cases outside that subset.</p>
<section id="mrs-shifting-towards-mrt." class="level3">
<h3 class="anchored" data-anchor-id="mrs-shifting-towards-mrt.">(1) <strong>MRS shifting towards MRT.</strong></h3>
<p>A choice set which varies in different attributes has an implicit rate of tradeoff between the attributes – i.e.&nbsp;the marginal rate of transformation (MRT) – and it is easy to think of cases where our preferences would naturally adapt to the implicit tradeoff, i.e., where our relative value, AKA our marginal rate of substitution (MRS), would rotate towards the tradeoff that is implicit in the choice set (i.e.&nbsp;the MRT).</p>
<p>The MRS shifting towards the MRT could be rationalized in two separate ways. First, suppose you believe the social MRS to be informative about your own MRS, and you believe that the choice set reflects the market price, and finally that the price reflects the social MRS (as it would in a competitive equilibrium). Second, suppose you believe the person who constructed the choice set to be cooperative (in the sense of Grice) - i.e., they only include things in the choice set which they think you might want: this implies that the MRT in the choice set reflects their beliefs about your MRS, which is itself informative. If I’m staying in your spare room and and you ask me “would you prefer a poached egg or gruel for breakfast?” then I will figure that your gruel must be pretty good.</p>
<p>If there are just two attributes (i,j) and two alternatives (a,b) then the implicit tradeoff is <img src="https://latex.codecogs.com/png.latex?MRT_%7Bi,j%7D=%5Cfrac%7B%5C%7Ca_%7Bi%7D-b_%7Bi%7D%5C%7C%7D%7B%5C%7Ca_%7Bj%7D-b_%7Bj%7D%5C%7C%7D">. If the MRS rotates to meet this MRT then the sensitivity to attribute <img src="https://latex.codecogs.com/png.latex?i"> will be decreasing in the range observed along that dimension, exactly as implied by the range-sensitivity theory (i.e., V, M&amp;C, BR&amp;S).</p>
<p><strong>However the MRS-MRT effect implies range sensitivity only for a 2-attribute, 2-alternative case. Outside of that case the intuitions depart.</strong></p>
<p>A marginal rate of transformation can only be directly identifed from a menu if the number of alternatives is equal to the number of attributes. I.e., to define a plane in <img src="https://latex.codecogs.com/png.latex?n"> dimensions from a set of points, you’ll need exactly <img src="https://latex.codecogs.com/png.latex?n"> points. If you have fewer then it becomes the statistical problem of fitting a line to a set of points. Here is a simple example where the MRT theory and other theories (e.g.&nbsp;range-sensitivity) give qualitatively different answers, and in which the MRS-MRT theory seems more faithful to the intuition. Suppose we have the following three options:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cxymatrix@C=1em@R=1em%7B%0A&amp;%20%5Cbinom%7B%5Cmbox%7B100K%20salary%7D%7D%7B%5Cmbox%7B199%20days%20off%7D%7D%5C%5C%0A%5C%5C%0A&amp;%20%20&amp;%20%5Cbinom%7B%5Cmbox%7B105K%20salary%7D%7D%7B%5Cmbox%7B189%20days%20off%7D%7D%20&amp;%20%5Cbinom%7B%5Cmbox%7B110K%20salary%7D%7D%7B%5Cmbox%7B189%20days%20off%7D%7D%5C%5C%0A%5C%5C%0A%5Car%5Brrrr%5D%5Car%5Buuuu%5D%20&amp;%20&amp;%20&amp;%20%20&amp;%20%20&amp;%20%5C,%20%5C%5C%0A%7D%0A"></p>
<p>The intermediate option is dominated by the by the option on the right, and intuitively - to me - the existence of the intermediate option makes the rightmost option more desirable - because the choice set makes 10-days-off seem to be worth between \$5K and \$10K, meaning the higher salary seems to come at a low cost in terms of days-off. This intuition is not captured by range sensitivity, because the intermediate option does not change the range in either dimension. However the intermediate option <em>does</em> change the implicit MRS, in the sense of the best-fitting line (e.g.&nbsp;orthogonal regression), and the change will be in favor of the rightmost option – fitting my own intuition.</p>
<p>Even in the 3-attribute 3-alternative case, it is no longer true that <img src="https://latex.codecogs.com/png.latex?MRT_%7Bi,j%7D"> is equal to the ratio of ranges on dimension i and j, it’s now a more complicated function.</p>
<p>When one attribute is a good and the other is a bad (e.g.&nbsp;price and quality; salary and hours) it is sometimes reasonable to think that choosing <em>neither</em> alternative is an additional implicit element of the choice set, i.e.&nbsp;the point (0,0). In these cases the ratio of the ranges reduces to the ratio of the maximum values (<img src="https://latex.codecogs.com/png.latex?%5Cfrac%7B%5Cmax_%7Bc%5Cin%20C%7Dc_%7Bi%7D%7D%7B%5Cmax_%7Bc%5Cin%20C%7Dc_%7Bj%7D%7D">), which has similar comparative statics to the theory in (C) - which depends on the ratio of average values - than the theory in (V,M&amp;C,BR&amp;S) - which depends on the ratio of the ranges.</p>
<p>An interesting fact: the effects of this MRS-MRT theory will not be detectable when the choice set is binary: suppose your MRS shifts towards the MRT implicit in the choice set, then although your final MRS will be closer to the MRT, it will not <em>cross</em> the MRT, i.e.&nbsp;the shift in MRS will not alter which of the two elements you prefer. This implies that the MRS-MRT theory cannot rationalize the existence of cycles in binary choice, and so cannot explain evidence for ‘subadditivity’ of different dimensions, such as probability, money, or delay (see Read (2001) for citations). For example, if your have a prior belief that <img src="https://latex.codecogs.com/png.latex?a"> is better than <img src="https://latex.codecogs.com/png.latex?b">, and then you observe a choice set containing <img src="https://latex.codecogs.com/png.latex?a"> and <img src="https://latex.codecogs.com/png.latex?b">, then you may revise upward your valuation of <img src="https://latex.codecogs.com/png.latex?b"> relative to <img src="https://latex.codecogs.com/png.latex?a">, but this observation wouldn’t cause you to switch preference, i.e.&nbsp;to think that <img src="https://latex.codecogs.com/png.latex?b"> is now better than <img src="https://latex.codecogs.com/png.latex?a">. (This could be violated under some unusual priors, e.g.&nbsp;if you had bimodal beliefs about the value of <img src="https://latex.codecogs.com/png.latex?b">).</p>
</section>
<section id="mrs-shifting-towards-demand." class="level3">
<h3 class="anchored" data-anchor-id="mrs-shifting-towards-demand.">(2) MRS shifting towards demand.</h3>
<p>There is a second strong intuition for choice sets influencing preferences: combinations <em>offered</em> often reflect combinations <em>desired</em>, so a relative increase in attribute 1 could be interpreted as a positive signal about the value of attribute 1. Suppose we manipulate the choice set, while keeping the relative price fixed, for example consider these two choice sets, trading off the price and quantity of some good:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cxymatrix@C=.5em@R=.5em%7B%5Car%5Brrrrr%5D%5Car%5Bddddd%5D%20&amp;%20&amp;%20&amp;%20%20&amp;%20%20&amp;%20%5Ctext%7Bapples%7D%5C%5C%0A&amp;%20%5Cbinom%7B%5Ctext%7B1%20apple%7D%7D%7B%5C$1%7D%5C%5C%0A&amp;%20%20&amp;%20%5Cbinom%7B%5Ctext%7B2%20apples%7D%7D%7B%5C$2%7D%5C%5C%0A&amp;%20%20&amp;%20&amp;%20%5Cbinom%7B%5Ctext%7B3%20apples%7D%7D%7B%5C$3%7D%5C%5C%0A%5C%5C%0A%5C%5C%0A%5C$%20%7D%0A"></p>
<p><img src="https://latex.codecogs.com/png.latex?%0A%5Cxymatrix@C=.5em@R=.5em%7B%5Car%5Brrrrr%5D%5Car%5Bddddd%5D%20&amp;%20&amp;%20&amp;%20&amp;%20%20&amp;%20%5Ctext%7Bapples%7D%5C%5C%0A&amp;%20%5C,%5C,%5C,%5C,%5C,%5C,%20&amp;%20%5C%5C%0A&amp;%20&amp;%20%5Cbinom%7B%5Ctext%7B2%20apples%7D%7D%7B%5C$2%7D%5C%5C%0A&amp;%20&amp;%20&amp;%20%5Cbinom%7B%5Ctext%7B3%20apples%7D%7D%7B%5C$3%7D%5C%5C%0A&amp;%20&amp;%20&amp;%20&amp;%20%5Cbinom%7B%5Ctext%7B4%20apples%7D%7D%7B%5C$4%7D%5C%5C%0A%5C%5C%0A%5C$%20%7D%0A"></p>
<p>A natural intuition is that people will tend to switch from <img src="https://latex.codecogs.com/png.latex?%5Cbinom%7B2%7D%7B%5C$2%7D"> to <img src="https://latex.codecogs.com/png.latex?%5Cbinom%7B3%7D%7B%5C$3%7D">, when going from the first to the second choice set. None of the theories discussed above gives an unambiguous prediction about the change in MRS between these two choice sets - because both the range and magnitude change by the same amount on each dimension - yet the intuition remains quite clear (I think).</p>
<p>This idea could be easily formalized - suppose people know the supply curve but are uncertain about the demand curve - then when they observe an increase in quantity they attribute this to a higher demand, and so they infer an increase in value of the good. I think this is similar to the intuition given in Kamenica (2008) - when you observe a higher price/quantity combination, you infer that demand is higher, and so you infer that the value of each marginal good must be higher. I think that a similar foundation is used in Drolet, Simonson, Tversky (2000) “Indifference Curves that Travel with the Choice Set”.</p>
<p>We have discussed diagonal shifts along the budget set, meaning that both attributes are varying at once; if only one attribute varied, it’s not clear what a consumer would infer from this. Of course we could formalize a model where the consumer is uncertain about both supply and demand; or we could combine this model with the prior one, where the consumer is uncertain about the price.</p>
</section>
<section id="magnitude-effects." class="level3">
<h3 class="anchored" data-anchor-id="magnitude-effects.">(3) Magnitude effects.</h3>
<p>Finally it’s easy to come up with a rational magnitude effect, such that when we observe a higher quantity <img src="https://latex.codecogs.com/png.latex?q"> we infer that the marginal value of each unit is less. Suppose we know the price and we know the consumption-value of a good, when measured in units that are familiar to us, but we do not know the units that are used in the packaging. Then if we observe other people consuming a higher quantity, measured in unfamiliar units, we infer that each unit is worth less: when we observe a 10,000 Kronor bank note we infer each Kronor is not worth a lot; when we observe a 10,000 Watt bulb we infer each Watt is not worth much; when we observe 200mg of Oxytocin we infer the marginal effect of a milligram is not too much.</p>
</section>
</section>
<section id="summary" class="level2">
<h2 class="anchored" data-anchor-id="summary">SUMMARY</h2>
<p>When we dig into the intuitions behind comparison effects, we often find that they resemble <em>inferences</em> that we would make every day. The laws that have been proposed to explain comparison effects only work because they coincide with one or other of the inferences in certain subsets of cases – e.g.&nbsp;range-sensitivity coincides with MRS-MRT inference, magnitude-sensitivity coincides with unit-value inference. But these overlaps occur only in a subset of cases, and stepping outside that subset we find that the law fails, while the inference remains.</p>
<p>Does this mean that comparison effects are all just rational inferences? What we would like to know is whether comparison effects occur even when inference can be entirely ruled out – e.g.&nbsp;when we run an experiment that explicitly randomizes the choice sets. Some papers do this, but few do it well. I am persuaded that comparisons <em>do</em> affect us on a pre-conscious level, i.e.&nbsp;that our instincts latch onto comparisons without being careful about the significance of the comparison in the particular circumstance, but there’s not a lot of unambiguous evidence on this. I can at least say that most people find the types of example listed above pretty beguiling: they get strong intuitions about relative value, but struggle to explain where the intuitions come from, implying that the inference isn’t entirely conscious.</p>
<p>So then why would we make bad inferences that resemble good inferences? I think for the same reason that our perception makes bad inferences that resemble good inferences – because perceptual processes interpret cues according to their <em>ordinary</em> significance, without adjusting for all relevant information. Perception is carried out in a cabinet, whirring through the sense data, and printing out conclusions for the conscious mind to read. The cabinet is locked, we only have access to the output. That is the argument of my paper on ‘implicit knowledge’.</p>
</section>
<section id="miscellaneous-notes" class="level2">
<h2 class="anchored" data-anchor-id="miscellaneous-notes">MISCELLANEOUS NOTES</h2>
<ul>
<li><p>It is useful to make a sharp distinction between “goods” and ’bads”. Examples of good-bad tradeoffs are quality vs price; salary vs hours worked. Examples of good-good tradeoffs are bedrooms vs bathrooms; MPG vs horsepower; salary vs holiday-days.</p></li>
<li><p>Parducci invented, as well as range-frequency theory, windsurfing.</p></li>
<li><p>Bordalo Gennaioli and Shleifer (2013) has a weird feature: the <em>salience</em> of an attribute depends on relative levels (<img src="https://latex.codecogs.com/png.latex?q-%5Cbar%7Bq%7D"> and <img src="https://latex.codecogs.com/png.latex?p-%5Cbar%7Bp%7D">), but the <em>utility</em> of an attribute depends on absolute levels (<img src="https://latex.codecogs.com/png.latex?q"> and <img src="https://latex.codecogs.com/png.latex?p">). I think this is just a mistake – the underlying intuition is matched much better if <img src="https://latex.codecogs.com/png.latex?U(q,p)%20=%20%5Chat%7B%5Ctheta%7D_q(q-%5Cbar%7Bq%7D)%20+%20%5Chat%7B%5Ctheta%7D_p(p-%5Cbar%7Bq%7D)"> instead of <img src="https://latex.codecogs.com/png.latex?U(q,p)%20=%20%5Chat%7B%5Ctheta%7D_q%20q%20+%20%5Chat%7B%5Ctheta%7D_p%20p">. This alternation removes a lot of the weird comparative statics of the theory, such as the severe non-monotonicity of the decoy effects. Another note: for two-alternative two-attribute choices the theory (as stated in the paper) has a utility representation, i.e.&nbsp;many of the predictions of that paper are equivalent to a model with menu-<em>independent</em> preferences. For a sufficiently large value of <img src="https://latex.codecogs.com/png.latex?M">:</p>
<p><img src="https://latex.codecogs.com/png.latex?%0AU(q,p)=%5Cbegin%7Bcases%7D%0A%5Cdelta%20q-p%20&amp;%20,%5C,q%3C%5Cdelta%20p%5C%5C%0AM+%5Cln%20q-%5Cln%20p%20&amp;%20,%5Cdelta%20p%3Cq%3C%5Cdelta%5E%7B-1%7Dp%5C%5C%0AM+q-%5Cdelta%20p%20&amp;%20,%5C,q%3E%5Cdelta%5E%7B-1%7Dp%0A%5Cend%7Bcases%7D%0A"></p></li>
</ul>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="tecunningham.github.io/posts/https:/www.dropbox.com/s/0xvbvy3m836zex8/scheletro_con_askoi.jpg?raw=1" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">skeleton</figcaption>
</figure>
</div>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">REFERENCES</h2>
<p>KS: Koszegi and Szeidl (2011) (KS)</p>
<p>BRS: Bushong Schwarzstein &amp; Rabin (2014)</p>
<p>MC: Mellers &amp; Cooke ()</p>
<p>C: Cunningham (2013)</p>
<p>BGS: Bordalo Gennaioli Shleifer (2012)</p>
<p>Simonson (2008) “Will I like a Medium Pillow?”</p>
<blockquote class="blockquote">
<p>“much of the evidence for preference construction reflects people’s difficulty in evaluating absolute attribute values and tradeoffs and their tendency to gravitate to available relative evaluations … These illustrations suggest that many forms of preference construction reflect a key underlying principle: decision makers tend to avoid absolute value judgments and gravitate to accessible relative evaluations … it is noteworthy that the evidence that has been accumulated to make the case for preference construction might be largely driven by a rather simple common principle. This rather simple, yet important absolute-to-relative principle lends itself to seemingly unrelated demonstrations, which have been treated as distinct phenomena and received unique labels.”</p>
</blockquote>
<p>David Stove (1991) <a href="http://web.maths.unsw.edu.au/~jim/wrongthoughts.html">“What is Wrong With our Thoughts?”</a></p>
</section>
<section id="another-illusion" class="level2">
<h2 class="anchored" data-anchor-id="another-illusion">Another Illusion</h2>
<p>Adelson’s “steps” illusion:</p>
<p><img src="tecunningham.github.io/posts/https:/www.dropbox.com/s/yhpp2qyd1x4yzke/AdelsonSteps.png?raw=1" class="img-fluid"></p>
<p>In the first picture the two squares with arrows on them look similar, but in the second picture they seem to have different shades. They are (as you guessed) all the same shade, and in fact the shades are all identical between the first and second image, just arranged a little differently.</p>
<p>In particular, the tilt gives an the impression of an angle, and so influences our judgment of where the illumination is coming from. In the first image both squares seem to be on the same plane; in the second image the upper square seems to be on a plane with light squares, and the lower square seems to be on a plane with dark squares. If we use, as proxies of illumination for a square, the shade of squares coplanar with it, then we get the predicted effect.</p>
</section>
<section id="more-examples" class="level2">
<h2 class="anchored" data-anchor-id="more-examples">More Examples</h2>
<ul>
<li>If you have to judge the value of 10 of something – say you’re bidding on 10 bottles of wine in an auction – they’ll seem more valuable if, at the same time, you’re also considering a single bottle of wine; and vice versa if you’re also considering 100 bottles of wine.</li>
<li>If you’re choosing between a low-price and medium-price version of a good, seeing that there’s also a high-price version makes the medium-price version seem relatively more attractive.</li>
<li>(See my ‘comparisons and choice’ paper for more examples).</li>
</ul>


</section>

 ]]></description>
  <guid>tecunningham.github.io/posts/2016-04-30-relative-thinking.html</guid>
  <pubDate>Sat, 30 Apr 2016 07:00:00 GMT</pubDate>
</item>
</channel>
</rss>
