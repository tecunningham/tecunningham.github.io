[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Knowledge-Creating LLMs\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2026\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nLLM verification\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2025\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nForecasts of AI & Economic Growth\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2025\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nEconomics and Transformative AI\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2025\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nOn Deriving Things\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2025\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nToo Much Good News is Bad News\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2024\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nPremature Optimization and the Valley of Confusion\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2024\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nPeer Effects, Culture, and Taxes\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2024\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nBloodhounds and Bulldogs\n\n\nOn Perception, Judgment, & Decision-Making\n\n\n\n\n\n\n\n\nApr 27, 2024\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nThe Influence of AI on Content Moderation and Communication\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2023\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nThe History of Automated Text Moderation\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2023\n\n\nIntegrity Institute collaborators: Alex Rosenblatt, Jeff Allen, Ejona Varangu, Dave Sullivan, Tom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nThinking About Tradeoffs? Draw an Ellipse\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\nTom Cunningham, OpenAI.\n\n\n\n\n\n\n\n\n\n\n\nExperiment Interpretation and Extrapolation\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nAn AI Which Imitates Humans Can Beat Humans\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nSushi-Roll Model of Online Media\n\n\nPreviously: “pizza model”, “salami model”\n\n\n\n\n\n\n\n\nSep 8, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n\n\n\n\n\nHow Much has Social Media affected Polarization?\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n\n\n\n\n\nThe Paradox of Small Effects\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n\n\n\n\n\nRanking by Engagement\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nSocial Media Suspensions of Prominent Accounts\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2023\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nOptimal Coronavirus Policy Should be Front-Loaded\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2020\n\n\n\n\n\n\n\n\n\n\n\nOn Unconscious Influences (Part 1)\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2017\n\n\n\n\n\n\n\n\n\n\n\nThe Work of Art in the Age of Mechanical Production\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2017\n\n\n\n\n\n\n\n\n\n\n\nRepulsion from the Prior\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2017\n\n\n\n\n\n\n\n\n\n\n\nThe Repeated Failure of Laws of Behaviour\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2017\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nEconomist Explorers\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2017\n\n\n\n\n\n\n\n\n\n\n\nSamuelson & Expected Utility\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2017\n\n\n\n\n\n\n\n\n\n\n\nWeber’s Law Doesn’t Imply Concave Representations or Concave Judgments\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2017\n\n\n\n\n\n\n\n\n\n\n\nRelative Thinking\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2016\n\n\nTom Cunningham\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html",
    "href": "posts/2023-01-31-social-media-suspensions-data.html",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "",
    "text": "Tom Cunningham. (@testingham) First version Jan 31 2023, data last updated April 2023, text updated July 2024.1\nThis note describes the suspension practices of the major social media platforms. I have collected a dataset of around 200 suspensions of prominent people across 12 platforms between 2011 and early 2023, stored in a google spreadsheet. The chart below summarizes the full dataset:\nThe data helps illuminate what platforms are doing. It is very difficult for an outside observer to see how a platform moderates their content. The advantages of studying the suspension of prominent users are that (1) the data is public and (2) the outcomes are comparable across platforms.\nKey findings.\nI am working on a separate essay about why platforms suspend users. It is difficult to give clear reasons why platforms suspend users. In a separate essay I try to break down how much their action can be attributed to influence from owners, from employees, from users, from advertisers, or from governments. Having this dataset of suspensions is very useful to be able to make generalizations about platform behavior."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#politicians",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#politicians",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Politicians",
    "text": "Politicians\nAmong US Federal politicians only Republicans have been suspended. In the US 8 Republicans have had one or more suspension, but no Democrats. Among the Republicans the suspensions were for a variety of reasons: related to the Jan 6 riots (Trump, Barry Moore, MTG), related to COVID (Ron Johnson, Rand Paul, MTG), for misgendering (Jim Banks), for tweeting a threat (Briscoe Cain), for animal blood on a profile photo (Steve Daines), one by a rogue employee (Trump).\nIt seems to me that the asymmetry in suspensions is primarily due to Republicans being more likely to violate the policies, rather than asymmetric enforcement of existing policies. I am not aware of any cases where a Democratic politician violated one of these policies but was not suspended.\n\n\n\n\n\n\n\n\n\nSuspension of national politicians outside the US has been relatively rare. My dataset contains 13 national politicians who were suspended in the world outside the US, compared to 8 in the US. This is a big asymmetry, and something of a puzzle. I have discussed this with a number of people who worked in enforcement and they attribute to a mixture of (1) less policy-violating behaviour from non-US politicians; (2) looser enforcement against non-US politicians; (3) lower overall social media usage outside the US; and (4) lower coverage of non-US politicians in my dataset."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#us-prominent-figures",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#us-prominent-figures",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "US Prominent Figures",
    "text": "US Prominent Figures\nThis shows all suspensions of US “notable people”:\n\n\n\n\n\n\n\n\n\nBetween 2015 and 2017 there were a series of alt-right personalities suspended from Twitter. The suspensions were often not for their views but their behaviour:\n\n2015: Charles Johnson from Twitter for a threat.\n2016: Milo Yiannopoulos from Twitter for harassment, Richard Spencer from Twitter for manipulation.\n2017: Roger Stone from Twitter for abuse.\n2018: Alex Jones from Twitter for incitement and abuse.\n\nBeginning in late 2017 more alt-right accounts were suspended. Either for hate speech, for offline behaviour, or without any public reason given:\n\nLate 2017: Baked Alaska from Twitter for hate speech.\n2018: Owen Benjamin from Twit with no reason given, Alex Jones from FB and YouTube for hate speech.\n2019: Nick Fuentes from Meta with no reason given.\n\nBetween November 2020 and January 2021 a large set of prominent figures were suspended for election-related reasons. The most suspensions were on Twitter but there were also from other platforms.\n\nSince November 2022 Twitter has unsuspended a large fraction of the suspended users that I track, probably around 1/2.\nSome people have been suspended simultaneously across multiple platforms (e.g. Alex Jones)"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#meta",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#meta",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Meta",
    "text": "Meta"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#twitter",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#twitter",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Twitter",
    "text": "Twitter\n\n\n\n\n\n\n\n\n\nThe following chart shows just accounts that were un-suspended under Musk, i.e. people with Twitter suspension that started before Oct 27 2022 and ended after that date. See below for a more fine-grained dataset of accounts unsuspended under Musk.\nYou can see that the primary original reasons for suspension were hate speech COVID misinformation. Kanye West and Nick Fuentes were re-suspended under Musk."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#youtube",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#youtube",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "YouTube",
    "text": "YouTube"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#tik-tok",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#tik-tok",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Tik Tok",
    "text": "Tik Tok"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Other Platforms",
    "text": "Other Platforms"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#hate-speech",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#hate-speech",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Hate Speech",
    "text": "Hate Speech"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#twitter-1",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#twitter-1",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Twitter",
    "text": "Twitter\nWikipedia page on Twitter Suspensions. Wikipedia has a list of around 400 Twitter suspensions. I chose not to create my own database (partly drawing from Wikipedia) for a few reasons: (1) I would want to add a lot of annotations to the Wikipedia data, e.g. about reasons for suspension or types of suspension. (2) Parsing the data is nontrivial: date ranges are given in various formats and would require some work on a regex to parse consistently. (3) There is some missing and inconsistent data, e.g. it has Trump’s suspension start-date but not end-date, and the names of people are not consistent (e.g. sometimes “Donald Trump”, sometimes “Donald J Trump”).\nThe Wikipedia dataset shows a similar basic pattern to what I document above: a dramatic increase in the rate of suspensions around mid-2017\n\n\n\n\n\nWikipedia-reported Twitter suspension by year\n\n\n\n\n\n\n\n\n\nAll Wikipedia-reported Twitter suspension, highlighting accounts with more than 1M followers (not all suspensions list the number of followers).\n\n\n\n\nTravis Brown: Twitter Watch This project appears to have data on almost all suspensions on Twitter since Feb 2022, and also tracks whether the suspension have been reversed. It does not include any suspensions which started prior to Feb 2022. There is a giant CSV file with 600K rows, suspensions.csv. Some visualizations:\n\n\n\n\n\nObservations by date of suspension\n\n\n\n\n\n\n\n\n\nObservation by date of unsuspension\n\n\n\n\n\n\n\n\n\nObservation by date of account creation\n\n\n\n\n\n\n\n\n\nSuspensions for accounts with &gt;1M followers\n\n\n\n\nTravis Brown: Twitter Unsuspensions. This is a collection of users who Twitter has un-suspended since Oct 27 2022 (when Musk took over). For some accounts there is a date of suspension but some have missing dates, I think suspension-date is only observed if after Feb 2022. (The content of this dataset is neither a subset nor a superset of the previous daatset). Unfortunately the dataset doesn’t have follower-count or twitter handle, so it’s not easy to join with other datasets or find the most prominent accounts.\n\n\n\n\n\nObservations by date of suspension\n\n\n\n\n\n\n\n\n\nObservations by date of unsuspension\n\n\n\n\nTravis Brown: Deleted Tweets / Suspended Accounts. This project scrapes profiles from the Wayback Machine, and seems to have a large set of accounts that were suspended with fairly long retention, I have not yet investigated further.\nTwitter Transparency Reports. This has data on the aggregate number of suspensions per half between July 2018 and Dec 2021. Note that the website is down but the CSV files can still be downloaded. \n\n\n\n\n\nTotal Accounts Suspended on Twitter by Reason, 2018H2-2021H2\n\n\n\n\nCounterHate list of unsuspensions. The organization CounterHate has a list of 10 large accounts reinstated by Twitter since Musk’s takeover. Note I believe they incorrectly listed Rizza Islam as an account re-activated by Twitter: I can find no evidence that the acccount @RizzaIslam was ever suspended, it seems to have been continuously tweeting from November 2022 through Feb 2023. I have added all 10 accounts to my database, and checked activity across all platforms."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#youtube-1",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#youtube-1",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "YouTube",
    "text": "YouTube\nWikipedia page on YouTube suspensions. See above for reasons why I chose not to use this dataset as the primary source.\nWikitubia: Terminated YouTubers. A list of around 2300 YouTubers that have been permanently banned, including date of ban, subscribers, reason for ban, and citation. They don’t have a date when unbanned."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#meta-facebook-instagram",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#meta-facebook-instagram",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Meta / Facebook / Instagram",
    "text": "Meta / Facebook / Instagram\nThere is no Wikipedia page of suspensions on Facebook, Instagram or WhatsApp.\nMeta’s “Community Standards Enforcement Report” is shown below. Meta’s data does not include any data on account suspensions, however there are a few other patterns of interest.\n\nContent actioned is relatively stable. There are fairly few notable upward or downward trends across the different types of content actioned: terrorism content actioned has increased significantly on both platforms, hate speech actions increased up to the end of 2020, then declined.\nThe proactive detection rate is close to 100% for most categories. there were dramatic improvements for bullying and for hate speech over 2017-2021. Note that the proactive detection rate is the share of actioned content that is automatically detected, the share of true positives that are automatically detected is surely much lower.\nThe prevalence of volations has fallen significantly. The log axis diminishes the magnitude of the decline: prevalence has fallen by a factor of 2-5 for nudity, bullying, hate speech, and graphic content. (I only show the prevalence upper bound, but the lower bound generally tracks the same course).\n\n\n\n\n\n\n\n\n\n\nFacebook’s dangerous organizations list. This list was leaked in 2021 by the Intercept. Unfortunately it does not include the dates of when each organization was added. The list is organized into the following categories:\n\nTerror Organizations (e.g. Islamic State)\nCrime Organizations (e.g. Bloods, Crips)\nHate Organizations (e.g. Aryan Nation, includes bands and websites)\nMilitarized Social Movements (e.g. United States Patrio Defense Force)\nViolent Non-State Actors (e.g. Free Syrian Army)\nHate (e.g. David Duke)\nIndividuals: Crime (e.g. Denton Suggs, Gangster Disciples)\nIndividuals: Terror (e.g. Osama bin Laden)"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#tiktok",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#tiktok",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "TikTok",
    "text": "TikTok\nCommunity Standards Report. Shows an increase in suspensions from around 1M accounts/quarter per 2020 to 6M accounts/quarter in 2023."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#twitch",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#twitch",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Twitch",
    "text": "Twitch\nStreamerBans. They seem to have a pretty comprehensive database of bans on Twitch."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms-1",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms-1",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Other Platforms",
    "text": "Other Platforms\n\nSpotify. The only unambiguous suspension from Spotify I found was Alex Jones’ podcast. Spotify removed some episodes of Joe Rogan’s podcast, and removed R Kelly and XXXtentacion’s music from playlists. They remove some white-supremacist artists and music. They removed all music from the band LostProphets after their lead singer was convicted of child sexual abuse.\nSubstack. I’m not aware of anybody who’s been kicked off Substack, they present themselves as very pro-free-speech.\nReddit. I’m not aware of any data on reddit account suspensions.\nRumble. The Rumble video-hosting platform has become quite large (they claim 70M MAU, and have a market cap of ). Their terms of service restrict content that is “abusive, inciting violence, harassing, harmful, hateful, anti-semitic, racist or threatening.” However I have not yet found a single example of a prominent user who has been suspended from Rumble."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#other-data-sources",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#other-data-sources",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Other Data Sources",
    "text": "Other Data Sources\nCan find more suspensions by searching Wikipedia for “suspended from XXX”. E.g. site:wikipedia.org \"suspended from facebook\". Possibly worth doing the same search for Google News.\nSocialBlade has data on number of followers by month since 2018, across Twitter, FB, YouTube. I’m not sure how easy it would be to scrape this data. They have a paid API, they say “up to 3 years of Historical statistics on creators.” However the website seems to have data back to at least April 2018.\nBallotpedia list of elected officials suspended from social media. It is an excellent resource, appears comprehensive and cites original reporting. I have added all of their data to the database as of January 2023.\nGlobal Internet Forum to Counter Terrorism (GIFCT). They mainly work on sharing hashes of terrorist content between platforms. They have some dicussion papers about “terror designation lists” but I don’t think they maintain any lists themselves.\nSpecially Designated National / Global Terrorist (SDN/SDGT). This is a public list maintained by the US government, and consumed by a number of tech companies. The full history is available, but it would be extremely difficult to parse.\nLumen. This has an international database of government takedown requests. They also seem to include whether the request was honored.\nCCDH Disinformation Dozen. This is a list from March 2021 of prominent accounts who were spreading anti-vax information on social media: original report, followup report from April 2021). They also have a “toxic ten” report. It’s probably worth adding both lists to the database."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#third-party-sources-on-platform-policies",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#third-party-sources-on-platform-policies",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Third-Party Sources on Platform Policies",
    "text": "Third-Party Sources on Platform Policies\nThere are a variety of third-party resources comparing policies across platforms, however none seem to have data comparable to the list above, i.e. a summary of specific content policy changes over time.\nComparisons at a single point in time.\n\nDNC (2020) Comparison of Misinformation Policies, 2020\nConsumer Reports (Aug 13 2020) Comparison of Misinformation Policies, 2020. Data as of 2020, with three levels: “allowed”, “sometimes”, and “prohibited”.\nUNC CITAP (May 22 2020) Comparison of Misinformation Policies, 2020. Has tables comparing misinfo policies as of 2020, three levels: “prohibited”, “flagged”, “allowed.”\nElection Integrity Partnership (Oct 28 2020) Comparison of Election Policies, 2020.\nCarnegie Endowment (April 1 2021) Existence of Policies, 2021. Table just marks whether a platform has a policy on some type of content, not nature of policy. They also they have a database of platform policies but it seems to only have data from February 2021.\nVirality Project Comparison of COVID Vaccine Policies in 2021.\n\nPolicies tracked over time.\n\nMchangama, Fanlo and Alkiviadou (2023) Scope Creep: An Assessment of 8 Social Media Platforms’ Hate Speech Policies. They document the hate speech policies over time for 8 platforms using a consistent rubric. They document that hate speech policies have become more broad-reaching over time. The data is available in Excel sheets here.\nKatie Harbath and Collier Fernenkes (August 2022) Election Policy Announcements, 2003-2022. Google spreadsheet with links to around 600 policy announcements, organized by platform, author, date, product-type, and country. Focussed on election-related policies, and they don’t include summaries of the policy announcement. They also wrote up analyses: (1) “A Brief History of Tech and Elections”; (2) 2022 election announcements.\nRanking Digital Rights Index, Comparison of Privacy and Transparency Policies, 2017-2022. They collect perhaps 100 different indicators across around 15 tech companies, mostly related to privacy and transparency, earliest data from 2017. All the data is available.\nGLAAD Comparison of LGBTQ user safety, 2021-2022\nCELE, Letra Chica. Tracks all public policy changes on Meta, YouTube, and Twitter. Most data from May 2020, but they go back to 2019 for Facebook by using FB’s Transparency Center. Each policy update includes a short summary of what’s changed. Tracks both Spanish and English versions. Data stored on coda.io, I think it’s queryable.\nLinterna Verdes, Circuito. Has about 15 in-depth case studies of platform moderation decisions.\nHumboldt Institute, Platform Governance Archive. Comprehensive archive of ToS, Privacy Policy, and Community Guidlines, from 2004 until late 2021, for FB, IG, Twitter, and YouTube. The data will not be updated.\nOpen Terms Archive. Started by the French Ambassador for Digital Affairs, but now a collaboration. Tracks terms for many different online services in a github repo. The Platform Governance Archive has moved to be part of this project, here.\nEFF, TOSback. Database of historical ToS documents from different services, with cross-platform comparisons. The most recent updates seem to be from May 2021, possibly was succeeded by Open Terms Archive.\nEuropean Commission, Copyright Content Moderation and Removal. This PDF report includes a lot of work which maps the copyright policies of major platforms.\n\nNarrative histories:\n\nCatherine Buni and Soraya Chemaly (2016, the Verge) History of Moderation.\nSarah Jeong (2016, Vice) The History of Twitter’s Rules\nBergen (2022) Like, Comment, Subscribe. A book on the history of YouTube, it has a lot of detail on policy changes."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-film-and-television",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-film-and-television",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Exclusions in Film and Television",
    "text": "Exclusions in Film and Television\n\n\n\n\n\n\n\n\n\n\n\nalleged crime\nresult\n\n\n\n\n1920s\nFatty Arbuckle\nrumors of immorality\nfilm industry blacklisted\n\n\n\n\n\n\n\n\n1940s\nOrson Welles\ncommunist associations\nblacklisted, moved to Switzerland\n\n\n\nDalton Trumbo\ncommunist associations\nblacklisted\n\n\n\n(around 100 people)\ncommunist associations\nblacklisted for a decade\n\n\n\n\n\n\n\n\n1950s\nCharlie Chaplin\ncommunist associations\nbanned from US\n\n\n\nElia Kazan\ntestifying before HUAC\nlost some relationships in Hollywood\n\n\n\n\n\n\n\n\n1960s\nJane Fonda\nopposition to Vietnam war\nblacklisted\n\n\n\n\n\n\n\n\n1970s\nRoman Polanski\nrape of 13yo girl\nmild disapproval from Hollywood\n\n\n\n\n\n\n\n\n1990s\nO J Simpson\nmurdered his wife\nblacklisted\n\n\n\nWoody Allen\nmolested 7yo daughter\n\n\n\n\n\n\n\n\n\n2000s\nMel Gibson\nracism & anti-semitism\n“blacklisted in Hollywood for almost a decade”\n\n\n\nMira Sorvino\nrejecting Harvey Weinstein\nblacklisted\n\n\n\nRose McGowan\nrejecting Harvey Weinstein\nblacklisted\n\n\n\nIsaiah Washington\nhomophobic remarks\nblacklisted\n\n\n\nMichael Richards\nracist remarks\nblacklisted\n\n\n\nKathy Griffin\n“told Jesus to suck it”\nbanned from talk shows and TV appearances\n\n\n\nSean Penn\nopposition to Iraq war\ndropped from movie\n\n\n\n\n\n\n\n\n2010s\nBill Cosby\nsexual assault\nblacklisted (& later imprisoned)\n\n\n\nHarvey Weinstein\nsexual assault\nblacklisted (& later imprisoned)\n\n\n\nStacy Dash\nconservative advocacy\nblacklisted\n\n\n\nKirk Camerson\ncriticism of homosexuality\nblacklisted\n\n\n\nJames Woods\nanti-Obama tweets\nblacklisted\n\n\n\nCeeLo Green\nsexual assault\nblacklisted\n\n\n\nLouis CK\nsexual harassment\nblacklisted\n\n\n\nKathy Griffin\nphoto with head of Trump\nfired by CNN, lost endorsement, cancelled tour\n\n\n\nT J Miller\nsubstance abuse, sexual assault\nblacklisted\n\n\n\nGina Carano\npolitical social media posts\nfired from TV show\n\n\n\nKevin Spacey\nsexual harassment\nlost roles in films\n\n\n\nJussie Smollett\nlied about an attack\nlost roles in TV shows\n\n\n\nNeil deGrasse Tyson\nrape, sexual harassment\ntemporarily lost roles in TV shows\n\n\n\nRoseanne Barr\nracist tweet\nlost TV show\n\n\n\n\n\n\n\n\n2020s\nWill Smith\nslapping someone at Oscars\nfilm projects put on hold\n\n\n\nJohnny Depp\ndomestic violence\nlost roles in films\n\n\n\nAmber Heard\ninvolvement in trial w Johnny Depp\nlost roles in films\n\n\n\nJustin Roiland\nsexual harassment & abuse\nlost roles in shows"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-music",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-music",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Exclusions in Music",
    "text": "Exclusions in Music\n\n\n\n\n\n\n\n\n\n\n\nalleged crime\nresult\n\n\n\n\n1940s\nPaul Robeson\ncommunist associations\nblacklist and passport revoked\n\n\n\n\n\n\n\n\n1950s\nLeonard Bernstein\ncommunist associations\nbrief blacklist\n\n\n\nLena Horne\ncommunist associations\nblacklist\n\n\n\nPete Seeger\ncommunist associations\nblacklist\n\n\n\n\n\n\n\n\n1960s\nBeatles\nsaying they’re bigger than Jesus\nconsumer boycott\n\n\n\nLovin Spoonful\ncooperating with FBI\nmusic industry boycott\n\n\n\nNina Simone\n“Mississippi Goddam”\nboycott in the South\n\n\n\nJohn Lennon\ncriticism of US and Vietnam war\nrefused entry into US\n\n\n\nEartha Kitt\ncriticism of Vietnam war\nblacklist through LBJ and CIA\n\n\n\n\n\n\n\n\n1970s\nSex Pistols\ncriticizing the Queen, swearing on TV\nbanned by the BBC, dropped by EMI\n\n\n\n\n\n\n\n\n1980s\nNWA\n“Fuck the Police” & similar songs\nradio station boycott, police boycott\n\n\n\n\n\n\n\n\n1990s\nBruce Springseen\nsong against police brutality\nbrief police boycott\n\n\n\nMarilyn Manson\ntransgressive lyrics\nbanned from performing in some states\n\n\n\nBody Count\nsong “cop killer”\nalbum withdrawn and reissued\n\n\n\n\n\n\n\n\n2000s\nDixie Chicks\nfor opposition to Iraq war\nblacklisting and consumer boycott\n\n\n\nJanet Jackson\nshowing nipple\nVH1, MTV, & Viacom radio stopped playing her music\n\n\n\nR Kelly\nsexual abuse\nbroad blacklist\n\n\n\nChris Brown\ndomestic violence\nweak boycott and blacklist\n\n\n\n\n\n\n\n\n2010s\nLostprophets\nsexual abuse\nbroad blacklist\n\n\n\nMichael Jackson\nchild molestation\nsome radio stations stop playing music\n\n\n\n\n\n\n\n\n2020s\nBeyonce\nsong against police brutality\nbrief police boycott\n\n\n\nMorgan Wallen\nusing n-word\ntemporarily dropped from radio/streaming playlists\n\n\n\nKanye West\npraise of Hitler\nlost sponsors\n\n\n\nOthers.\n\nIn radio: Father Coughlin, Rush Limbaugh, Don Imus fired from CBS for calling womens’ basketball team “nappy-headed hos”, Howard Stern fired from various radio shows for comments.\nIn sport. Colin Kapaernick blacklisted from NFL for kneeling for the anthem. Pete Rose banned from MLB for gambling.\nNazi sympathisers/collaborators. Charles Lindbergh, Henry Ford, Charles Coughlin, PG Wodehouse, Ezra Pound.\nWriters: DH Lawrence, Henry Miller, Salman Rushdie (Nicole Bonoff).\nJournalists. Jeffrey Toobin (New Yorker writer masturbated on zoom call),\nNote on R Kelly disappearing from radio"
  },
  {
    "objectID": "posts/2025-10-19-forecasts-of-AI-growth.html",
    "href": "posts/2025-10-19-forecasts-of-AI-growth.html",
    "title": "Forecasts of AI & Economic Growth",
    "section": "",
    "text": "Validation Checks\n\nOverall: ⚠️ Warning\n\n✅ [36/36] Cited sources exist in posts/ai.bib (programmatic)\n✅ [26/26] Table rows have required fields (programmatic)\n✅ [26/26] QMD quotes match posts/ai.bib (programmatic)\n✅ [26/26] QMD growth values match posts/ai.bib (programmatic)\n⚠️ [34/36] Abstracts present for all cited sources (programmatic)\n❌ [15/18] Bib quotes present in local fulltext version (programmatic)\n\nLast checked: 2026-02-22"
  },
  {
    "objectID": "posts/2025-10-19-forecasts-of-AI-growth.html#footnotes",
    "href": "posts/2025-10-19-forecasts-of-AI-growth.html#footnotes",
    "title": "Forecasts of AI & Economic Growth",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI classified Epoch’s GATE model (Erdil et al. (2025)) as by “AI people”, though the authors are a mixture of academic economists and people who work in AI.↩︎\nIt seems to me quite plausible that these papers over-estimate the productivity impact of existing LLMs: (1) the AB tests showing productivity improvements are on unrepresentatively self-contained tasks and are likely distorted by publication selection; (2) the Eloundou et al. (2023) estimates of very large time-savings from GPT-4 are based just on intuitions.↩︎\nComin and Mestieri (2014) say “the average adoption lag across all technologies (and countries) is 44 years,” but since the 1950s it has been 7-18 years.↩︎\n“Between 1 and 5% of all work hours are currently assisted by generative AI, and respondents report time savings equivalent to 1.4% of total work hours. … implies a potential productivity gain of 1.1%.”↩︎\nSuppose the total valuation of AI-related companies is $10T, which is perhaps around 10% of all capital stock. Using P/E of 15, a $10T valuation implies a stream of $600B in earnings/year, which is 2% of GDP.↩︎"
  },
  {
    "objectID": "posts/2026-01-29-knowledge-creating-llms.html",
    "href": "posts/2026-01-29-knowledge-creating-llms.html",
    "title": "Knowledge-Creating LLMs",
    "section": "",
    "text": "Thanks to Zoë Hitzig & Parker Whitfill, among others, for helpful comments."
  },
  {
    "objectID": "posts/2026-01-29-knowledge-creating-llms.html#footnotes",
    "href": "posts/2026-01-29-knowledge-creating-llms.html#footnotes",
    "title": "Knowledge-Creating LLMs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMany other technologies share knowledge – speaking, writing, printing, the internet – LLMs just continue this progression but further lower the costs of sharing.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Tom Cunningham",
    "section": "",
    "text": "Economics/AI Research, METR (ex-OpenAI)."
  },
  {
    "objectID": "about.html#recent-talks",
    "href": "about.html#recent-talks",
    "title": "About Tom Cunningham",
    "section": "Recent Talks",
    "text": "Recent Talks\n\n\n\nDec 11 2025\nBay Area Tech-Economics seminar\n\n\nDec 5-6 2025\nZurich Economics of AI Keynote\n\n\nNov 25 2025\nBank of England Methods Seminar\n\n\nOct 24 2025\nChicago Center for Applied AI seminar\n\n\nOct 23 2025\nYale Research in Motion seminar\n\n\nSep 15 2025\nStanford lab talk, Erik Brynjolfsson lab"
  },
  {
    "objectID": "about.html#blog",
    "href": "about.html#blog",
    "title": "About Tom Cunningham",
    "section": "Blog",
    "text": "Blog"
  },
  {
    "objectID": "about.html#resume",
    "href": "about.html#resume",
    "title": "About Tom Cunningham",
    "section": "Resume",
    "text": "Resume"
  },
  {
    "objectID": "about.html#working-papers",
    "href": "about.html#working-papers",
    "title": "About Tom Cunningham",
    "section": "Working Papers",
    "text": "Working Papers\n\n2025: How People use ChatGPT, with Aaron Chatterji, David J. Deming, Zoe Hitzig, Christopher Ong, Carl Yan Shan & Kevin Wadman.\n2023: Implicit Preferences, with Jon de Quidt.\n2019: Interpreting Experiments with Multiple Outcomes (presented at CODE 2019) with Josh Kim.\n2015: Biases and Implicit Knowledge\n2015: Equilibrium Persuasion with Ines Moreno de Barreda.\n2013: Comparisons and Choice\n2013: Relative Thinking and Markups"
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "About Tom Cunningham",
    "section": "Publications",
    "text": "Publications\n\n2025: Ranking by Engagement and Non-Engagement Signals: Learnings from Industry, with Sana Pandey, Leif Sigerson, Jonathan Stray, Jeff Allen, Bonnie Barrilleaux, Ravi Iyer, Mohit Kothari, Behnam Rezaei, Sanjay Kairam, Smitha Milli.\n2019: Improving Treatment Effect Estimators Through Experiment Splitting, WWW, with Dominic Coey.\n2013: The Incumbency Effects of Signalling  Economica, with Ines Moreno de Barreda, Francesco Caselli, and Massimo Morelli.\n2009: Leader Behaviour and the Natural Resource Curse Oxford Economic Papers, with Francesco Caselli."
  },
  {
    "objectID": "about.html#miscellaneous",
    "href": "about.html#miscellaneous",
    "title": "About Tom Cunningham",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\n2010: “LSE CEP 2010 Election Analysis: Macroeconomics and Public Finance” (with Ethan Ilzetski)\n\nComment pieces for the Guardian (2008/2009)\n\nDon’t worry about inflation\nThe great financial stitch-up\nThe rate cut wasn’t big enough\nStuck in the middle\nWe need to outgrow our debt\nYou say inflation, I say deflation\nConfidently predicting darker days"
  },
  {
    "objectID": "posts/2026-02-26-hallucinations-and-alignment.html",
    "href": "posts/2026-02-26-hallucinations-and-alignment.html",
    "title": "Hallucinations and Alignment",
    "section": "",
    "text": "Project spec (human)\n\n\n\n\n\n\nThis project is structured with spec (human) &gt; plan (LLM) &gt; document (LLM). You can write to the spec only if you get affirmative approval from the human author for the exact changes.\nOverall goal: a two-page blog post with some obsevations about hallucinations in language models. Then appendices to back up (1) data; (2) literature review; (3) derivations.\nBasic model:\n\nThe user has to make a choice between N options, they get \\(\\pi_s&gt;0\\) if they choose the right one, otherwise \\(\\pi_f&lt;0\\). But they can also abstain and get \\(\\pi_a=0\\).\nchoose between a few options (multi-choice), the LLM has probabilities on each answer.\nThe LLM will always return the most-likely option\n\nanswer (i.e. most likely option)\nanswer or abstain (if P(answer) is below some threshold)\nanswer with likelihood\n\nThe user has some outside option from not choosing, which is better than choosing the wrong option. Thus they will only choose the recommended option if the probability is sufficiently high\nExtension: the user can pay a cost to verify\nIn the appendix discuss a model where the user can pay a cost to (a) verify the LLM’s answer, or (b) find the right answer themselves.\n\nClaims:\n\nDiagram: show p on the x-axis, represents probability of the most-likely alternative (i.e. LLM’s beliefs).\nThe usefulness of a binary-output LLM to a user is convex in its avg accuracy, this means the value of benchmark scores is convex.\nIf you can abstain then the threshold for making a claim is p^*=(_a-_f)/(_s-_f).\nTraining with a reward only for accuracy encourages guessing over abstention.\nSimplex representation:\n\nWe can illustrate different user preferences over succeed/fail/abstain on a simplex: reward accuracy; punish failure; F1.\nWe can illustrate different empirical results: SimpleQA, Abstain-QA. Put the simplex in the body, numerical results in the appendix.\n\nNote that if\n\nAdditional notes\n\nWe will use these papers for terminology: (1) Wen et al (2025) “know your limits: a survey of abstention in large language models”; (2) Kalai et al. (2025) “Why Language Models Hallucinate”.\nRelated literature: start with a chronological list of related papers, Chow, Herbei and Wegkamp (you can mention there are other followsup on “classification with a reject option”), Kalai et al., Kadavath. Don’t need to mention conformal prediction or calibration & scoring.\nRelated literature: add a short “recent mechanisms” subsection on LLM-specific ways of implementing abstain/verify and confidence signals (e.g. refusal-aware tuning; explicit IDK tokens; verification loops; sampling-based or semantic-uncertainty detection). Include a short caveat that self-check/uncertainty can miss high-confidence hallucinations and can fail in some reasoning settings.\nRelated literature: one-line note that abstention/refusal is now being studied beyond factual QA, including math and coding benchmarks (e.g. Mohamadi et al. 2025; Jha et al. 2026; Dai et al. 2025; Oehri et al. 2025).\nDetailed discussion of Chow (1970) and Kalai et al. (2025), list their claims precisely.\nThe diagrams should be super clear. Make sure you look at the diagrams to see that they are readable.\nPlot data from different studies on simplex diagrams. Also give comments on the diagrams, on what the takewaay is about tradeoffs here, & see that’s consistent with what the original papers say.\nNote early on different terminology: “abstain”, “refuse”, “reject”, “IDK/i don’t know”, “forfeit”, “concede”, “fold” (others?)"
  },
  {
    "objectID": "posts/2026-02-26-hallucinations-and-alignment.html#introduction",
    "href": "posts/2026-02-26-hallucinations-and-alignment.html#introduction",
    "title": "Hallucinations and Alignment",
    "section": "Introduction",
    "text": "Introduction\nHallucinations are usually described as the model “making things up”. But in many applications, hallucinations are the predictable outcome of a mis-specified payoff function: the user cares about the tradeoff between being right, being wrong, and not answering (or escalating to verification), while most training and evaluation pipelines implicitly reward “answer something” much more than “know when to stop”.\nThis post treats question answering as a small decision problem with payoffs \\((\\pi_s,\\pi_f,\\pi_a)\\) for succeed / fail / abstain. That framing connects directly to Chow’s classic reject-option rule in pattern recognition (Chow 1970), to the Marschak-Machina probability simplex, and to recent results arguing that binary evaluation systematically pressures models to guess (Kalai et al. 2025)."
  },
  {
    "objectID": "posts/2026-02-26-hallucinations-and-alignment.html#basic-model",
    "href": "posts/2026-02-26-hallucinations-and-alignment.html#basic-model",
    "title": "Hallucinations and Alignment",
    "section": "Basic model",
    "text": "Basic model\nConsider a multiple-choice question with \\(k\\) options and a hidden correct answer \\(y^\\star\\). The model observes an input \\(x\\) and has a posterior distribution \\(p(y\\mid x)\\) over options.\nThe user can take one of three actions:\n\nAnswer: pick an option \\(\\hat y\\).\nAbstain: do not answer (or defer to a safer outside option).\nVerify (optional extension): pay a cost \\(c\\) to obtain the correct answer by some other means.\n\nFor now, summarize payoffs as constants:\n\nSucceed (pick \\(\\hat y=y^\\star\\)): payoff \\(\\pi_s\\).\nFail (pick \\(\\hat y\\neq y^\\star\\)): payoff \\(\\pi_f\\).\nAbstain (outside option): payoff \\(\\pi_a\\).\n\nAssume \\(\\pi_s&gt;\\pi_a&gt;\\pi_f\\): being right is best; abstaining is better than being wrong.\n\nOutputs: binary, ternary, continuous\nWe can distinguish LLM “answer formats” by how much information they expose about \\(p(y\\mid x)\\):\n\nBinary: the model returns a single recommended option \\(\\hat y\\).\nTernary: the model either returns \\(\\hat y\\) or abstains.\nContinuous: the model returns (an approximation to) the full distribution \\(p(y\\mid x)\\) over options."
  },
  {
    "objectID": "posts/2026-02-26-hallucinations-and-alignment.html#claims",
    "href": "posts/2026-02-26-hallucinations-and-alignment.html#claims",
    "title": "Hallucinations and Alignment",
    "section": "Claims",
    "text": "Claims\n\nA single threshold organizes abstention. If an “attempt” succeeds with probability \\(p\\) and fails with probability \\(1-p\\), then attempting has expected payoff \\[\n\\mathrm{E}[\\pi\\mid \\text{attempt}] = p\\,\\pi_s + (1-p)\\,\\pi_f.\n\\] Attempting is optimal iff \\(\\mathrm{E}[\\pi\\mid \\text{attempt}] \\ge \\pi_a\\), i.e. \\[\np \\ge p^* \\equiv \\frac{\\pi_a-\\pi_f}{\\pi_s-\\pi_f}.\n\\]\nBinary usefulness is convex in average accuracy. A user with an outside option chooses to rely on a binary-output model only when \\(p\\ge p^*\\). So the user’s value is the max of an outside option and a linear function of \\(p\\), which is convex.\nContinuous output is (weakly) best. If the model provides \\(p(y\\mid x)\\), the user can compute the expected payoff of each action (answer/abstain/verify) and implement the optimal policy. Any coarser output (binary or ternary) throws away information, and cannot improve expected utility.\nThe simplex makes preferences and objectives visible. In the Marschak-Machina simplex over \\((p_s,p_f,p_a)\\), user preferences correspond to indifference lines whose slope is determined by payoff ratios; training/evaluation objectives correspond to different directions in the same triangle.\nHallucinations are a consequence of the objective. Accuracy-only training rewards and binary grading make abstention suboptimal and encourage guessing. Chow (1970) derives the optimal reject rule in this exact payoff model (Chow 1970); Kalai et al. (2025) argue modern LLM pipelines effectively ignore this reject option and therefore pressure models to hallucinate (Kalai et al. 2025)."
  },
  {
    "objectID": "posts/2026-02-26-hallucinations-and-alignment.html#from-probabilities-to-actions",
    "href": "posts/2026-02-26-hallucinations-and-alignment.html#from-probabilities-to-actions",
    "title": "Hallucinations and Alignment",
    "section": "From probabilities to actions",
    "text": "From probabilities to actions\nFor a multiple-choice question, suppose the user answers by choosing the MAP option \\(\\hat y(x)=\\arg\\max_y p(y\\mid x)\\). Under the symmetric payoff model above (only “correct vs incorrect” matters), the probability of success from attempting is \\[\np_{\\max}(x)\\equiv \\max_y p(y\\mid x).\n\\]\nSo the attempt-vs-abstain decision is driven by a single number: answer iff \\(p_{\\max}(x)\\ge p^*\\).\nIf verification is available at cost \\(c\\), one simple version is: verifying yields certain success with payoff \\(\\pi_s-c\\). Then for each question the user compares three quantities:\n\nAttempt: \\(p_{\\max}(x)\\,\\pi_s + (1-p_{\\max}(x))\\,\\pi_f\\).\nAbstain: \\(\\pi_a\\).\nVerify: \\(\\pi_s-c\\).\n\nThis makes the “payoff mis-specification” point concrete: evaluation regimes that treat abstention as failure implicitly set \\(\\pi_a\\approx \\pi_f\\), eliminating the region where “don’t answer” is optimal.\n\nConvex value of a binary-output model\nIn the most constrained interface, a binary-output model only returns \\(\\hat y\\) and the user cannot condition on per-question confidence (they only know the model’s average accuracy \\(p\\) on the relevant distribution). Then the user’s best policy is either to follow the model or to abstain, and the resulting value is \\[\nV_{\\text{binary}}(p)=\\max\\Bigl\\{\\pi_a,\\; p\\,\\pi_s + (1-p)\\,\\pi_f\\Bigr\\}.\n\\]\nThis is the maximum of two affine functions of \\(p\\), so it is convex. It has a kink at the threshold \\(p=p^*\\): below the threshold the user abstains and additional accuracy has (locally) zero value; above the threshold, value increases linearly with accuracy. This is one reason “small accuracy gains” can feel useless until a system crosses a reliability threshold.\n\n\nWhy continuous output is (weakly) best\nContinuous output (the distribution \\(p(y\\mid x)\\), or any sufficiently rich summary like \\((\\hat y,p_{\\max})\\)) lets the user implement the payoff-optimal policy question-by-question: answer only when it clears their \\(p^*\\), abstain otherwise, and (if available) trigger verification in the middle region.\nBinary and ternary outputs are strict coarsenings of the posterior: they discard information about confidence. By a standard “more information cannot hurt” argument (Blackwell ordering), a user who observes a more informative signal can always simulate a less informative one by ignoring information, but not vice versa (Blackwell 1953). So a continuous interface is weakly better than any binary/ternary interface for any fixed payoff function."
  },
  {
    "objectID": "posts/2026-02-26-hallucinations-and-alignment.html#probability-payoff-diagram",
    "href": "posts/2026-02-26-hallucinations-and-alignment.html#probability-payoff-diagram",
    "title": "Hallucinations and Alignment",
    "section": "Probability-Payoff Diagram",
    "text": "Probability-Payoff Diagram\nThis figure visualizes the threshold rule: as confidence \\(p\\) rises, the expected payoff of attempting rises linearly from \\(\\pi_f\\) (when \\(p=0\\)) to \\(\\pi_s\\) (when \\(p=1\\)). Abstaining yields the flat payoff \\(\\pi_a\\). The optimal policy is to attempt when the blue line crosses the abstain line.\n\n\n\n\n\n\n\n\n\nIn the normalized example shown in the diagram, \\((\\pi_s,\\pi_a,\\pi_f)=(2,0,-2)\\), so \\[\np^*=\\frac{0-(-2)}{2-(-2)}=\\tfrac{1}{2}.\n\\] So in this example, the right behavior is “answer only when you’re at least 50% confident.”\nAppendix Verification option shows how the threshold changes if the user can pay a cost to verify."
  },
  {
    "objectID": "posts/2026-02-26-hallucinations-and-alignment.html#simplex-representation",
    "href": "posts/2026-02-26-hallucinations-and-alignment.html#simplex-representation",
    "title": "Hallucinations and Alignment",
    "section": "Simplex Representation",
    "text": "Simplex Representation\nThe probability simplex has three vertices corresponding to the three pure outcomes: certain success (\\(p_s=1\\)), certain failure (\\(p_f=1\\)), and certain abstention (\\(p_a=1\\)). Any lottery over outcomes is a point in this triangle.\nDifferent training/evaluation objectives induce different indifference-curve sets over \\((p_s,p_f,p_a)\\). The figure below contrasts three illustrative objectives.\n\n\n\n\n\n\n\n\n\nEach point in the simplex is a lottery over outcomes: a model might succeed with probability \\(p_s\\), fail with probability \\(p_f\\), and abstain with probability \\(p_a\\). The panels show three different objective families:\n\nAccuracy-only (\\(U=p_s\\)): success is rewarded, but failure and abstention are treated the same. This creates pressure to guess rather than abstain.\nPenalize failure (linear expected utility): failure is explicitly penalized relative to abstention, expanding the region where abstaining is optimal.\nF1 (a non-linear metric): indifference curves bend, reflecting that the metric itself builds in a particular tradeoff between attempting and being correct.\n\n\nIndifference-curve slope derivation\nLet \\(p_s, p_f, p_a = 1-p_s-p_f\\) denote the probabilities of succeed, fail, and abstain. Expected utility is\n\\[\nU = \\pi_s\\, p_s + \\pi_f\\, p_f + \\pi_a\\, p_a.\n\\]\n\nSubstitute \\(p_a = 1 - p_s - p_f\\): \\[\n\\begin{aligned}\nU\n&= \\pi_a \\\\\n&\\quad+ (\\pi_s-\\pi_a)\\,p_s \\\\\n&\\quad+ (\\pi_f-\\pi_a)\\,p_f.\n\\end{aligned}\n\\]\nHold \\(U = \\bar U\\) and solve for \\(p_f\\): \\[\n\\begin{aligned}\np_f\n&= \\frac{\\bar U - \\pi_a}{\\pi_f-\\pi_a}\n- \\frac{\\pi_s-\\pi_a}{\\pi_f-\\pi_a}\\,p_s.\n\\end{aligned}\n\\]\nThe slope of the indifference curve in the \\((p_s, p_f)\\) plane is therefore: \\[\n\\frac{dp_f}{dp_s}\\bigg|_{U=\\bar U}\n= -\\frac{\\pi_s-\\pi_a}{\\pi_f-\\pi_a}.\n\\]\nNormalizing \\(\\pi_a=0\\), this simplifies to: \\[\n\\frac{dp_f}{dp_s}\\bigg|_{U=\\bar U}\n= -\\frac{\\pi_s}{\\pi_f},\n\\qquad\np_f = \\frac{\\bar U}{\\pi_f} - \\frac{\\pi_s}{\\pi_f}\\,p_s.\n\\]\n\nThe slope depends only on payoff differences relative to abstain. When failure is very costly (\\(|\\pi_f|\\) large after normalizing \\(\\pi_a=0\\)), the curves are flatter: the decision-maker tolerates little additional failure probability in exchange for more success probability."
  },
  {
    "objectID": "posts/2026-02-26-hallucinations-and-alignment.html#related-literature",
    "href": "posts/2026-02-26-hallucinations-and-alignment.html#related-literature",
    "title": "Hallucinations and Alignment",
    "section": "Related Literature",
    "text": "Related Literature\n\nChronological sketch\n\nChow (1970): introduces the Bayes-optimal reject option (“indecision”) and derives the posterior-threshold rule.\nHerbei and Wegkamp (2006): formalizes “classification with a reject option” in modern statistical learning terms; there are many followups on learning and using reject policies (e.g. (Bartlett and Wegkamp 2008; El-Yaniv and Wiener 2010; Geifman and El-Yaniv 2019)).\nKadavath et al. (2022): finds that language models can often estimate whether their own answers are correct, which is exactly the signal needed to implement a threshold rule in practice.\nRecent LLM work tries to implement the missing abstain/verify channel via refusal-aware tuning and explicit “IDK” tokens (Zhang et al. 2024; Cohen et al. 2024), verification loops (Dhuliawala et al. 2023; Altinisik et al. 2026), and black-box uncertainty proxies like sampling-based checks or semantic uncertainty (Manakul, Liusie, and Gales 2023; Farquhar et al. 2024).\nKalai et al. (2025): argues LLM hallucinations are a predictable consequence of binary grading that penalizes abstention; proposes a scoring rule that makes abstaining optimal below a stated confidence threshold.\n\n\n\nChow (1970): Optimal reject rules\nChow (1970) introduces the reject option (which he calls the “Indecision class” \\(I\\)) into pattern recognition and derives the optimal error-reject tradeoff. Chow’s terminology maps directly onto ours:\n\n\n\nChow’s term\nOur term\n\n\n\n\ncorrect recognition\nsucceed\n\n\nerror (misclassification)\nfail\n\n\nrejection / indecision\nabstain\n\n\n\nChow’s setup adds a third action (reject) to ordinary classification. In one common normalization, the costs are \\(0\\) for a correct classification, \\(1\\) for an error, and \\(t\\) for a rejection.\nThe key result (“Chow’s rule”) is a posterior-threshold rule: accept and classify when confident enough, otherwise reject: \\[\n\\max_i P(G_i \\mid x) \\ge 1-t\n\\quad\\Rightarrow\\quad \\text{accept;}\n\\qquad\n\\max_i P(G_i \\mid x) &lt; 1-t\n\\quad\\Rightarrow\\quad \\text{reject.}\n\\]\nIt is optimal in the sense that, for a given rejection threshold (equivalently, a given reject rate), no other rule achieves a lower error rate.\nThe threshold \\(t\\) is related to the costs of the three outcomes as \\(t = (C_r - C_c)/(C_e - C_c)\\), where \\(C_e, C_r, C_c\\) are the costs of error, rejection, and correct recognition. In our payoff notation \\((\\pi_s,\\pi_f,\\pi_a)\\), Chow’s rule becomes: predict iff\n\\[\n\\max_y P(y \\mid x) \\ge \\frac{\\pi_a - \\pi_f}{\\pi_s - \\pi_f}.\n\\]\nIn the Marschak-Machina triangle, this threshold corresponds to one of the indifference lines: the boundary between the region where prediction is preferred and the region where abstention is preferred.\n\n\nHerbei and Wegkamp (2006) and followups: Learning a reject option\nHerbei and Wegkamp (2006) (and a large followup literature) reframes the reject option as a learning problem: you want a predictor that is accurate on the examples it attempts, while explicitly controlling how often it refuses.\nFor this post, the key takeaway is less about any one algorithm and more about the framing: “abstention is a first-class action” is standard in the statistical decision-theory literature, and the same expected-utility thresholds show up once you make the third outcome explicit.\n\n\nKadavath et al. (2022): Models can sometimes score their own answers\nIf you want the simple \\(p^*\\) rule to be usable, you need some per-question measure of correctness probability (like \\(p_{\\max}(x)\\) or a direct \\(P(\\text{correct}\\mid x)\\) proxy).\nKadavath et al. (2022) study this in language models, finding that they can often estimate whether their own answers are correct. That makes “give the user a probability” a practical interface choice, not just a theoretical one.\n\n\nKalai, Nachum, Vempala, and Zhang (2025): Why language models hallucinate\nKalai et al. (2025) argue that hallucinations are not a mysterious glitch but a predictable consequence of how models are trained and evaluated. Their central thesis is that the three-outcome structure (succeed, fail, abstain) is systematically distorted by binary evaluation:\nThe paper makes two distinct arguments:\n1. Pretraining origin. Even with error-free training data, the statistical objective of pretraining produces hallucinations. The authors reduce the problem to binary classification (“Is-It-Valid”), showing that\n\\[\n\\text{generative error rate} \\gtrsim 2 \\cdot \\text{IIV misclassification rate}.\n\\]\nFor arbitrary facts (like someone’s birthday) where there is no learnable pattern, the hallucination rate after pretraining is at least the fraction of facts appearing exactly once in the training data.\n2. Post-training persistence. Even after RLHF and other interventions, hallucinations persist because nearly all evaluation benchmarks use binary grading that penalizes abstention:\nThe fix they propose is exactly the payoff structure from our Marschak-Machina framework: penalize errors more than abstentions, with an explicit confidence threshold \\(t\\) stated in the prompt. Their proposed scoring rule awards \\(+1\\) for a correct answer, \\(-t/(1-t)\\) for an incorrect answer, and \\(0\\) for abstaining—so that answering is optimal iff confidence exceeds \\(t\\). This is Chow’s reject-option rule rediscovered in the LLM evaluation context.\n\n\nRecent mechanisms: refusal, uncertainty signals, and verification\nOur model is deliberately abstract: it assumes the user has access to (i) a usable confidence signal \\(p\\) and (ii) an abstain / verify channel. A lot of recent work can be read as ways of engineering those two ingredients:\n\nMake abstention an explicit output. Refusal-aware fine-tuning can teach a model to say “I don’t know” on out-of-knowledge questions (Zhang et al. 2024). Similarly, adding an explicit uncertainty token and training it to soak up probability mass on incorrect predictions effectively adds an abstain action to the model’s output space (Cohen et al. 2024).\nPay a cost to verify. Inference-time verification loops like Chain-of-Verification (CoVe) can be viewed as “spend extra tokens/compute to reduce \\(p_f\\)” (Dhuliawala et al. 2023). Very recent work trains this kind of behavior directly, with structured self-verification traces and an explicit final decision to answer vs abstain (Altinisik et al. 2026).\nConstruct a confidence signal without logits. When output probabilities are unavailable (or untrustworthy), disagreement across samples can act as a proxy confidence signal. SelfCheckGPT does this with sampling-based consistency checks (Manakul, Liusie, and Gales 2023); semantic-uncertainty methods like semantic entropy similarly use semantic variability across generations to predict and filter confabulations (Farquhar et al. 2024), and followup work proposes cheaper “semantic entropy probes” in the same spirit (Kossen et al. 2024).\n\nA cautionary note: neither uncertainty proxies nor “self-verification” are automatically reliable. Some hallucinations happen with high confidence (so uncertainty-based filters can miss them) (Simhi et al. 2025), and in logical reasoning settings models can struggle to identify their own errors (so internal self-checks can fail without external grounding) (Hong et al. 2024).\nBeyond factual QA, Mohamadi, Wang, and Li (2025) show on GSM8K/MedQA/GPQA that replacing binary RLVR rewards with a ternary scheme \\((+1,0,-\\lambda)\\) produces controllable answer-vs-abstain tradeoffs and useful abstention-aware cascades. Jha et al. (2026) report on MedMCQA and Hendrycks Math that moderate abstention rewards reduce wrong answers without collapsing coverage, especially when paired with supervised abstention training. In code generation, Dai et al. (2025) frame the task as “find a correct program or abstain” and use semantic triangulation to improve abstention decisions on LiveCodeBench/CodeElo. Complementarily, Oehri et al. (2025) fuse multiple uncertainty signals into calibrated correctness probabilities and enforce user-specified risk budgets via refusal, including experiments on code generation with execution tests."
  },
  {
    "objectID": "posts/2026-02-26-hallucinations-and-alignment.html#implications-for-alignment",
    "href": "posts/2026-02-26-hallucinations-and-alignment.html#implications-for-alignment",
    "title": "Hallucinations and Alignment",
    "section": "Implications for “alignment”",
    "text": "Implications for “alignment”\nIf you view hallucinations through this lens, “alignment” is not a mysterious property of model internals. It is the much more mundane question: does the model’s training and evaluation objective implement the user’s payoff function?\nPractical implications:\n\nExpose confidence. If the user cannot observe confidence, they cannot implement the threshold rule; binary outputs force guessing.\nScore abstention explicitly. Benchmarks that collapse abstain into “wrong” implicitly set \\(\\pi_a=\\pi_f\\) and will select for guessing.\nPrefer probabilities or verification hooks. Either give users a usable \\(p_{\\max}\\), or route medium-confidence cases to a verification workflow (a second pass, a tool call, or a structured self-check) (Dhuliawala et al. 2023; Farquhar et al. 2024)."
  },
  {
    "objectID": "posts/2026-02-26-hallucinations-and-alignment.html#appendix-verification",
    "href": "posts/2026-02-26-hallucinations-and-alignment.html#appendix-verification",
    "title": "Hallucinations and Alignment",
    "section": "Appendix: Verification option",
    "text": "Appendix: Verification option\nSuppose the user has a way to pay a cost \\(c\\) to obtain the correct answer (e.g. look it up, run an expensive check, ask a human). In the simplest model, verification yields certain success with payoff \\(\\pi_s-c\\).\nSince abstaining and verifying are both “outside options” (their payoff does not depend on the model’s confidence), the only relevant outside-option payoff is \\[\n\\pi_{\\text{outside}}=\\max\\{\\pi_a,\\;\\pi_s-c\\}.\n\\]\nThe attempt rule is the same threshold logic as before: attempt iff \\[\np \\ge \\frac{\\pi_{\\text{outside}}-\\pi_f}{\\pi_s-\\pi_f}.\n\\]\nIn the example shown below, \\((\\pi_s,\\pi_a,\\pi_f)=(2,0,-2)\\) and \\(\\pi_s-c=1\\), so \\[\np^*_{\\mathrm{verify}}=\\frac{1-(-2)}{2-(-2)}=\\tfrac{3}{4}.\n\\]\nOperationally, “verify” can mean many things: a web lookup, a separate fact-checking model, retrieval + citation, or even a structured self-checking loop that spends extra tokens before committing to an answer (Dhuliawala et al. 2023; Altinisik et al. 2026)."
  },
  {
    "objectID": "posts/2026-02-26-hallucinations-and-alignment.html#appendix-cross-benchmark-outcome-table",
    "href": "posts/2026-02-26-hallucinations-and-alignment.html#appendix-cross-benchmark-outcome-table",
    "title": "Hallucinations and Alignment",
    "section": "Appendix: Cross-Benchmark Outcome Table",
    "text": "Appendix: Cross-Benchmark Outcome Table\nTo make cross-model plotting easier, the table below standardizes outputs from multiple benchmarks into a common schema.\n\n\n\nbenchmark\nmodel\np_s_pct\np_f_pct\np_a_pct\n\n\n\n\nSimpleQA\nClaude-3-haiku (2024-03-07)\n5.1\n19.6\n75.3\n\n\nSimpleQA\nClaude-3-sonnet (2024-02-29)\n5.7\n19.3\n75.0\n\n\nSimpleQA\nClaude-3-opus (2024-02-29)\n23.5\n36.9\n39.6\n\n\nSimpleQA\nClaude-3.5-sonnet (2024-06-20)\n28.9\n36.1\n35.0\n\n\nSimpleQA\nGPT-4o-mini\n8.6\n90.5\n0.9\n\n\nSimpleQA\nGPT-4o\n38.2\n60.8\n1.0\n\n\nSimpleQA\nOpenAI o1-mini\n8.1\n63.4\n28.5\n\n\nSimpleQA\nOpenAI o1-preview\n42.7\n48.1\n9.2\n\n\nAbstain-QA\nGPT-4 Turbo\n66.1\n19.7\n14.2\n\n\nAbstain-QA\nGPT-4 32K\n72.0\n19.1\n8.9\n\n\nAbstain-QA\nGPT-3.5 Turbo\n61.1\n37.4\n1.5\n\n\nAbstain-QA\nMixtral 8x7b\n54.1\n37.0\n8.9\n\n\nAbstain-QA\nMixtral 8x22b\n59.0\n29.1\n11.9\n\n\n\nNotes: - Table columns are constructed to look like probabilities in the \\((p_s,p_f,p_a)\\) simplex: \\(p_s=\\text{p\\_s\\_pct}/100\\), \\(p_f=\\text{p\\_f\\_pct}/100\\), \\(p_a=\\text{p\\_a\\_pct}/100\\). - For SimpleQA, these correspond directly to {Correct, Incorrect, Not attempted} shares. - For Abstain-QA, these are constructed from the paper’s summary metrics; interpret them as a mapping into a common coordinate system, not as identical underlying evaluation protocols.\n\nData extraction details by source\nSimpleQA (Wei et al., 2024) (Wei et al. 2024). The table uses all model rows shown in the main SimpleQA model-comparison table (8 models). Here, p_s_pct is Correct, p_a_pct is Not attempted, and p_f_pct is computed as \\(100-\\text{Correct}-\\text{Not attempted}\\). I chose this slice because it is the paper’s canonical cross-model summary and directly exposes explicit non-attempt behavior.\nAbstain-QA (Madhusudhan et al., 2024) (Madhusudhan et al. 2024). This is a deliberate subset, not all values in the paper: I take the MMLU / Standard clause / Base rows (5 models) from the main result table. The paper reports AAC (answerable accuracy) and AR (abstention rate). I set \\(p_{a,\\%}=\\text{AR}\\) and construct \\(p_{s,\\%}\\) and \\(p_{f,\\%}\\) by treating AAC as attempt-conditional accuracy: \\[\np_s = \\text{AAC}\\cdot(1-p_a),\\qquad\np_f = (1-\\text{AAC})\\cdot(1-p_a),\n\\] all expressed in percent.\nImportant comparability caveats. Even after mapping all results into \\((p_s,p_f,p_a)\\) coordinates, the underlying tasks and abstention protocols differ: SimpleQA is short-form factual QA with optional non-attempts, while Abstain-QA is multiple-choice QA with an explicit IDK/NOTA option. So the combined table is useful for geometric intuition and directional comparisons, but not for strict leaderboard ranking across benchmarks.\nSources: Wei et al. (2024); Madhusudhan et al. (2024)."
  },
  {
    "objectID": "posts/2026-02-26-hallucinations-and-alignment.html#appendix-benchmark-points-in-the-simplex",
    "href": "posts/2026-02-26-hallucinations-and-alignment.html#appendix-benchmark-points-in-the-simplex",
    "title": "Hallucinations and Alignment",
    "section": "Appendix: Benchmark Points in the Simplex",
    "text": "Appendix: Benchmark Points in the Simplex\nThe next two figures plot benchmark observations as points on a Marschak-Machina simplex using a common transformation from the table columns:\n\\[\np_s = \\text{p\\_s\\_pct}/100,\\qquad\np_f = \\text{p\\_f\\_pct}/100,\\qquad\np_a = \\text{p\\_a\\_pct}/100.\n\\]\n\nSimpleQA\n\n\n\n\n\n\n\n\n\n\n\nAbstain-QA\n\n\n\n\n\n\n\n\n\n\n\nReading the simplex plots\n\nThe horizontal axis is \\(p_s\\) (succeed share) and the vertical axis is \\(p_f\\) (fail share). The remaining probability is \\(p_a=1-p_s-p_f\\) (abstain share), so points closer to the diagonal edge have lower abstention.\nMoving down (lower \\(p_f\\)) corresponds to reducing failures; whether that is best depends on how much worse failure is than abstention (\\(\\pi_f\\) vs \\(\\pi_a\\)).\nThe labeled points illustrate that a model can look good under an “answer-everything” regime by pushing \\(p_a\\) toward zero, but that is exactly the regime that a payoff function with a harsh \\(\\pi_f\\) would discourage.\nDon’t over-interpret cross-benchmark comparisons: each source defines abstention differently, so these plots are best read as a geometric visualization of tradeoffs, not as a single unified leaderboard."
  }
]