[
  {
    "objectID": "posts/2017-09-27-work-of-art-age-mechnical-production.html",
    "href": "posts/2017-09-27-work-of-art-age-mechnical-production.html",
    "title": "The Work of Art in the Age of Mechanical Production",
    "section": "",
    "text": "When I heard about the neural nets that copy the styles of famous painters I thought it would be the same old junk.\nAcademics have been saying forever that they were on the verge of discovering the principles of aesthetics, and that they would soon be able to automate the production of beauty – melody, harmony, proportion, plot.\nWhen I was a kid I was excited to read about this sort of thing. But they always turn out to be fatuous, catastrophically oversimplified and overconfident, written - I’m guessing - by people who are intimidated & resentful of the culture around them. Our technical understanding of what makes something look good is still weak, and I don’t think it’s improving very fast. I learned to, when I come across an article about art written by a scientist, turn the page.\nBut now I think that maybe the automatic production of beauty will arrive soon. The machine learning algorithms work by extrapolating from existing examples, which means that they can produce new examples that fit some pattern (such as the pattern of beauty) without anyone involved having any explicit understanding of what the pattern is or how it can be defined.\nThis extrapolation without understanding is what happened in the study of visual perception – i.e. making inferences from images. Our understanding of perception is slowly moving forward, as it has been for centuries, but our ability to automate perception has shot ahead. In the 15th century Leonard da Vinci studied how the light reflected by an object is related to its distance – more distant objects tend to be bluer – these are relationships that we all know unconsciously, but which take a lot of work to dig out, such that we consciously understand them. Psychologists and computer scientists are still discovering things about the physics of light which we all know unconsciously. But computer models which incorporate our explicit knowledge of the physics of light are being thrashed by pure machine-learning models, which are fed a huge databases of pictures and simply extrapolate from what they’ve already seen.[2]\nI think the same basic point is true of aesthetic things. We really struggle trying to explain why we like a picture or dislike a melody, because most of the work is done at an unconscious level. The progress in understanding those principles will probably continue to be slow.\nBut now it seems likely to me that, before long, machines will be able to do all these things on demand – play some brand new Mozart, make elegant little drawings of animals, write a pretty good pop song. And the programmer who implements them could be – probably will be – some bozo who has no clue why it works.\n\n[1] 2017-05: SIGGRAPH video with style transfer - https://www.youtube.com/watch?v=HYhzZ-Abku8\n[2] 2017-05: Michael Elad “Deep, Deep Trouble: Deep Learning’s Impact on Image Processing, Mathematics, and Humanity” https://sinews.siam.org/Details-Page/deep-deep-trouble-4"
  },
  {
    "objectID": "posts/2017-09-27-work-of-art-age-mechnical-production.html#aka-machine-learning-aesthetics-the-unconscious",
    "href": "posts/2017-09-27-work-of-art-age-mechnical-production.html#aka-machine-learning-aesthetics-the-unconscious",
    "title": "The Work of Art in the Age of Mechanical Production",
    "section": "",
    "text": "When I heard about the neural nets that copy the styles of famous painters I thought it would be the same old junk.\nAcademics have been saying forever that they were on the verge of discovering the principles of aesthetics, and that they would soon be able to automate the production of beauty – melody, harmony, proportion, plot.\nWhen I was a kid I was excited to read about this sort of thing. But they always turn out to be fatuous, catastrophically oversimplified and overconfident, written - I’m guessing - by people who are intimidated & resentful of the culture around them. Our technical understanding of what makes something look good is still weak, and I don’t think it’s improving very fast. I learned to, when I come across an article about art written by a scientist, turn the page.\nBut now I think that maybe the automatic production of beauty will arrive soon. The machine learning algorithms work by extrapolating from existing examples, which means that they can produce new examples that fit some pattern (such as the pattern of beauty) without anyone involved having any explicit understanding of what the pattern is or how it can be defined.\nThis extrapolation without understanding is what happened in the study of visual perception – i.e. making inferences from images. Our understanding of perception is slowly moving forward, as it has been for centuries, but our ability to automate perception has shot ahead. In the 15th century Leonard da Vinci studied how the light reflected by an object is related to its distance – more distant objects tend to be bluer – these are relationships that we all know unconsciously, but which take a lot of work to dig out, such that we consciously understand them. Psychologists and computer scientists are still discovering things about the physics of light which we all know unconsciously. But computer models which incorporate our explicit knowledge of the physics of light are being thrashed by pure machine-learning models, which are fed a huge databases of pictures and simply extrapolate from what they’ve already seen.[2]\nI think the same basic point is true of aesthetic things. We really struggle trying to explain why we like a picture or dislike a melody, because most of the work is done at an unconscious level. The progress in understanding those principles will probably continue to be slow.\nBut now it seems likely to me that, before long, machines will be able to do all these things on demand – play some brand new Mozart, make elegant little drawings of animals, write a pretty good pop song. And the programmer who implements them could be – probably will be – some bozo who has no clue why it works.\n\n[1] 2017-05: SIGGRAPH video with style transfer - https://www.youtube.com/watch?v=HYhzZ-Abku8\n[2] 2017-05: Michael Elad “Deep, Deep Trouble: Deep Learning’s Impact on Image Processing, Mathematics, and Humanity” https://sinews.siam.org/Details-Page/deep-deep-trouble-4"
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html",
    "title": "Repulsion from the Prior",
    "section": "",
    "text": "the shortest version: contrary to recent reports, I do not think it’s possible for you to be a Bayesian and consistently exaggerate things.\n{: .center-image }"
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#short-version",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#short-version",
    "title": "Repulsion from the Prior",
    "section": "Short Version",
    "text": "Short Version\n\nIf we think of perception as inference, it has implications about the types of biases we would have.\nYet many biases and illusions seems to go in the exact opposite direction – sometimes called “anti-Bayesian” biases – in particular there are ubiquitous contrast effects, while Bayesian inference seems to imply assimilation effects.\nWei and Stocker (2015) say they can rationalize these contrast effects, under the assumption that our sensory mechanisms are tuned to the environment, such that they are relatively more sensitive to more likely signals. They say that this will imply contrast effects (that the bias is inversely proportional to the slope of the prior).\nYet their results contradict some simple laws of Bayesian inference – the law of iterated expectations, and law of total variance – so there is something odd going on.\n(If this explanation doesn’t work, then why do we get repulsion? I think that Ted Adelson explained the basic reason in the 70s. Will write another post on this.)"
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#shortish-version",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#shortish-version",
    "title": "Repulsion from the Prior",
    "section": "Shortish Version",
    "text": "Shortish Version\n\nHere’s a nice crisp problem: in what cases does inference attract towards the prior, and in what cases does it repulse away from it?\nGiven an unknown variable \\(x\\) and a signal \\(s\\), let’s say that there’s “attraction” at a given value of \\(x\\) if the average inferred value of \\(x\\) is closer to the prior than \\(x\\) itself is –\n\\[|E[E[x|s]|x]-\\mu|&lt;|x-\\mu|\\]\nAttraction effects are typically treated as the norm. For example if \\(x\\) is drawn from a normal distribution and if \\(s\\) is equal to \\(x\\) plus normal noise, then you’ll always get attraction to the prior. I.e., if \\(x\\) is above the mean, then it’ll be, on average, estimated to be closer to the mean than it actually is.\nHowever this has sometimes been treated as a puzzle in studies of perception: perception seems like inference, but we also find what look like repulsion effects. For example “contrast” effects, in which an object seems less dark when you put it next to another, darker, object. If we assume that the colour of the neighboring objects affects your prior about the target object, then this would imply an attraction effect. Yet repulsion effects seems to be the norm across all sorts of judgments (lightness, colour, volume, orientation, size), and similar contrast effects occur in time as well as in space (i.e., something seems less dark if it is preceded by something darker) – though of course there are exceptions. These types of illusion are sometimes called “anti-Bayesian.”\nA common explanation of these contrast effects is that we ‘code for differences’ – i.e. that something about our neural wiring causes us to encode differences, rather than levels, and this causes us to exaggerate differences, i.e. get contrast effects.\nBut this assumes that we encode the difference and then forget to decode (AKA coding catastrophe, AKA the el Greco fallacy). If you write down a Bayesian model, which makes its best effort to infer the level from the difference, you typically do not find the desired contrast effects (Schwartz, Hsu & Dayan (2007)).\nWei and Stocker (2015) announce that they have made a breakthrough – a fully Bayesian model which generates contrast/repulsion effects generically. They say that the key assumption is that we are more sensitive to differences in areas where signals are more likely to fall – i.e., sensitivity is proportional to the density of the prior.\nFormally, let \\(x\\sim f\\), and \\[s=F(x)+\\varepsilon.\\] This means that sensitivity is proportional to the density of the prior – and it implies that \\(s\\) will be roughly uniformly distributed – so in some sense it’s an efficient use of signal capacity. Given this setup, and some simplifications, they find that the bias is proportional to the slope of the prior – so if the prior is symmetric & single-peaked then for values above the mean, the bias will be positive, and vice versa – i.e. repulsion away from the prior everywhere.\nIn the note below I give a proof that implies that it is impossible to have repulsion effects everywhere – which seems to contradict the results of Wei & Stocker.\nI’m not sure what the source of the contradiction is – it could be either (a) Wei & Stocker’s results are true locally, but do not apply at the tails of the distribution, and so things balance out that way; (b) there is a difference in the implicit assumption used when taking conditional expectations (AKA the Borel-Kolmogorov paradox); or (c) I made a mistake.\nI also mention below a related result, that there cannot be a consistent upward or downward bias (i.e., it cannot be that \\(E[\\hat{x}\\|x]&gt;x\\) for all \\(x\\)). This is relevant for Wei & Stocker’s result applied to asymmetric priors – e.g. if the prior is everywhere decreasing – where the result seems to imply a consistent upward bias. \n\n{: .center-image }"
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#summary-of-proof",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#summary-of-proof",
    "title": "Repulsion from the Prior",
    "section": "Summary of proof",
    "text": "Summary of proof\n\nSuppose that there is repulsion from the prior everywhere, i.e. for all \\(x\\), \\(\\|E[\\hat{x}\\|x]-\\mu\\|&gt;\\|x-\\mu\\|\\).\nThis implies that \\(Var[\\hat{x}]&gt;Var[x]\\).\nBut this contradicts the law of total variance, which says that \\(Var[E[A\\|B]]\\leq Var[A]\\)."
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#detail",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#detail",
    "title": "Repulsion from the Prior",
    "section": "Detail:",
    "text": "Detail:\nSuppose there are two random variables \\(x\\) and \\(s\\), and let \\(\\hat{x}=E[x\\|s]\\). Let \\(x\\) be mean-zero, and let’s assume repulsion from the prior everywhere, i.e. for all \\(x\\):\n\\[\nE[\\hat{x}|x]|&gt;|x|\n\\]\nFrom this repulsion assumption I think it’s clear that there’s more variance in \\(E[\\hat{x}\\|x]\\) than in \\(x\\):\n\\[Var[E[\\hat{x}|x]]&gt;Var[x]\\]\nNow let’s apply the law of total variance:\n\\[\n\\begin{aligned}\nVar[A]=& E[Var[A|B]]+Var[E[A|B]] \\\\\\\\\nVar[\\hat{x}]=& E[Var[\\hat{x}|x]]+Var[E[\\hat{x}|x]]\n\\end{aligned}\n\\]\nThus implying that:\n\\[Var[\\hat{x}]\\equiv Var[E[x|s]]&gt;Var[x]\\]\nApplying the law of total variance again we get:\n\\[\\begin{aligned}\nVar[x]=& E[Var[x|s]]+Var[E[x|s]] \\\\\\\\\n      &gt;& Var[x]\n\\end{aligned}\\]\nA contradiction."
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#no-consistent-upwarddownward-bias",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#no-consistent-upwarddownward-bias",
    "title": "Repulsion from the Prior",
    "section": "No consistent upward/downward bias",
    "text": "No consistent upward/downward bias\nThe law of iterated expectations states that, for any \\(A\\) and \\(B\\):\n\\[E[E[A|B]]=E[A]\\]\nThis implies that there cannot be a consistent upward or downward bias, i.e. it cannot be true that:\n\\[E[\\hat{x}|x]&gt;x, \\forall x\\]"
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#references",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#references",
    "title": "Repulsion from the Prior",
    "section": "References",
    "text": "References\n\nSchwartz, Hsu & Dayan (2007, Nature Review Neuro) “Space and Time in Visual Context”\nWei & Stocker (2015, Nature Neuroscience) “A Bayesian observer model constrained by efficient coding can explain ‘anti-Bayesian’ percepts”"
  },
  {
    "objectID": "posts/2020-04-05-front-loading-restrictions.html",
    "href": "posts/2020-04-05-front-loading-restrictions.html",
    "title": "Optimal Coronavirus Policy Should be Front-Loaded",
    "section": "",
    "text": "Q: How should you sequence policies over time? E.g. suppose you want to manage the epidemic until a vaccine arrives and you have policies (lockdowns, distancing, masks) each of which is associated with a certain effect on the growth-rate of cases, but each also has some fixed social cost per day. How should you apply the policies over time?\nA: The severity of the policies should be gradually decreasing, i.e. they should gradually become less severe, as you approach the availability of a vaccine. There should not be zig-zagging between policies in this setup.\nAny justification for zig-zagging must come from some additional consideration like (a) non-separabilities in the costs, e.g. psychological/economic need for occasional respite, (b) uncertainty about the end-date, (c) uncertainty about the effect of the policies, such that there is informational-value from varying policies, or (d) desire to maintain a steady flow of cases, in order to reach herd immunity (the “mitigation” strategy)."
  },
  {
    "objectID": "posts/2020-04-05-front-loading-restrictions.html#corollary-you-should-never-expect-policy-to-get-stricter",
    "href": "posts/2020-04-05-front-loading-restrictions.html#corollary-you-should-never-expect-policy-to-get-stricter",
    "title": "Optimal Coronavirus Policy Should be Front-Loaded",
    "section": "Corollary: you should never expect policy to get stricter",
    "text": "Corollary: you should never expect policy to get stricter\nYou should never find yourself in the situation where you expect policy to get stricter in the future. If you anticipate that a stricter policy will be appropriate next week then that strict policy is appropriate this week!\nCountries in early stages of the epidemic should be doing as much or more as countries in later stages."
  },
  {
    "objectID": "posts/2020-04-05-front-loading-restrictions.html#intuition",
    "href": "posts/2020-04-05-front-loading-restrictions.html#intuition",
    "title": "Optimal Coronavirus Policy Should be Front-Loaded",
    "section": "Intuition",
    "text": "Intuition\nSuppose that there’s some tradeoff across policiers between the growth-rate and the social cost.\nThen given any fixed time-path of policies: e.g., (A,A,B,C), if it is not monotonically decreasing in severity from high-cost to low-cost, then you can do strictly better by rearranging the path of policies to be monotonically decreasing. The social cost will be identical, because the set of policies will be the same, but the number of cases will be lower at every point in time, since at any given point the cumulative growth rates, up to that point, will be lower. Thus the final cumulative number of cases will be lower."
  },
  {
    "objectID": "posts/2020-04-05-front-loading-restrictions.html#additional-reason-to-front-load-extinction",
    "href": "posts/2020-04-05-front-loading-restrictions.html#additional-reason-to-front-load-extinction",
    "title": "Optimal Coronavirus Policy Should be Front-Loaded",
    "section": "Additional Reason to Front-Load: Extinction",
    "text": "Additional Reason to Front-Load: Extinction\nAll of this is treating the number of cases as a continuous variable which means you can never completely extinguish the disease. However if that’s a possibility that’s within sight (e.g. as in NZ), then that’s a significantly stronger case for starting with very severe policies, to try to kill the disease entirely, and then you can go back to the garden of Eden."
  },
  {
    "objectID": "posts/2020-04-05-front-loading-restrictions.html#prior-discussion",
    "href": "posts/2020-04-05-front-loading-restrictions.html#prior-discussion",
    "title": "Optimal Coronavirus Policy Should be Front-Loaded",
    "section": "Prior Discussion",
    "text": "Prior Discussion\nThere’s been some discussion of zig-zagging by the Imperial group (paper) and by Timothy Gowers (twitter & post)\nGowers says the optimal policy is very short zig-zags (changing policy every other day), however I think this is misleading. It comes from fixing the lower-threshold and optimizing the upper-threshold. If instead you fixed the upper-threshold and optimized the lower-threshold, then the optimal cycle-length will be long.\nIf you choose both the upper and lower threshold (both T and S) then he notes that they’ll both be arbitarily low. However this ignores the cost of getting to zero given current cases.\nInstead a well-defined problem is to choose an optimal time-path of policy given some start-point and end-point. In that case it’ll be a path of gradually decreasing strictness (without zig-zags).\nYou can see the intuition in the diagram below: the total infections is approximately the area under the zig-zag (not quite: because the y-axis is ln(cases), but this won’t matter for the argument). Thus you can reduce the area under the line by lowering the upper threshold. However if you instead take the upper threshold as fixed, then it’s optimal to choose a lower threshold that is as low as possible, i.e. you want long cycles, not short cycles.\n\n\n\nabc"
  },
  {
    "objectID": "posts/2017-02-25-samuelson-expected-utility.html",
    "href": "posts/2017-02-25-samuelson-expected-utility.html",
    "title": "Samuelson & Expected Utility",
    "section": "",
    "text": "clown\n\n\n\n“If in every event which can possibly occur the consequence of action I is not preferred to that of action II, and if in some possible event the consequence of II is preferred to that of I, then any sane preferer would prefer II to I.”\n\nThis sentence, in a letter from Savage in 1950, finally persuaded Samuelson that rational choices must obey expected utility. He had been skeptical – thinking that expected utility was just a simple approximation, like exponential discounting or like separability. Marschack and Savage and Friedman all wrote letters trying to persuade him, and though they were right, they kept using bad arguments, and Samuelson disposed of them.\nSavage and Friedman wrote a paper saying that, because people buy both insurance and lottery tickets, expected utility implies that the utility-of-money must be concave then convex. Samuelson saw that this was ridiculous, & said “there’s as much to be learned about gambling from Dostoyevsky as from Pascal.”\nBut the sentence from the letter above finally persuaded Samuelson.\nThis is all from Ivan Moscati’s “How Economists Came to Accept Expected Utility Theory”."
  },
  {
    "objectID": "posts/2023-06-06-effect-of-ai-on-communication.html",
    "href": "posts/2023-06-06-effect-of-ai-on-communication.html",
    "title": "The Influence of AI on Content Moderation and Communication",
    "section": "",
    "text": "Tom Cunningham11 tom.cunningham@gmail, @testingham, resident fellow at the Integrity Institute. I worked at FB for 5 years, and Twitter for 1 year, but this note entirely based on public information.\nFirst version June 6 2023. Still a draft, don’t circulate!\nIt is difficult to anticipate the effect of AI on communication. AI models have already had big effects on content moderation (detecting spam, bots, misinformation), but they are also starting to have big effects on content production (obfuscation, generating synthetic spam, deepfakes).\nIn this note I will try to make some predictions about the equilibrium impact of AI. Below I discuss and give predictions about the impact on a variety of outcomes such as the prevalence of policy-violating content on platforms (e.g. porn, hate speech), the prevalence of bots, the prevalence and influence of synthetic media (e.g. deepfakes), and the shift of overall communication between platforms.\nIt is useful to distinguish between “internal” and “external” properties of content:\nThe remainder of this note:\nI also have a short appendices on the history of forged documents, and on relevant forecasting questions from Metaculus."
  },
  {
    "objectID": "posts/2023-06-06-effect-of-ai-on-communication.html#model",
    "href": "posts/2023-06-06-effect-of-ai-on-communication.html#model",
    "title": "The Influence of AI on Content Moderation and Communication",
    "section": "Model",
    "text": "Model\nThe diagram below illustrates three sets of posts: 1212 I’m assuming that the universe is all possible posts, e.g. all possible strings of text, or sets of pixels. Then sets \\(V\\) and \\(\\hat{V}\\) and \\(M\\) will be enormous, but the actually existing posts are only a tiny tiny fraction of the set of possible posts. When calculating prevalence we use cardinality of these sets, implying (1) posts are drawn uniformly from the sets, and (2) there are no identical posts, i.e. each post is drawn at most 1 time.\n\n\\(V\\): posts that are violating – according to expert human raters. I’m concentrating on “self-contained” rules, e.g. nudity, hate speech, holocaust denial, where the violation depends only on the direct content of the post. I’m not considering categories like misinformation, deepfakes, or spam where violation depends on whether the content is true, or the provenance of the content.\n\\(\\hat{V}\\): posts that are classified as violating – by the platform’s classifier, can parameterize with a threshold (\\(\\hat{V}(k)\\)). The red segments below represent the false positives and false negatives of the classifier:\n\\(M\\): posts that the producer wants to send – i.e., posts that express their message, \\(M\\). We can classify some producers as “good” (only wants to send non-violating \\(M\\cap V=\\varnothing\\)), or as bad (only want to send violating \\(M\\cap V=M\\))."
  },
  {
    "objectID": "posts/2023-06-06-effect-of-ai-on-communication.html#implications-of-this-model",
    "href": "posts/2023-06-06-effect-of-ai-on-communication.html#implications-of-this-model",
    "title": "The Influence of AI on Content Moderation and Communication",
    "section": "Implications of this Model",
    "text": "Implications of this Model\nSuppose that we remove all detected violating posts (i.e. \\(\\hat{V}\\)). In the case illustrated above the posts removed (\\(M\\cap\\hat{V}\\)) would be the majority of the producer’s posts, and of those that remain only a small share are violating. We can make a few observations given this setup:\n\nImprovements to the platform’s classifier will reduce prevalence of violating content. If we hold fixed the producer’s production, then as the classifier \\(\\hat{V}\\) gets closer to \\(V\\) we should generally expect fewer false-positives and fewer false-negatives, so overall a better outcome for the platform (there are cases where this wouldn’t happen but they seem unusual.)\nImprovements to producer’s classifier will probably increase prevalence of violating content. Suppose that producers have their own estimate of the platform’s classifier, \\(\\tilde{V}\\), and they will produce posts uniformly from the set they believe will not be removed, i.e. from the set \\(M \\backslash \\tilde{V}\\). If the producer learns the true classifier \\(\\tilde{V}=\\hat{V}\\) then they will reduce their rate of deleted posts, which is good for the producer, but the effect on the platform can be ambiguous:\n\nFor good producers: For producer who do not wish to produce violating content (\\(M\\cap V=\\varnothing\\)) knowledge of the classifier will prevent them from producing false positives, and they will produce more true negatives, which is good for the platform.\nFor bad producers: For producers who only want to produce violating content (\\(M\\cap V=M\\)) they will increase their rate of violating but non-detected posts (false negatives), which is bad for the platform.\nFor neutral producers: For neutral producers (where \\(M\\) is neither a subset nor mutually exclusive with being violating), the net effect is ambiguous.\n\nIn fact the net effect seems likely to be negative because in most cases people who produce violating content do it on purpose, e.g. those who produce nudity and hate speech and holocaust denial do it intentionally, and they couldn’t achieve their purpose without violating the policy.\nWhat this looks like in practice: tricking classifiers by misspelling words, using roundabout phrasing, inverting colours in a photo, adding noise to a video. Earlier classifiers had difficulty with these manipulations but deep neural nets are much better at recognizing the deep content and are harder to confuse.\nQualification. In the argument above I’m holding fixed the platform’s classifier \\(\\hat{V}\\). However if the producer changes the posts they produce then the performance of that classifier will change, e.g. precision might fall, and so the full effect should include an adjustment to that classifier. Could add an equilibrium condition that the classifier’s threshold \\(k\\) will be set such that precision will be \\(p\\) (\\(\\frac{|\\hat{V}(k)\\cap V \\cap M|}{|\\hat{V}(k)\\cap M|}=p\\)). I don’t think this would change the qualitative conclusion.\nImprovements to a classifier used by both platform and producer will lower prevalence of violating content. Suppose both parties used the same classifier (\\(\\tilde{V}=\\hat{V}\\)), and that classifier was perfect (\\(\\tilde{V}=\\hat{V}=V\\)). Then there will be no violating posts shown: negative producers won’t post anything, and neutral and positive producers will only produce non-violating posts. This argument applies to the limit of having a perfect model, but it seems reasonable to presume that on the path to that limit the rate of violating posts will generally decline.\nContent will cluster just below the classifier threshold. So far we have discussed everything in binary terms, thought realistically most things are continuous. If we action posts in a binary manner, e.g. filtering posts with a classifier score above some threshold, then producers will have an incentive to produce posts which are just below the threshold for filtering."
  },
  {
    "objectID": "posts/2023-06-06-effect-of-ai-on-communication.html#bender-gebru-mcmillan-major-mitchell-2021-facct-on-the-dangers-of-stochastic-parrots-can-language-models-be-too-big",
    "href": "posts/2023-06-06-effect-of-ai-on-communication.html#bender-gebru-mcmillan-major-mitchell-2021-facct-on-the-dangers-of-stochastic-parrots-can-language-models-be-too-big",
    "title": "The Influence of AI on Content Moderation and Communication",
    "section": "Bender, Gebru, McMillan-Major, Mitchell (2021, FAccT) “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?”",
    "text": "Bender, Gebru, McMillan-Major, Mitchell (2021, FAccT) “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?”\n\nDiscuss environmental cost. &gt; “LMs are not performing natural language understanding (NLU), and only have success in tasks that can be approached by manipulating linguistic form [14].”\n\n\n“the tendency of human interlocutors to impute meaning where there is none can mislead both NLP researchers and the general public into taking synthetic text as meaningful.”\n\n\n“Combined with the ability of LMs to pick up on both subtle biases and overtly abusive language patterns in training data, this leads to risks of harms, including encountering derogatory language and experiencing discrimination at the hands of others who reproduce racist, sexist, ableist, extremist or other harmful ideologies rein- forced through interactions with synthetic language.”\n\nCoherence in the Eye of the Beholder. It produces “apparently” coherent text but not really coherent.\nRisks and Harms. Generally absorbing hegemonic worldview. E.g. (1) assuming doctor will be male, nurse female; (2) outputting abusive language; (3) could generate meaningless text & used, e.g., to recruit terrorists. / Especially this point: apparent fluency will mislead people into thinking that there’s some genuine content. “the human tendency to attribute meaning to text, in combination with large LMs’ ability to learn patterns of forms that humans associate with various bi- ases and other harmful attitudes, leads to risks of real-world harm.”\nProposal. Value-sensitive design."
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking/2016-04-30-relative-thinking.html",
    "href": "posts/2016-04-30-relative-thinking/2016-04-30-relative-thinking.html",
    "title": "Relative Thinking",
    "section": "",
    "text": "peaches\nThere are a lot of cute thought experiments where the apparent value of something depends on what it’s compared to:\nMany people have felt that there’s a common principle at work, in particular: that the sensitivity to an attribute (price, probability, square feet) depends on the set of quantities that you’re considering. But different people have proposed different principles:\nAll of these models can be thought about as indifference curves that change slope as you change the elements in the choice set, e.g. below adding option C makes the indifference curves rotate clockwise, and so makes you prefer B to A:\n\\[\n   \\xymatrix{\\, &  &  &  &  & . & \\,\\, &  & \\,\\\\\n   \\, & A &  &  &  &  & A\\\\\n   \\, &  &  & B & \\ar@{-}[uullll] &  &  &  & B & \\ar@{-}[uul]\\\\\n   \\, &  &  &  &  &  &  &  &  &  C \\\\\n   \\ar[uuuu]\\ar[rrrr] &  &  &  & \\ar@{-}[uullll] & \\ar[uuuu]\\ar[rrrr] &  &  & \\ar@{-}[uuuull] & \\,\n   }\n\\]\nBut each theory has different assumption about how the slope of the indifference curves depends on the placement of the options.\nI’m going to try to make the following points:\nA few other points that I’ll leave for later:"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking/2016-04-30-relative-thinking.html#digression-perception",
    "href": "posts/2016-04-30-relative-thinking/2016-04-30-relative-thinking.html#digression-perception",
    "title": "Relative Thinking",
    "section": "DIGRESSION: PERCEPTION",
    "text": "DIGRESSION: PERCEPTION\nComparison effects have been studied in perception for a long time, and the same points I make here also apply there. At first people proposed that perceptual comparison effects were hardwired things, mechanical effects, but on further study it turned out that they were context-dependent, in a way that makes them look like sensible inferences.\nHere’s a classic contrast effect:\n\nthe same shade of grey looks darker when surrounded by white, than when surrounded by black.\nFor a long time psychology textbooks gave this as an example of a hardwired contrast effect – i.e. this is caused by the basic wiring of neurons in our eyes. (Some still do).\nBut take a look at this (White’s illusion):\n\nhere the same shade of grey looks lighter on the left than on the right, despite the surroundings being relatively lighter on the left than on the right. This is exactly the opposite of what’s predicted by a hardwired contrast effect in perception.\n(An even cleaner example would be ceteris paribus, where making some part of the background lighter, all else held equal, makes the foreground appear lighter. The figure above does imply that there must exist at least one such case: imagine a third set of grey rectangles which are surrounded only by white. That third case must serve as a ceteris paribus case for one or other of the two cases above, probably both.)\nThe general point is this: There is not a stable relationship between the perceived-lightness of an object and the lightness of surrounding objects. There isn’t even an all-else-equal relationship. The relationship can run in either direction depending on the context.\nBut that’s not the last word. The set of contexts where it goes one way or the other way aren’t arbitrary (“contrast effects” and “assimilation effects”). Adelson (2001) shows that you can usually predict when you’ll observe one effect or the other: roughly, whether or not the surrounding lightness is a positive or negative ecological cue for illumination. I.e., in typical circumstances, is the surrounding lightness positively or negatively associated with illumination? (See an example at the bottom of this post.)"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking/2016-04-30-relative-thinking.html#digression-2",
    "href": "posts/2016-04-30-relative-thinking/2016-04-30-relative-thinking.html#digression-2",
    "title": "Relative Thinking",
    "section": "DIGRESSION 2",
    "text": "DIGRESSION 2\nI would write out lists of comparison-effect examples over and over while working on my PhD. My train of thought would get detached and I’d end up asking myself questions like: What are you doing sitting in this office, a continent away from your friends and an ocean and a continent away from your family? Why do you spend your weekends in this sad building, where people stare at the carpet when they pass each other? What rock did you hit in adolescence that knocked you out of orbit, and sent you here? Are you trying to make your mother proud? Avenge your father? Do these professors you work with look like the kind of man you want to be? Did you stumble into one of those academic fields that people snigger about? Why, when you talk about your work, do the people you admire glaze over, and the people who bore you perk up? Do you think that giving your life to intellectual things makes you better than other people? Do you look down on people who don’t think so clearly? What are you doing on a Friday night at the NBER eating a tuna subway sandwich and reading reddit? If it takes you 5 years to get straight one point about relative thinking – one corner of one shelf in one cupboard – then how long is it going to take to tidy up the whole house? When an undergraduate corners you, asking earnest & tedious questions, doesn’t it remind you of yourself?\n\n\n\ndesk"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking/2016-04-30-relative-thinking.html#alternative-heuristics",
    "href": "posts/2016-04-30-relative-thinking/2016-04-30-relative-thinking.html#alternative-heuristics",
    "title": "Relative Thinking",
    "section": "ALTERNATIVE HEURISTICS",
    "text": "ALTERNATIVE HEURISTICS\nOK. Well, here’s the body of the argument: I’m going to discuss a few perfectly reasonable reasons why we might infer the value of different attributes from the choice set, and each reason will imply one of the above laws in a subset of cases. However I also show that the same reason can imply the exact opposite of that law, for cases outside that subset.\n\n(1) MRS shifting towards MRT.\nA choice set which varies in different attributes has an implicit rate of tradeoff between the attributes – i.e. the marginal rate of transformation (MRT) – and it is easy to think of cases where our preferences would naturally adapt to the implicit tradeoff, i.e., where our relative value, AKA our marginal rate of substitution (MRS), would rotate towards the tradeoff that is implicit in the choice set (i.e. the MRT).\nThe MRS shifting towards the MRT could be rationalized in two separate ways. First, suppose you believe the social MRS to be informative about your own MRS, and you believe that the choice set reflects the market price, and finally that the price reflects the social MRS (as it would in a competitive equilibrium). Second, suppose you believe the person who constructed the choice set to be cooperative (in the sense of Grice) - i.e., they only include things in the choice set which they think you might want: this implies that the MRT in the choice set reflects their beliefs about your MRS, which is itself informative. If I’m staying in your spare room and and you ask me “would you prefer a poached egg or gruel for breakfast?” then I will figure that your gruel must be pretty good.\nIf there are just two attributes (i,j) and two alternatives (a,b) then the implicit tradeoff is \\(MRT_{i,j}=\\frac{\\|a_{i}-b_{i}\\|}{\\|a_{j}-b_{j}\\|}\\). If the MRS rotates to meet this MRT then the sensitivity to attribute \\(i\\) will be decreasing in the range observed along that dimension, exactly as implied by the range-sensitivity theory (i.e., V, M&C, BR&S).\nHowever the MRS-MRT effect implies range sensitivity only for a 2-attribute, 2-alternative case. Outside of that case the intuitions depart.\nA marginal rate of transformation can only be directly identifed from a menu if the number of alternatives is equal to the number of attributes. I.e., to define a plane in \\(n\\) dimensions from a set of points, you’ll need exactly \\(n\\) points. If you have fewer then it becomes the statistical problem of fitting a line to a set of points. Here is a simple example where the MRT theory and other theories (e.g. range-sensitivity) give qualitatively different answers, and in which the MRS-MRT theory seems more faithful to the intuition. Suppose we have the following three options:\n\\[\n\\xymatrix@C=1em@R=1em{\n& \\binom{\\mbox{100K salary}}{\\mbox{199 days off}}\\\\\n\\\\\n&  & \\binom{\\mbox{105K salary}}{\\mbox{189 days off}} & \\binom{\\mbox{110K salary}}{\\mbox{189 days off}}\\\\\n\\\\\n\\ar[rrrr]\\ar[uuuu] & & &  &  & \\, \\\\\n}\n\\]\nThe intermediate option is dominated by the by the option on the right, and intuitively - to me - the existence of the intermediate option makes the rightmost option more desirable - because the choice set makes 10-days-off seem to be worth between \\$5K and \\$10K, meaning the higher salary seems to come at a low cost in terms of days-off. This intuition is not captured by range sensitivity, because the intermediate option does not change the range in either dimension. However the intermediate option does change the implicit MRS, in the sense of the best-fitting line (e.g. orthogonal regression), and the change will be in favor of the rightmost option – fitting my own intuition.\nEven in the 3-attribute 3-alternative case, it is no longer true that \\(MRT_{i,j}\\) is equal to the ratio of ranges on dimension i and j, it’s now a more complicated function.\nWhen one attribute is a good and the other is a bad (e.g. price and quality; salary and hours) it is sometimes reasonable to think that choosing neither alternative is an additional implicit element of the choice set, i.e. the point (0,0). In these cases the ratio of the ranges reduces to the ratio of the maximum values (\\(\\frac{\\max_{c\\in C}c_{i}}{\\max_{c\\in C}c_{j}}\\)), which has similar comparative statics to the theory in (C) - which depends on the ratio of average values - than the theory in (V,M&C,BR&S) - which depends on the ratio of the ranges.\nAn interesting fact: the effects of this MRS-MRT theory will not be detectable when the choice set is binary: suppose your MRS shifts towards the MRT implicit in the choice set, then although your final MRS will be closer to the MRT, it will not cross the MRT, i.e. the shift in MRS will not alter which of the two elements you prefer. This implies that the MRS-MRT theory cannot rationalize the existence of cycles in binary choice, and so cannot explain evidence for ‘subadditivity’ of different dimensions, such as probability, money, or delay (see Read (2001) for citations). For example, if your have a prior belief that \\(a\\) is better than \\(b\\), and then you observe a choice set containing \\(a\\) and \\(b\\), then you may revise upward your valuation of \\(b\\) relative to \\(a\\), but this observation wouldn’t cause you to switch preference, i.e. to think that \\(b\\) is now better than \\(a\\). (This could be violated under some unusual priors, e.g. if you had bimodal beliefs about the value of \\(b\\)).\n\n\n(2) MRS shifting towards demand.\nThere is a second strong intuition for choice sets influencing preferences: combinations offered often reflect combinations desired, so a relative increase in attribute 1 could be interpreted as a positive signal about the value of attribute 1. Suppose we manipulate the choice set, while keeping the relative price fixed, for example consider these two choice sets, trading off the price and quantity of some good:\n\\[\n\\xymatrix@C=.5em@R=.5em{\\ar[rrrrr]\\ar[ddddd] & & &  &  & \\text{apples}\\\\\n& \\binom{\\text{1 apple}}{\\$1}\\\\\n&  & \\binom{\\text{2 apples}}{\\$2}\\\\\n&  & & \\binom{\\text{3 apples}}{\\$3}\\\\\n\\\\\n\\\\\n\\$ }\n\\]\n\\[\n\\xymatrix@C=.5em@R=.5em{\\ar[rrrrr]\\ar[ddddd] & & & &  & \\text{apples}\\\\\n& \\,\\,\\,\\,\\,\\, & \\\\\n& & \\binom{\\text{2 apples}}{\\$2}\\\\\n& & & \\binom{\\text{3 apples}}{\\$3}\\\\\n& & & & \\binom{\\text{4 apples}}{\\$4}\\\\\n\\\\\n\\$ }\n\\]\nA natural intuition is that people will tend to switch from \\(\\binom{2}{\\$2}\\) to \\(\\binom{3}{\\$3}\\), when going from the first to the second choice set. None of the theories discussed above gives an unambiguous prediction about the change in MRS between these two choice sets - because both the range and magnitude change by the same amount on each dimension - yet the intuition remains quite clear (I think).\nThis idea could be easily formalized - suppose people know the supply curve but are uncertain about the demand curve - then when they observe an increase in quantity they attribute this to a higher demand, and so they infer an increase in value of the good. I think this is similar to the intuition given in Kamenica (2008) - when you observe a higher price/quantity combination, you infer that demand is higher, and so you infer that the value of each marginal good must be higher. I think that a similar foundation is used in Drolet, Simonson, Tversky (2000) “Indifference Curves that Travel with the Choice Set”.\nWe have discussed diagonal shifts along the budget set, meaning that both attributes are varying at once; if only one attribute varied, it’s not clear what a consumer would infer from this. Of course we could formalize a model where the consumer is uncertain about both supply and demand; or we could combine this model with the prior one, where the consumer is uncertain about the price.\n\n\n(3) Magnitude effects.\nFinally it’s easy to come up with a rational magnitude effect, such that when we observe a higher quantity \\(q\\) we infer that the marginal value of each unit is less. Suppose we know the price and we know the consumption-value of a good, when measured in units that are familiar to us, but we do not know the units that are used in the packaging. Then if we observe other people consuming a higher quantity, measured in unfamiliar units, we infer that each unit is worth less: when we observe a 10,000 Kronor bank note we infer each Kronor is not worth a lot; when we observe a 10,000 Watt bulb we infer each Watt is not worth much; when we observe 200mg of Oxytocin we infer the marginal effect of a milligram is not too much."
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking/2016-04-30-relative-thinking.html#summary",
    "href": "posts/2016-04-30-relative-thinking/2016-04-30-relative-thinking.html#summary",
    "title": "Relative Thinking",
    "section": "SUMMARY",
    "text": "SUMMARY\nWhen we dig into the intuitions behind comparison effects, we often find that they resemble inferences that we would make every day. The laws that have been proposed to explain comparison effects only work because they coincide with one or other of the inferences in certain subsets of cases – e.g. range-sensitivity coincides with MRS-MRT inference, magnitude-sensitivity coincides with unit-value inference. But these overlaps occur only in a subset of cases, and stepping outside that subset we find that the law fails, while the inference remains.\nDoes this mean that comparison effects are all just rational inferences? What we would like to know is whether comparison effects occur even when inference can be entirely ruled out – e.g. when we run an experiment that explicitly randomizes the choice sets. Some papers do this, but few do it well. I am persuaded that comparisons do affect us on a pre-conscious level, i.e. that our instincts latch onto comparisons without being careful about the significance of the comparison in the particular circumstance, but there’s not a lot of unambiguous evidence on this. I can at least say that most people find the types of example listed above pretty beguiling: they get strong intuitions about relative value, but struggle to explain where the intuitions come from, implying that the inference isn’t entirely conscious.\nSo then why would we make bad inferences that resemble good inferences? I think for the same reason that our perception makes bad inferences that resemble good inferences – because perceptual processes interpret cues according to their ordinary significance, without adjusting for all relevant information. Perception is carried out in a cabinet, whirring through the sense data, and printing out conclusions for the conscious mind to read. The cabinet is locked, we only have access to the output. That is the argument of my paper on ‘implicit knowledge’."
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking/2016-04-30-relative-thinking.html#miscellaneous-notes",
    "href": "posts/2016-04-30-relative-thinking/2016-04-30-relative-thinking.html#miscellaneous-notes",
    "title": "Relative Thinking",
    "section": "MISCELLANEOUS NOTES",
    "text": "MISCELLANEOUS NOTES\n\nIt is useful to make a sharp distinction between “goods” and ’bads”. Examples of good-bad tradeoffs are quality vs price; salary vs hours worked. Examples of good-good tradeoffs are bedrooms vs bathrooms; MPG vs horsepower; salary vs holiday-days.\nParducci invented, as well as range-frequency theory, windsurfing.\nBordalo Gennaioli and Shleifer (2013) has a weird feature: the salience of an attribute depends on relative levels (\\(q-\\bar{q}\\) and \\(p-\\bar{p}\\)), but the utility of an attribute depends on absolute levels (\\(q\\) and \\(p\\)). I think this is just a mistake – the underlying intuition is matched much better if \\(U(q,p) = \\hat{\\theta}_q(q-\\bar{q}) + \\hat{\\theta}_p(p-\\bar{q})\\) instead of \\(U(q,p) = \\hat{\\theta}_q q + \\hat{\\theta}_p p\\). This alternation removes a lot of the weird comparative statics of the theory, such as the severe non-monotonicity of the decoy effects. Another note: for two-alternative two-attribute choices the theory (as stated in the paper) has a utility representation, i.e. many of the predictions of that paper are equivalent to a model with menu-independent preferences. For a sufficiently large value of \\(M\\):\n\\[\nU(q,p)=\\begin{cases}\n\\delta q-p & ,\\,q&lt;\\delta p\\\\\nM+\\ln q-\\ln p & ,\\delta p&lt;q&lt;\\delta^{-1}p\\\\\nM+q-\\delta p & ,\\,q&gt;\\delta^{-1}p\n\\end{cases}\n\\]\n\n\n\n\nskeleton"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking/2016-04-30-relative-thinking.html#references",
    "href": "posts/2016-04-30-relative-thinking/2016-04-30-relative-thinking.html#references",
    "title": "Relative Thinking",
    "section": "REFERENCES",
    "text": "REFERENCES\nKS: Koszegi and Szeidl (2011) (KS)\nBRS: Bushong Schwarzstein & Rabin (2014)\nMC: Mellers & Cooke ()\nC: Cunningham (2013)\nBGS: Bordalo Gennaioli Shleifer (2012)\nSimonson (2008) “Will I like a Medium Pillow?”\n\n“much of the evidence for preference construction reflects people’s difficulty in evaluating absolute attribute values and tradeoffs and their tendency to gravitate to available relative evaluations … These illustrations suggest that many forms of preference construction reflect a key underlying principle: decision makers tend to avoid absolute value judgments and gravitate to accessible relative evaluations … it is noteworthy that the evidence that has been accumulated to make the case for preference construction might be largely driven by a rather simple common principle. This rather simple, yet important absolute-to-relative principle lends itself to seemingly unrelated demonstrations, which have been treated as distinct phenomena and received unique labels.”\n\nDavid Stove (1991) “What is Wrong With our Thoughts?”"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking/2016-04-30-relative-thinking.html#another-illusion",
    "href": "posts/2016-04-30-relative-thinking/2016-04-30-relative-thinking.html#another-illusion",
    "title": "Relative Thinking",
    "section": "Another Illusion",
    "text": "Another Illusion\nAdelson’s “steps” illusion:\n\nIn the first picture the two squares with arrows on them look similar, but in the second picture they seem to have different shades. They are (as you guessed) all the same shade, and in fact the shades are all identical between the first and second image, just arranged a little differently.\nIn particular, the tilt gives an the impression of an angle, and so influences our judgment of where the illumination is coming from. In the first image both squares seem to be on the same plane; in the second image the upper square seems to be on a plane with light squares, and the lower square seems to be on a plane with dark squares. If we use, as proxies of illumination for a square, the shade of squares coplanar with it, then we get the predicted effect."
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking/2016-04-30-relative-thinking.html#more-examples",
    "href": "posts/2016-04-30-relative-thinking/2016-04-30-relative-thinking.html#more-examples",
    "title": "Relative Thinking",
    "section": "More Examples",
    "text": "More Examples\n\nIf you have to judge the value of 10 of something – say you’re bidding on 10 bottles of wine in an auction – they’ll seem more valuable if, at the same time, you’re also considering a single bottle of wine; and vice versa if you’re also considering 100 bottles of wine.\nIf you’re choosing between a low-price and medium-price version of a good, seeing that there’s also a high-price version makes the medium-price version seem relatively more attractive.\n(See my ‘comparisons and choice’ paper for more examples)."
  },
  {
    "objectID": "posts/2017-02-25-economist-explorers.html",
    "href": "posts/2017-02-25-economist-explorers.html",
    "title": "Economist Explorers",
    "section": "",
    "text": "explorers\n\n\nImagine a group of adventurers set out to explore a new continent, each one choosing a different valley to map. And suppose that, instead of sending back reports of what they found (“a forest, a swamp, mosquitoes”), each one wrote reports calculated to appear as exciting as possible (“a forest, abundant water, rich fauna, couldn’t disconfirm rumors of a city of gold”).\nThis is how I feel when I’m refereeing economics papers: everyone’s trying to tell an exciting story, and it takes a great deal of work to figure out what actual novel facts they have discovered. It’s frustrating because it’s such an inefficient way to explore the territory: so many people have spent so much time on this, and we have so little to show. What have we discovered about decision-making in the last 50 years after proposing thousands of different models, running tens of thousands of experiments, and regressing millions of variables? Couldn’t we have accumulated more knowledge if we’d organized things differently?\n(Although I have to admit that many of my own papers do include some speculation about cities of gold. But I think the most useful thing a referee can do is to suggest changes to the title and abstract, to make it more transparent exactly what the paper has found.)\nMy original post on Facebook\n–\nFollowup: what have we discovered about decision-making in the last 50 years?\nOnce, when I taught a graduate class in behavioural economics, a couple of students came from civil engineering & dentistry, hoping to learn something useful that they could use in modelling decision-making - e.g. in modelling how people make decisions about commuting. I was embarrassed in how little I could help them. I was able to think of a lot of useful stuff about decision making that’s ~50 years or older: utility & expected utility, exponential (& hyperbolic) discounting, Engel curves, responses to permanent & temporary income, tractable demand estimation, the offsetting income & substitution effects of wages. All of this is immediately useful in quantifying decision-making. But I can think of few recent examples of quantitatively useful findings. Special mention to kahneman & tversky. If you ask, say, about attitudes to uncertainty, to time, to temptation, to intertemporal complementarities, I would begin my answer with “there are various schools of thought…”."
  },
  {
    "objectID": "posts/2017-04-15-the-mechanical-and-the-rational.html",
    "href": "posts/2017-04-15-the-mechanical-and-the-rational.html",
    "title": "The Repeated Failure of Laws of Behaviour",
    "section": "",
    "text": "krazy kat"
  },
  {
    "objectID": "posts/2017-04-15-the-mechanical-and-the-rational.html#footnotes",
    "href": "posts/2017-04-15-the-mechanical-and-the-rational.html#footnotes",
    "title": "The Repeated Failure of Laws of Behaviour",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlso Seligman (1970) On the Generality of Laws of Learning, “That all events are equally associable and obey common laws is a central assumption of general process learning theory … A review of data from the traditional learning paradigms shows that the assumption of equivalent associability is false … it is speculated that the laws of learning themselves may vary with the preparedness of the organism for the associa- tion and that different physiological and cognitive mechanisms may covary with the dimension.”↩︎"
  },
  {
    "objectID": "posts/2017-02-25-weber-fechner-law.html",
    "href": "posts/2017-02-25-weber-fechner-law.html",
    "title": "Weber’s Law Doesn’t Imply Concave Representations or Concave Judgments",
    "section": "",
    "text": "runningman\nNutshell."
  },
  {
    "objectID": "posts/2017-02-25-weber-fechner-law.html#linear-representation-multiplicative-noise",
    "href": "posts/2017-02-25-weber-fechner-law.html#linear-representation-multiplicative-noise",
    "title": "Weber’s Law Doesn’t Imply Concave Representations or Concave Judgments",
    "section": "Linear Representation & Multiplicative Noise",
    "text": "Linear Representation & Multiplicative Noise\nAssume people get signals about underlying value with multiplicative noise, \\(s=v\\cdot e\\), with \\(e\\) lognormal. For conciseness let \\(\\delta=JND(v_{1},p)\\), then \\(\\delta\\) can be implicitly defined as:\n\\[\n\\begin{aligned}\np   =&  P(E[v_{1}+\\delta|s_{2}]&gt;E[v_{1}|s_{1}]) \\\\\n    =&  P((v_{1}+\\delta)e_{2}&gt;v_{1}e_{1}) \\\\\n    =&  P(\\ln(v_{1}+\\delta)+\\ln e_{2}&gt;\\ln v_{1}+\\ln e_{1}) \\\\\n    =&  \\Phi\\left(\\frac{\\ln(v_{1}+\\delta)-\\ln v_{1}}{\\sigma_{e}^{2}+\\sigma_{e}^{2}}\\right)\n\\end{aligned}\n\\]\nwhere \\(\\Phi\\) is the CDF of a standard normal distribution. Then,\n\\[\n\\begin{aligned}\n\\ln(v_{1}+\\delta)-\\ln v_{1} =&   2\\sigma_{e}^{2}\\Phi^{-1}(p) \\\\\n\\frac{v_{1}+\\delta}{v_{1}}  =&   \\exp(2\\sigma_{e}^{2}\\Phi^{-1}(p))\\\\\nJND(v_{1},p)=\\delta         =&   v_{1}\\left[\\exp(2\\sigma_{e}^{2}\\Phi^{-1}(p))-1\\right]\n\\end{aligned}\n\\]\nIn other words, the just noticeable difference is proportional to the value, \\(v_{1}\\), as found by Weber."
  },
  {
    "objectID": "posts/2017-02-25-weber-fechner-law.html#a-concave-representation-additive-noise",
    "href": "posts/2017-02-25-weber-fechner-law.html#a-concave-representation-additive-noise",
    "title": "Weber’s Law Doesn’t Imply Concave Representations or Concave Judgments",
    "section": "A Concave Representation & Additive Noise",
    "text": "A Concave Representation & Additive Noise\nSuppose that the decision-maker receives a concave signal of value with additive noise, i.e. \\(s=\\ln v+e\\), with Gaussian \\(e\\). Then the derivation is very similar:\n\\[\n\\begin{aligned}\n  p =& P(E[v_{1}|s_{2}]&gt;E[v_{1}|s_{1}]) \\\\\n    =& P(\\ln(v_{1}+\\delta)+e_{2}&gt;\\ln v_{1}+e_{1}).\n\\end{aligned}\n\\]\nThe rest of the derivation is the same: i.e., the JND in the neighborhood of \\(v_{1}\\) will be proportional to \\(v_{1}\\)."
  },
  {
    "objectID": "posts/2017-02-25-weber-fechner-law.html#multiplicative-noise-posteriors-are-concave-in-v",
    "href": "posts/2017-02-25-weber-fechner-law.html#multiplicative-noise-posteriors-are-concave-in-v",
    "title": "Weber’s Law Doesn’t Imply Concave Representations or Concave Judgments",
    "section": "Multiplicative Noise => Posteriors are Concave in \\(v\\)",
    "text": "Multiplicative Noise =&gt; Posteriors are Concave in \\(v\\)\nSuppose we have lognormal priors for both \\(v\\) and \\(e\\):\n\\[\n\\begin{eqnarray*}\n\\ln v & \\sim & N(\\mu_{v},\\sigma_{v}^{2})\\\\\n\\ln e & \\sim & N(\\mu_{e},\\sigma_{e}^{2}),\n\\end{eqnarray*}\n\\]\nand \\(s=v\\cdot e\\), then we will have posteriors like:\n\\[\n\\begin{eqnarray*}\nf(\\ln v|s) & \\sim & N(\\frac{\\sigma_{v}^{2}}{\\sigma_{e}^{2}+\\sigma_{v}^{2}}\\ln s,\\left(\\sigma_{v}^{-2}+\\sigma_{e}^{-2}\\right)^{-1})\\\\\nE[v|s] & = & \\exp\\left(\\frac{\\sigma_{v}^{2}}{\\sigma_{e}^{2}+\\sigma_{v}^{2}}\\ln s+\\frac{1}{2}\\left(\\sigma_{v}^{-2}+\\sigma_{e}^{-2}\\right)^{-1}\\right)\\\\\n& = & s^{\\frac{\\sigma_{v}^{2}}{\\sigma_{e}^{2}+\\sigma_{v}^{2}}}e^{\\frac{1}{2}(\\sigma_{v}^{-2}+\\sigma_{e}^{-2})^{-1}}\n\\end{eqnarray*}\n\\]\nThis means that the expected \\(v\\) is concave in the signal \\(s\\) (because the exponent is less than one). Intuitively: a doubling of the value, which causes a doubling of the stimulus, will cause a less than doubling of the expected value conditional on that stimulus, because it will cause us to revise upwards our beliefs about both \\(v\\) and \\(e\\).\nFinally, we are also interested in the average posterior for a given \\(v\\). This will also be concave (abbreviating \\(\\alpha=\\frac{\\sigma_{v}^{2}}{\\sigma_{e}^{2}+\\sigma_{v}^{2}}\\), and dropping the constant coefficient in \\(E[v|s]\\)):\n\\[\n\\begin{eqnarray*}\nE[E[v|s]|v] & = & \\int(v\\cdot e)^{\\alpha}f(e)de\\\\\n& = & v^{\\alpha}\\int e^{\\alpha}f(e)de.\n\\end{eqnarray*}\n\\]"
  },
  {
    "objectID": "posts/2017-02-25-weber-fechner-law.html#additive-noise-posteriors-are-linear-in-v",
    "href": "posts/2017-02-25-weber-fechner-law.html#additive-noise-posteriors-are-linear-in-v",
    "title": "Weber’s Law Doesn’t Imply Concave Representations or Concave Judgments",
    "section": "Additive Noise => Posteriors are Linear in \\(v\\)",
    "text": "Additive Noise =&gt; Posteriors are Linear in \\(v\\)\nSuppose again that the decision-maker receives a logarithmic signal with additive noise: \\(s=\\ln v+u\\), and let \\(u\\) be Gaussian. (I changed notation from \\(e\\) to \\(u\\) because I use a lot of exponential functions in the derivation.) Now assume that, in addition, \\(v\\) is drawn from an improper uniform \\((0,\\infty)\\). Consider the expected value of \\(v\\) given the signal \\(s\\) (I drop the constant term from the Gaussian distribution for conciseness):\n\\[\n\\begin{eqnarray*}\nE[v|s] & = & \\frac{\\int_{0}^{\\infty}ve^{-\\left(s-\\ln v\\right)^{2}}dv}{\\int_{0}^{\\infty}e^{-\\left(s-\\ln v\\right)^{2}}dv}.\n\\end{eqnarray*}\n\\]\nNow exchange variables, so that \\(v=e^{z}\\):\n\\[\n\\begin{eqnarray*}\nE[v|s] & = & \\frac{\\int_{-\\infty}^{\\infty}e^{z}e^{-(s-z)^{2}}e^{z}dz}{\\int_{-\\infty}^{\\infty}e^{-(s-z)^{2}}e^{z}dz}\\\\\n& = & \\frac{\\int_{-\\infty}^{\\infty}e^{-s^{2}+2(1+s)z-z^{2}}dz}{\\int_{-\\infty}^{\\infty}e^{z-(s-z)^{2}}dz}\\\\\n& = & \\frac{\\int_{-\\infty}^{\\infty}e^{-(z-1-s)^{2}}e^{1+2s}dz}{\\int_{-\\infty}^{\\infty}e^{s+\\frac{1}{4}}e^{-((s+\\frac{1}{2})-z)^{2}}dz}\\\\\n& = & e^{s}e^{3/4}\\frac{\\int_{-\\infty}^{\\infty}e^{-(z-1-s)^{2}}dz}{\\int_{-\\infty}^{\\infty}e^{-((s+\\frac{1}{2})-z)^{2}}dz}\n\\end{eqnarray*}\n\\]\nNote that both of the integrals are independent of \\(s\\) (because the integration is between \\(-\\infty\\) and \\(\\infty\\)), so there exists some \\(\\kappa\\) such that:\n\\[\nE[x|s]=\\text{e}^{s}\\kappa.\n\\]\nFinally we are interested in the average posterior for a given \\(v\\) (here I’m again ignoring all constant terms):\n\\[\n\\begin{eqnarray*}\nE[E[v|s]|v] & = & \\int_{-\\infty}^{\\infty}E[v|s=\\ln v+u]\\text{e}^{-u^{2}}du\\\\\n& = & \\int_{-\\infty}^{\\infty}\\text{e}^{(\\ln v+u)}\\kappa\\text{e}^{-u^{2}}du\\\\\n& = & v\\int_{-\\infty}^{\\infty}\\kappa\\text{e}^{u-u^{2}}du.\n\\end{eqnarray*}\n\\]\nI.e., despite the logarithmic internal representation, the average posterior is linear in the value."
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html",
    "href": "posts/2017-12-10-unconscious-influences.html",
    "title": "On Unconscious Influences (Part 1)",
    "section": "",
    "text": "Over a couple of years I spent a lot of time in offices looking out the window, thinking about decision-making & the unconscious, scribbling little bits & pieces in a notebook.\n\n\n\nNBER\n\n\nI ended up writing two papers - “Hierarchical Aggregation of Information and Decision-Making” by myself and “Implicit Preferences Inferred from Choice” with Jon de Quidt. The papers are fairly technical, and this post is going to be a layperson’s guide to the background, what’s known about unconscious knowledge, and a tiny bit about the ideas in those papers.\nHere is the argument in a nutshell:\n\nThere are plenty of reasons to think that unconscious influences are strong – in other words, that people have limited insight into what factors influence their decisions.\nThe idea of unconscious influences has been in and out of the mainstream of psychology for the last 200 years, but always hounded by arguments over what it means, i.e. over what evidence would be sufficient to show that a decision was influenced by an unconscious factor. The battle has had many reversals: a new types of evidence has been proposed which is thought to reveal unconscious influences, and then later the technique or interpretation is shown to have substantial flaws and the line of inquiry fizzles out. A couple of decades pass and a different approach becomes popular.\nTwo broad classes of evidence are the following: (A) people reveal their unconscious preoccupations in their involuntary responses – in how their pupils dilate, how quickly they respond to a stimulus, in their word associations, dreams, slips of the tongue; (B) people reveal unconscious influences in discrepancies between how they act and how they explain their behaviour. Both sources of evidence have got tangled in debates about interpretation, and there are substantial camps on either side with not much agreement on what constitutes sufficient evidence for unconscious influences.\nA third type of evidence is less common but, I think, more powerful: evidence from inconsistencies in decision-making. The idea being that unconscious factors are by their nature isolated from conscious factors, i.e. they don’t interact with conscious beliefs and desires, and this isolation will cause certain characteristic inconsistencies among decisions.\nThis can be made precise with an analogy: the relationship between the conscious and unconscious brain is like the relationship between a blind man and his guide dog. The blind man makes decisions based, in part, on which direction the guide dog is pulling towards, so the guide dog’s beliefs and desires influence the man’s decisions, but without the man knowing exactly what those beliefs and desires are, and so he couldn’t tell you how much any particular factor contributed to his decision. Testing for unconscious influences in behaviour is just testing the degree to which your brain is being led by a guide dog.\nThe internal-consistency definition of unconscious influences implies two ways of looking for them: (1) testing whether people can accurately answer hypothetical questions about decisions they would make if factors changed - i.e. navigating without your guide dog; and (2) testing whether people make consistent judgments when judging two outcomes at a time.\nFirst, hypothetical questions. We can ask people, how would your judgment change if this factor changed? Would you still like this painting if the name of the artist was different? Would this drawing look more like your cousin if the nostrils were bigger? Unconscious influences imply that people will not be able to give accurate answers to these hypothetical questions because if the description of the situation is abstract then their unconscious brain won’t be able to evaluate it (AKA, they don’t know which direction they would go in without knowing what their guide dog will say).\nThe second way of testing for unconscious influences is what my paper with Jon is about: unconscious influences particularly leave their mark in comparisons, where you evaluate two outcomes simultaneously or consecutively, or when you choose between two outcomes. When confronted with two outcomes you surface two unconscious judgments and that gives you some insight into what is affecting those judgments, which in turn will affect your conscious decision.\nSuppose you had an unconscious preference for men over women, but a conscious preference to be indifferent, this will manifest in the following: (A) when you see two CVs which are identical, except that one is a man and one is a woman, then you’re indifferent between them; (B) when you see two CVs which differ in some other respect (e.g. one has a PhD, the other has an MBA), then you consistently have a preference for the CV belonging to the man. Your guide dog has a bias towards men, which you’re not aware of: the bias will only sway your decision in the second case because, in the second case, when your guide dog pulls you towards the man with a PhD, you cannot figure out how much of that pull is due to his being a man, and how much is due to his PhD.\nIn the end I think that our brains are full of guide dogs all pulling in different directions. If we had the stomach for it we could plot out our decisions all on a map – measure how each factor influences our judgment – and we would be able to see both the surface influences and the deeper latent influences.\n\n\n\n\nMotivating examples\nSome definitions & theory\nWays of measuring implicit preferences\nThe proposal\n\n\n\n\nLittauer"
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html#contents",
    "href": "posts/2017-12-10-unconscious-influences.html#contents",
    "title": "On Unconscious Influences (Part 1)",
    "section": "",
    "text": "Motivating examples\nSome definitions & theory\nWays of measuring implicit preferences\nThe proposal\n\n\n\n\nLittauer"
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html#involuntary-responses",
    "href": "posts/2017-12-10-unconscious-influences.html#involuntary-responses",
    "title": "On Unconscious Influences (Part 1)",
    "section": "Involuntary responses",
    "text": "Involuntary responses\nFreud is the most famous theorist of extracting unconscious factors from involuntary responses – he wrote three books on different methods: one on dreams, one on jokes, one on mistakes (mis-reading, mis-hearing, mis-speaking). An example from the last book: “A woman who is very anxious to get children always reads ‘storks’ instead of ‘stocks’.” Most of Freud’s examples of unconscious influences are much more complex than this one, and more often the hidden factor influencing behaviour is something unpleasant or shameful.\nAnother way of measuring unconscious cognition is through measuring arousal. Most famous is the “Iowa card task” from Bechara et al. (1996). They had their subjects choose among playing cards, and receive rewards if they chose certain cards. They found that people gradually learned which types of cards were rewarded, but they also found that the subjects’ automatic responses (measured by skin conductance, i.e. sweating) would show an awareness of the pattern more quickly than the subjects’ choices would: after a while, when the subject’s hand hovered over one of the cards which was rewarded, the subject would sweat a little more, even though the subject wasn’t any more likely to choose that card. They said that this showed that unconscious learning was outpacing conscious learning. Antonio Damasio, one of the authors of this study, went on to write Descartes’ Error which accused Descartes’ of starting the great misapprehension that emotions and reason are in competition – Damasio said that his experiments show how emotions inform reason and improve decision-making. A lot of subsequent papers tried to show that snap decisions, which avoid conscious processing, can produce better outcomes than slow considered decisions.\nEven more famous is the “Implicit Association Test” (IAT) (Greenwald, McGhee and Schwartz (1998)). Subjects are told to press a button whenever they see something from either of two different categories of stimuli, e.g. press the button if you see either a black face or a word with a positive association. Their finding, much-replicated, was that people are relatively quicker at tasks (meaning they have shorter response times) when they are asked to identify a set such as “black face or negative word” or “white face or positive word” than to identify a set like “black face or positive word” or “white face or negative word.” They find that this occurs even among people who report no conscious negative feelings towards black people, and they interpret this as revealing an unconscious association between black people and negative feelings, and they argue that this association could affect your decision-making without you being aware of it.\nMany other measures of automatic responses have been popular at different times: hypnosis and word association (Freud used both of these before moving to talking therapy); Rorsach blots (AKA inkblot tests); thematic apperception test (interpret an ambiguous drawing, still widely used); lie detectors AKA polygraphs (they measure autonomic responses - blood pressure, pulse, respiration, and skin conductivity - as you are asked different questions).\nUnfortunately a great deal of this research turns out to be both hard to replicate, and reliant on strong assumptions in order to interpret as surfacing unconscious associations. Newell and Shanks (2014) give strong arguments for both of these points, covering many of the methods I mentioned here.\nIt is worth mentioning that, although Freud’s more elaborate theories died off, his idea that psychosomatic illness is an indirect expression of a psychological stress, especially about something shameful, I believe remains one of the standard theories of modern neurology (O’Sullivan, 2015).\n\nHowever even if we had solid evidence for unconscious influences on involuntary responses, this still stops short of unconscious influences on decision-making. It’s possible that our associations show up in sweating, response time, and dreams, but have little effect on decision-making, and if that’s so then unconscious associations are not terribly important for social science. Most of the authors in this literature have assumed that the unconscious factors they identify affect real decisions but have left that extrapolation untested. Blanton et al. (2009) say that there’s no persuasive evidence that implicit racial bias, as measured by the IAT, predicts peoples’ decision-making, once you control for measures of explicit racial bias, i.e. when you just ask people how they feel about black people. (Singal (2016) has a long discussion on this point)."
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html#ability-to-describe-the-influence",
    "href": "posts/2017-12-10-unconscious-influences.html#ability-to-describe-the-influence",
    "title": "On Unconscious Influences (Part 1)",
    "section": "Ability to Describe the Influence",
    "text": "Ability to Describe the Influence\nA second type of evidence is to compare self-reported influences on behaviour with actual influences on behaviour. Here are some examples:\n\nIn the mid 20th-century behaviourists found that they could shape their subjects choices through conditioning with rewards and punishments, and the subjects seemed to remain ignorant of this shaping. For example if you say ‘mm-hmm’ whenever someone uses a plural noun, then after a while that person ends up using plural nouns more often, apparently unaware of the influence (Thorndike and Rock (1934); Greenspoon (1955)).\nSince the 1970s social psychologists have published all sorts of experiments in which they vary an apparently irrelevant factor and find that this can affect peoples’ decision-making. Nisbett and Wilson (1977) summarize a lot of experiments and say “subjects are sometimes (a) unaware of the existence of a stimulus that importantly influenced a response, (b) unaware of the existence of the response, and (c) unaware that the stimulus has affected the response.”\nAnother paradigm from the 1970s asks people to make a judgement - e.g. which stock to pick - and also to rate the importance of factors which contributed to their decision. Slovic et al. (1972) find a low correlation (0.34) between the ratings that a stockbrokers put on factors, and the actual influence of these factors on their decisions. There is a small literature with similar findings across a variety of tasks.\nFinally, since the 1970s a smaller group of psychologists have been running experiments in which people learn a complicated pattern, and then are asked about their insight into it. E.g. in Arthur Reber’s “artificial grammar” experiments subjects learn, through trial and error, to discriminate between two categories of words. After some time they become very good at the task, but when asked to explain how they are making decisions they often say they don’t know, or they come up with rules that do not match their actual performance.\n\nAs in the previous category, a lot of this evidence is very fragile: either hard to replicate, or based on delicate interpretations of what is happening in the experiment. Newell and Shanks (2014) again give a good summary.\nAn additional problem is that these findings could reflect knowledge being difficult to articulate, without it being unconscious. And this literature is full of reversals which bear this out: when experiments are repeated it has often turned out that the subjects do report awareness of the pattern that they have learned if they are asked the question in a different way. Mitchell et al. (2009) say “[i]t is very difficult to provide a satisfactory demonstration of unaware conditioning simply by showing conditioning in the absence of awareness. This is because it is very difficult to be sure that the awareness measure and the conditioning measure are equally sensitive.”\n\n\n\n\n\nIIES"
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html#isolation-of-unconscious-influences",
    "href": "posts/2017-12-10-unconscious-influences.html#isolation-of-unconscious-influences",
    "title": "On Unconscious Influences (Part 1)",
    "section": "Isolation of Unconscious Influences",
    "text": "Isolation of Unconscious Influences\nFinally there’s a third type of evidence which is more strictly behavioural: an unconscious factor is one which is isolated from your other conscious beliefs and desires – i.e. it does not interact with conscious factors – and that isolation will be reflected in your behaviour. This isolation criterion has been given various names, but I don’t think it’s ever been explained as clearly as it could be.\n\nTo be precise think of the blind man (the conscious part of the brain) and the guide dog (the unconscious). The guide dog can know something – e.g. she knows when the crossing light is flashing – which the man does not know, and her knowledge will influence the man’s decisions through her recommendation of when to cross. However the guide dog’s knowledge is isolated from the man’s knowledge: it only influences his decisions through the narrow channel of pulling on the leash. Suppose you tell the man that the crossing lights are not working properly, and so whatever color they show is entirely at random and uninformative. The man and dog, considered as a system, has two pieces of information: (1) the light is green (i.e. indicating ready to cross); and (2) the color is uninformative. However the two pieces of information are known by different actors, implying that they will not be integrated, because neither the man or dog knows both. This will be reflected in the man’s behaviour: he will be influenced by the guide-dog’s recommendation, because the dog sees other things in addition to the crossing-light, such as oncoming traffic. And so the man’s behaviour will still be influenced by the color of the light, even though he knows that the color is irrelevant.\nIf information is separated in the brain, we ought to see characteristic patterns of that in behavior. I know of just a few cases where the isolation of knowledge has come up clearly in trying to define or measure unconscious influences.\n\nStich (1978) said that certain mental states are “inferentially unintegrated”: \n\n“[unconscious beliefs are] largely inferentially isolated from the large body of inferentially integrated beliefs to which a subject has access”\n\nHe gives an example: suppose Noam Chomsky has a theory of grammar, and that there exists some grammatical rule which is a counterexample to that theory. If a linguist knows that rule consciously, then the linguist will immediately infer that Chomsky’s theory is false. But if the linguist only knows the rule unconsciously, then they won’t be able to make that inference, because the knowledge is “inferentially unintegrated” – i.e. the knowledge is isolated from the knowledge regarding Chomsky’s theory. 1\nA separate place where this separation has come up is in the work of Zenon Pylyshyn and Jerry Fodor since the 1980s regarding perception being “cognitively impenetrable,” or “informationally encapsulated.” They mean that perceptual processes often make inferences without taking into account all the information that is available, i.e. by drawing only on a subset of information. Their principal argument was from perceptual illusions: they argue that illusions can typically be understood as rational inferences from a subset of the information available. Helmholtz had a nice example: if you close one eye and press with your finger on the edge of your eyelid then you’ll perceive a point of light, but the light will be coming from the opposite side of your field of vision from where your finger is. This is because the left side of your retina receives light from the right side of your visual field and vice versa. So when your retina receives some stimulation on the left-hand side your brain makes infers that light is coming from the right-hand side. This is a sensible inference given only the information that your eye has, i.e. just the information from the retina. In this case there is additional information - the fact your finger is pressing on your eyelid - which should give a different interpretation to the stimulation, but your visual cortex is not wired up to incorporate that information, and so it misinterpret the signals it receives.\nThe Helmholtz-Fodor-Pylyshyn model of encapsulated inference isn’t quite the same as the case of the blind man and the guide dog. In their examples the pre-conscious process have a strict subset of the information available to the conscious brain. In other words the man isn’t blind, it’s just a case where the dog leads in a different direction than the man would. Fodor (1983) does have a brief discussion on whether early perceptual processes have access to information not available to the conscious brain, which would imply unconscious influences, in my sense.\nFinally the isolation argument has appeared in the literature on human “associative learning,” in testing whether or not the associations that we learn through conditioning are conscious. A typical experiment involves ringing a bell and then giving subjects a small electric shock. After a while people learn to flinch when they hear the bell. For a long time psychologists tried to map out the logic of how such associations would form, trying to figure out the rule which governed learning of associations. However in the last few decades an argument has been made that these learned associations are not in fact mechanical - there is no simple rule - instead they are more-or-less optimal responses to the environment based on the entirety of the information available, i.e. they are not isolated from other knowledge, though the argument isn’t usually put in terms of conscious vs unconscious knowledge. For example Colgan (1970) told subjects, after they learned an association, that the association is no longer valid (“from now on the bell will not signal an electric shock”) and he found that, although this didn’t entirely extinguish the flinching, it did cause it to markedly decrease. This implies the flinching is not isolated from your conscious knowledge: the association, at least to some degree, interacts with more abstract knowledge. There are many other circumstances where rule-based theories of association-learning have foundered because it turns out that peoples’ responses respond to outside considerations. De Houwer, Vandorpe and Beckers (2005) summarize the evidence against associative models (which can be interpreted as models with unconscious knowledge):\n\nThe two types of models can be differentiated … by manipulating variables that influence the likelihood that people will reason in a certain manner but that should have no impact on the operation of the associative model. We have seen that such variables (e.g., instructions, secondary tasks, ceiling effects, nature of the cues and outcomes) do indeed have a huge effect. Given these results, it is justified to entertain the belief that participants are using controlled processes such as reasoning and to look for new ways to model and understand these processes.\n\nMitchell says:\n\n“The results consistently show evidence for skin conductance [effects] only in participants who are aware of the [relationship] … [a]lthough there are many papers arguing for unaware conditioning, close inspection reveals, in almost all cases, that the measure of conditioning was most likely more sensitive than that of awareness.”\n\nIn retrospect a lot of behavior that was studied in the lab, which was thought to be telling us about the wiring of the animals, actually was telling us about the world outside the animal, because it has turned out that the animals’ response is the optimal response to the typical circumstances it faces in the world. (See my other post The Repeated Failure of Laws of Behaviour , and also Mitchell et al. (2009) section 4.3)\nIf this line of thought were entirely correct – if all information was integrated and fed into every decision – then there would be no unconscious influences in my sense. However I do think that there’s plenty of evidence that remains for a lack of integratation between cognitive processes.\nIn Part 2 of this essay I will give a more formal statement of how decisions can reveal unconscious knowledge (and unconscious motivations), and a survey what I think is the strength of the evidence.\n\n\n\n\nCaltech"
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html#footnotes",
    "href": "posts/2017-12-10-unconscious-influences.html#footnotes",
    "title": "On Unconscious Influences (Part 1)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nQuine says you shouldn’t call this type of thing unconscious knowledge – your linguistic practice may obey some rule, but you can’t say that you unconsciously know that rule, because there are infinitely many different rules that would imply that pattern of behavior. But this skeptical objection is too tough: Quine would deny that a cow can have a belief about where a water trough is, & instead admit only that the cow’s behavior is consistent with a particular belief among infinitely many others.↩︎"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "tecunningham.github.io",
    "section": "",
    "text": "The Influence of AI on Content Moderation and Communication\n\n\n\n\n\n\n\n\n\n\n\n\nJun 6, 2023\n\n\n\n\n\n\n  \n\n\n\n\nRanking by Engagement\n\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\nTom Cunningham\n\n\n\n\n\n\n  \n\n\n\n\nSocial Media Suspensions of Prominent Accounts\n\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2023\n\n\nTom Cunningham\n\n\n\n\n\n\n  \n\n\n\n\nOptimal Coronavirus Policy Should be Front-Loaded\n\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2020\n\n\n\n\n\n\n  \n\n\n\n\nOn Unconscious Influences (Part 1)\n\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2017\n\n\n\n\n\n\n  \n\n\n\n\nThe Work of Art in the Age of Mechanical Production\n\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2017\n\n\n\n\n\n\n  \n\n\n\n\nRepulsion from the Prior\n\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2017\n\n\n\n\n\n\n  \n\n\n\n\nThe Repeated Failure of Laws of Behaviour\n\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2017\n\n\n\n\n\n\n  \n\n\n\n\nSamuelson & Expected Utility\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2017\n\n\n\n\n\n\n  \n\n\n\n\nEconomist Explorers\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2017\n\n\n\n\n\n\n  \n\n\n\n\nWeber’s Law Doesn’t Imply Concave Representations or Concave Judgments\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2017\n\n\n\n\n\n\n  \n\n\n\n\nRelative Thinking\n\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2016\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html",
    "href": "posts/2023-01-31-social-media-suspensions-data.html",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "",
    "text": "Tom Cunningham. (@testingham) First version Jan 31 2023, last updated April 12 2023.11 Thanks to comments from Sahar Massachi, Katie Harbath, Nichole Sessego, and many others.\nThis note describes the suspension practices of the major social media platforms. I have collected a dataset of around 200 suspensions of prominent people across 12 platforms, stored in a google spreadsheet. The chart below summarizes the full dataset:\nThe data helps illuminate what platforms are doing. It is very difficult for an outside observer to see how a platform moderates their content. The advantages of studying the suspension of prominent users are that (1) the data is public and (2) the outcomes are comparable across platforms.\nKey findings.\nI am working on a separate essay about why platforms suspend users. It is difficult to give clear reasons why platforms suspend users. In a separate essay I try to break down how much their action can be attributed to influence from owners, from employees, from users, from advertisers, or from governments. Having this dataset of suspensions is very useful to be able to make generalizations about platform behavior."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#politicians",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#politicians",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Politicians",
    "text": "Politicians\nAmong US Federal politicians only Republicans have been suspended. In the US 8 Republicans have had one or more suspension, but no Democrats. Among the Republicans the suspensions were for a variety of reasons: related to the Jan 6 riots (Trump, Barry Moore, MTG), related to COVID (Ron Johnson, Rand Paul, MTG), for misgendering (Jim Banks), for tweeting a threat (Briscoe Cain), for animal blood on a profile photo (Steve Daines), one by a rogue employee (Trump).\nIt seems to me that the asymmetry in suspensions is primarily due to Republicans being more likely to violate the policies, rather than asymmetric enforcement of existing policies. I am not aware of any cases where a Democratic politician violated one of these policies but was not suspended.\n\n\n\n\n\nSuspension of national politicians outside the US has been relatively rare. My dataset contains 13 national politicians who were suspended in the world outside the US, compared to 8 in the US. This is a big asymmetry, and something of a puzzle. I have discussed this with a number of people who worked in enforcement and they attribute to a mixture of (1) less policy-violating behaviour from non-US politicians; (2) looser enforcement against non-US politicians; (3) lower overall social media usage outside the US; and (4) lower coverage of non-US politicians in my dataset.\n\n\nScale for x is already present.\nAdding another scale for x, which will replace the existing scale.\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#us-prominent-figures",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#us-prominent-figures",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "US Prominent Figures",
    "text": "US Prominent Figures\nThis shows all suspensions of US “notable people”:\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`).\n\n\n\n\n\nBetween 2015 and 2017 there were a series of alt-right personalities suspended from Twitter. The suspensions were often not for their views but their behaviour:\n\n2015: Charles Johnson from Twitter for a threat.\n2016: Milo Yiannopoulos from Twitter for harassment, Richard Spencer from Twitter for manipulation.\n2017: Roger Stone from Twitter for abuse.\n2018: Alex Jones from Twitter for incitement and abuse.\n\nBeginning in late 2017 more alt-right accounts were suspended. Either for hate speech, for offline behaviour, or without any public reason given:\n\nLate 2017: Baked Alaska from Twitter for hate speech.\n2018: Owen Benjamin from Twit with no reason given, Alex Jones from FB and YouTube for hate speech.\n2019: Nick Fuentes from Meta with no reason given.\n\nBetween November 2020 and January 2021 a large set of prominent figures were suspended for election-related reasons. The most suspensions were on Twitter but there were also from other platforms.\n\nSince November 2022 Twitter has unsuspended a large fraction of the suspended users that I track, probably around 1/2.\nSome people have been suspended simultaneously across multiple platforms (e.g. Alex Jones)"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#meta",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#meta",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Meta",
    "text": "Meta\n\n\nWarning: Removed 2 rows containing missing values (`geom_point()`)."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#twitter",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#twitter",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Twitter",
    "text": "Twitter\n\n\nWarning: Removed 1 rows containing missing values (`geom_point()`).\n\n\nWarning in grid.Call.graphics(C_points, x$x, x$y, x$pch, x$size): unimplemented\npch value '28'\n\nWarning in grid.Call.graphics(C_points, x$x, x$y, x$pch, x$size): unimplemented\npch value '28'\n\nWarning in grid.Call.graphics(C_points, x$x, x$y, x$pch, x$size): unimplemented\npch value '28'\n\n\nWarning in grid.Call.graphics(C_points, x$x, x$y, x$pch, x$size): unimplemented\npch value '29'\n\n\nWarning in grid.Call.graphics(C_points, x$x, x$y, x$pch, x$size): unimplemented\npch value '27'\n\n\nWarning in grid.Call.graphics(C_points, x$x, x$y, x$pch, x$size): unimplemented\npch value '29'\n\nWarning in grid.Call.graphics(C_points, x$x, x$y, x$pch, x$size): unimplemented\npch value '29'\n\n\nWarning in grid.Call.graphics(C_points, x$x, x$y, x$pch, x$size): unimplemented\npch value '28'\n\n\nWarning in grid.Call.graphics(C_points, x$x, x$y, x$pch, x$size): unimplemented\npch value '29'\n\n\nWarning in grid.Call.graphics(C_points, x$x, x$y, x$pch, x$size): unimplemented\npch value '26'\n\nWarning in grid.Call.graphics(C_points, x$x, x$y, x$pch, x$size): unimplemented\npch value '26'\n\n\nWarning in grid.Call.graphics(C_points, x$x, x$y, x$pch, x$size): unimplemented\npch value '27'\n\n\nWarning in grid.Call.graphics(C_points, x$x, x$y, x$pch, x$size): unimplemented\npch value '28'\n\n\nWarning in grid.Call.graphics(C_points, x$x, x$y, x$pch, x$size): unimplemented\npch value '29'\n\n\n\n\n\nThe following chart shows just accounts that were un-suspended under Musk, i.e. people with Twitter suspension that started before Oct 27 2022 and ended after that date. See below for a more fine-grained dataset of accounts unsuspended under Musk.\nYou can see that the primary original reasons for suspension were hate speech COVID misinformation. Kanye West and Nick Fuentes were re-suspended under Musk."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#youtube",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#youtube",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "YouTube",
    "text": "YouTube"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#tik-tok",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#tik-tok",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Tik Tok",
    "text": "Tik Tok"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Other Platforms",
    "text": "Other Platforms"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#hate-speech",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#hate-speech",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Hate Speech",
    "text": "Hate Speech"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#twitter-1",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#twitter-1",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Twitter",
    "text": "Twitter\nWikipedia page on Twitter Suspensions. Wikipedia has a list of around 400 Twitter suspensions. I chose not to create my own database (partly drawing from Wikipedia) for a few reasons: (1) I would want to add a lot of annotations to the Wikipedia data, e.g. about reasons for suspension or types of suspension. (2) Parsing the data is nontrivial: date ranges are given in various formats and would require some work on a regex to parse consistently. (3) There is some missing and inconsistent data, e.g. it has Trump’s suspension start-date but not end-date, and the names of people are not consistent (e.g. sometimes “Donald Trump”, sometimes “Donald J Trump”).\nThe Wikipedia dataset shows a similar basic pattern to what I document above: a dramatic increase in the rate of suspensions around mid-2017\n\n\nWarning: There were 2 warnings in `mutate()`.\nThe first warning was:\nℹ In argument: `followers = as.numeric(...)`.\nCaused by warning:\n! NAs introduced by coercion\nℹ Run `dplyr::last_dplyr_warnings()` to see the 1 remaining warning.\n\n\nWarning: Removed 14 rows containing non-finite values (`stat_bin()`).\n\n\n\n\n\nWikipedia-reported Twitter suspension by year\n\n\n\n\n\n\n\n\n\nAll Wikipedia-reported Twitter suspension, highlighting accounts with more than 1M followers (not all suspensions list the number of followers).\n\n\n\n\nTravis Brown: Twitter Watch This project appears to have data on almost all suspensions on Twitter since Feb 2022, and also tracks whether the suspension have been reversed. It does not include any suspensions which started prior to Feb 2022. There is a giant CSV file with 600K rows, suspensions.csv. Some visualizations:\n\n\nWarning: `label_number_si()` was deprecated in scales 1.2.0.\nℹ Please use the `scale_cut` argument of `label_number()` instead.\n\n\n\n\n\nObservations by date of suspension\n\n\n\n\n\n\nWarning: Removed 1 rows containing missing values (`position_stack()`).\n\n\n\n\n\nObservation by date of unsuspension\n\n\n\n\n\n\n\n\n\nObservation by date of account creation\n\n\n\n\n\n\n\n\n\nSuspensions for accounts with &gt;1M followers\n\n\n\n\nTravis Brown: Twitter Unsuspensions. This is a collection of users who Twitter has un-suspended since Oct 27 2022 (when Musk took over). For some accounts there is a date of suspension but some have missing dates, I think suspension-date is only observed if after Feb 2022. (The content of this dataset is neither a subset nor a superset of the previous daatset). Unfortunately the dataset doesn’t have follower-count or twitter handle, so it’s not easy to join with other datasets or find the most prominent accounts.\n\n\nWarning: Removed 1 rows containing missing values (`position_stack()`).\n\n\n\n\n\nObservations by date of suspension\n\n\n\n\n\n\n\n\n\nObservations by date of unsuspension\n\n\n\n\nTravis Brown: Deleted Tweets / Suspended Accounts. This project scrapes profiles from the Wayback Machine, and seems to have a large set of accounts that were suspended with fairly long retention, I have not yet investigated further.\nTwitter Transparency Reports. This has data on the aggregate number of suspensions per half between July 2018 and Dec 2021. Note that the website is down but the CSV files can still be downloaded. \n\n\nRows: 87 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr  (3): Rule name, Accounts suspended, Content removed\ndbl  (1): Accounts actioned\ndate (1): Time period start\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n`summarise()` has grouped output by 'start'. You can override using the `.groups` argument.\n\n\nWarning: `position_stack()` requires non-overlapping x intervals\n`position_stack()` requires non-overlapping x intervals\n`position_stack()` requires non-overlapping x intervals\n`position_stack()` requires non-overlapping x intervals\n`position_stack()` requires non-overlapping x intervals\n`position_stack()` requires non-overlapping x intervals\n`position_stack()` requires non-overlapping x intervals\n`position_stack()` requires non-overlapping x intervals\n\n\n\n\n\nTotal Accounts Suspended on Twitter by Reason, 2018H2-2021H2\n\n\n\n\nCounterHate list of unsuspensions. The organization CounterHate has a list of 10 large accounts reinstated by Twitter since Musk’s takeover. Note I believe they incorrectly listed Rizza Islam as an account re-activated by Twitter: I can find no evidence that the acccount @RizzaIslam was ever suspended, it seems to have been continuously tweeting from November 2022 through Feb 2023. I have added all 10 accounts to my database, and checked activity across all platforms."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#youtube-1",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#youtube-1",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "YouTube",
    "text": "YouTube\nWikipedia page on YouTube suspensions. See above for reasons why I chose not to use this dataset as the primary source.\nWikitubia: Terminated YouTubers. A list of around 2300 YouTubers that have been permanently banned, including date of ban, subscribers, reason for ban, and citation. They don’t have a date when unbanned."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#meta-facebook-instagram",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#meta-facebook-instagram",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Meta / Facebook / Instagram",
    "text": "Meta / Facebook / Instagram\nThere is no Wikipedia page of suspensions on Facebook, Instagram or WhatsApp.\nMeta’s “Community Standards Enforcement Report” is shown below. Meta’s data does not include any data on account suspensions, however there are a few other patterns of interest.\n\nContent actioned is relatively stable. There are fairly few notable upward or downward trends across the different types of content actioned: terrorism content actioned has increased significantly on both platforms, hate speech actions increased up to the end of 2020, then declined.\nThe proactive detection rate is close to 100% for most categories. there were dramatic improvements for bullying and for hate speech over 2017-2021. Note that the proactive detection rate is the share of actioned content that is automatically detected, the share of true positives that are automatically detected is surely much lower.\nThe prevalence of volations has fallen significantly. The log axis diminishes the magnitude of the decline: prevalence has fallen by a factor of 2-5 for nudity, bullying, hate speech, and graphic content. (I only show the prevalence upper bound, but the lower bound generally tracks the same course).\n\n\n\nRows: 2780 Columns: 5\n── Column specification ────────────────────────────────────────────────────────\nDelimiter: \",\"\nchr (5): app, policy_area, metric, period, value\n\nℹ Use `spec()` to retrieve the full column specification for this data.\nℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `value_numeric = as.numeric(...)`.\nCaused by warning:\n! NAs introduced by coercion\n\n\nWarning: Removed 300 rows containing missing values (`geom_point()`).\n\n\nWarning: Removed 8 rows containing missing values (`geom_line()`).\n\n\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n`geom_line()`: Each group consists of only one observation.\nℹ Do you need to adjust the group aesthetic?\n\n\n\n\n\nFacebook’s dangerous organizations list. This list was leaked in 2021 by the Intercept. Unfortunately it does not include the dates of when each organization was added. The list is organized into the following categories:\n\nTerror Organizations (e.g. Islamic State)\nCrime Organizations (e.g. Bloods, Crips)\nHate Organizations (e.g. Aryan Nation, includes bands and websites)\nMilitarized Social Movements (e.g. United States Patrio Defense Force)\nViolent Non-State Actors (e.g. Free Syrian Army)\nHate (e.g. David Duke)\nIndividuals: Crime (e.g. Denton Suggs, Gangster Disciples)\nIndividuals: Terror (e.g. Osama bin Laden)"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#tiktok",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#tiktok",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "TikTok",
    "text": "TikTok\nCommunity Standards Report. Shows an increase in suspensions from around 1M accounts/quarter per 2020 to 6M accounts/quarter in 2023."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#twitch",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#twitch",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Twitch",
    "text": "Twitch\nStreamerBans. They seem to have a pretty comprehensive database of bans on Twitch."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms-1",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms-1",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Other Platforms",
    "text": "Other Platforms\n\nSpotify. The only unambiguous suspension from Spotify I found was Alex Jones’ podcast. Spotify removed some episodes of Joe Rogan’s podcast, and removed R Kelly and XXXtentacion’s music from playlists. They remove some white-supremacist artists and music. They removed all music from the band LostProphets after their lead singer was convicted of child sexual abuse.\nSubstack. I’m not aware of anybody who’s been kicked off Substack, they present themselves as very pro-free-speech.\nReddit. I’m not aware of any data on reddit account suspensions.\nRumble. The Rumble video-hosting platform has become quite large (they claim 70M MAU, and have a market cap of ). Their terms of service restrict content that is “abusive, inciting violence, harassing, harmful, hateful, anti-semitic, racist or threatening.” However I have not yet found a single example of a prominent user who has been suspended from Rumble."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#other-data-sources",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#other-data-sources",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Other Data Sources",
    "text": "Other Data Sources\nCan find more suspensions by searching Wikipedia for “suspended from XXX”. E.g. site:wikipedia.org \"suspended from facebook\". Possibly worth doing the same search for Google News.\nSocialBlade has data on number of followers by month since 2018, across Twitter, FB, YouTube. I’m not sure how easy it would be to scrape this data. They have a paid API, they say “up to 3 years of Historical statistics on creators.” However the website seems to have data back to at least April 2018.\nBallotpedia list of elected officials suspended from social media. It is an excellent resource, appears comprehensive and cites original reporting. I have added all of their data to the database as of January 2023.\nGlobal Internet Forum to Counter Terrorism (GIFCT). They mainly work on sharing hashes of terrorist content between platforms. They have some dicussion papers about “terror designation lists” but I don’t think they maintain any lists themselves.\nSpecially Designated National / Global Terrorist (SDN/SDGT). This is a public list maintained by the US government, and consumed by a number of tech companies. The full history is available, but it would be extremely difficult to parse.\nLumen. This has an international database of government takedown requests. They also seem to include whether the request was honored.\nCCDH Disinformation Dozen. This is a list from March 2021 of prominent accounts who were spreading anti-vax information on social media: original report, followup report from April 2021). They also have a “toxic ten” report. It’s probably worth adding both lists to the database."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#third-party-sources-on-platform-policies",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#third-party-sources-on-platform-policies",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Third-Party Sources on Platform Policies",
    "text": "Third-Party Sources on Platform Policies\nThere are a variety of third-party resources comparing policies across platforms, however none seem to have data comparable to the list above, i.e. a summary of specific content policy changes over time.\nComparisons at a single point in time.\n\nDNC (2020) Comparison of Misinformation Policies, 2020\nConsumer Reports (Aug 13 2020) Comparison of Misinformation Policies, 2020. Data as of 2020, with three levels: “allowed”, “sometimes”, and “prohibited”.\nUNC CITAP (May 22 2020) Comparison of Misinformation Policies, 2020. Has tables comparing misinfo policies as of 2020, three levels: “prohibited”, “flagged”, “allowed.”\nElection Integrity Partnership (Oct 28 2020) Comparison of Election Policies, 2020.\nCarnegie Endowment (April 1 2021) Existence of Policies, 2021. Table just marks whether a platform has a policy on some type of content, not nature of policy. They also they have a database of platform policies but it seems to only have data from February 2021.\nVirality Project Comparison of COVID Vaccine Policies in 2021.\n\nPolicies tracked over time.\n\nKatie Harbath and Collier Fernenkes (August 2022) Election Policy Announcements, 2003-2022. Google spreadsheet with links to around 600 policy announcements, organized by platform, author, date, product-type, and country. Focussed on election-related policies, and they don’t include summaries of the policy announcement. They also wrote up analyses: (1) “A Brief History of Tech and Elections”; (2) 2022 election announcements.\nRanking Digital Rights Index, Comparison of Privacy and Transparency Policies, 2017-2022. They collect perhaps 100 different indicators across around 15 tech companies, mostly related to privacy and transparency, earliest data from 2017. All the data is available.\nGLAAD Comparison of LGBTQ user safety, 2021-2022\nCELE, Letra Chica. Tracks all public policy changes on Meta, YouTube, and Twitter. Most data from May 2020, but they go back to 2019 for Facebook by using FB’s Transparency Center. Each policy update includes a short summary of what’s changed. Tracks both Spanish and English versions. Data stored on coda.io, I think it’s queryable.\nLinterna Verdes, Circuito. Has about 15 in-depth case studies of platform moderation decisions.\nHumboldt Institute, Platform Governance Archive. Comprehensive archive of ToS, Privacy Policy, and Community Guidlines, from 2004 until late 2021, for FB, IG, Twitter, and YouTube. The data will not be updated.\nOpen Terms Archive. Started by the French Ambassador for Digital Affairs, but now a collaboration. Tracks terms for many different online services in a github repo. The Platform Governance Archive has moved to be part of this project, here.\nEFF, TOSback. Database of historical ToS documents from different services, with cross-platform comparisons. The most recent updates seem to be from May 2021, possibly was succeeded by Open Terms Archive.\nEuropean Commission, Copyright Content Moderation and Removal. This PDF report includes a lot of work which maps the copyright policies of major platforms.\n\nNarrative histories:\n\nCatherine Buni and Soraya Chemaly (2016, the Verge) History of Moderation.\nSarah Jeong (2016, Vice) The History of Twitter’s Rules\nBergen (2022) Like, Comment, Subscribe. A book on the history of YouTube, it has a lot of detail on policy changes."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-film-and-television",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-film-and-television",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Exclusions in Film and Television",
    "text": "Exclusions in Film and Television\n\n\n\n\n\n\n\n\n\n\n\nalleged crime\nresult\n\n\n\n\n1920s\nFatty Arbuckle\nrumors of immorality\nfilm industry blacklisted\n\n\n\n\n\n\n\n\n1940s\nOrson Welles\ncommunist associations\nblacklisted, moved to Switzerland\n\n\n\nDalton Trumbo\ncommunist associations\nblacklisted\n\n\n\n(around 100 people)\ncommunist associations\nblacklisted for a decade\n\n\n\n\n\n\n\n\n1950s\nCharlie Chaplin\ncommunist associations\nbanned from US\n\n\n\nElia Kazan\ntestifying before HUAC\nlost some relationships in Hollywood\n\n\n\n\n\n\n\n\n1960s\nJane Fonda\nopposition to Vietnam war\nblacklisted\n\n\n\n\n\n\n\n\n1970s\nRoman Polanski\nrape of 13yo girl\nmild disapproval from Hollywood\n\n\n\n\n\n\n\n\n1990s\nO J Simpson\nmurdered his wife\nblacklisted\n\n\n\nWoody Allen\nmolested 7yo daughter\n\n\n\n\n\n\n\n\n\n2000s\nMel Gibson\nracism & anti-semitism\n“blacklisted in Hollywood for almost a decade”\n\n\n\nMira Sorvino\nrejecting Harvey Weinstein\nblacklisted\n\n\n\nRose McGowan\nrejecting Harvey Weinstein\nblacklisted\n\n\n\nIsaiah Washington\nhomophobic remarks\nblacklisted\n\n\n\nMichael Richards\nracist remarks\nblacklisted\n\n\n\nKathy Griffin\n“told Jesus to suck it”\nbanned from talk shows and TV appearances\n\n\n\nSean Penn\nopposition to Iraq war\ndropped from movie\n\n\n\n\n\n\n\n\n2010s\nBill Cosby\nsexual assault\nblacklisted (& later imprisoned)\n\n\n\nHarvey Weinstein\nsexual assault\nblacklisted (& later imprisoned)\n\n\n\nStacy Dash\nconservative advocacy\nblacklisted\n\n\n\nKirk Camerson\ncriticism of homosexuality\nblacklisted\n\n\n\nJames Woods\nanti-Obama tweets\nblacklisted\n\n\n\nCeeLo Green\nsexual assault\nblacklisted\n\n\n\nLouis CK\nsexual harassment\nblacklisted\n\n\n\nKathy Griffin\nphoto with head of Trump\nfired by CNN, lost endorsement, cancelled tour\n\n\n\nT J Miller\nsubstance abuse, sexual assault\nblacklisted\n\n\n\nGina Carano\npolitical social media posts\nfired from TV show\n\n\n\nKevin Spacey\nsexual harassment\nlost roles in films\n\n\n\nJussie Smollett\nlied about an attack\nlost roles in TV shows\n\n\n\nNeil deGrasse Tyson\nrape, sexual harassment\ntemporarily lost roles in TV shows\n\n\n\nRoseanne Barr\nracist tweet\nlost TV show\n\n\n\n\n\n\n\n\n2020s\nWill Smith\nslapping someone at Oscars\nfilm projects put on hold\n\n\n\nJohnny Depp\ndomestic violence\nlost roles in films\n\n\n\nAmber Heard\ninvolvement in trial w Johnny Depp\nlost roles in films\n\n\n\nJustin Roiland\nsexual harassment & abuse\nlost roles in shows"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-music",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-music",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Exclusions in Music",
    "text": "Exclusions in Music\n\n\n\n\n\n\n\n\n\n\n\nalleged crime\nresult\n\n\n\n\n1940s\nPaul Robeson\ncommunist associations\nblacklist and passport revoked\n\n\n\n\n\n\n\n\n1950s\nLeonard Bernstein\ncommunist associations\nbrief blacklist\n\n\n\nLena Horne\ncommunist associations\nblacklist\n\n\n\nPete Seeger\ncommunist associations\nblacklist\n\n\n\n\n\n\n\n\n1960s\nBeatles\nsaying they’re bigger than Jesus\nconsumer boycott\n\n\n\nLovin Spoonful\ncooperating with FBI\nmusic industry boycott\n\n\n\nNina Simone\n“Mississippi Goddam”\nboycott in the South\n\n\n\nJohn Lennon\ncriticism of US and Vietnam war\nrefused entry into US\n\n\n\nEartha Kitt\ncriticism of Vietnam war\nblacklist through LBJ and CIA\n\n\n\n\n\n\n\n\n1970s\nSex Pistols\ncriticizing the Queen, swearing on TV\nbanned by the BBC, dropped by EMI\n\n\n\n\n\n\n\n\n1980s\nNWA\n“Fuck the Police” & similar songs\nradio station boycott, police boycott\n\n\n\n\n\n\n\n\n1990s\nBruce Springseen\nsong against police brutality\nbrief police boycott\n\n\n\nMarilyn Manson\ntransgressive lyrics\nbanned from performing in some states\n\n\n\nBody Count\nsong “cop killer”\nalbum withdrawn and reissued\n\n\n\n\n\n\n\n\n2000s\nDixie Chicks\nfor opposition to Iraq war\nblacklisting and consumer boycott\n\n\n\nJanet Jackson\nshowing nipple\nVH1, MTV, & Viacom radio stopped playing her music\n\n\n\nR Kelly\nsexual abuse\nbroad blacklist\n\n\n\nChris Brown\ndomestic violence\nweak boycott and blacklist\n\n\n\n\n\n\n\n\n2010s\nLostprophets\nsexual abuse\nbroad blacklist\n\n\n\nMichael Jackson\nchild molestation\nsome radio stations stop playing music\n\n\n\n\n\n\n\n\n2020s\nBeyonce\nsong against police brutality\nbrief police boycott\n\n\n\nMorgan Wallen\nusing n-word\ntemporarily dropped from radio/streaming playlists\n\n\n\nKanye West\npraise of Hitler\nlost sponsors\n\n\n\nOthers.\n\nIn radio: Father Coughlin, Rush Limbaugh, Don Imus fired from CBS for calling womens’ basketball team “nappy-headed hos”, Howard Stern fired from various radio shows for comments.\nIn sport. Colin Kapaernick blacklisted from NFL for kneeling for the anthem. Pete Rose banned from MLB for gambling.\nNazi sympathisers/collaborators. Charles Lindbergh, Henry Ford, Charles Coughlin, PG Wodehouse, Ezra Pound.\nWriters: DH Lawrence, Henry Miller, Salman Rushdie (Nicole Bonoff).\nJournalists. Jeffrey Toobin (New Yorker writer masturbated on zoom call),\nNote on R Kelly disappearing from radio"
  },
  {
    "objectID": "posts/2023-04-28-ranking-by-engagement.html",
    "href": "posts/2023-04-28-ranking-by-engagement.html",
    "title": "Ranking by Engagement",
    "section": "",
    "text": "Tom Cunningham, May 8 2023.11 tom.cunningham@gmail, @testingham. I worked at FB for 5 years, and Twitter for 1 year, now affiliated with the Integrity Institute. This note entirely based on public information. Thanks to comments from Jeff Allen, Jacquelyn Zehner, David Evan Harris, Jonathan Stray, and others.\nSix observations on ranking by engagement:\nIn an appendix I formalize the argument. I show that all these observations can be expressed as covariances between different properties of content, e.g. between the retentiveness, predicted engagement rates, and other measures of content quality. From those covariances we can derive Pareto frontiers and visualize how platforms are trading-off between different outcomes."
  },
  {
    "objectID": "posts/2023-04-28-ranking-by-engagement.html#formal-observations",
    "href": "posts/2023-04-28-ranking-by-engagement.html#formal-observations",
    "title": "Ranking by Engagement",
    "section": "Formal Observations",
    "text": "Formal Observations\nHere I describe a few formal properties of a model of ranking based on a joint-normal distribution of attributes. I have a longer writeup with proofs of these results which I hope to publish soon, I am happy to share a draft on request.\n\nThe covariance between item attributes will determine a Pareto frontier among outcomes. Suppose we know the joint distribution of attributes and we can choose a subset with share \\(p\\) of the distribution (e.g. a fixed number of impressions given a pool of possible stories to show), and we want to calculate the average value of each attribute in the subset of content shown to the user. Then we can describe the Pareto frontier over subsets, i.e. the set of realized average outcomes, and it will be a function of the covariances among attributes over pieces of content. With 2 attributes the Pareto frontier will be an ellipse with shape exactly equal to an isoprobability curve from the joint density.\nThe shape of the ellipse has a simple interpretation. If two attributes are positively correlated then the Pareto frontier will be tight meaning there is little tradeoff, i.e. we will have similar aggregate outcomes independent of the relative weights put on each outcome in ranking. If instead two attributes are negatively correlated then the Pareto frontier will be loose meaning outcomes will vary a lot with the relative weights used in ranking.\nOur assumption that the share \\(p\\) is fixed is equivalent to assuming that any ranking rule will get the same number of impressions. This assumption obviously has some tension with retentiveness being an outcome variable: if some ranking rule has low retentiveness, then we would expect lower impressions. Accounting for this would make the Pareto frontier significantly more complicated to model, for simplicity we can interpret every attribute except retentiveness as a short-run outcome. Alternatively we could interpret them as relative instead of absolute outcomes, e.g. as engagement/impression or engagement/DAU.\nImproving a classifiers will stretch the Pareto frontier. As a classifier gets better the average prediction will stay the same but the variance will increase, meaning the Pareto frontier will stretch out, and given a linear indifference curve we can derive the effect on outcomes.\nThe joint distribution plus utility weights will determine ranking weights. If we observe only some outcomes then we can calculate the conditional expectation for other outcomes. Typically we want to know retentiveness, and we can write the conditional expectation as follows: \\[E[\\text{retentiveness}|\n   \\text{engagement},\\ldots,\\text{user preference}].\\] This expectation has a closed-form solution when the covariance matrix is joint normal. When we have just two signals, for example engagement and quality, we can write:\n\\[\\begin{aligned}\n   E[r|e,q] &= \\frac{1}{1-\\gamma^2}(\\rho_e-\\gamma\\rho_q)e +\n               \\frac{1}{1-\\gamma^2}(\\rho_q-\\gamma\\rho_e)q\\\\\n   r     &= \\text{retentiveness}\\\\\n   e     &= \\text{engagement (predicted)}\\\\\n   q     &= \\text{quality (predicted)}\\\\\n   \\rho_{e}     &= \\text{covariance of engagement and retentiveness}\\\\\n   \\rho_{q}     &= \\text{covariance of quality and retentiveness}\\\\\n   \\gamma     &= \\text{covariance of engagement and quality}\n\\end{aligned}\\]\nNote that the slope of the iso-retentiveness line in \\((e,q)\\)-space will be \\(-\\frac{\\rho_e-\\gamma\\rho_q}{\\rho_q-\\gamma\\rho_e}\\).\nExperiments which vary ranking weights tell us about covariances. We can write findings from experiments as follows. First, suppose we find that retention is higher when ranked by engagement than when unranked, this can be written:\n\\[\\begin{aligned}\n      \\utt{E[r|e&gt;e^*]}{ranked by}{engagement} &&gt; \\ut{E[r]}{unranked}\n   \\end{aligned}\\]\nHere \\(e^*\\) is chosen such that \\(P(e&gt;e^*)=p\\) for some \\(p\\), representing the share of potential inventory that the user consumes. This implies that engagement must positively correlate with retentiveness, \\(\\rho_e&gt;0\\).\nNext we can express that retention is higher when we put some weight \\(\\beta\\) on quality:\n\\[\\begin{aligned}\n   \\utt{E[r|e+\\beta q&gt;\\kappa^*]}{ranked by}{engagement and quality} &&gt; \\utt{E[r|e&gt;e^*]}{ranked by}{engagement}\n\\end{aligned}\\]\nHere \\(\\kappa^*\\) is chosen such that \\(P(e+\\beta q &gt; \\kappa^*)=P(e&gt;e^*)=p\\). If \\(\\beta\\) is fairly small then we can infer that the iso-retentiveness line is downward-sloping, implying: \\[\\frac{\\rho_e-\\gamma\\rho_q}{\\rho_q-\\gamma\\rho_e}&gt;0.\\]\nThis implies that both engagement and quality have the same sign. I don’t think they both can be negative, so they both must be positive:\n\\[\\begin{aligned}\n      \\rho_e - \\gamma \\rho_q &&gt; 0 \\\\\n      \\rho_q - \\gamma \\rho_e &&gt; 0.\n   \\end{aligned}\\]\nI think it’s reasonable to treat preferences as locally linear. To have a well-defined maximization problem (with an interior solution) we need either nonlinear preferences or a nonlinear Pareto frontier. It’s always easier to treat things as linear when you can, so a relevant question is which of these two is closer to linear? Internally companies often treat their preferences as nonlinear, e.g. setting specific goals and guardrails, but those are always flexible and often have justifications as incentive devices. Typical metric changes are small, only single-digit percentage points, over that range the Pareto frontier does show significant diminishing returns while (it seems to me) value to the company does not.\n\n\n\n\n\n\n# Appendix: Literature\nMilli, Pierson and Garg (2023) Choosing the Right Weights: Balancing Value, Strategy, and Noise in Recommender Systems\n\n“model in which two producers compete for the attention of one user. The recommender system ranks producers based on a linear combination of predictions of k behaviors. However, producers can strategically adapt their items to increase the probability of different user behaviors. User utility depends on being shown a high value producer, and a producer’s utility is the probability they are ranked highly minus their costs of strategic manipulation.”\n\nE.g. considering weights on like, RT, and reply: (1) which one is closer to true user preference; (2) which has higher signal-noise ratio (); (3) .\n\nTwo items, i{-1,1}, and k behaviours (e.g. like, comment). We have prediction of each, y^k. Final score is ^T. Predictions are ."
  },
  {
    "objectID": "posts quarantine/2023-01-31-social-media-suspensions-data.html",
    "href": "posts quarantine/2023-01-31-social-media-suspensions-data.html",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "",
    "text": "Tom Cunningham. (@testingham) First version Jan 31 2023, last updated April 12 2023.11 Thanks to comments from Sahar Massachi, Katie Harbath, Nichole Sessego, and many others.\nThis note describes the suspension practices of the major social media platforms. I have collected a dataset of around 200 suspensions of prominent people across 12 platforms, stored in a google spreadsheet. The chart below summarizes the full dataset:\nThe data helps illuminate what platforms are doing. It is very difficult for an outside observer to see how a platform moderates their content. The advantages of studying the suspension of prominent users are that (1) the data is public and (2) the outcomes are comparable across platforms.\nKey findings.\nI am working on a separate essay about why platforms suspend users. It is difficult to give clear reasons why platforms suspend users. In a separate essay I try to break down how much their action can be attributed to influence from owners, from employees, from users, from advertisers, or from governments. Having this dataset of suspensions is very useful to be able to make generalizations about platform behavior."
  },
  {
    "objectID": "posts quarantine/2023-01-31-social-media-suspensions-data.html#politicians",
    "href": "posts quarantine/2023-01-31-social-media-suspensions-data.html#politicians",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Politicians",
    "text": "Politicians\nAmong US Federal politicians only Republicans have been suspended. In the US 8 Republicans have had one or more suspension, but no Democrats. Among the Republicans the suspensions were for a variety of reasons: related to the Jan 6 riots (Trump, Barry Moore, MTG), related to COVID (Ron Johnson, Rand Paul, MTG), for misgendering (Jim Banks), for tweeting a threat (Briscoe Cain), for animal blood on a profile photo (Steve Daines), one by a rogue employee (Trump).\nIt seems to me that the asymmetry in suspensions is primarily due to Republicans being more likely to violate the policies, rather than asymmetric enforcement of existing policies. I am not aware of any cases where a Democratic politician violated one of these policies but was not suspended.\n\n\n\n\n\nSuspension of national politicians outside the US has been relatively rare. My dataset contains 13 national politicians who were suspended in the world outside the US, compared to 8 in the US. This is a big asymmetry, and something of a puzzle. I have discussed this with a number of people who worked in enforcement and they attribute to a mixture of (1) less policy-violating behaviour from non-US politicians; (2) looser enforcement against non-US politicians; (3) lower overall social media usage outside the US; and (4) lower coverage of non-US politicians in my dataset."
  },
  {
    "objectID": "posts quarantine/2023-01-31-social-media-suspensions-data.html#us-prominent-figures",
    "href": "posts quarantine/2023-01-31-social-media-suspensions-data.html#us-prominent-figures",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "US Prominent Figures",
    "text": "US Prominent Figures\nThis shows all suspensions of US “notable people”:\n\n\n\n\n\nBetween 2015 and 2017 there were a series of alt-right personalities suspended from Twitter. The suspensions were often not for their views but their behaviour:\n\n2015: Charles Johnson from Twitter for a threat.\n2016: Milo Yiannopoulos from Twitter for harassment, Richard Spencer from Twitter for manipulation.\n2017: Roger Stone from Twitter for abuse.\n2018: Alex Jones from Twitter for incitement and abuse.\n\nBeginning in late 2017 more alt-right accounts were suspended. Either for hate speech, for offline behaviour, or without any public reason given:\n\nLate 2017: Baked Alaska from Twitter for hate speech.\n2018: Owen Benjamin from Twit with no reason given, Alex Jones from FB and YouTube for hate speech.\n2019: Nick Fuentes from Meta with no reason given.\n\nBetween November 2020 and January 2021 a large set of prominent figures were suspended for election-related reasons. The most suspensions were on Twitter but there were also from other platforms.\n\nSince November 2022 Twitter has unsuspended a large fraction of the suspended users that I track, probably around 1/2.\nSome people have been suspended simultaneously across multiple platforms (e.g. Alex Jones)"
  },
  {
    "objectID": "posts quarantine/2023-01-31-social-media-suspensions-data.html#meta",
    "href": "posts quarantine/2023-01-31-social-media-suspensions-data.html#meta",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Meta",
    "text": "Meta"
  },
  {
    "objectID": "posts quarantine/2023-01-31-social-media-suspensions-data.html#twitter",
    "href": "posts quarantine/2023-01-31-social-media-suspensions-data.html#twitter",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Twitter",
    "text": "Twitter\n\n\n\n\n\nThe following chart shows just accounts that were un-suspended under Musk, i.e. people with Twitter suspension that started before Oct 27 2022 and ended after that date. See below for a more fine-grained dataset of accounts unsuspended under Musk.\nYou can see that the primary original reasons for suspension were hate speech COVID misinformation. Kanye West and Nick Fuentes were re-suspended under Musk."
  },
  {
    "objectID": "posts quarantine/2023-01-31-social-media-suspensions-data.html#youtube",
    "href": "posts quarantine/2023-01-31-social-media-suspensions-data.html#youtube",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "YouTube",
    "text": "YouTube"
  },
  {
    "objectID": "posts quarantine/2023-01-31-social-media-suspensions-data.html#tik-tok",
    "href": "posts quarantine/2023-01-31-social-media-suspensions-data.html#tik-tok",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Tik Tok",
    "text": "Tik Tok"
  },
  {
    "objectID": "posts quarantine/2023-01-31-social-media-suspensions-data.html#other-platforms",
    "href": "posts quarantine/2023-01-31-social-media-suspensions-data.html#other-platforms",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Other Platforms",
    "text": "Other Platforms"
  },
  {
    "objectID": "posts quarantine/2023-01-31-social-media-suspensions-data.html#hate-speech",
    "href": "posts quarantine/2023-01-31-social-media-suspensions-data.html#hate-speech",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Hate Speech",
    "text": "Hate Speech"
  },
  {
    "objectID": "posts quarantine/2023-01-31-social-media-suspensions-data.html#twitter-1",
    "href": "posts quarantine/2023-01-31-social-media-suspensions-data.html#twitter-1",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Twitter",
    "text": "Twitter\nWikipedia page on Twitter Suspensions. Wikipedia has a list of around 400 Twitter suspensions. I chose not to create my own database (partly drawing from Wikipedia) for a few reasons: (1) I would want to add a lot of annotations to the Wikipedia data, e.g. about reasons for suspension or types of suspension. (2) Parsing the data is nontrivial: date ranges are given in various formats and would require some work on a regex to parse consistently. (3) There is some missing and inconsistent data, e.g. it has Trump’s suspension start-date but not end-date, and the names of people are not consistent (e.g. sometimes “Donald Trump”, sometimes “Donald J Trump”).\nThe Wikipedia dataset shows a similar basic pattern to what I document above: a dramatic increase in the rate of suspensions around mid-2017\n\n\n\n\n\nWikipedia-reported Twitter suspension by year\n\n\n\n\n\n\n\n\n\nAll Wikipedia-reported Twitter suspension, highlighting accounts with more than 1M followers (not all suspensions list the number of followers).\n\n\n\n\nTravis Brown: Twitter Watch This project appears to have data on almost all suspensions on Twitter since Feb 2022, and also tracks whether the suspension have been reversed. It does not include any suspensions which started prior to Feb 2022. There is a giant CSV file with 600K rows, suspensions.csv. Some visualizations:\n\n\n\n\n\nObservations by date of suspension\n\n\n\n\n\n\n\n\n\nObservation by date of unsuspension\n\n\n\n\n\n\n\n\n\nObservation by date of account creation\n\n\n\n\n\n\n\n\n\nSuspensions for accounts with &gt;1M followers\n\n\n\n\nTravis Brown: Twitter Unsuspensions. This is a collection of users who Twitter has un-suspended since Oct 27 2022 (when Musk took over). For some accounts there is a date of suspension but some have missing dates, I think suspension-date is only observed if after Feb 2022. (The content of this dataset is neither a subset nor a superset of the previous daatset). Unfortunately the dataset doesn’t have follower-count or twitter handle, so it’s not easy to join with other datasets or find the most prominent accounts.\n\n\n\n\n\nObservations by date of suspension\n\n\n\n\n\n\n\n\n\nObservations by date of unsuspension\n\n\n\n\nTravis Brown: Deleted Tweets / Suspended Accounts. This project scrapes profiles from the Wayback Machine, and seems to have a large set of accounts that were suspended with fairly long retention, I have not yet investigated further.\nTwitter Transparency Reports. This has data on the aggregate number of suspensions per half between July 2018 and Dec 2021. Note that the website is down but the CSV files can still be downloaded. \n\n\n\n\n\nTotal Accounts Suspended on Twitter by Reason, 2018H2-2021H2\n\n\n\n\nCounterHate list of unsuspensions. The organization CounterHate has a list of 10 large accounts reinstated by Twitter since Musk’s takeover. Note I believe they incorrectly listed Rizza Islam as an account re-activated by Twitter: I can find no evidence that the acccount @RizzaIslam was ever suspended, it seems to have been continuously tweeting from November 2022 through Feb 2023. I have added all 10 accounts to my database, and checked activity across all platforms."
  },
  {
    "objectID": "posts quarantine/2023-01-31-social-media-suspensions-data.html#youtube-1",
    "href": "posts quarantine/2023-01-31-social-media-suspensions-data.html#youtube-1",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "YouTube",
    "text": "YouTube\nWikipedia page on YouTube suspensions. See above for reasons why I chose not to use this dataset as the primary source.\nWikitubia: Terminated YouTubers. A list of around 2300 YouTubers that have been permanently banned, including date of ban, subscribers, reason for ban, and citation. They don’t have a date when unbanned."
  },
  {
    "objectID": "posts quarantine/2023-01-31-social-media-suspensions-data.html#meta-facebook-instagram",
    "href": "posts quarantine/2023-01-31-social-media-suspensions-data.html#meta-facebook-instagram",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Meta / Facebook / Instagram",
    "text": "Meta / Facebook / Instagram\nThere is no Wikipedia page of suspensions on Facebook, Instagram or WhatsApp.\nMeta’s “Community Standards Enforcement Report” is shown below. Meta’s data does not include any data on account suspensions, however there are a few other patterns of interest.\n\nContent actioned is relatively stable. There are fairly few notable upward or downward trends across the different types of content actioned: terrorism content actioned has increased significantly on both platforms, hate speech actions increased up to the end of 2020, then declined.\nThe proactive detection rate is close to 100% for most categories. there were dramatic improvements for bullying and for hate speech over 2017-2021. Note that the proactive detection rate is the share of actioned content that is automatically detected, the share of true positives that are automatically detected is surely much lower.\nThe prevalence of volations has fallen significantly. The log axis diminishes the magnitude of the decline: prevalence has fallen by a factor of 2-5 for nudity, bullying, hate speech, and graphic content. (I only show the prevalence upper bound, but the lower bound generally tracks the same course).\n\n\n\n\n\n\nFacebook’s dangerous organizations list. This list was leaked in 2021 by the Intercept. Unfortunately it does not include the dates of when each organization was added. The list is organized into the following categories:\n\nTerror Organizations (e.g. Islamic State)\nCrime Organizations (e.g. Bloods, Crips)\nHate Organizations (e.g. Aryan Nation, includes bands and websites)\nMilitarized Social Movements (e.g. United States Patrio Defense Force)\nViolent Non-State Actors (e.g. Free Syrian Army)\nHate (e.g. David Duke)\nIndividuals: Crime (e.g. Denton Suggs, Gangster Disciples)\nIndividuals: Terror (e.g. Osama bin Laden)"
  },
  {
    "objectID": "posts quarantine/2023-01-31-social-media-suspensions-data.html#tiktok",
    "href": "posts quarantine/2023-01-31-social-media-suspensions-data.html#tiktok",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "TikTok",
    "text": "TikTok\nCommunity Standards Report. Shows an increase in suspensions from around 1M accounts/quarter per 2020 to 6M accounts/quarter in 2023."
  },
  {
    "objectID": "posts quarantine/2023-01-31-social-media-suspensions-data.html#twitch",
    "href": "posts quarantine/2023-01-31-social-media-suspensions-data.html#twitch",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Twitch",
    "text": "Twitch\nStreamerBans. They seem to have a pretty comprehensive database of bans on Twitch."
  },
  {
    "objectID": "posts quarantine/2023-01-31-social-media-suspensions-data.html#other-platforms-1",
    "href": "posts quarantine/2023-01-31-social-media-suspensions-data.html#other-platforms-1",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Other Platforms",
    "text": "Other Platforms\n\nSpotify. The only unambiguous suspension from Spotify I found was Alex Jones’ podcast. Spotify removed some episodes of Joe Rogan’s podcast, and removed R Kelly and XXXtentacion’s music from playlists. They remove some white-supremacist artists and music. They removed all music from the band LostProphets after their lead singer was convicted of child sexual abuse.\nSubstack. I’m not aware of anybody who’s been kicked off Substack, they present themselves as very pro-free-speech.\nReddit. I’m not aware of any data on reddit account suspensions.\nRumble. The Rumble video-hosting platform has become quite large (they claim 70M MAU, and have a market cap of ). Their terms of service restrict content that is “abusive, inciting violence, harassing, harmful, hateful, anti-semitic, racist or threatening.” However I have not yet found a single example of a prominent user who has been suspended from Rumble."
  },
  {
    "objectID": "posts quarantine/2023-01-31-social-media-suspensions-data.html#other-data-sources",
    "href": "posts quarantine/2023-01-31-social-media-suspensions-data.html#other-data-sources",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Other Data Sources",
    "text": "Other Data Sources\nCan find more suspensions by searching Wikipedia for “suspended from XXX”. E.g. site:wikipedia.org \"suspended from facebook\". Possibly worth doing the same search for Google News.\nSocialBlade has data on number of followers by month since 2018, across Twitter, FB, YouTube. I’m not sure how easy it would be to scrape this data. They have a paid API, they say “up to 3 years of Historical statistics on creators.” However the website seems to have data back to at least April 2018.\nBallotpedia list of elected officials suspended from social media. It is an excellent resource, appears comprehensive and cites original reporting. I have added all of their data to the database as of January 2023.\nGlobal Internet Forum to Counter Terrorism (GIFCT). They mainly work on sharing hashes of terrorist content between platforms. They have some dicussion papers about “terror designation lists” but I don’t think they maintain any lists themselves.\nSpecially Designated National / Global Terrorist (SDN/SDGT). This is a public list maintained by the US government, and consumed by a number of tech companies. The full history is available, but it would be extremely difficult to parse.\nLumen. This has an international database of government takedown requests. They also seem to include whether the request was honored.\nCCDH Disinformation Dozen. This is a list from March 2021 of prominent accounts who were spreading anti-vax information on social media: original report, followup report from April 2021). They also have a “toxic ten” report. It’s probably worth adding both lists to the database."
  },
  {
    "objectID": "posts quarantine/2023-01-31-social-media-suspensions-data.html#third-party-sources-on-platform-policies",
    "href": "posts quarantine/2023-01-31-social-media-suspensions-data.html#third-party-sources-on-platform-policies",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Third-Party Sources on Platform Policies",
    "text": "Third-Party Sources on Platform Policies\nThere are a variety of third-party resources comparing policies across platforms, however none seem to have data comparable to the list above, i.e. a summary of specific content policy changes over time.\nComparisons at a single point in time.\n\nDNC (2020) Comparison of Misinformation Policies, 2020\nConsumer Reports (Aug 13 2020) Comparison of Misinformation Policies, 2020. Data as of 2020, with three levels: “allowed”, “sometimes”, and “prohibited”.\nUNC CITAP (May 22 2020) Comparison of Misinformation Policies, 2020. Has tables comparing misinfo policies as of 2020, three levels: “prohibited”, “flagged”, “allowed.”\nElection Integrity Partnership (Oct 28 2020) Comparison of Election Policies, 2020.\nCarnegie Endowment (April 1 2021) Existence of Policies, 2021. Table just marks whether a platform has a policy on some type of content, not nature of policy. They also they have a database of platform policies but it seems to only have data from February 2021.\nVirality Project Comparison of COVID Vaccine Policies in 2021.\n\nPolicies tracked over time.\n\nKatie Harbath and Collier Fernenkes (August 2022) Election Policy Announcements, 2003-2022. Google spreadsheet with links to around 600 policy announcements, organized by platform, author, date, product-type, and country. Focussed on election-related policies, and they don’t include summaries of the policy announcement. They also wrote up analyses: (1) “A Brief History of Tech and Elections”; (2) 2022 election announcements.\nRanking Digital Rights Index, Comparison of Privacy and Transparency Policies, 2017-2022. They collect perhaps 100 different indicators across around 15 tech companies, mostly related to privacy and transparency, earliest data from 2017. All the data is available.\nGLAAD Comparison of LGBTQ user safety, 2021-2022\nCELE, Letra Chica. Tracks all public policy changes on Meta, YouTube, and Twitter. Most data from May 2020, but they go back to 2019 for Facebook by using FB’s Transparency Center. Each policy update includes a short summary of what’s changed. Tracks both Spanish and English versions. Data stored on coda.io, I think it’s queryable.\nLinterna Verdes, Circuito. Has about 15 in-depth case studies of platform moderation decisions.\nHumboldt Institute, Platform Governance Archive. Comprehensive archive of ToS, Privacy Policy, and Community Guidlines, from 2004 until late 2021, for FB, IG, Twitter, and YouTube. The data will not be updated.\nOpen Terms Archive. Started by the French Ambassador for Digital Affairs, but now a collaboration. Tracks terms for many different online services in a github repo. The Platform Governance Archive has moved to be part of this project, here.\nEFF, TOSback. Database of historical ToS documents from different services, with cross-platform comparisons. The most recent updates seem to be from May 2021, possibly was succeeded by Open Terms Archive.\nEuropean Commission, Copyright Content Moderation and Removal. This PDF report includes a lot of work which maps the copyright policies of major platforms.\n\nNarrative histories:\n\nCatherine Buni and Soraya Chemaly (2016, the Verge) History of Moderation.\nSarah Jeong (2016, Vice) The History of Twitter’s Rules\nBergen (2022) Like, Comment, Subscribe. A book on the history of YouTube, it has a lot of detail on policy changes."
  },
  {
    "objectID": "posts quarantine/2023-01-31-social-media-suspensions-data.html#exclusions-in-film-and-television",
    "href": "posts quarantine/2023-01-31-social-media-suspensions-data.html#exclusions-in-film-and-television",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Exclusions in Film and Television",
    "text": "Exclusions in Film and Television\n\n\n\n\n\n\n\n\n\n\n\nalleged crime\nresult\n\n\n\n\n1920s\nFatty Arbuckle\nrumors of immorality\nfilm industry blacklisted\n\n\n\n\n\n\n\n\n1940s\nOrson Welles\ncommunist associations\nblacklisted, moved to Switzerland\n\n\n\nDalton Trumbo\ncommunist associations\nblacklisted\n\n\n\n(around 100 people)\ncommunist associations\nblacklisted for a decade\n\n\n\n\n\n\n\n\n1950s\nCharlie Chaplin\ncommunist associations\nbanned from US\n\n\n\nElia Kazan\ntestifying before HUAC\nlost some relationships in Hollywood\n\n\n\n\n\n\n\n\n1960s\nJane Fonda\nopposition to Vietnam war\nblacklisted\n\n\n\n\n\n\n\n\n1970s\nRoman Polanski\nrape of 13yo girl\nmild disapproval from Hollywood\n\n\n\n\n\n\n\n\n1990s\nO J Simpson\nmurdered his wife\nblacklisted\n\n\n\nWoody Allen\nmolested 7yo daughter\n\n\n\n\n\n\n\n\n\n2000s\nMel Gibson\nracism & anti-semitism\n“blacklisted in Hollywood for almost a decade”\n\n\n\nMira Sorvino\nrejecting Harvey Weinstein\nblacklisted\n\n\n\nRose McGowan\nrejecting Harvey Weinstein\nblacklisted\n\n\n\nIsaiah Washington\nhomophobic remarks\nblacklisted\n\n\n\nMichael Richards\nracist remarks\nblacklisted\n\n\n\nKathy Griffin\n“told Jesus to suck it”\nbanned from talk shows and TV appearances\n\n\n\nSean Penn\nopposition to Iraq war\ndropped from movie\n\n\n\n\n\n\n\n\n2010s\nBill Cosby\nsexual assault\nblacklisted (& later imprisoned)\n\n\n\nHarvey Weinstein\nsexual assault\nblacklisted (& later imprisoned)\n\n\n\nStacy Dash\nconservative advocacy\nblacklisted\n\n\n\nKirk Camerson\ncriticism of homosexuality\nblacklisted\n\n\n\nJames Woods\nanti-Obama tweets\nblacklisted\n\n\n\nCeeLo Green\nsexual assault\nblacklisted\n\n\n\nLouis CK\nsexual harassment\nblacklisted\n\n\n\nKathy Griffin\nphoto with head of Trump\nfired by CNN, lost endorsement, cancelled tour\n\n\n\nT J Miller\nsubstance abuse, sexual assault\nblacklisted\n\n\n\nGina Carano\npolitical social media posts\nfired from TV show\n\n\n\nKevin Spacey\nsexual harassment\nlost roles in films\n\n\n\nJussie Smollett\nlied about an attack\nlost roles in TV shows\n\n\n\nNeil deGrasse Tyson\nrape, sexual harassment\ntemporarily lost roles in TV shows\n\n\n\nRoseanne Barr\nracist tweet\nlost TV show\n\n\n\n\n\n\n\n\n2020s\nWill Smith\nslapping someone at Oscars\nfilm projects put on hold\n\n\n\nJohnny Depp\ndomestic violence\nlost roles in films\n\n\n\nAmber Heard\ninvolvement in trial w Johnny Depp\nlost roles in films\n\n\n\nJustin Roiland\nsexual harassment & abuse\nlost roles in shows"
  },
  {
    "objectID": "posts quarantine/2023-01-31-social-media-suspensions-data.html#exclusions-in-music",
    "href": "posts quarantine/2023-01-31-social-media-suspensions-data.html#exclusions-in-music",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Exclusions in Music",
    "text": "Exclusions in Music\n\n\n\n\n\n\n\n\n\n\n\nalleged crime\nresult\n\n\n\n\n1940s\nPaul Robeson\ncommunist associations\nblacklist and passport revoked\n\n\n\n\n\n\n\n\n1950s\nLeonard Bernstein\ncommunist associations\nbrief blacklist\n\n\n\nLena Horne\ncommunist associations\nblacklist\n\n\n\nPete Seeger\ncommunist associations\nblacklist\n\n\n\n\n\n\n\n\n1960s\nBeatles\nsaying they’re bigger than Jesus\nconsumer boycott\n\n\n\nLovin Spoonful\ncooperating with FBI\nmusic industry boycott\n\n\n\nNina Simone\n“Mississippi Goddam”\nboycott in the South\n\n\n\nJohn Lennon\ncriticism of US and Vietnam war\nrefused entry into US\n\n\n\nEartha Kitt\ncriticism of Vietnam war\nblacklist through LBJ and CIA\n\n\n\n\n\n\n\n\n1970s\nSex Pistols\ncriticizing the Queen, swearing on TV\nbanned by the BBC, dropped by EMI\n\n\n\n\n\n\n\n\n1980s\nNWA\n“Fuck the Police” & similar songs\nradio station boycott, police boycott\n\n\n\n\n\n\n\n\n1990s\nBruce Springseen\nsong against police brutality\nbrief police boycott\n\n\n\nMarilyn Manson\ntransgressive lyrics\nbanned from performing in some states\n\n\n\nBody Count\nsong “cop killer”\nalbum withdrawn and reissued\n\n\n\n\n\n\n\n\n2000s\nDixie Chicks\nfor opposition to Iraq war\nblacklisting and consumer boycott\n\n\n\nJanet Jackson\nshowing nipple\nVH1, MTV, & Viacom radio stopped playing her music\n\n\n\nR Kelly\nsexual abuse\nbroad blacklist\n\n\n\nChris Brown\ndomestic violence\nweak boycott and blacklist\n\n\n\n\n\n\n\n\n2010s\nLostprophets\nsexual abuse\nbroad blacklist\n\n\n\nMichael Jackson\nchild molestation\nsome radio stations stop playing music\n\n\n\n\n\n\n\n\n2020s\nBeyonce\nsong against police brutality\nbrief police boycott\n\n\n\nMorgan Wallen\nusing n-word\ntemporarily dropped from radio/streaming playlists\n\n\n\nKanye West\npraise of Hitler\nlost sponsors\n\n\n\nOthers.\n\nIn radio: Father Coughlin, Rush Limbaugh, Don Imus fired from CBS for calling womens’ basketball team “nappy-headed hos”, Howard Stern fired from various radio shows for comments.\nIn sport. Colin Kapaernick blacklisted from NFL for kneeling for the anthem. Pete Rose banned from MLB for gambling.\nNazi sympathisers/collaborators. Charles Lindbergh, Henry Ford, Charles Coughlin, PG Wodehouse, Ezra Pound.\nWriters: DH Lawrence, Henry Miller, Salman Rushdie (Nicole Bonoff).\nJournalists. Jeffrey Toobin (New Yorker writer masturbated on zoom call),\nNote on R Kelly disappearing from radio"
  },
  {
    "objectID": "posts quarantine/2023-04-28-ranking-by-engagement.html",
    "href": "posts quarantine/2023-04-28-ranking-by-engagement.html",
    "title": "Ranking by Engagement",
    "section": "",
    "text": "Tom Cunningham, May 8 2023.11 tom.cunningham@gmail, @testingham. I worked at FB for 5 years, and Twitter for 1 year, now affiliated with the Integrity Institute. This note entirely based on public information. Thanks to comments from Jeff Allen, Jacquelyn Zehner, David Evan Harris, Jonathan Stray, and others.\nSix observations on ranking by engagement:\nIn an appendix I formalize the argument. I show that all these observations can be expressed as covariances between different properties of content, e.g. between the retentiveness, predicted engagement rates, and other measures of content quality. From those covariances we can derive Pareto frontiers and visualize how platforms are trading-off between different outcomes."
  },
  {
    "objectID": "posts quarantine/2023-04-28-ranking-by-engagement.html#formal-observations",
    "href": "posts quarantine/2023-04-28-ranking-by-engagement.html#formal-observations",
    "title": "Ranking by Engagement",
    "section": "Formal Observations",
    "text": "Formal Observations\nHere I describe a few formal properties of a model of ranking based on a joint-normal distribution of attributes. I have a longer writeup with proofs of these results which I hope to publish soon, I am happy to share a draft on request.\n\nThe covariance between item attributes will determine a Pareto frontier among outcomes. Suppose we know the joint distribution of attributes and we can choose a subset with share \\(p\\) of the distribution (e.g. a fixed number of impressions given a pool of possible stories to show), and we want to calculate the average value of each attribute in the subset of content shown to the user. Then we can describe the Pareto frontier over subsets, i.e. the set of realized average outcomes, and it will be a function of the covariances among attributes over pieces of content. With 2 attributes the Pareto frontier will be an ellipse with shape exactly equal to an isoprobability curve from the joint density.\nThe shape of the ellipse has a simple interpretation. If two attributes are positively correlated then the Pareto frontier will be tight meaning there is little tradeoff, i.e. we will have similar aggregate outcomes independent of the relative weights put on each outcome in ranking. If instead two attributes are negatively correlated then the Pareto frontier will be loose meaning outcomes will vary a lot with the relative weights used in ranking.\nOur assumption that the share \\(p\\) is fixed is equivalent to assuming that any ranking rule will get the same number of impressions. This assumption obviously has some tension with retentiveness being an outcome variable: if some ranking rule has low retentiveness, then we would expect lower impressions. Accounting for this would make the Pareto frontier significantly more complicated to model, for simplicity we can interpret every attribute except retentiveness as a short-run outcome. Alternatively we could interpret them as relative instead of absolute outcomes, e.g. as engagement/impression or engagement/DAU.\nImproving a classifiers will stretch the Pareto frontier. As a classifier gets better the average prediction will stay the same but the variance will increase, meaning the Pareto frontier will stretch out, and given a linear indifference curve we can derive the effect on outcomes.\nThe joint distribution plus utility weights will determine ranking weights. If we observe only some outcomes then we can calculate the conditional expectation for other outcomes. Typically we want to know retentiveness, and we can write the conditional expectation as follows: \\[E[\\text{retentiveness}|\n   \\text{engagement},\\ldots,\\text{user preference}].\\] This expectation has a closed-form solution when the covariance matrix is joint normal. When we have just two signals, for example engagement and quality, we can write:\n\\[\\begin{aligned}\n   E[r|e,q] &= \\frac{1}{1-\\gamma^2}(\\rho_e-\\gamma\\rho_q)e +\n               \\frac{1}{1-\\gamma^2}(\\rho_q-\\gamma\\rho_e)q\\\\\n   r     &= \\text{retentiveness}\\\\\n   e     &= \\text{engagement (predicted)}\\\\\n   q     &= \\text{quality (predicted)}\\\\\n   \\rho_{e}     &= \\text{covariance of engagement and retentiveness}\\\\\n   \\rho_{q}     &= \\text{covariance of quality and retentiveness}\\\\\n   \\gamma     &= \\text{covariance of engagement and quality}\n\\end{aligned}\\]\nNote that the slope of the iso-retentiveness line in \\((e,q)\\)-space will be \\(-\\frac{\\rho_e-\\gamma\\rho_q}{\\rho_q-\\gamma\\rho_e}\\).\nExperiments which vary ranking weights tell us about covariances. We can write findings from experiments as follows. First, suppose we find that retention is higher when ranked by engagement than when unranked, this can be written:\n\\[\\begin{aligned}\n      \\utt{E[r|e&gt;e^*]}{ranked by}{engagement} &&gt; \\ut{E[r]}{unranked}\n   \\end{aligned}\\]\nHere \\(e^*\\) is chosen such that \\(P(e&gt;e^*)=p\\) for some \\(p\\), representing the share of potential inventory that the user consumes. This implies that engagement must positively correlate with retentiveness, \\(\\rho_e&gt;0\\).\nNext we can express that retention is higher when we put some weight \\(\\beta\\) on quality:\n\\[\\begin{aligned}\n   \\utt{E[r|e+\\beta q&gt;\\kappa^*]}{ranked by}{engagement and quality} &&gt; \\utt{E[r|e&gt;e^*]}{ranked by}{engagement}\n\\end{aligned}\\]\nHere \\(\\kappa^*\\) is chosen such that \\(P(e+\\beta q &gt; \\kappa^*)=P(e&gt;e^*)=p\\). If \\(\\beta\\) is fairly small then we can infer that the iso-retentiveness line is downward-sloping, implying: \\[\\frac{\\rho_e-\\gamma\\rho_q}{\\rho_q-\\gamma\\rho_e}&gt;0.\\]\nThis implies that both engagement and quality have the same sign. I don’t think they both can be negative, so they both must be positive:\n\\[\\begin{aligned}\n      \\rho_e - \\gamma \\rho_q &&gt; 0 \\\\\n      \\rho_q - \\gamma \\rho_e &&gt; 0.\n   \\end{aligned}\\]\nI think it’s reasonable to treat preferences as locally linear. To have a well-defined maximization problem (with an interior solution) we need either nonlinear preferences or a nonlinear Pareto frontier. It’s always easier to treat things as linear when you can, so a relevant question is which of these two is closer to linear? Internally companies often treat their preferences as nonlinear, e.g. setting specific goals and guardrails, but those are always flexible and often have justifications as incentive devices. Typical metric changes are small, only single-digit percentage points, over that range the Pareto frontier does show significant diminishing returns while (it seems to me) value to the company does not.\n\n\n\n\n\n\n# Appendix: Literature\nMilli, Pierson and Garg (2023) Choosing the Right Weights: Balancing Value, Strategy, and Noise in Recommender Systems\n\n“model in which two producers compete for the attention of one user. The recommender system ranks producers based on a linear combination of predictions of k behaviors. However, producers can strategically adapt their items to increase the probability of different user behaviors. User utility depends on being shown a high value producer, and a producer’s utility is the probability they are ranked highly minus their costs of strategic manipulation.”\n\nE.g. considering weights on like, RT, and reply: (1) which one is closer to true user preference; (2) which has higher signal-noise ratio (); (3) .\n\nTwo items, i{-1,1}, and k behaviours (e.g. like, comment). We have prediction of each, y^k. Final score is ^T. Predictions are ."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "tecunningham.github.io",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nfn↩︎"
  }
]