[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Tom Cunningham",
    "section": "",
    "text": "Data Science at OpenAI."
  },
  {
    "objectID": "about.html#blog",
    "href": "about.html#blog",
    "title": "About Tom Cunningham",
    "section": "Blog",
    "text": "Blog"
  },
  {
    "objectID": "about.html#resume",
    "href": "about.html#resume",
    "title": "About Tom Cunningham",
    "section": "Resume",
    "text": "Resume"
  },
  {
    "objectID": "about.html#working-papers",
    "href": "about.html#working-papers",
    "title": "About Tom Cunningham",
    "section": "Working Papers",
    "text": "Working Papers\n\n2023: Implicit Preferences, with Jon de Quidt.\n2019: Interpreting Experiments with Multiple Outcomes (presented at CODE 2019) with Josh Kim.\n2015: Biases and Implicit Knowledge\n2015: Equilibrium Persuasion with Ines Moreno de Barreda.\n2013: Comparisons and Choice\n2013: Relative Thinking and Markups"
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "About Tom Cunningham",
    "section": "Publications",
    "text": "Publications\n\n2019: Improving Treatment Effect Estimators Through Experiment Splitting, WWW, with Dominic Coey.\n2013: The Incumbency Effects of Signalling  Economica, with Ines Moreno de Barreda, Francesco Caselli, and Massimo Morelli.\n2009: Leader Behaviour and the Natural Resource Curse Oxford Economic Papers, with Francesco Caselli."
  },
  {
    "objectID": "about.html#miscellaneous",
    "href": "about.html#miscellaneous",
    "title": "About Tom Cunningham",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\n2010: “LSE CEP 2010 Election Analysis: Macroeconomics and Public Finance” (with Ethan Ilzetski)\n\nComment pieces for the Guardian (2008/2009)\n\nDon’t worry about inflation\nThe great financial stitch-up\nThe rate cut wasn’t big enough\nStuck in the middle\nWe need to outgrow our debt\nYou say inflation, I say deflation\nConfidently predicting darker days"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html",
    "href": "posts/2016-04-30-relative-thinking.html",
    "title": "Relative Thinking",
    "section": "",
    "text": "There are a lot of cute thought experiments where the apparent value of something depends on what it’s compared to:\nMany people have felt that there’s a common principle at work, in particular: that the sensitivity to an attribute (price, probability, square feet) depends on the set of quantities that you’re considering. But different people have proposed different principles:\nAll of these models can be thought about as indifference curves that change slope as you change the elements in the choice set, e.g. below adding option C makes the indifference curves rotate clockwise, and so makes you prefer B to A:\n\\[\n   \\xymatrix{\\, &  &  &  &  & . & \\,\\, &  & \\,\\\\\n   \\, & A &  &  &  &  & A\\\\\n   \\, &  &  & B & \\ar@{-}[uullll] &  &  &  & B & \\ar@{-}[uul]\\\\\n   \\, &  &  &  &  &  &  &  &  &  C \\\\\n   \\ar[uuuu]\\ar[rrrr] &  &  &  & \\ar@{-}[uullll] & \\ar[uuuu]\\ar[rrrr] &  &  & \\ar@{-}[uuuull] & \\,\n   }\n\\]\nBut each theory has different assumption about how the slope of the indifference curves depends on the placement of the options.\nI’m going to try to make the following points:\nA few other points that I’ll leave for later:"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#digression-perception",
    "href": "posts/2016-04-30-relative-thinking.html#digression-perception",
    "title": "Relative Thinking",
    "section": "DIGRESSION: PERCEPTION",
    "text": "DIGRESSION: PERCEPTION\nComparison effects have been studied in perception for a long time, and the same points I make here also apply there. At first people proposed that perceptual comparison effects were hardwired things, mechanical effects, but on further study it turned out that they were context-dependent, in a way that makes them look like sensible inferences.\nHere’s a classic contrast effect:\n\n\n\n \n\n\nthe same shade of grey looks darker when surrounded by white, than when surrounded by black.\nFor a long time psychology textbooks gave this as an example of a hardwired contrast effect – i.e. this is caused by the basic wiring of neurons in our eyes. (Some still do).\nBut take a look at this (White’s illusion):\n\n\n\n \n\n\nhere the same shade of grey looks lighter on the left than on the right, despite the surroundings being relatively lighter on the left than on the right. This is exactly the opposite of what’s predicted by a hardwired contrast effect in perception.\n(An even cleaner example would be ceteris paribus, where making some part of the background lighter, all else held equal, makes the foreground appear lighter. The figure above does imply that there must exist at least one such case: imagine a third set of grey rectangles which are surrounded only by white. That third case must serve as a ceteris paribus case for one or other of the two cases above, probably both.)\nThe general point is this: There is not a stable relationship between the perceived-lightness of an object and the lightness of surrounding objects. There isn’t even an all-else-equal relationship. The relationship can run in either direction depending on the context.\nBut that’s not the last word. The set of contexts where it goes one way or the other way aren’t arbitrary (“contrast effects” and “assimilation effects”). Adelson (2001) shows that you can usually predict when you’ll observe one effect or the other: roughly, whether or not the surrounding lightness is a positive or negative ecological cue for illumination. I.e., in typical circumstances, is the surrounding lightness positively or negatively associated with illumination? (See an example at the bottom of this post.)"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#digression-2",
    "href": "posts/2016-04-30-relative-thinking.html#digression-2",
    "title": "Relative Thinking",
    "section": "DIGRESSION 2",
    "text": "DIGRESSION 2\nI would write out lists of comparison-effect examples over and over while working on my PhD. My train of thought would get detached and I’d end up asking myself questions like: What are you doing sitting in this office, a continent away from your friends and an ocean and a continent away from your family? Why do you spend your weekends in this sad building, where people stare at the carpet when they pass each other? What rock did you hit in adolescence that knocked you out of orbit, and sent you here? Are you trying to make your mother proud? Avenge your father? Do these professors you work with look like the kind of man you want to be? Did you stumble into one of those academic fields that people snigger about? Why, when you talk about your work, do the people you admire glaze over, and the people who bore you perk up? Do you think that giving your life to intellectual things makes you better than other people? Do you look down on people who don’t think so clearly? What are you doing on a Friday night at the NBER eating a tuna subway sandwich and reading reddit? If it takes you 5 years to get straight one point about relative thinking – one corner of one shelf in one cupboard – then how long is it going to take to tidy up the whole house? When an undergraduate corners you, asking earnest & tedious questions, doesn’t it remind you of yourself?"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#alternative-heuristics",
    "href": "posts/2016-04-30-relative-thinking.html#alternative-heuristics",
    "title": "Relative Thinking",
    "section": "ALTERNATIVE HEURISTICS",
    "text": "ALTERNATIVE HEURISTICS\nOK. Well, here’s the body of the argument: I’m going to discuss a few perfectly reasonable reasons why we might infer the value of different attributes from the choice set, and each reason will imply one of the above laws in a subset of cases. However I also show that the same reason can imply the exact opposite of that law, for cases outside that subset.\n\n(1) MRS shifting towards MRT.\nA choice set which varies in different attributes has an implicit rate of tradeoff between the attributes – i.e. the marginal rate of transformation (MRT) – and it is easy to think of cases where our preferences would naturally adapt to the implicit tradeoff, i.e., where our relative value, AKA our marginal rate of substitution (MRS), would rotate towards the tradeoff that is implicit in the choice set (i.e. the MRT).\nThe MRS shifting towards the MRT could be rationalized in two separate ways. First, suppose you believe the social MRS to be informative about your own MRS, and you believe that the choice set reflects the market price, and finally that the price reflects the social MRS (as it would in a competitive equilibrium). Second, suppose you believe the person who constructed the choice set to be cooperative (in the sense of Grice) - i.e., they only include things in the choice set which they think you might want: this implies that the MRT in the choice set reflects their beliefs about your MRS, which is itself informative. If I’m staying in your spare room and and you ask me “would you prefer a poached egg or gruel for breakfast?” then I will figure that your gruel must be pretty good.\nIf there are just two attributes (i,j) and two alternatives (a,b) then the implicit tradeoff is \\(MRT_{i,j}=\\frac{\\|a_{i}-b_{i}\\|}{\\|a_{j}-b_{j}\\|}\\). If the MRS rotates to meet this MRT then the sensitivity to attribute \\(i\\) will be decreasing in the range observed along that dimension, exactly as implied by the range-sensitivity theory (i.e., V, M&C, BR&S).\nHowever the MRS-MRT effect implies range sensitivity only for a 2-attribute, 2-alternative case. Outside of that case the intuitions depart.\nA marginal rate of transformation can only be directly identifed from a menu if the number of alternatives is equal to the number of attributes. I.e., to define a plane in \\(n\\) dimensions from a set of points, you’ll need exactly \\(n\\) points. If you have fewer then it becomes the statistical problem of fitting a line to a set of points. Here is a simple example where the MRT theory and other theories (e.g. range-sensitivity) give qualitatively different answers, and in which the MRS-MRT theory seems more faithful to the intuition. Suppose we have the following three options:\n\\[\n\\xymatrix@C=1em@R=1em{\n& \\binom{\\mbox{100K salary}}{\\mbox{199 days off}}\\\\\n\\\\\n&  & \\binom{\\mbox{105K salary}}{\\mbox{189 days off}} & \\binom{\\mbox{110K salary}}{\\mbox{189 days off}}\\\\\n\\\\\n\\ar[rrrr]\\ar[uuuu] & & &  &  & \\, \\\\\n}\n\\]\nThe intermediate option is dominated by the by the option on the right, and intuitively - to me - the existence of the intermediate option makes the rightmost option more desirable - because the choice set makes 10-days-off seem to be worth between \\$5K and \\$10K, meaning the higher salary seems to come at a low cost in terms of days-off. This intuition is not captured by range sensitivity, because the intermediate option does not change the range in either dimension. However the intermediate option does change the implicit MRS, in the sense of the best-fitting line (e.g. orthogonal regression), and the change will be in favor of the rightmost option – fitting my own intuition.\nEven in the 3-attribute 3-alternative case, it is no longer true that \\(MRT_{i,j}\\) is equal to the ratio of ranges on dimension i and j, it’s now a more complicated function.\nWhen one attribute is a good and the other is a bad (e.g. price and quality; salary and hours) it is sometimes reasonable to think that choosing neither alternative is an additional implicit element of the choice set, i.e. the point (0,0). In these cases the ratio of the ranges reduces to the ratio of the maximum values (\\(\\frac{\\max_{c\\in C}c_{i}}{\\max_{c\\in C}c_{j}}\\)), which has similar comparative statics to the theory in (C) - which depends on the ratio of average values - than the theory in (V,M&C,BR&S) - which depends on the ratio of the ranges.\nAn interesting fact: the effects of this MRS-MRT theory will not be detectable when the choice set is binary: suppose your MRS shifts towards the MRT implicit in the choice set, then although your final MRS will be closer to the MRT, it will not cross the MRT, i.e. the shift in MRS will not alter which of the two elements you prefer. This implies that the MRS-MRT theory cannot rationalize the existence of cycles in binary choice, and so cannot explain evidence for ‘subadditivity’ of different dimensions, such as probability, money, or delay (see Read (2001) for citations). For example, if your have a prior belief that \\(a\\) is better than \\(b\\), and then you observe a choice set containing \\(a\\) and \\(b\\), then you may revise upward your valuation of \\(b\\) relative to \\(a\\), but this observation wouldn’t cause you to switch preference, i.e. to think that \\(b\\) is now better than \\(a\\). (This could be violated under some unusual priors, e.g. if you had bimodal beliefs about the value of \\(b\\)).\n\n\n(2) MRS shifting towards demand.\nThere is a second strong intuition for choice sets influencing preferences: combinations offered often reflect combinations desired, so a relative increase in attribute 1 could be interpreted as a positive signal about the value of attribute 1. Suppose we manipulate the choice set, while keeping the relative price fixed, for example consider these two choice sets, trading off the price and quantity of some good:\n\\[\n\\xymatrix@C=.5em@R=.5em{\\ar[rrrrr]\\ar[ddddd] & & &  &  & \\text{apples}\\\\\n& \\binom{\\text{1 apple}}{\\$1}\\\\\n&  & \\binom{\\text{2 apples}}{\\$2}\\\\\n&  & & \\binom{\\text{3 apples}}{\\$3}\\\\\n\\\\\n\\\\\n\\$ }\n\\]\n\\[\n\\xymatrix@C=.5em@R=.5em{\\ar[rrrrr]\\ar[ddddd] & & & &  & \\text{apples}\\\\\n& \\,\\,\\,\\,\\,\\, & \\\\\n& & \\binom{\\text{2 apples}}{\\$2}\\\\\n& & & \\binom{\\text{3 apples}}{\\$3}\\\\\n& & & & \\binom{\\text{4 apples}}{\\$4}\\\\\n\\\\\n\\$ }\n\\]\nA natural intuition is that people will tend to switch from \\(\\binom{2}{\\$2}\\) to \\(\\binom{3}{\\$3}\\), when going from the first to the second choice set. None of the theories discussed above gives an unambiguous prediction about the change in MRS between these two choice sets - because both the range and magnitude change by the same amount on each dimension - yet the intuition remains quite clear (I think).\nThis idea could be easily formalized - suppose people know the supply curve but are uncertain about the demand curve - then when they observe an increase in quantity they attribute this to a higher demand, and so they infer an increase in value of the good. I think this is similar to the intuition given in Kamenica (2008) - when you observe a higher price/quantity combination, you infer that demand is higher, and so you infer that the value of each marginal good must be higher. I think that a similar foundation is used in Drolet, Simonson, Tversky (2000) “Indifference Curves that Travel with the Choice Set”.\nWe have discussed diagonal shifts along the budget set, meaning that both attributes are varying at once; if only one attribute varied, it’s not clear what a consumer would infer from this. Of course we could formalize a model where the consumer is uncertain about both supply and demand; or we could combine this model with the prior one, where the consumer is uncertain about the price.\n\n\n(3) Magnitude effects.\nFinally it’s easy to come up with a rational magnitude effect, such that when we observe a higher quantity \\(q\\) we infer that the marginal value of each unit is less. Suppose we know the price and we know the consumption-value of a good, when measured in units that are familiar to us, but we do not know the units that are used in the packaging. Then if we observe other people consuming a higher quantity, measured in unfamiliar units, we infer that each unit is worth less: when we observe a 10,000 Kronor bank note we infer each Kronor is not worth a lot; when we observe a 10,000 Watt bulb we infer each Watt is not worth much; when we observe 200mg of Oxytocin we infer the marginal effect of a milligram is not too much."
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#summary",
    "href": "posts/2016-04-30-relative-thinking.html#summary",
    "title": "Relative Thinking",
    "section": "SUMMARY",
    "text": "SUMMARY\nWhen we dig into the intuitions behind comparison effects, we often find that they resemble inferences that we would make every day. The laws that have been proposed to explain comparison effects only work because they coincide with one or other of the inferences in certain subsets of cases – e.g. range-sensitivity coincides with MRS-MRT inference, magnitude-sensitivity coincides with unit-value inference. But these overlaps occur only in a subset of cases, and stepping outside that subset we find that the law fails, while the inference remains.\nDoes this mean that comparison effects are all just rational inferences? What we would like to know is whether comparison effects occur even when inference can be entirely ruled out – e.g. when we run an experiment that explicitly randomizes the choice sets. Some papers do this, but few do it well. I am persuaded that comparisons do affect us on a pre-conscious level, i.e. that our instincts latch onto comparisons without being careful about the significance of the comparison in the particular circumstance, but there’s not a lot of unambiguous evidence on this. I can at least say that most people find the types of example listed above pretty beguiling: they get strong intuitions about relative value, but struggle to explain where the intuitions come from, implying that the inference isn’t entirely conscious.\nSo then why would we make bad inferences that resemble good inferences? I think for the same reason that our perception makes bad inferences that resemble good inferences – because perceptual processes interpret cues according to their ordinary significance, without adjusting for all relevant information. Perception is carried out in a cabinet, whirring through the sense data, and printing out conclusions for the conscious mind to read. The cabinet is locked, we only have access to the output. That is the argument of my paper on ‘implicit knowledge’."
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#miscellaneous-notes",
    "href": "posts/2016-04-30-relative-thinking.html#miscellaneous-notes",
    "title": "Relative Thinking",
    "section": "MISCELLANEOUS NOTES",
    "text": "MISCELLANEOUS NOTES\n\nIt is useful to make a sharp distinction between “goods” and ’bads”. Examples of good-bad tradeoffs are quality vs price; salary vs hours worked. Examples of good-good tradeoffs are bedrooms vs bathrooms; MPG vs horsepower; salary vs holiday-days.\nParducci invented, as well as range-frequency theory, windsurfing.\nBordalo Gennaioli and Shleifer (2013) has a weird feature: the salience of an attribute depends on relative levels (\\(q-\\bar{q}\\) and \\(p-\\bar{p}\\)), but the utility of an attribute depends on absolute levels (\\(q\\) and \\(p\\)). I think this is just a mistake – the underlying intuition is matched much better if \\(U(q,p) = \\hat{\\theta}_q(q-\\bar{q}) + \\hat{\\theta}_p(p-\\bar{q})\\) instead of \\(U(q,p) = \\hat{\\theta}_q q + \\hat{\\theta}_p p\\). This alternation removes a lot of the weird comparative statics of the theory, such as the severe non-monotonicity of the decoy effects. Another note: for two-alternative two-attribute choices the theory (as stated in the paper) has a utility representation, i.e. many of the predictions of that paper are equivalent to a model with menu-independent preferences. For a sufficiently large value of \\(M\\):\n\\[\nU(q,p)=\\begin{cases}\n\\delta q-p & ,\\,q&lt;\\delta p\\\\\nM+\\ln q-\\ln p & ,\\delta p&lt;q&lt;\\delta^{-1}p\\\\\nM+q-\\delta p & ,\\,q&gt;\\delta^{-1}p\n\\end{cases}\n\\]"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#references",
    "href": "posts/2016-04-30-relative-thinking.html#references",
    "title": "Relative Thinking",
    "section": "REFERENCES",
    "text": "REFERENCES\nKS: Koszegi and Szeidl (2011) (KS)\nBRS: Bushong Schwarzstein & Rabin (2014)\nMC: Mellers & Cooke (1994)\nC: Cunningham (2013)\nBGS: Bordalo Gennaioli Shleifer (2012)\nSimonson (2008) “Will I like a Medium Pillow?”\n\n“much of the evidence for preference construction reflects people’s difficulty in evaluating absolute attribute values and tradeoffs and their tendency to gravitate to available relative evaluations … These illustrations suggest that many forms of preference construction reflect a key underlying principle: decision makers tend to avoid absolute value judgments and gravitate to accessible relative evaluations … it is noteworthy that the evidence that has been accumulated to make the case for preference construction might be largely driven by a rather simple common principle. This rather simple, yet important absolute-to-relative principle lends itself to seemingly unrelated demonstrations, which have been treated as distinct phenomena and received unique labels.”\n\nDavid Stove (1991) “What is Wrong With our Thoughts?”"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#another-illusion",
    "href": "posts/2016-04-30-relative-thinking.html#another-illusion",
    "title": "Relative Thinking",
    "section": "Another Illusion",
    "text": "Another Illusion\nAdelson’s “steps” illusion:\n\n\n\n \n\n\nIn the first picture the two squares with arrows on them look similar, but in the second picture they seem to have different shades. They are (as you guessed) all the same shade, and in fact the shades are all identical between the first and second image, just arranged a little differently.\nIn particular, the tilt gives an the impression of an angle, and so influences our judgment of where the illumination is coming from. In the first image both squares seem to be on the same plane; in the second image the upper square seems to be on a plane with light squares, and the lower square seems to be on a plane with dark squares. If we use, as proxies of illumination for a square, the shade of squares coplanar with it, then we get the predicted effect."
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#more-examples",
    "href": "posts/2016-04-30-relative-thinking.html#more-examples",
    "title": "Relative Thinking",
    "section": "More Examples",
    "text": "More Examples\n\nIf you have to judge the value of 10 of something – say you’re bidding on 10 bottles of wine in an auction – they’ll seem more valuable if, at the same time, you’re also considering a single bottle of wine; and vice versa if you’re also considering 100 bottles of wine.\nIf you’re choosing between a low-price and medium-price version of a good, seeing that there’s also a high-price version makes the medium-price version seem relatively more attractive.\n(See my ‘comparisons and choice’ paper for more examples)."
  },
  {
    "objectID": "posts/2023-04-28-ranking-by-engagement.html",
    "href": "posts/2023-04-28-ranking-by-engagement.html",
    "title": "Ranking by Engagement",
    "section": "",
    "text": "Thanks to comments from Jeff Allen, Jacquelyn Zehner, David Evan Harris, Jonathan Stray, and others. If you find this note useful for your work send me an email and tell me :).\nSix observations on ranking by engagement on social media platforms:\nIn an appendix I formalize the argument. I show that all these observations can be expressed as covariances between different properties of content, e.g. between the retentiveness, predicted engagement rates, and other measures of content quality. From those covariances we can derive Pareto frontiers and visualize how platforms are trading-off between different outcomes."
  },
  {
    "objectID": "posts/2023-04-28-ranking-by-engagement.html#formal-observations",
    "href": "posts/2023-04-28-ranking-by-engagement.html#formal-observations",
    "title": "Ranking by Engagement",
    "section": "Formal Observations",
    "text": "Formal Observations\nHere I describe a few formal properties of a model of ranking based on a joint-normal distribution of attributes. Fuller proofs of some of these results are here, other are in notes I’m preparing and available on request.\n\nThe covariance between item attributes will determine a Pareto frontier among outcomes. Suppose we know the joint distribution of attributes and we can choose a subset with share \\(p\\) of the distribution (e.g. a fixed number of impressions given a pool of possible stories to show), and we want to calculate the average value of each attribute in the subset of content shown to the user. Then we can describe the Pareto frontier over subsets, i.e. the set of realized average outcomes, and it will be a function of the covariances among attributes over pieces of content. With 2 attributes the Pareto frontier will be an ellipse with shape exactly equal to an isoprobability curve from the joint density.\nThe shape of the ellipse has a simple interpretation. If two attributes are positively correlated then the Pareto frontier will be tight meaning there is little tradeoff, i.e. we will have similar aggregate outcomes independent of the relative weights put on each outcome in ranking. If instead two attributes are negatively correlated then the Pareto frontier will be loose meaning outcomes will vary a lot with the relative weights used in ranking.\nOur assumption that the share \\(p\\) is fixed is equivalent to assuming that any ranking rule will get the same number of impressions. This assumption obviously has some tension with retentiveness being an outcome variable: if some ranking rule has low retentiveness, then we would expect lower impressions. Accounting for this would make the Pareto frontier significantly more complicated to model, for simplicity we can interpret every attribute except retentiveness as a short-run outcome. Alternatively we could interpret them as relative instead of absolute outcomes, e.g. as engagement/impression or engagement/DAU.\nImproving a classifiers will stretch the Pareto frontier. As a classifier gets better the average prediction will stay the same but the variance will increase, meaning the Pareto frontier will stretch out, and given a linear indifference curve we can derive the effect on outcomes.\nThe joint distribution plus utility weights will determine ranking weights. If we observe only some outcomes then we can calculate the conditional expectation for other outcomes. Typically we want to know retentiveness, and we can write the conditional expectation as follows: \\[E[\\text{retentiveness}|\n   \\text{engagement},\\ldots,\\text{user preference}].\\] This expectation has a closed-form solution when the covariance matrix is joint normal. When we have just two signals, for example engagement and quality, we can write:\n\\[\\begin{aligned}\n   E[r|e,q] &= \\frac{1}{1-\\gamma^2}(\\rho_e-\\gamma\\rho_q)e +\n               \\frac{1}{1-\\gamma^2}(\\rho_q-\\gamma\\rho_e)q\\\\\n   r     &= \\text{retentiveness}\\\\\n   e     &= \\text{engagement (predicted)}\\\\\n   q     &= \\text{quality (predicted)}\\\\\n   \\rho_{e}     &= \\text{covariance of engagement and retentiveness}\\\\\n   \\rho_{q}     &= \\text{covariance of quality and retentiveness}\\\\\n   \\gamma     &= \\text{covariance of engagement and quality}\n\\end{aligned}\\]\nNote that the slope of the iso-retentiveness line in \\((e,q)\\)-space will be \\(-\\frac{\\rho_e-\\gamma\\rho_q}{\\rho_q-\\gamma\\rho_e}\\).\nExperiments which vary ranking weights tell us about covariances. We can write findings from experiments as follows. First, suppose we find that retention is higher when ranked by engagement than when unranked, this can be written:\n\\[\\begin{aligned}\n      \\utt{E[r|e&gt;e^*]}{ranked by}{engagement} &&gt; \\ut{E[r]}{unranked}\n   \\end{aligned}\\]\nHere \\(e^*\\) is chosen such that \\(P(e&gt;e^*)=p\\) for some \\(p\\), representing the share of potential inventory that the user consumes. This implies that engagement must positively correlate with retentiveness, \\(\\rho_e&gt;0\\).\nNext we can express that retention is higher when we put some weight \\(\\beta\\) on quality:\n\\[\\begin{aligned}\n   \\utt{E[r|e+\\beta q&gt;\\kappa^*]}{ranked by}{engagement and quality} &&gt; \\utt{E[r|e&gt;e^*]}{ranked by}{engagement}\n\\end{aligned}\\]\nHere \\(\\kappa^*\\) is chosen such that \\(P(e+\\beta q &gt; \\kappa^*)=P(e&gt;e^*)=p\\). If \\(\\beta\\) is fairly small then we can infer that the iso-retentiveness line is downward-sloping, implying: \\[\\frac{\\rho_e-\\gamma\\rho_q}{\\rho_q-\\gamma\\rho_e}&gt;0.\\]\nThis implies that both engagement and quality have the same sign. I don’t think they both can be negative, so they both must be positive:\n\\[\\begin{aligned}\n      \\rho_e - \\gamma \\rho_q &&gt; 0 \\\\\n      \\rho_q - \\gamma \\rho_e &&gt; 0.\n   \\end{aligned}\\]\nI think it’s reasonable to treat preferences as locally linear. To have a well-defined maximization problem (with an interior solution) we need either nonlinear preferences or a nonlinear Pareto frontier. It’s always easier to treat things as linear when you can, so a relevant question is which of these two is closer to linear? Internally companies often treat their preferences as nonlinear, e.g. setting specific goals and guardrails, but those are always flexible and often have justifications as incentive devices. Typical metric changes are small, only single-digit percentage points, over that range the Pareto frontier does show significant diminishing returns while (it seems to me) value to the company does not."
  },
  {
    "objectID": "posts/2023-04-28-ranking-by-engagement.html#overviews-of-recommender-systems-chronological",
    "href": "posts/2023-04-28-ranking-by-engagement.html#overviews-of-recommender-systems-chronological",
    "title": "Ranking by Engagement",
    "section": "Overviews of Recommender Systems (chronological)",
    "text": "Overviews of Recommender Systems (chronological)\n\nAdomavicius and Tuzhilin (2005) “Toward the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible Extensions”\n\nAn influential overview of recommender systems (14,000 citations!). The canonical example is recommending movies to get the highest predicted rating. They use “rating” as similar to “engagement”. A more recent survey is Roy and Dutta (2022).\n\nDavidson et al. (2010) “The YouTube Video Recommendation System”\n\n\n“[videos] are scored and ranked using … signals [which] can be broadly categorized into three groups corresponding to three different stages of ranking: 1) video quality, 2) user specificity and 3) diversification.””\n\n\n\n\n“The primary metrics we consider include click through rate (CTR), long CTR (only counting clicks that led to watches of a substantial fraction of the video), session length, time until first long watch, and recommendation coverage (the fraction of logged in users with recommendations).”\n\n\n\nThey say recommendations are good because they have high click-through rate. - A blog post from 2012 discusses a switch from views to watch time: “Our video discovery features were previously designed to drive views. This rewarded videos that were successful at attracting clicks, rather than the videos that actually kept viewers engaged. (Cleavage thumbnails, anyone?)”\n\nGomez-Uribe and Hunt (2015) “The Netflix Recommender System: Algorithms, Business Value, and Innovation”\n\nClearly states that they evaluate AB tests using engagement, but it is regarded as an imperfect proxy for retention:\n\n\n\n“we have observed that improving engagement—the time that our members spend viewing Netflix content—is strongly correlated with improving retention. Accordingly, we design randomized, controlled experiments … to compare the medium-term engagement with Netflix along with member cancellation rates across algorithm variants. Algorithms that improve these A/B test metrics are considered better.”\n\n\n\nCovington et al. (2016) “Deep Neural Networks for YouTube Recommendations”\nThis paper proposed a very influential architecture for content recommendation (the paper has 3000 citations). They say:\n\n“Our final ranking objective is constantly being tuned based on live A/B testing results but is generally a simple function of expected watch time per impression. Ranking by click-through rate often promotes deceptive videos that the user does not complete (“clickbait”) whereas watch time better captures engagement”\n\nLada, Wang, & Yan (2021, FB Blog) How does news feed predict what you want to see?\n\nThorburn, Bengani, & Stray (2022, Understanding Recommenders) “How Platform Recommenders Work”\n\nThis is an excellent short article with description and illustration of the stages in building a slate of content: moderation, candidate generation, ranking, and reranking. Includes links to many posts from platforms describing their systems.\n\n\n\n\n\n\n\nArvin Narayanan (2023) “Understanding Social Media Recommendation Algorithms”\n\nA good overview of recommendation algorithms, with an in-depth discussion of Facebook’s MSI.\n\n\nCriticisms of social media recommendation: (1) harm users because “implicit-feedback-based feeds cater to our basest impulses,” (2) harm creators because “engagement optimization … is a fickle overlord,” (3) harms society because “social media platforms are weakening institutions by undermining their quality standards and making them less trustworthy. While this has been widely observed in the case of news … my claim is that every other institution is being affected, even if not to the same degree.”\n\n\nThe technical part of the essay is excellent but I found some of the arguments about harm and social effects hard to follow.\n\n\nKleinberg, Mullainathan, and Raghavan (2022)"
  },
  {
    "objectID": "posts/2023-04-28-ranking-by-engagement.html#proposals-for-change-chronological",
    "href": "posts/2023-04-28-ranking-by-engagement.html#proposals-for-change-chronological",
    "title": "Ranking by Engagement",
    "section": "Proposals for Change (chronological)",
    "text": "Proposals for Change (chronological)\n\nAndrew Mauboussin (2022, SurgeAI) “Moving Beyond Engagement: Optimizing Facebook’s Algorithms for Human Values”\n\nSays that the problem is “the most engaging content is often the most toxic.” They propose using human raters, e.g. ask people “did this post make you feel closer to your friends and family on a 1-5 scale?” They label a small set of FB posts as a proof of concept.\n\nBengani, Stray, & Thorburn (2022,Medium) “What’s Right and What’s Wrong with Optimizing for Engagement”\n\nThey define engagement as “a set of user behaviors, generated in the normal course of interaction with the platform, which are thought to correlate with value to the user, the platform, or other stakeholders.” Reviews evidence for good and bad effects of ranking by engagement.\n\n\nOvadya & Thorburn (2023). Bridging Systems: Open Problems for Countering Destructive Divisiveness across Ranking, Recommenders, and Governance\n\nStray, Iyer, Larrauri (2023) “The Algorithmic Management of Polarization and Violence on Social Media”\n\n\nOur overall goal should be to minimize “destructive conflict”.\n\n\n\n\nThe major lever used has been content moderation: changing the visibility of content based on semantic criteria (e.g. downranking toxic, disallowing hate speech).\n\n\n\n\nHowever we should put relatively more work on system design, e.g. adding friction or changing the mechanics of sharing or engagement-based ranking. In part because there’s a robust correlation between content that causes destructive conflict and content that is engaging.\n\n\n\nMilli, Belli, and Hardt (2021) (2021) “From Optimizing Engagement to Measuring Value”\n\nMilli, Pierson and Garg (2023) Choosing the Right Weights: Balancing Value, Strategy, and Noise in Recommender Systems\n\nI find the model a little hard to follow. \n\n\n\n\nLubin & Gilbert (2023) “Accountability Infrastructure: How to implement limits on platform optimization to protect population health”\n\nA very wide-ranging and loose discussion of issues related to ranking content. Makes an analogy with 19th century measures to control public health. I think the main proposal is that firms come up with metrics to measure their effect on social problems such as mental health, and regularly report on how they’re doing. They suggest requirements for platforms of different sizes:\n\n\n\n\n\n\n\n\n\n1M+\nSubmitted plan for metrics and methods for evaluation of potential structural harms\n\n\n10M+\nConsistent data collection on potential structural harms\n\n\n50M+\nQuarterly, enforceable assessments on product aggregate effects on structural harms, with breakouts for key subgroups\n\n\n100M+\nMonthly, enforceable assessments on product aggregate effects as well as targeted assessments of specific product rollouts for any subproduct used by at least 50 million users, with breakouts for key subgroups"
  },
  {
    "objectID": "posts/2023-07-27-meta-2020-elections-experiments.html",
    "href": "posts/2023-07-27-meta-2020-elections-experiments.html",
    "title": "How Much has Social Media affected Polarization?",
    "section": "",
    "text": "TL;DR: The experiments run by Meta during the 2020 elections were not big enough to test the theory that social media has made a substantial contribution to polarization in the US. Nevertheless there are other reasons to doubt it."
  },
  {
    "objectID": "posts/2023-07-27-meta-2020-elections-experiments.html#discussion",
    "href": "posts/2023-07-27-meta-2020-elections-experiments.html#discussion",
    "title": "How Much has Social Media affected Polarization?",
    "section": "Discussion",
    "text": "Discussion\nTrends in affective polarization. Boxell et al. (2022) document affective polarization across a dozen countries, 1978-2020:\n\n\nIn the US affective polarization index increased from around 25 to 50, “an increase of 1.08 standard deviations as measured in the 1978 distribution.” (I’m not sure if the SD increased).\nAcross the world there’s no clear trend: some countries increased, other countries decreased. This weakens the simple argument that polarization has increased at the same time as social media use.\nIn the US the trend seems to be almost entirely due to increasing negative feelings about the opposing party:\n\n\nThe US timeseries can be seen online from the ANES.\nObservational data finds that much of the growth in polarization in the US was among people who were not online. Boxell et al. (2017) say\n\n“the growth in polarization in recent years [1996-2012] is largest for the demographic groups least likely to use the internet and social media”\n\nContent on Meta platforms. Guess et al. (2023b) has data from the control group in their 2020 experiments:\n\n\n\nShare of Impressions\nFacebook\nInstagram\n\n\n\n\nPolitical content\n14%\n5%\n\n\nPolitical news content\n6%\n-\n\n\nContent from untrustworthy sources\n3%\n1%\n\n\nUncivil content\n3%\n2%\n\n\n\nPew 2022 has data on where people get their news from:\n\n\n\n\npct adults regularly get news from\n\n\n\n\ntelevision\n65%\n\n\nnews websites\n63%\n\n\nsearch\n60%\n\n\nsocial media\n50%\n\n\nradio\n47%\n\n\nprint\n33%\n\n\npodcasts\n23%\n\n\n\nRadio show popularity. Around half of the top 20 most-listened radio shows in the US are conservative talk, with around 90M weekly listeners (this is double-counting overlapping users). Data from 2021.\nTelevision. Fox News is Cable TV’s most-watched network with around 5M regular viewers. (source from 2016).\nTime spent on social media. Statista: Average time-spent 150 minutes/day/person on social networks\nThe academic literature has identified other possible causes of polarization. Some potential causes: southern realignment, 1968 changes to the primary system, the Obama presidency, the tea party movement (though each of these could be in part proximal causes). Martin & Yurcoglu (2017) argue that a large part of recent growth is due to cable news: &gt; “the cable news channels can explain an increase in political polarization of similar size to that observed in the US population over [2000-2008]. … In absolute terms, however, this increase is fairly small.”\nSee also Haidt and Bail’s long document Social Media and Political Dysfunction: A Collaborative Review\nDoes Allcott et al. (2020) find that Facebook use increases polarization? This paper reports on an experiment paying people to stop using Facebook for a month. They find an effect of -0.16 SDs (\\(\\pm\\) 0.08) on a measure they describe as “political polarization,” however there are some subtleties:\n\n\nUnlike the questions used in typical population surveys the questions were explicitly about their feelings during the period of the experiment, e.g. “Thinking back over the last 4 weeks, how warm or cold did you feel towards the parties and the president on the feeling thermometer?”\nPolarization is measured by a composite of different measures. By far the largest effect was on the “congenial news exposure” question: “over the last 4 weeks how often did you see news that made you better understand the point of view of the Democrat (Republican) party?” The score was the difference between the answer for their own party vs the other-side party. It seems to me that it’s not surprising that deactivating Facebook would affect one’s exposure to such news, but that this wouldn’t normally be called a measure of “polarization” in the literature. The paper mentions in a footnote that “the effect on the political polarization index is robust to excluding each of the seven individual component variables,” but it turns out that removing “congenial news exposure” halves the effect-size and shifts the p-value from 0.00 to 0.09 (i.e. from very significant to non-significant). I’m not sure I would describe this as a finding that is “robust”.\nThe paper finds no significant effect on their two “affective polarization” measures (-0.08 \\(\\pm\\) 0.08 SD, and 0 \\(\\pm\\) 0.04 SD), however the 2020 papers which cite Allcott et al. (2020) seem to treat it as finding that Facebook has a positive effect on “polarization” without noting that it has a null effect on affective polarization."
  },
  {
    "objectID": "posts/2024-05-10-premature-optimization.html",
    "href": "posts/2024-05-10-premature-optimization.html",
    "title": "Premature Optimization and the Valley of Confusion",
    "section": "",
    "text": "When formalizing tradeoffs our decisions typically get worse before they get better.\nThose who climb the mountain of efficiency first pass through the valley of confusion.\nData scientists are often asked to provide a formula to calculate the expected costs and benefits of a decision but this is hard. There are often many subtleties which we grasp intuitively but do not know how to formalize. For this reason using a simple model is often worse than no model.\nSome examples of common mistakes:\n\nCausal effects assumed to be equal to correlations.\nDecisions depend on a sharp threshold of statistical significance.\nDecisions depend on average rather than marginal value.\nLack of accounting for opportunity cost.\nLack of accounting for option value.\nLack of accounting for network effects.\nConfusion over whether an outcome is intrinsically desirable or instrumentally desirable.\nEstimating the topline impact of product improvements with estimates from AB tests (i.e. accounting only on effects for existing users, not new users).\n\nThese are all avoidable mistakes but it takes time and experience to model them correctly. In fact many of the tools we use to formalize optimization problems are relatively recent discoveries: over the last few centuries we figured out how to write down probabilities and expected value, linear programming, and dynamic programming. But before these discoveries we were still able to make quite subtle and complex decisions – we were able to build pyramids, welfare states, aeroplanes – our informal and intuitive methods were sufficient.\nThis is independent of whether decisions should be formalized. There are some situation where no formalization of costs and benefits will be sufficient, for various reasons. My point is different: even when we truly care only about a well-defined outcome it’s often better to rely on our instincts than to use a model if we don’t have time to deeply invest in that model.\nIt’s a common idea that formalizing the costs and benefits of a decision can lead to worse decision-making. People talk about “MacNamara’s fallacy,” but this can be taken in two ways: whether problems are intrinsically unquantifiable or just difficult to quantify. In tech companies I think many problems are quantifiable in principle but it’s sufficiently hard that it’s sensible to leave off quantifying until you’re confident that you have a good model.\nThe process of formalizing a decision often looks like a dialogue. When you try to write down a model to represent the tradeoffs of a situation very often it feels like a dialogue with your intuition: you first write a simple model, and see that it implies that we should be making a radically different decision. You try to think through, intuitively, the implications of that decision, and see that there is some important factor that the model is missing, and so revise the model, and then look again at the implications.\nAvoiding the valley of confusion. I have been talking about the choice between using judgment or relying on a model. There is an alternative route: instead of replacing judgment the goal should be to augment it. Broadly speaking by trying to summarize data in a way that helps inform intuitions about tradeoffs. If we continue this process we may end up at a comprehensive formal model which we can trust.\nSome examples:\n\nIf we are trying to improve decisions about which experiments to ship then we can build a table of prior experiments, and for each new experiment summarize how it compares with prior ones. (My post on experiment interpretation expands on this.)\nIf we are trying to improve decisions about the value of metrics as surrogates for causal effects, we can start by visualizing the distributions of each metric and the correlations between them to help build judgment about how they relate to user experience.\nIf we are trying to improve evaluation of a given initiative we should start by mapping out prior initatives and compare their inputs (e.g. headcount, time) and outputs (metrics), to benchmark estimates.\n\nGeneral considerations that apply to all evaluations:\n\nAll models should be described as fundamentally aids to human judgment, and it’s always legitimate to override their recommendations.\nModel output should not be described with a single number but a visualization to show how the broad patterns in data.\nWe should summarize what decisions other people have made in similar situations, both inside and outside the company."
  },
  {
    "objectID": "posts/2023-04-18-experiment-interpretation-extrapolation.html",
    "href": "posts/2023-04-18-experiment-interpretation-extrapolation.html",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "",
    "text": "I give a simple Bayesian way of thinking about experiments, and implications for interpretation and extrapolation.\n\n\nThanks to J. Mark Hou for comments. \nSetup: The canonical tech problem is to choose a policy to maximize long-run user retention. Because the policy space is high-dimensional it’s not feasible to run experiments on every alternative (there are trillions), instead most of the decision-making is done with human intuition based on observational data, and experiments are run to confirm those intuitions.\n\nThe inference problem. The basic problem of experimentation is to estimate the true effect given the observed effect. The problem can become complicated when we have a set of different observed effects, e.g. across experiments, across metrics, across subgroups, or across time. \nTwo common approaches are: (1) adjust confidence intervals (e.g. Bonferroni, always-valid, FDR-adjusted); (2) adjust point estimates based on the distribution (empirical Bayes). Both have significant drawbacks: my suggested approach is to let decision-makers make their own best-estimates of the true effects but provide them with an informative set of benchmark statistics so they can compare the results of any given experiment to the results from a reference group.1\nThe extrapolation problem. Given an effect on metric A what’s our best estimate of the effect on metric B? This problem is common to observational inference, proximal goals, and extrapolation.\nThere are three approaches to solving this: (1) using raw priors; (2) using correlation across units (surrogacy); (3) using correlation across experiments (meta-analysis). I argue that approach #3 is generally the best option but reasonable care needs to be taken in interpreting the results.\n\n1 If the decision-maker is not technical then a data scientist or engineer can summarize for the decision-maker their best-estimate of the true impact on long-run outcomes, taking into account the evidence from the experiment and other sources of evidence, including the distribution of effects from other experiments.I also briefly discuss two additional problems:\n\nThe explore-exploit problem. We would like to choose which experiments to run in an efficient and automated way. I think the technical solution is relatively clear but tech companies have struggled to implement it because good execution requires some discipline. I describe a simple algorithm that is not optimal but very simple and robust.\nThe culture problem. Inside tech companies people keep misusing experiments and misinterpreting the results, especially (1) running under-powered experiments, (2) selectively choosing results, and (3) looking at correlations without thinking about identification.\nA common response is to restrict access to only a subset of experiment resuts. However this often backfires because (1) it is difficult to formally specify the right subset; (2) it reinforces a perception that experimental results can be interpreted as best-estimates of true treatment effects; (3) it reinforces a norm of selecting experimental results as arguments for a desired outcome. I think a better alternative is to explicitly frame the problem as one of predicting the true effect given imperfect evidence, and benchmark peoples’ prior performance in predicting the true effect of an intervention. (This section is unfinished, I hope to add more)."
  },
  {
    "objectID": "posts/2023-04-18-experiment-interpretation-extrapolation.html#strategic-problems",
    "href": "posts/2023-04-18-experiment-interpretation-extrapolation.html#strategic-problems",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "1.1 Strategic Problems",
    "text": "1.1 Strategic Problems\nThere are additionally some strategic problems in experiment interpretation.\n\nStrategic stopping (“peeking”). An engineer will wait until an experiment has a high estimated impact, or low p-value, before presenting it for launch review. A common proposed remedy is that all experiments should be evaluated after the same length of time, or that engineers should pre-specify the length of experiments.\nSelection of treatments (“winners curse”). An engineer will run a dozen variants and only present for launch review the best-performing one. A common proposed remedy is that every variant should be officially presented in launch reviews, even the poorly-performing ones.\nSelection of metrics (“cherry picking”). An engineer will choose to show the experiment results on the metrics that are favorable, not those that are unfavorable. A common proposed remedy is that the set of metrics should be standardized for all launches, or that the set of evaluation metrics should be pre-specified by the engineer (AKA a pre-analysis plan).\n\nI will argue that the commonly proposed remedies are highly imperfect fixes. These are complicated things to think about because the mix together issues of statistical inference and of strategic behaviour. In the discussion that follows I try to separate those out as clearly as possible."
  },
  {
    "objectID": "posts/2023-04-18-experiment-interpretation-extrapolation.html#strategic-stopping",
    "href": "posts/2023-04-18-experiment-interpretation-extrapolation.html#strategic-stopping",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "1.2 Strategic Stopping",
    "text": "1.2 Strategic Stopping\nI will ignore dynamic effects. For simplicity assume that all effects are constant, so the length of an experiment effectively determines just the sample size of that experiment. I.e. I will ignore time-dependent and exposure-dependent effects.\nStopping rules are irrelevant to expected effect sizes. Suppose an experiment has a given estimate. Does it matter to your estimate of the true causal effect if you learn that the experimenter chose the sample size \\(N\\) by a data-dependent rule, e.g. continuing to collect data until the estimate was statistically significant? If you are estimating the true causal effect, \\(E[t|\\hat{t}]\\) then it doesn’t matter, your posterior will be identical either way.7 A simple proof: suppose we observe two noisy signals, \\(x_1\\) and \\(x_2\\): \\[\\begin{aligned}\n         x_1 &= v + e_1 \\\\\n         x_2 &= v + e_2 \\\\\n         v,e_1,e_2 &\\sim  N(0,1)\n      \\end{aligned}\\] Suppose a peeker will report \\(x_1\\) only if \\(x_1&gt;0\\), otherwise they will report \\(x_1+x_2\\). We can compare the expectation of \\(v\\) given the sum, depending on whther the engineer peeked: \\[\\utt{E[v|x_1+x_2]}{estimate}{without peeking} =\n         \\utt{E[v|x_1+x_2|x_1&lt;0]}{estimate}{with peeking}\\] This holds because \\(x_1+x_2\\) is a sufficient statistic for the distribution, i.e. \\(x_1&lt;0\\) does not tell us any additional information. Note that peeking is not irrelevant to interpretation of a result if (1) the engineer can choose to report either \\(x_1\\) or \\(x_2\\), (2) the engineer can choose to report \\(x_1\\) alone after observing \\(x_2\\).[^deng]\n7 This argument holds if the engineer always has to report the most-recent estimate. If they can choose to ignore later datapoints, and report an earlier result, this is essentially a “selection of metrics” case as below, and so the selection rule is relevant for interpretation.  [^deng]: See @deng2016continuous for a fuller argument that stopping rules are irrelevant, and a review of the prior literature.\nStopping rules would be relevant if we made decisions based on statistical-significance. A stopping rule would be relevant if we conditioned only on statistical-signficance instead of the full estimate. In other words the expected true effect, conditioning only on whether or not the estimated effect is statistically significant, will depend on the stopping rule. For example if people kept running experiments until they were significant then significant experiments would tend to have small effect sizes. However it is clearly bad practice to condition only on this binary piece of information when you have the full estimate, and if you have the full estimate then the stopping rule becomes irrelevant.\nThe optimal stopping rule is data-dependent. The discussion above took a stopping rule as given, we can also ask what’s the efficient stopping rule. It’s clear that a fixed length is inefficient: we should stop an experiment sooner if it does unexpectedly well or unexpectedly badly, in both of those cases the value of collecting more information has decreased because it’s less likely to change our mind about a launch decision. Thus enforcing a static or pre-specific experiment length will lead to inefficient decision-making.\nConsidering engineers’ incentives. Now consider the launch process as a game, with the engineers trying to persuade the director to launch their feature. Suppose the director’s ex post optimal strategy is to launch if \\(E[t|\\hat{t}]&gt;0\\), and suppose the engineers get a bonus whenever their feature is launched. In equilibrium the engineers will keep their experiments running until \\(E[t|\\hat{t}]&gt;0\\), which will cause a skew distribution: the distribution of posteriors will show a cluster just above the threshold. The director’s strategy is ex post optimal but it’s not an efficient use of experimentation resources. In this game the director would likely wish to pre-commit to a different threshold which induces more efficient effort by engineers. However a more direct solution would be to align engineers’ incentives with those of the director by rewarding them for their true impact, i.e. setting their bonuses proportional to \\(\\max\\{E[t|\\hat{t}],0\\}\\), instead of discontinuously rewarding them for whether or not they launched."
  },
  {
    "objectID": "posts/2023-04-18-experiment-interpretation-extrapolation.html#selection-of-treatments",
    "href": "posts/2023-04-18-experiment-interpretation-extrapolation.html#selection-of-treatments",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "1.3 Selection of Treatments",
    "text": "1.3 Selection of Treatments\nIf you learn an experiment is the top-performing variant it should change your asssessment. Suppose we have a result \\(\\hat{t}_1\\), and we are estimating the true treatment effect, \\(t_1\\). If we learn that another variant has a lower treatment effect, \\(\\hat{t_1}&gt;\\hat{t}_2\\), then it is rational to update our assessment of \\(t_1\\):\n\\[\\utt{E[t_1|\\hat{t}_1,\\hat{t}_1&gt;\\hat{t}_2]}{assessment knowing}{it's winner}&lt;\n      \\utt{E[t_1|\\hat{t}_1]}{assessment}{given outcome}\n      \\]\nThis will hold whenever \\(Cov(t_1,t_2)&gt;0\\), i.e. when we have some shared source of uncertainty about the two treatment effects.8 We can write a model for this, however conditioning on this binary information (whether a variant is the winner) is not an efficient way of using the information at your disposal.\n8 Because \\(t_1\\) and \\(t_2\\) represent independent experiments we’ll have \\(cov(e_1,e_2)=0\\).9 Andrews et al. (2019) describes some unbiased estimates for treatment effects conditional on them being winners. In general I would say this is an inefficient use of information, because we know much more about the distribution of treatment effects than just whether a specific variant is the winner. However that paper does argue that empirical Bayes estimates struggle when the sample-size is small or when we are estimating the tails of when variants are non-exchangeable, and in those cases the unbiased estimators may be useful.It’s better to condition on the whole distribution. In almost all cases we know much more than whether \\(\\hat{t}_1\\) is the winner, we also know the value of \\(\\hat{t}_2\\), and then this reduces simply to the empirical Bayes problem, i.e. we simply wish to estimate: \\[E[t_1|\\hat{t}_1,\\ldots,\\hat{t}_n],\\] and we can do that in the usual way.9 E.g. if we have a Normal prior over treatment effects then we can estimate \\(\\sigma_t^2\\) from \\(Var(\\hat{t})\\) and \\(\\sigma_e^2\\). Once we have conditioned on \\(\\sigma_t^2\\) then it becomes irrelevant whether variant 1 is the winner or not, i.e.: \\[E[t_1|\\hat{t}_1,\\sigma_t^2]=E[t_1|\\hat{t}_1,\\sigma_t^2,\\hat{t}_1&gt;\\hat{t}_2].\\]\nPut another way: the selection rule is irrelevant (just as the stopping rule is irrelevant) once we condition on the distribution of observed outcomes.\nImplication: show the distribution. If we are worried that engineers are selecting variants based on their outcomes then the simplest and cleanest fix is to calculate the distribution of variants and use that to discount any experiment results, either explicitly with an empirical Bayes estimator, or implicitly by showing the decision-maker the distribution."
  },
  {
    "objectID": "posts/2023-04-18-experiment-interpretation-extrapolation.html#selection-of-metrics",
    "href": "posts/2023-04-18-experiment-interpretation-extrapolation.html#selection-of-metrics",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "1.4 Selection of Metrics",
    "text": "1.4 Selection of Metrics\nSuppose engineers are selectively presenting the most favorable metrics. Suppose there are two outcome metrics from a single experiment, and the engineer will present whichever is the most favorable. Knowing this fact should rationally affect your judgment of the treatment effect on the presented metric: \\[\\utt{E[t_1|\\hat{t}_1]}{assessment knowing}{only metric 1} &gt;\n      \\utt{E[t_1|\\hat{t}_1,\\hat{t}_1&gt;\\hat{t}_2]}{assessment knowing}{metric 1 beats metric 2}\\]\nImplication: engineers should present all outcome metrics."
  },
  {
    "objectID": "posts/2023-04-18-experiment-interpretation-extrapolation.html#on-launch-criteria",
    "href": "posts/2023-04-18-experiment-interpretation-extrapolation.html#on-launch-criteria",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "1.5 On Launch Criteria",
    "text": "1.5 On Launch Criteria\nChoosing weights on metrics for a launch decisions involves many considerations: network effects, noise, cross-metric proxy effects, and dynamic effects. In addition launch rules serve a bureaucratic role, and engineers will often want the launch rule to be public and without discretion. To make clear decisions it’s important to peel apart these layers, I recommend these steps:\n\nChoose a set of final metrics. These are the metrics we would care about if we had perfect knowledge of the experimental effect. We can define tradeoffs between them, it’s convenient to express those tradeoffs in terms of percentage changes, e.g. we might be indifferent between 1% DAU, 2% time/DAU, and 5% prevalence of bad content.10\nChoose a set of proximal metrics. These are the metrics on which we are confident we can detect our experiment’s effect, meaning the measured impact will be close to the true impact on these metrics (i.e. has a high signal-noise ratio). To determine whether a metric is moved we can use the fraction of a given class of experiments that have a statistically-significant effect on that metric: if the share is greater than 50% then we can be confident that the estimated effect is close to the true effect.\nIdentify conversion factors between proximal and final metrics. These tell us the best-estimate impact on final metrics given the impact on proximal metrics. Conversion factors can be estimated either from (a) long-running tuning experiments; (b) a meta-analysis of prior experiments with similar designs.\nA final linear launch criteria can then be expressed as a set of conversion-factor weights applied to each of the proximal metrics.11\n\n10 Arguably revenue or profit is a more truly final metric, and these are just proxies, but these are probably close enough to final for most purposes.11 For derivation see Cunningham and Kim (2019)."
  },
  {
    "objectID": "posts/2023-04-18-experiment-interpretation-extrapolation.html#comparing-launch-rules",
    "href": "posts/2023-04-18-experiment-interpretation-extrapolation.html#comparing-launch-rules",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "1.6 Comparing Launch Rules",
    "text": "1.6 Comparing Launch Rules\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShip if sum is positive\n\n\n\n\n\n\n\n\n\n\n\n\nI find it useful to visualize different launch rules. For simplicity suppose our utility function is linear: we have two metrics, 1 and 2, and we care about them equally: \\[U(t_1,t_2)=t_1+t_2.\\] But we only observe noisy estimates \\(\\hat{t}_1,\\hat{t}_2\\).\nKohavi et al. (2020) recommend a stat-sig shipping rule. They say (p105):\n\nIf no metrics are positive-significant then do not ship\nIf some are positive-significant and none are negative-significant then ship\nIf some are positive-significant and some are negative-significant then “decide based on the tradeoffs.\n\nI represent this in the first diagram (but I treat condition 3 as a non-ship). The dotted line represents \\(\\hat{t}_1+\\hat{t}_2=0\\).\nThe stat-sig shipping rule has strange consequences. You can see that this rule will recommend shipping things even with negative face-value utility (\\(U(\\hat{t}_1,\\hat{t}_2)&lt;0\\)), when there’s a negative outcome on the relatively noisier metric. This will still hold if we evaluate utility with shrunk estimates, when there’s equal proportional shrinkage on the two metrics, but if there’s greater shrinkage on the noisier metric it will not hold.\nLinear shipping rules are better. In the margin I illustrate (1) a rule to ship wherever the sum is positive; (2) a rule to ship wherever the sum is stat-sig positive. I have drawn the second assuming that \\(cov(\\hat{t}_1,\\hat{t}_2)=0\\). With a positive covariance the threshold would be higher.\n\n\n\n\n\n\n\n\n\nThe Leontief sandwich. I assumed above that our true utility function is linear. In fact tech companies often explicitly give nonlinear objective functions to teams, e.g.: \\[\\begin{aligned}\n      \\max_k &\\ A(k)\n         && \\text{(goal)} \\\\\n      \\text{s.t.} &\\ B(k)\\leq \\bar{B}\n         && \\text{(guardrail)}\n   \\end{aligned}\n   \\]\nThis is illustrated at right, the indifference curves are L-shaped so I’ll call it a Leontief utility. Having Leontief preferences can cause some unintuitive decision-making, in particular the tradeoff between \\(A\\) and \\(B\\) will varies drastically depending on your location. One important observation is that if your goal is assessed at the end of some time-point (e.g. at the end of the half) then optimal launch decisions will depend on your future expectations, e.g. you’d be willing to launch a feature that boosts A at the cost of B only if you expect a future launch to make up that deficit in B.\nIn practice I think it’s useful to think of this nonlinear objective function as sitting in the middle of the hierarchy of an organization, with approximately linear objective functions above and below it, i.e. a “Leontief sandwich.”\nAt the highest layer the CEO (or shareholders) care about all the metrics in way that is locally linear, i.e. they do not have sharp discontinuities in how they assess the company’s health. At the lowest layer engineers and data scientists are trying to make individual changes that achieve the Org’s overall goals, but because they only account for a small share of the overall org’s impact they can treat their objectives as locally linear (& likewise in a value function we make linear tradeoffs between objectives because we’re in such a small region). Finally even for orgs which have nonlinear objective functions it’s often reasonable to think of the nonlinearities as “soft”, e.g. if an org comes in slightly below a guardrail the punishment is slight, and if they come in above the guardrail then they will be rewarded. This softening makes the effective objective function much closer to linear, and so I think for many practical purposes it’s reasonable to start with a linear objective function."
  },
  {
    "objectID": "posts/2023-04-18-experiment-interpretation-extrapolation.html#with-meta-analysis",
    "href": "posts/2023-04-18-experiment-interpretation-extrapolation.html#with-meta-analysis",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "2.1 With Meta-Analysis",
    "text": "2.1 With Meta-Analysis\nWith \\(n\\) metrics we can write the underlying model as: \\[\\utt{\\pmatrix{\\hat{t}_1\\\\\\vdots\\\\\\hat{t}_n}}{observed}{effects}\n      = \\utt{\\pmatrix{t_1\\\\\\vdots\\\\t_n}}{true}{effects}\n         +\\utt{\\pmatrix{e_1\\\\\\vdots\\\\e_n}}{noise}{(=user variation)}\n   \\]\nHere we are treating \\(\\Delta \\text{DAU}_{SR}\\) and \\(\\Delta \\text{DAU}_{LR}\\) as two different metrics, but for some experiments we only observe the first. We thus want to estimate the effect on long-run retention (DAU\\(_{LR}\\)) given short-run metrics. \\[E[\\Delta\\text{DAU}_{LR} |\n       \\Delta \\widehat{\\text{DAU}}_{SR}, \\ldots, \\Delta\\widehat{\\text{engagement}}_{SR}]\n   \\]\nwhere \\[\\begin{aligned}\n      \\Delta\\text{DAU}_{LR}   &= \\textit{true}\\text{ effect on long-run daily active users (AKA retention)}\\\\\n      \\Delta\\widehat{\\text{DAU}}_{SR} &= \\textit{estimated}\\text{ effect on short-run daily active users} \\\\\n      \\Delta\\widehat{\\text{engagement}}_{SR} &= \\textit{estimated}\\text{ effect on short-run engagement}\n   \\end{aligned}\n   \\]\nRunning a Regression will be Biased. The obvious thing to do is run a regression across experiments: \\[\\Delta\\widehat{\\text{DAU}}_{LR} \\sim\n      \\Delta \\widehat{\\text{DAU}}_{SR} + \\ldots + \\Delta\\widehat{\\text{engagement}}_{SR}\n   \\]\nHowever this will be biased. The simplest way to demonstrate the bias is to show that even with AA tests (where there is zero treatment effect on either metric) we will still get a strong predictive relationship between the observed treatment effects on each of the two metrics (see figure).\n\n\n\n\n\nA simulated scatter-plot showing 20 experiments, with N=1,000,000, \\(\\sigma_{e1}^2=\\sigma_{e2}^2=1\\), with correlation 0.8. The experiments are all AA-tests, i.e. there are no true treatment effects, yet a regression of \\(\\hat{t}_2\\) on \\(\\hat{t}_1\\) will consistently yield statistically-significant coefficients of around 0.8.\n\n\nThe bias is because in the regression our LHS variable is estimated retention (\\(\\Delta\\widehat{\\text{DAU}}_{LR}\\) instead of \\(\\Delta\\text{DAU}_{LR}\\)), and the noise in that estimate will be correlated with the noise in the estimates of short-run metrics. In the linear bivariate case (where we have just one RHS variable) then we can write: \\[\\begin{aligned}\n      \\ut{\\frac{cov(\\hat{t}_2,\\hat{t}_1)}{var(\\hat{t}_1)}}{regression}\n      = \\utt{\\frac{cov(t_2,\\hat{t}_1)}{var(\\hat{t_1})}}{what we}{want to know}\n         + \\ut{\\frac{cov(e_2,e_1)}{var(\\hat{t}_1)}}{bias}\n   \\end{aligned}\\]\nThe bias will be small if the short-run metrics have high signal-noise ratios (SNR), \\(\\frac{var(t_1)}{var(e_1)}\\gg 0\\). A simple test for SNR ratio is the distribution of p-values: if most experiments are significant then the SNR is high. However in the typical case (1) \\(\\Delta \\widehat{\\text{DAU}}_{SR}\\) is the best predictor of \\(\\Delta \\widehat{\\text{DAU}}_{LR}\\); and (2) \\(\\Delta \\widehat{\\text{DAU}}_{SR}\\) has a low signal-noise ratio (i.e. few outcomes are stat-sig). This means the bias is large, and so results are hard to interpret.\n\n2.1.1 Adjusting for the Bias\nHere are some alternatives:\n\nRun a regression just using the high-SNR metrics. We could just drop \\(\\Delta\\widehat{\\text{DAU}}_{SR}\\) as a regressor because of the bias, but we lose predictive power (\\(R^2\\)) so it’s hard to know when this will be a good idea without an explicit model.\nAdjust for bias in linear estimator. If we want a linear estimator then we can estimate and adjust for the bias. \\[\\begin{aligned}\n   \\utt{\\frac{cov(t_2,\\hat{t}_1)}{var(\\hat{t_1})}}{BLUE for}{$t_2$ given $\\hat{t}_1$}\n      &= \\frac{cov(t_2,t_1)}{var(\\hat{t}_1)}\n      = \\ut{\\frac{cov(\\hat{t}_2,\\hat{t}_1)}{var(\\hat{t}_1)}}{regression result}\n         - \\utt{\\frac{cov(e_2,e_1)}{var(\\hat{t}_1)}}{observable}{variables}\n\\end{aligned}\n\\]\nIf everything is joint normal then the expectation is itself linear, and so this will be optimal. In practice the true distribution of effect-sizes is somewhat fat-tailed, which imply that the conditional expectation will be nonlinear in the observables. Nevertheless I think this is a good start. (One other complication is that the SNR is more complicated to calculate when experiments vary in their sample size).12\nUse experiment splitting. You can randomly assign users in each experiment to one or other sub-experiments. You now effectively have a set of pairs of experiments, each of which has experiments with identical treatment effects (\\(\\Delta \\text{DAU}_{LR}\\)) but independent noise. Thus you can run a regression with LHS from one split, and RHS from other split, and you’ll get an unbiased estimate. Additionally you can easily fit a nonlinear model (Coey and Cunningham (2019) has details of how to do an experiment-splitting).\nRun a regression just using the strongest experiments. If the distribution of experiments is fat-tailed then the strongest experiments will have higher SNR, and so lower bias. A worry about this is that you’re only estimating the relationship from outliers, so nonlinearities are more of a worry. At the same time the assumption of fat-tailed treatment-effects gives reason to believe the expectation will be nonlinear. (This is roughly how I interpret the Peysakhovich and Eckles (2018) experiments-as-instruments paper. They propose using L0 regularization and experiment-splitting cross-validations, which I think effectively selects the strongest experiments.)\n\n12 See Cunningham and Kim (2019), and see Tripuraneni et al. (2023) for a slightly different setup with weaker assumptions.Choosing a Reference Class. It is important to think about the reference-class of experiments which we use to calibrate our estimates. The long-run DAU prediction can be though of as an empirical-bayes estimate, which is our best estimate conditional on the experiment being a random draw from this class of experiments.\nIn many cases a company’s experiments will naturally fall into different classes: e.g. some have a very steep relationship between engagement and DAU, others have a very flat. It’s important to both (1) visualize all the experiments, so that a reference-class can be chosen sensibly; (2) calculate the \\(R^2\\) across experiments, so we can have some sense of confidence in our extrapolation."
  },
  {
    "objectID": "posts/2023-04-18-experiment-interpretation-extrapolation.html#observational-inference",
    "href": "posts/2023-04-18-experiment-interpretation-extrapolation.html#observational-inference",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "2.2 Observational Inference",
    "text": "2.2 Observational Inference\nWhat we want to know: Given the short-run effect of a content experiment on engagement we want to predict the long-run effect on DAU. We can start with a simple regression along these lines: \\[\\utt{\\text{DAU}_{u,t+1}}{long-run}{retention} \\sim \\utt{\\text{engagement}_{u,t}}{short-run}{engagement}\\]\nWe could set up a DAG and discuss the surrogacy conditions. The condition are that (1) all effects of an experiment on DAU are via short-run engagement; and (2) there is no unobserved factor which affects both SR engagement and LR DAU:\n\\[\\xymatrix{\n      &  *+[F-:&lt;6pt&gt;]\\txt{unobserved}\\ar@{.&gt;}[d] \\ar@{.&gt;}[dr] \\\\\n         *+[F]{\\text{experiment}} \\ar[r] \\ar@{.&gt;}@/_1pc/[rr]\n         & *+[F]{\\text{SR engagement}}\\ar[r]\n         & *+[F]{\\text{LR DAU}}\n      }\n\\]\nIn fact we know that engagement doesn’t literally lie on the causal chain, instead we think engagement is a good proxy for content which might lie on the causal chain.\nIn any case I find the following setup an easier way to think about the assumptions necessary for identification:\nWe can write it out a simple structural model as follows (for compactness I leave out coefficients):\n\\[\\begin{array}{rcccccccc}\n   \\text{engagement}_{u,t}\n      &=& \\utt{\\text{temperament}_{u}}{user-specific}{propensity to engage}  \n      &+& \\utt{\\text{mood}_{u,t}}{time-varying}{mood/holiday/etc.}\n      &+& \\utt{\\text{content}_{u,t}}{content seen}{on platform}\n      &+& \\utt{\\text{distractions}_{u,t}}{other platform effects}{e.g. messages, notifs}\\\\\n   \\text{DAU}_{u,t}\n      &=& \\text{temperament}_{u}\n      &+&\\text{mood}_{u,t}\n      &+&\\utt{\\sum_{s=1}^\\infty\\beta^s\\text{content}_{u,t-s}}{prior experience}{w content}\n      &+&\\text{distractions}_{u,t}\\\\\n\\end{array}\n\\]\nSome general observations:\n\nWe would get a more credible estimate if we could directly measure content quality. E.g. if we could use the quality of the content available to the user on the RHS, instead of just their engagement on that content. This wouldn’t get perfect identification but it would help.\nThe relative shares of variation in the RHS is important. If most of the variation in engagement is due to variation in content (i.e. high \\(R^2\\) from content), then we don’t need to worry much about confounding from other effects. We can think of introducing control variables as a way of increasing the share of varation in engagement due to content.\nWe should control for distractions. If we have measures of app-related events that don’t affect content-seen but do affect engagement, e.g. notifications, messages, then we should use those as controls. This will increase the relative share of variation in engagement due to content.\nControlling for pre-treatment outcomes changes variation used. If we control for engagement\\(_{t-1}\\) this will change the relative contribution of each factor in the variation of engagement. Specifically it will reduce the share of the terms with higher autocorrelation. Thus by definition temperament will reduce its contribution. However it’s unclear whether mood or content has higher autocorrelation, and so controlling for pre-treatment could either increase or decrease the relative contribution of content. It’s probably worth doing some simple decomposition of variation in engagement into (1) user, (2) content, and (3) mood (the residual), both statically and over time.\nUnivariate linear prediction is usually pretty good. In my experience you can get a fairly good prediction of most user-level metrics with a linear function of the lagged values. If you use a multivariate or nonlinear function you’ll get a better fit but only by a small amount (one exception: when predicting discrete variables like DAU it’s useful to use a continuous lagged variable like time-spent). So I’m skeptical that adding more regressors or adding nonlinearity will significantly change the estimates or the credibility of the estimates.\nEstimand is not \\(\\beta\\) but \\(\\frac{1}{1-\\beta}\\). Suppose we see that 1 unit of engagement causes a certain increase in DAU over the following weeks. We then want to apply that estimate to an experiment which permanently increases engagement by 1 unit. We thus should take the integral over all the subsequent DAU effects. In the simple exponential case the effect of a shock at period \\(t\\) on DAU at period \\(t+s\\) will be \\(\\beta^s\\), and so the cumulative effect on all subsequent periods will be \\(1+\\beta+\\beta^2+\\ldots=\\frac{1}{1-\\beta}\\).\nAutocorrelation in content makes things messier. If there is significant autocorrelation in content then the interpretation of DAU~engagement is more difficult. E.g. if we see that engagement on \\(t\\) is correlated with DAU on \\(t+1\\) this could be because either (1) content on \\(t\\) content caused the DAU on \\(t+1\\), or (2) good content on \\(t\\) is correlated with good content on \\(t+1\\), which in turn causes DAU on \\(t+1\\). I don’t think controlling for pre-treatment levels or trends solves this."
  },
  {
    "objectID": "posts/2024-10-27-from-citlali.html",
    "href": "posts/2024-10-27-from-citlali.html",
    "title": "Tom Cunningham",
    "section": "",
    "text": "hi papi"
  },
  {
    "objectID": "posts/2017-02-25-economist-explorers.html",
    "href": "posts/2017-02-25-economist-explorers.html",
    "title": "Economist Explorers",
    "section": "",
    "text": "explorers\n\n\nImagine a group of adventurers set out to explore a new continent, each one choosing a different valley to map. And suppose that, instead of sending back reports of what they found (“a forest, a swamp, mosquitoes”), each one wrote reports calculated to appear as exciting as possible (“a forest, abundant water, rich fauna, couldn’t disconfirm rumors of a city of gold”).\nThis is how I feel when I’m refereeing economics papers: everyone’s trying to tell an exciting story, and it takes a great deal of work to figure out what actual novel facts they have discovered. It’s frustrating because it’s such an inefficient way to explore the territory: so many people have spent so much time on this, and we have so little to show. What have we discovered about decision-making in the last 50 years after proposing thousands of different models, running tens of thousands of experiments, and regressing millions of variables? Couldn’t we have accumulated more knowledge if we’d organized things differently?\n(Although I have to admit that many of my own papers do include some speculation about cities of gold. But I think the most useful thing a referee can do is to suggest changes to the title and abstract, to make it more transparent exactly what the paper has found.)\nMy original post on Facebook\n–\nFollowup: what have we discovered about decision-making in the last 50 years?\nOnce, when I taught a graduate class in behavioural economics, a couple of students came from civil engineering & dentistry, hoping to learn something useful that they could use in modelling decision-making - e.g. in modelling how people make decisions about commuting. I was embarrassed in how little I could help them. I was able to think of a lot of useful stuff about decision making that’s ~50 years or older: utility & expected utility, exponential (& hyperbolic) discounting, Engel curves, responses to permanent & temporary income, tractable demand estimation, the offsetting income & substitution effects of wages. All of this is immediately useful in quantifying decision-making. But I can think of few recent examples of quantitatively useful findings. Special mention to kahneman & tversky. If you ask, say, about attitudes to uncertainty, to time, to temptation, to intertemporal complementarities, I would begin my answer with “there are various schools of thought…”."
  },
  {
    "objectID": "posts/2020-10-02-on-deriving-things.html",
    "href": "posts/2020-10-02-on-deriving-things.html",
    "title": "On Deriving Things",
    "section": "",
    "text": "I’ve spent a lot of time trying to prove things.\n\nWith diagrams and algebra, back and forth between clipboard whiteboard blackboard & keyboard.\n\nI can’t talk about what it’s like for a good mathematician but I can talk about it for a hack.\n\nI can prove true & interesting things occasionally but only after wallowing in it for a long time, and after a half-dozen proofs of things that I later realize are not interesting or not true.\n\nThe representation of a problem becomes gradually more abstract.\n\nI start with a full set of equations, then I gradually omit things that seem inessential, and I introduce a new symbol which feels like it denotes something but I’m not sure what, and then another higher level of abstraction, a jenga tower to the ceiling. I’m holding it very delicately and it’s crucial not to lose that concentration as I can see it teeter. Sometimes I stare out the window too long and when I look back down at the page the symbols no longer make sense: was this a matrix and that a vector? Why was I multiplying them together in this weird way?\n\nI take a problem and present it to my intuition to see if there’s a glimmer of recognition.\n\nI show it from different angles: vary the notation, write worked examples, draw arrow diagrams, draw venn diagrams, hoping that one of these approaches will trigger a memory of something similar.\nI walk around the house checking the windows and doors. I check whether the answers are in the right units: dollars/person, quantity/year, and I check whether the signs go in the right direction.\n\nI lose track of which way the inequality should be pointing, left or right.\n\nMy intuition doesn’t give me a strong signal so I just choose one and stick with it hoping that once I arrive at the conclusion I can work backwards and fix the directions.\n\n\nMargaret Bray told me that, when she worked correcting Stiglitz’s proofs in the 1970s, she would sometimes find an odd number of sign errors instead of an even number. When that happened she would tell him it and he would quickly rewrite the discussion of that result: substituting an intuitive explanation of why the effect of \\(\\theta\\) on F was negative for what had been an explanation of why the effect of \\(\\theta\\) on F was positive.\n\nSometimes the goal is to maximize the distance between the assumptions and the conclusion.\n\nI’m trying to make it seem more impressive. At first the starting point and end point are within sight of each other, and I hack away at either side, loosening the assumptions, tightening the conclusions, until the path stretches as far as possible.\n\n\nI sometimes end up chasing what looks impressive. I twist the wording of the assumptions to make them seem weaker than they are, the conclusions to seem stronger, or hide ancillary assumptions in the body of the derivation. Looking back to some of my old papers I see signs of this and I’m ashamed of it.\n\nThere are a half-dozen different things I need to keep ambiguous.\n\nI’m not sure which disambiguation will be the right one - whether to use strict or weak preferences; whether the numbering should start at zero or one; whether naturals or wholes or integers. Making a decision at one stage has implications for others. This part is Sudoku, finding something which fits many constraints.\n\nI switch between two modes: is this true? How do I show everyone it’s true?\n\nIn the second mode it’s just trying to keep everything straight to nail it down but then occasionally a sickening feeling arises that it’s not my clumsiness that’s holding me back but perhaps the thing isn’t true after all, and I have to return to the first mode.\n\nSometimes I’m stumbling through equations trying to derive one from another but I have lost track of what they mean.\n\nLike going through a room in the dark grasping for a door handle. I want to go back and draw some diagrams to get an intuition of what’s going on, some mental picture, but sometimes I try and try and nothing comes.\n\nMy coauthor suggests modifying an assumption in the setup.\n\nTaking apart a Lego car to rearrange the unsatisfactory front wheels but it’s hard to anticipate all the downstream consequences of that; I don’t remember all the considerations I had in mind when I put together the front wheels.\n\nEventually the relief of finding the hidden door.\n\nSuddenly I see the inequalities holding down variables like saxophone keys, which gives me the lemma: a condition for a set of inequalities between the elements of two vectors to imply an inequality between the sums of the vectors. I don’t need to write it down, I can take a break and go for a run, it’s enough to remember the idea of saxophone keys. Later I find out this is called Hall’s marriage theorem.\n\n\n\n\nI’ll be writing out an equation for the 3rd time and suddenly a dusty image will be released.\n\nWhen I quiet my brain to concentrate on just one thing the background noise becomes audible. I can hear the mice scratching in the walls. Not just the familiar memories, unfamiliar long lost memories come out too.\nA sparse park in Manchester, a man who blanked me in a corridor in London, a cheese shop in Stockholm.\n\nMy desk is covered with notes: should write up what I have or push on towards the summit?\n\nIt’s a dangerous tradeoff. In retrospect it seems like I’ve often made the wrong decision: too-often tried for the summit, failed, and then left without a proper writeup. I come back to make another attempt, see my old footsteps and regret that I didn’t spend more time hammering nails into the rock.\n\nSometimes a gap opens up between what’s important and what’s orthodox.\n\nIf I make an assumption that is somewhat stronger than typical then everything is much more elegant and it feels like I can keep focus on what’s important. But then I get memories of being in seminars and seeing the speaker making an unorthodox assumption and how the energy drains out of the room.\n\nFor a while I tried to solve things falling asleep.\n\nOnce, in Birkbeck in 2008, as my mind untethered itself from its dock and I suddenly glimpsed that the integral could be taken vertically instead of horizontally. The problem was solved. For a few months afterwards I would put my book aside before I was ready to go to sleep and bring to mind some other unsolved problem but it never worked as well again.\n\nMore memories are released.\n\nBeer at the dreary Birkbeck bar: we were all grasping around for things to say to each other to fill up an evening with. When we heard the kids roaring by the pool table we all looked up.\nSaturday morning on the way back from the vegetable market in Farringdon. I sit down with a sandwich in a churchyard and I see my German neighbors are there too. I’m self conscious that my loneliness is exposed.\nMaking Will’s grandmother a shandy of beer and lemonade. Noticing the shadows of the roses in her garden, how they would change when I stood near them, the proximity of my own shadow would somehow make the shadows of the roses come into focus.\n\n\n\n\nPerhaps 1/3 of the time is actually concentrating on the problem.\n\nIf it’s writing or programming I can just bring up a window and type away. If it’s deriving things then my mind is constantly drifting, trying to find some other path to go down – in reverie, doodling Zeus, refilling my pen, or looking something up.\n\nOn a good day it’s like swimming in cold water.\n\nI don’t want to get in but once I’m in I don’t want to get out.\nOnce my chair is firmly pushed in at the desk I don’t want to get up to get a snack, to check the mail, I don’t want to go to the bathroom, I’ll be working through proofs while shaking my legs trying to stretch my bladder out.\nI procrastinate starting work on a revision but once the document is open and the seal is broken I see things to fix and it’s difficult to stop.\nWhen daughter was a baby she would push away a bottle of warm milk until the nipple was in her lips and then she’d clutch it tight while she’d drink.\n\nOn a bad day I’m rotating slices of a Rubik’s cube.\n\nI’m checking to see if it’s solved then rotating again in some other direction.\n\nOften I’ll over-estimate what I can accomplish.\n\nI’ll have some cocky confidence that, this time, I can prove something I’ve failed to prove before. Or I’ll have a sketch and decide that it’s good-enough, that I can fill in the details later. Then later get a sickening feeling discovering that it’s really nothing, the bits omitted from the sketch were exactly the difficult parts. The voice in my head that causes these over-optimistic judgments causes other griefs in my life I think.\n\nI get flashes of recollection of other peoples’ seminars and papers.\n\nThey seemed so boring and now my paper seems so similar to theirs.\n\nThe ideal state of mind has both (1) the clarity of fresh eyes; and (2) the suppleness of familiarity.\n\nFor that reason it makes sense to spend an entire day thinking about it, not little blocks of hours.\n\n\n\nWeaselly unworthy thoughts bubble up.\n\nI find myself thinking that a reader will be impressed by the quantity and the complexity of the derivation and give me credit for that.\n\nWhen I’m trying to concentrate on something my weasel thinks of something I could order on Amazon.\n\nI can suppress that. My weasel tries to get me to look up how to do a LaTeX symbol that would be useful (delta over equals). I can suppress that. The weasel sees a pair of brackets that could have an extra space. I give in. I fix the brackets. But when I’m fixing brackets I’m more vulnerable to each of the other temptations, and then after a while the weasel tells me it’s almost lunchtime. I look back on my morning it looks like swiss cheese.\n\nI’m a grown adult but my concentration is still not under my control.\n\nI put out treats for it, entreating it like a dog. Teach it bad habits. Give in to its whims.\n\nWhen I switched from programming to studying economics I missed those numb hours that would pass by.\n\nSome people seem to get into that state when they’re deriving things but it’s difficult for me to achieve that.\nWhen you’re programming you get incremental feedback: you can see the mountain peak and you’re slowly getting closer to it. With proofs you’re going through the jungle and you don’t know if you’re getting closer or farther away. You could be on the brink of emerging into a clearing or it could be months more in the forest.\n\nWhy am I doing this?\n\nAs I’m feeling around a problem I think of a guy I knew from seminars at Harvard who would pause when a new slide came up and then nod quietly when he understood. Why do I spend my time at this when so many other people are better at this than me? Am I doing this for intrinsic or extrinsic reasons? One instinct is to untangle the threads of means and ends to see where they lead, another instinct tells me to leave it tangled, and I trust the second instinct more.\n\nI look up from the corner I’m stitching.\n\nI see the quilt goes to the end of the bed, out the window, across the fields all the way to the horizon.\n\nReading through an old draft like a landlord returning to his estate after a long trip.\n\nTo be reminded of the state of things. I’m more often surprised unpleasantly than pleasantly. My memory must be pickling things in a sweet vinegar.\n\nComing back to writing a paper after 5 years at Facebook I had a different kind of confidence.\n\nOld anxieties were about failing to do things in the orthodox way. Now I feel that if it makes sense to me I won’t be embarrassed by it. I might be wrong but I’m not a fraud.\n\nI’m comparing myself, and it seems unhealthy, but then also necessary.\n\nAm I clearer sighted than I was last month? My decision about whether to attempt to solve this problem depends on that judgment.\nI see that the problem I’m attempting is one that was notably not solved in a paper by Benabou and Tirole: whether I should give up now depends on how I assess my ability compared to theirs.\nI’ve spent two hours trying out different matrices to see if they have a certain property, looking for a generalization about which matrices satisfy it, I still haven’t got one and now I don’t recall why I felt this was important – how much do I trust my decision to go down this road, and at what point do I go back to reconsider whether this problem is worth solving?\n\nIn Tel Aviv I would swim in the sea in the mornings.\n\nBy the third week I was still going down to the beach, getting in the water, and starting to swim, but after 30 seconds I would let my legs fall to the sand and just walk through the water staring into space, thinking. After a while I would remember to start swimming again.\n\n\n\nSome things I proved\nIt’s likely that some of these had been proved before, but they each seemed useful enough to prove in the time and place.\n\nThe interpretation of an experimental outcome will depend on other outcomes.\n\nYour estimate of the treatment effect on one outcome will depend on the observed effect on the other outcome, and the weight will be proportional to the difference in covariances between the treatment effects and the noise (if everything is joint Normal).\n\nIf item values are Normally distributed then selections will look like ellipses.\n\nIf you’re ranking items by predictions, e.g. ranking by \\(p(A)+\\beta p(B)\\), and the probabilities have a joint Gaussian distribution, then the overall tradeoff between total \\(A\\) and total \\(B\\) will be an ellipse, equal to an isovalue of the joint distribution of \\(F(p(A),p(B))\\).\n\nModularity in the brain will cause characteristic inconsistencies in decisions.\n\nIf information is dispersed in the brain, and aggregated sequentially, then (a) mistakes will occur when there are interactions in your posterior (nonseparabilities), and (b) certain characteristic inconsistencies which will reveal those nonseparabilities.\n\nImplicit preferences can be inferred from choices. (with Jon de Quidt)\n\nObserving choices over outcomes which differ in attributes can reveal the difference between implicit and explicit preferences – where implicit preferences can be due to either (a) tacit knowledge, (b) signaling motives, or (c) constraints on which decisions are allowable.\n\nNoisy signalling advantages senders. (with Ines Moreno de Barreda)\n\nSuppose you’ll admitting a student to your PhD program only if they have a GRE score sufficient to imply their ability is above median. If students can exert effort to inflate their scores then you’ll end up admitting more than half of the students, even if you rationally adjust for the inflation.\n\n\n\n\n\n\nAnatomy of a Mistake\nI missed a malignant ambiguity because there was another ambiguity next to it, and that other ambiguity was benign.\nThere were two versions of a claim, and I was a little vague about which I was assuming:\n\\[\\theta(x,x') = \\theta(x'-x)\\tag{A weak}\\]\n\\[\\theta(x,x') = \\theta(|x'-x|)\\tag{A strong}\\]\nMy conclusion required the strong version but my assumptions only justified the weak version.\nWhy did I miss this? Because there was another ambiguity that was floating around, claim B:\n\\[\\theta(x,x') - \\theta(x+1,x') &gt; 0. \\tag{B}\\]\nIt happens that assuming \\(\\mu=0\\) (never mind what that is) implies both (B) and (A-strong). My thought process:\n\nI don’t want to restrict to \\(\\mu=0\\).\nI knew that (B) is not true for all \\(\\mu\\), but it is true for sufficiently small values, i.e. \\(|\\mu|&lt;k\\) for some \\(k\\).\nSo I was juggling two versions of an assumption about \\(\\mu\\) (\\(\\mu\\) is zero, \\(\\mu\\) is small), and two versions of (A).\nWhen my mind wandered onto whether claim (A strong) was justified I was reassured by remembering that it’ll work when \\(\\mu=0\\) (true for both claim A and B), and additionally that for \\(\\mu=\\varepsilon\\) it’ll be OK (true only of claim B).\n\nThe key thing: if claim B had not have interfered I wouldn’t have made this mistake.\nIn the same way a pickpocket will wait for you to move before taking something off you, because the friction you feel on your buttock is attributed to the walking, not to the wallet leaving your pocket."
  },
  {
    "objectID": "posts/2023-10-23-pareto-frontiers-experiments-ranking.html",
    "href": "posts/2023-10-23-pareto-frontiers-experiments-ranking.html",
    "title": "Thinking About Tradeoffs? Draw an Ellipse",
    "section": "",
    "text": "This material was first presented at MIT CODE 2021. Thanks to Sean Taylor among others for comments.\nThinking about tradeoffs? draw an ellipse. When making a tradeoff between two outcomes, \\(X\\) and \\(Y\\), it’s useful to sketch out what the tradeoff looks like, and an ellipse is often a good first-order approximation. The ellipse helps visualize the most interesting parameter: the tightness, i.e. how much the rate of tradeoff between \\(X\\) and \\(Y\\) varies as you increase \\(X\\).\nIn addition we can show that if the Pareto frontier is formed by the sum of vectors, and the vectors are drawn from a joint Normal distribution, then the expected frontier will be exactly an ellipse.\nConcrete Applications:\n\nChoosing launch criteria? draw an ellipse. Suppose you have a set of features each of which has some metric impact, \\(\\Delta X\\) and \\(\\Delta Y\\). If we assume that the effects are additive then we can construct a Pareto frontier, i.e. a set of all the aggregate effects on \\(\\Delta X\\) and \\(\\Delta Y\\) achievable by selection from the set of features. The frontier will typically look like an ellipse. You can prove that the Pareto frontier will be exactly an ellipse if the set of experiment-effects have a joint Normal distribution.\nChoosing ranking weights? draw an ellipse. Suppose you are ranking items for a user using a set of features e.g. p(Like), p(Comment), etc. It is useful to sketch out the Pareto frontier, i.e. the set of outcomes achievable by different ranking algorithms. If the outcomes are additively separable functions of the item-features, and if the joint distribution of features is Normal, then the Pareto frontier will be an ellipse.\nAllocating headcount? draw an ellipse. When you shuffle headcount around a company it’s hard to precisely measure the impact on different goals, however I have found it useful to sketch ellipses to make explicit the tradeoffs you face. This is particularly useful for visualizing the differences between within-team vs between-team reallocation of effort.\n\n\nTight and Loose Tradeoffs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose we care about two metrics, \\(X\\) and \\(Y\\). E.g. suppose we care about DAU and time-spent, or revenue and retention, or engagement and misinformation.\nIt is useful to draw a Pareto frontier. A Pareto frontier will show the set of achievable outcomes for X and Y, to make the tradeoff precise. If we have a well-defined objective function then we can visually represent the optimal choice where the indifference curve is tangent to the Pareto frontier.\nIf your frontier is “tight” then there is not much tradeoff. The first figure shows a tight frontier, meaning that there is not much tradeoff available between X and Y. With a tight tradeoff it doesn’t matter whether we maximize X or Y or a weighted average, we’ll end up in roughly the same place anyway. Suppose we are choosing among experiments: if we observe a high positive correlation between \\(\\Delta X\\) and \\(\\Delta Y\\) then the choice of shipping criteria is relatively unimportant, most criteria would select the same experiments anyway. Suppose instead we are calibrating a recommender system: if we observe a high positive correlation between predictions of the two outcomes then the choice of weights is relatively unimportant, we would end up showing the same items anyway.\nIf your frontier is “loose” then there is a lot of tradeoff. The second figure shows a loose tradeoff: in this case the outcome does depend substantially on the relative weight we put on \\(X\\) and \\(Y\\).\n\n\nEllipses for Experiments\n\n\n\n\n\n\n\n\n\nSuppose we have a set of experiments. Each experiment has some impact on two metrics, \\(\\Delta X\\) and \\(\\Delta Y\\). We visualize such a set of experiments at right.\nIf the set of features is separable, meaning that the impact of each feature is independent of what other features are launched, then a natural question will be the shape of the Pareto frontier formed by all possible combination of experiments.\nIf the distribution of experiments is mean zero and joint Normal then the Pareto frontier will be an ellipse, and it will have exactly the shape of an isovalue of the density of experiments. Thus knowing the variance and covariance of experiment results allows us to characterize the nature of the Pareto frontier we face.\n\n\nEllipses for Ranking\n\n\n\n\n\n\n\n\n\nSuppose we are choosing a fixed set of items to show to a user, based on two metrics \\(x_1\\) and \\(x_2\\). E.g. pLike and pComment, or pDAU and quality etc. A natural question will be the shape of the Pareto frontier formed by alternative selections of items.\nThe Pareto frontier will be an ellipse. We show below that, if the predictions are well calibrated, the outcomes are independent (i.e. additive), and the distribution of prediction obeys a joint Normal distribution, then the Pareto frontier will be an ellipse and it will have exactly the shape of an isovalue of the density of predictions. Thus knowing the variance and covariance of predictions allows us to exactly characterize the nature of the aggregate tradeoffs we face.\n\n\nEllipses for Company Strategy\nTradeoffs are looser higher in the decision hierarchy. Suppose a company cares about two outcomes, \\(X\\) and \\(Y\\). Many different people will be making tradeoff decisions between X and Y, we can distinguish between four objectives used at different levels in the company hierarchy: \\[\\substack{\\text{company objective}\\\\\\text{(choose headcount)}}\n      &gt; \\substack{\\text{team objective}\\\\\\text{(choose projects)}}\n      &gt; \\substack{\\text{shipping objective}\\\\\\text{(choose experiments)}}\n      &gt; \\substack{\\text{algorithm objective}\\\\\\text{(choose items)}}\n      \\]\nWe can think of each successive level as holding more variables fixed, and so we expect the Pareto frontiers to become successively tighter (Le Chatelier principle). We thus expect the tradeoff to be loosest at the level of overall company objectives, where we reallocate headcount. For this reason we should expect that, if the company as a whole pivots form metric \\(X\\) to metric \\(Y\\), the principal effect will be a reallocation of effort between products rather than reallocation within products.\nWe now walk through some of the different levels of optimization:\n\n\n\n\n\n\n\n\n\nDifferent product areas have different Pareto frontiers. Typically two different product areas will have substantially different ability to affect different metrics, and we will often observe a situation like that shown on the right: team A’s choices primarily affect metric \\(X\\), team B’s choices primarily affect metric \\(Y\\).\n\n\n\n\n\n\n\n\n\n\nWe can also draw a combined Pareto frontier. Here we add up the Pareto frontiers of team A and B. In this case the combined frontier is somewhat tight, because the two constituent frontiers are tight. Neither individual Pareto frontier shows a substantial effect from changing weights (if we restrict weights to be positive), and so accordingly the combined Pareto frontier shows little response to a change in weights.\nNote that I have drawn the frontier only approximately, the frontier achieved by combining two ellipses does not have a simple representation. When the two constituent frontiers are straight lines then the combination will be a parallelogram. (Note also that when the two frontiers are circles then the combination will be a circle).\n\n\n\n\n\n\n\n\n\nGreater investment will shift Pareto frontiers out. Here we visualize reallocating employees from team B (the frontier shifts in) to team A (the frontier shifts out).\n\n\n\n\n\n\n\n\n\n\nA combined company Pareto frontier will be loose. Here the green curve represents all the possible outcomes as you shift resources between team A and B: we have now turned a tight tradeoff into a loose tradeoff. In this case this represents that a change in company objectives will be reflected mainly in reallocation of effort between teams rather than within teams.\n\n\n\nAppendix: Model for Normal Distributions\nSuppose we have a set of items with, \\(x_1\\) and \\(x_2\\), distributed Normally: \\[\\binom{x_1}{x_2}\\sim N\\left(\\binom{0}{0},\n      \\begin{pmatrix}\\sigma_1^2 & \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_1\\sigma_2 & \\sigma_2^2\\end{pmatrix}\\right).\\]\nWe additionally let each item have a score, \\(v\\), which is simply a weighted sum of the two characteristics (normalizing the weight on the first characteristic to be 1): \\[v=x_1 + wx_2.\\]\nWe can write the covariance between the characteristics and the score as follows:\n\\[Cov\\begin{bmatrix}x_1\\\\x_2\\\\v\\end{bmatrix}=\n\\begin{bmatrix}\n    \\sigma^2_1           & \\sigma_1\\sigma_2\\rho & \\sigma_1^2+ w\\rho\\sigma_1\\sigma_2 \\\\\n    \\sigma_1\\sigma_2\\rho &  \\sigma_2^2          & \\rho\\sigma_1\\sigma_2+w\\sigma_2^2 \\\\\n    \\sigma_1^2+w\\rho\\sigma_1\\sigma_2          &\n          \\rho\\sigma_1\\sigma_2+w\\sigma_2^2    &\n          \\sigma_1^2+w^2\\sigma_2^2 + 2\\rho w\\sigma_1\\sigma_2 \\\\\n\\end{bmatrix}\\]\nWe wish to know the total number of actions of each type, \\(X_1\\) and \\(X_2\\), for a given score threshold \\(\\bar{v}\\):\n\\[\\begin{aligned}\n   X_1   &=P(v\\geq \\bar{v})E[x_1|v\\geq \\bar{v}] \\\\\n   X_2   &=P(v\\geq \\bar{v})E[x_2|v\\geq \\bar{v}].\n\\end{aligned}\\]\nWe first calculate the conditional expectations:\n\\[\\begin{aligned}\nE[x_1|v\\geq \\bar{v}]\n   =& \\sigma_1 \\frac{Cov(x_1,v)}{\\sqrt{Var(x_1)Var(v)}}\n      \\frac{\\phi(\\frac{\\bar{v}}{\\sqrt{Var(v)}})}{\\Phi(\\frac{\\bar{v}}{\\sqrt{Var(v)}})} \\\\\n   =& \\sigma_1\n      \\frac{\\sigma_1^2+w\\rho\\sigma_1\\sigma_2}\n         {\\sqrt{\\sigma_1^2(\\sigma_1^2+w^2\\sigma_2^2 + 2\\rho w\\sigma_1\\sigma_2)}}\n      \\frac{\\phi(\\frac{\\bar{v}}{\\sqrt{Var(v)}})}{\\Phi(\\frac{\\bar{v}}{\\sqrt{Var(v)}})}\\\\\n  =&  \\frac{\\sigma_1^2+w\\rho\\sigma_1\\sigma_2}\n           {\\sqrt{\\sigma_1^2+w^2\\sigma_2^2 + 2\\rho w\\sigma_1\\sigma_2}}\n   \\frac{\\phi(\\frac{\\bar{v}}{\\sqrt{Var(v)}})}{\\Phi(\\frac{\\bar{v}}{\\sqrt{Var(v)}})}\n\\end{aligned}\\]\nNext we will assume that the expected quantity of items is fixed. This implies that both both \\(P(v\\geq \\bar{v})\\) and \\(\\frac{\\bar{v}}{\\sqrt{Var(v)}}\\) will be constant, and we will define: \\[\\begin{aligned}\n      \\gamma\\equiv\n         &\\frac{\\phi\\left(\\frac{\\bar{v}}{\\sqrt{Var(v)}}\\right)}\n            {\\Phi\\left(\\frac{\\bar{v}}{\\sqrt{Var(v)}}\\right)}P(v\\geq \\bar{v}) \\\\\n      X_1 =& \\frac{\\sigma_1^2+w\\rho\\sigma_1\\sigma_2}\n                  {\\sqrt{\\sigma_1^2+w^2\\sigma_2^2 + 2\\rho w\\sigma_1\\sigma_2}}\\gamma \\\\\n      X_2 =& \\frac{w\\sigma_2^2+\\rho\\sigma_1\\sigma_2}\n                  {\\sqrt{\\sigma_1^2+w^2\\sigma_2^2 + 2\\rho w\\sigma_1\\sigma_2}}\\gamma\n   \\end{aligned}\\]\nWe thus have expressions for \\(X_1\\) and \\(X_2\\) as a function of the relative weight \\(w\\). We wish to rearrange these to express \\(X_1\\) directly in terms of \\(X_2\\). To help we turn to Mathematica, with the following input:1\n1 see notebookF1[w_,p_,s1_,s2_,g_]:=g(s1^2+w p s1 s2 )/Sqrt[s1^2+w^2 s2^2+2w p s1 s2]\nF2[w_,p_,s1_,s2_,g_]:=g(w s2^2 +p s1 s2)/Sqrt[s1^2+w^2 s2^2+2w p s1 s2]\nSolve[{X1==F1[w,p,s1,s2,g],X2==F2[w,p,s1,s2,g]}, {X1,w}]\nSimplify[First[%1]]\nThis returns a large expression:\n\\[\\begin{aligned}\n   X_1(X_2) &=\n      \\frac{\n         \\gamma^2 \\rho \\sigma_1 \\sigma_2^3 X_2\n         - p \\sigma_1 \\sigma_2 X_2^3\n         + \\gamma^3 \\sigma_2^4\n            \\sqrt{\\frac{-\\gamma^2 (-1 + p^2) \\sigma_1^2 \\sigma_2^2}{\\gamma^2 \\sigma_2^2 - X_2^2}}\n         - \\gamma \\sigma_2^2 X_2^2\n            \\sqrt{-\\frac{\\gamma^2 (-1 + p^2) \\sigma_1^2 \\sigma_2^2}{\\gamma^2 \\sigma_2^2 - X_2^2}}\n         + X_2 \\sqrt{(-1 + p^2) \\sigma_1^2 \\sigma_2^2 X_2^2 (-\\gamma^2 \\sigma_2^2 + X_2^2)}\n      }{\\gamma^2 \\sigma_2^4 - \\sigma_2^2 X_2^2}\\end{aligned}\\]\nWe can however substantially simplify this: \\[\\begin{aligned}\n   X_1 &= \\frac{\n         \\sigma_1\\sigma_2X_2 (\\gamma^2 \\rho \\sigma_2^2  - p X_2^2)\n         + \\gamma^2\\sigma_2^2(\\gamma^2 \\sigma_2^2-  X_2^2)\n            \\sqrt{-\\frac{(-1 + p^2) \\sigma_1^2 \\sigma_2^2}{\\gamma^2 \\sigma_2^2 - X_2^2}}\n         - X_2^2\\sigma_1\\sigma_2 \\sqrt{(p^2-1) (\\gamma^2 \\sigma_2^2-X_2^2)}\n      }{\\gamma^2 \\sigma_2^4 - \\sigma_2^2 X_2^2} \\\\\n   &= \\frac{\n         \\sigma_1\\sigma_2X_2 p(\\gamma^2 \\sigma_2^2  - X_2^2)\n         + \\gamma \\sigma_2^3\\sigma_1 \\gamma\n            \\sqrt{(p^2-1)(\\gamma^2 \\sigma_2^2 - X_2^2)}\n         - X_2^2\\sigma_1\\sigma_2 \\sqrt{(p^2-1) (\\gamma^2 \\sigma_2^2-X_2^2)}\n      }{\\sigma_2^2(\\gamma^2 \\sigma_2^2 - X_2^2)} \\\\\n   &= \\frac{\\sigma_1}{\\sigma_2}X_2p + \\frac{\n         \\sigma_1\\sigma_2(\\gamma^2\\sigma_2^2 - X_2^2 )\n            \\sqrt{(p^2-1) (X_2^2-\\gamma^2 \\sigma_2^2)}\n      }{\\sigma_2^2(\\gamma^2 \\sigma_2^2 - X_2^2)} \\\\\n   &= X_2 \\rho \\frac{\\sigma_1}{\\sigma_2}\n      +\\frac{\\sigma_1}{\\sigma_2}\\sqrt{(p^2-1) (X_2^2-\\gamma^2 \\sigma_2^2)}.\n\\end{aligned}\\]\n\n\n\n\n\n\n\n\n\nWe now wish to show that this curve is equal to an isovalue of the joint distribution of \\(x_1\\) and \\(x_2\\) (illustrated at right). We can write the isovalue of the joint Normal distribution of \\((x_1,x_2)\\) for any given \\(k\\) as follows:2\n2 From Bertsekas and Tsitsiklis (2002) “Introduction to Probability”, Section 4.7\\[k = \\frac{x_1^2}{\\sigma_1^2}+\\frac{x_2^2}{\\sigma_2^2}-2\\rho\\frac{x_1x_2}{\\sigma_1\\sigma_2}.\\]\nSolving this quadratic we can write: \\[\\begin{aligned}\n      x_1 &= x_2 \\rho \\frac{\\sigma_1}{\\sigma_2}\n               \\pm \\frac{\\sigma_1}{\\sigma_2}\\sqrt{-x_2^2+x_2^2\\rho^2+k\\sigma_2^2} \\\\\n         &= x_2 \\rho \\frac{\\sigma_1}{\\sigma_2}\n               \\pm \\frac{\\sigma_1}{\\sigma_2}\\sqrt{k\\sigma_2^2-(1-\\rho^2)x_2^2}.\n   \\end{aligned}\\]\nWe can see that this will be identical to the relationship between \\(X_1\\) and \\(X_2\\) above when \\(k=\\frac{\\sigma_2^2}{\\sigma_1^2}(\\rho^2-1)\\gamma^2\\).\n\n\nAppendix: Simulations for Non-Normal Distributions\nIn this section I compare Pareto frontiers generated from different distribution of \\((X,Y)\\) from different joint distribution, and then draw the Pareto frontiers. There are a few points of interest:\n\nOther distributions apart from the Normal do not have the property that the Pareto frontier is equal to an isovalue of the joint density.\nIn my examples the Pareto frontier of many other distributions does look roughly elliptical though not precisely an ellipse. This makes me more comfortable to use an ellipse as a first-order approximation of a Pareto frontier.\n\n\nThese simulations show something slightly different from what is proved in the prior section. These simulations show that as the number of experiments is large (\\(N\\rightarrow\\infty\\)) then the Pareto frontier begins to resemble an ellipse. What was proved in the previous section is that for any given \\(N\\) the expected Pareto frontier will be an ellipse, where we calculate the expected value of \\(X\\) and \\(Y\\) for a given rate of tradeoff.\nIn each case the left-hand plot shows the raw distribution, the right-hand plot shows the Pareto frontier.\nJoint normal with positive correlation, small \\(N\\):\n\n\n\n\n\n\n\n\n\nJoint normal with positive correlation, \\(N\\)=100:\n\n\n\n\n\n\n\n\n\nJoint t Distribution with 3 degrees of freedom, \\(N\\)=100.\n\n\n\n\n\n\n\n\n\nIndependent Laplace, \\(N\\)=100:\n\n\n\n\n\n\n\n\n\nCommon Laplace factor with independent Gaussian noise, \\(N\\)=100:\n\n\n\n\n\n\n\n\n\nIndependent uniform, \\(N\\)=100:\n\n\n\n\n\n\n\n\n\nCommon uniform factor plus independent uniform noise, \\(N\\)=100:"
  },
  {
    "objectID": "posts/2017-02-25-samuelson-expected-utility.html",
    "href": "posts/2017-02-25-samuelson-expected-utility.html",
    "title": "Samuelson & Expected Utility",
    "section": "",
    "text": "clown\n\n\n\n“If in every event which can possibly occur the consequence of action I is not preferred to that of action II, and if in some possible event the consequence of II is preferred to that of I, then any sane preferer would prefer II to I.”\n\nThis sentence, in a letter from Savage in 1950, finally persuaded Samuelson that rational choices must obey expected utility. He had been skeptical – thinking that expected utility was just a simple approximation, like exponential discounting or like separability. Marschack and Savage and Friedman all wrote letters trying to persuade him, and though they were right, they kept using bad arguments, and Samuelson disposed of them.\nSavage and Friedman wrote a paper saying that, because people buy both insurance and lottery tickets, expected utility implies that the utility-of-money must be concave then convex. Samuelson saw that this was ridiculous, & said “there’s as much to be learned about gambling from Dostoyevsky as from Pascal.”\nBut the sentence from the letter above finally persuaded Samuelson.\nThis is all from Ivan Moscati’s “How Economists Came to Accept Expected Utility Theory”."
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html",
    "title": "Repulsion from the Prior",
    "section": "",
    "text": "the shortest version: contrary to recent reports, I do not think it’s possible for you to be a Bayesian and consistently exaggerate things.\n{: .center-image }"
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#short-version",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#short-version",
    "title": "Repulsion from the Prior",
    "section": "Short Version",
    "text": "Short Version\n\nIf we think of perception as inference, it has implications about the types of biases we would have.\nYet many biases and illusions seems to go in the exact opposite direction – sometimes called “anti-Bayesian” biases – in particular there are ubiquitous contrast effects, while Bayesian inference seems to imply assimilation effects.\nWei and Stocker (2015) say they can rationalize these contrast effects, under the assumption that our sensory mechanisms are tuned to the environment, such that they are relatively more sensitive to more likely signals. They say that this will imply contrast effects (that the bias is inversely proportional to the slope of the prior).\nYet their results contradict some simple laws of Bayesian inference – the law of iterated expectations, and law of total variance – so there is something odd going on.\n(If this explanation doesn’t work, then why do we get repulsion? I think that Ted Adelson explained the basic reason in the 70s. Will write another post on this.)"
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#shortish-version",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#shortish-version",
    "title": "Repulsion from the Prior",
    "section": "Shortish Version",
    "text": "Shortish Version\n\nHere’s a nice crisp problem: in what cases does inference attract towards the prior, and in what cases does it repulse away from it?\nGiven an unknown variable \\(x\\) and a signal \\(s\\), let’s say that there’s “attraction” at a given value of \\(x\\) if the average inferred value of \\(x\\) is closer to the prior than \\(x\\) itself is –\n\\[|E[E[x|s]|x]-\\mu|&lt;|x-\\mu|\\]\nAttraction effects are typically treated as the norm. For example if \\(x\\) is drawn from a normal distribution and if \\(s\\) is equal to \\(x\\) plus normal noise, then you’ll always get attraction to the prior. I.e., if \\(x\\) is above the mean, then it’ll be, on average, estimated to be closer to the mean than it actually is.\nHowever this has sometimes been treated as a puzzle in studies of perception: perception seems like inference, but we also find what look like repulsion effects. For example “contrast” effects, in which an object seems less dark when you put it next to another, darker, object. If we assume that the colour of the neighboring objects affects your prior about the target object, then this would imply an attraction effect. Yet repulsion effects seems to be the norm across all sorts of judgments (lightness, colour, volume, orientation, size), and similar contrast effects occur in time as well as in space (i.e., something seems less dark if it is preceded by something darker) – though of course there are exceptions. These types of illusion are sometimes called “anti-Bayesian.”\nA common explanation of these contrast effects is that we ‘code for differences’ – i.e. that something about our neural wiring causes us to encode differences, rather than levels, and this causes us to exaggerate differences, i.e. get contrast effects.\nBut this assumes that we encode the difference and then forget to decode (AKA coding catastrophe, AKA the el Greco fallacy). If you write down a Bayesian model, which makes its best effort to infer the level from the difference, you typically do not find the desired contrast effects (Schwartz, Hsu & Dayan (2007)).\nWei and Stocker (2015) announce that they have made a breakthrough – a fully Bayesian model which generates contrast/repulsion effects generically. They say that the key assumption is that we are more sensitive to differences in areas where signals are more likely to fall – i.e., sensitivity is proportional to the density of the prior.\nFormally, let \\(x\\sim f\\), and \\[s=F(x)+\\varepsilon.\\] This means that sensitivity is proportional to the density of the prior – and it implies that \\(s\\) will be roughly uniformly distributed – so in some sense it’s an efficient use of signal capacity. Given this setup, and some simplifications, they find that the bias is proportional to the slope of the prior – so if the prior is symmetric & single-peaked then for values above the mean, the bias will be positive, and vice versa – i.e. repulsion away from the prior everywhere.\nIn the note below I give a proof that implies that it is impossible to have repulsion effects everywhere – which seems to contradict the results of Wei & Stocker.\nI’m not sure what the source of the contradiction is – it could be either (a) Wei & Stocker’s results are true locally, but do not apply at the tails of the distribution, and so things balance out that way; (b) there is a difference in the implicit assumption used when taking conditional expectations (AKA the Borel-Kolmogorov paradox); or (c) I made a mistake.\nI also mention below a related result, that there cannot be a consistent upward or downward bias (i.e., it cannot be that \\(E[\\hat{x}\\|x]&gt;x\\) for all \\(x\\)). This is relevant for Wei & Stocker’s result applied to asymmetric priors – e.g. if the prior is everywhere decreasing – where the result seems to imply a consistent upward bias. \n\n{: .center-image }"
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#summary-of-proof",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#summary-of-proof",
    "title": "Repulsion from the Prior",
    "section": "Summary of proof",
    "text": "Summary of proof\n\nSuppose that there is repulsion from the prior everywhere, i.e. for all \\(x\\), \\(\\|E[\\hat{x}\\|x]-\\mu\\|&gt;\\|x-\\mu\\|\\).\nThis implies that \\(Var[\\hat{x}]&gt;Var[x]\\).\nBut this contradicts the law of total variance, which says that \\(Var[E[A\\|B]]\\leq Var[A]\\)."
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#detail",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#detail",
    "title": "Repulsion from the Prior",
    "section": "Detail:",
    "text": "Detail:\nSuppose there are two random variables \\(x\\) and \\(s\\), and let \\(\\hat{x}=E[x\\|s]\\). Let \\(x\\) be mean-zero, and let’s assume repulsion from the prior everywhere, i.e. for all \\(x\\):\n\\[\nE[\\hat{x}|x]|&gt;|x|\n\\]\nFrom this repulsion assumption I think it’s clear that there’s more variance in \\(E[\\hat{x}\\|x]\\) than in \\(x\\):\n\\[Var[E[\\hat{x}|x]]&gt;Var[x]\\]\nNow let’s apply the law of total variance:\n\\[\n\\begin{aligned}\nVar[A]=& E[Var[A|B]]+Var[E[A|B]] \\\\\\\\\nVar[\\hat{x}]=& E[Var[\\hat{x}|x]]+Var[E[\\hat{x}|x]]\n\\end{aligned}\n\\]\nThus implying that:\n\\[Var[\\hat{x}]\\equiv Var[E[x|s]]&gt;Var[x]\\]\nApplying the law of total variance again we get:\n\\[\\begin{aligned}\nVar[x]=& E[Var[x|s]]+Var[E[x|s]] \\\\\\\\\n      &gt;& Var[x]\n\\end{aligned}\\]\nA contradiction."
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#no-consistent-upwarddownward-bias",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#no-consistent-upwarddownward-bias",
    "title": "Repulsion from the Prior",
    "section": "No consistent upward/downward bias",
    "text": "No consistent upward/downward bias\nThe law of iterated expectations states that, for any \\(A\\) and \\(B\\):\n\\[E[E[A|B]]=E[A]\\]\nThis implies that there cannot be a consistent upward or downward bias, i.e. it cannot be true that:\n\\[E[\\hat{x}|x]&gt;x, \\forall x\\]"
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#references",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#references",
    "title": "Repulsion from the Prior",
    "section": "References",
    "text": "References\n\nSchwartz, Hsu & Dayan (2007, Nature Review Neuro) “Space and Time in Visual Context”\nWei & Stocker (2015, Nature Neuroscience) “A Bayesian observer model constrained by efficient coding can explain ‘anti-Bayesian’ percepts”"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html",
    "href": "posts/2023-01-31-social-media-suspensions-data.html",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "",
    "text": "Tom Cunningham. (@testingham) First version Jan 31 2023, data last updated April 2023, text updated July 2024.1\nThis note describes the suspension practices of the major social media platforms. I have collected a dataset of around 200 suspensions of prominent people across 12 platforms between 2011 and early 2023, stored in a google spreadsheet. The chart below summarizes the full dataset:\nThe data helps illuminate what platforms are doing. It is very difficult for an outside observer to see how a platform moderates their content. The advantages of studying the suspension of prominent users are that (1) the data is public and (2) the outcomes are comparable across platforms.\nKey findings.\nI am working on a separate essay about why platforms suspend users. It is difficult to give clear reasons why platforms suspend users. In a separate essay I try to break down how much their action can be attributed to influence from owners, from employees, from users, from advertisers, or from governments. Having this dataset of suspensions is very useful to be able to make generalizations about platform behavior."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#politicians",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#politicians",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Politicians",
    "text": "Politicians\nAmong US Federal politicians only Republicans have been suspended. In the US 8 Republicans have had one or more suspension, but no Democrats. Among the Republicans the suspensions were for a variety of reasons: related to the Jan 6 riots (Trump, Barry Moore, MTG), related to COVID (Ron Johnson, Rand Paul, MTG), for misgendering (Jim Banks), for tweeting a threat (Briscoe Cain), for animal blood on a profile photo (Steve Daines), one by a rogue employee (Trump).\nIt seems to me that the asymmetry in suspensions is primarily due to Republicans being more likely to violate the policies, rather than asymmetric enforcement of existing policies. I am not aware of any cases where a Democratic politician violated one of these policies but was not suspended.\n\n\n\n\n\n\n\n\n\nSuspension of national politicians outside the US has been relatively rare. My dataset contains 13 national politicians who were suspended in the world outside the US, compared to 8 in the US. This is a big asymmetry, and something of a puzzle. I have discussed this with a number of people who worked in enforcement and they attribute to a mixture of (1) less policy-violating behaviour from non-US politicians; (2) looser enforcement against non-US politicians; (3) lower overall social media usage outside the US; and (4) lower coverage of non-US politicians in my dataset."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#us-prominent-figures",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#us-prominent-figures",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "US Prominent Figures",
    "text": "US Prominent Figures\nThis shows all suspensions of US “notable people”:\n\n\n\n\n\n\n\n\n\nBetween 2015 and 2017 there were a series of alt-right personalities suspended from Twitter. The suspensions were often not for their views but their behaviour:\n\n2015: Charles Johnson from Twitter for a threat.\n2016: Milo Yiannopoulos from Twitter for harassment, Richard Spencer from Twitter for manipulation.\n2017: Roger Stone from Twitter for abuse.\n2018: Alex Jones from Twitter for incitement and abuse.\n\nBeginning in late 2017 more alt-right accounts were suspended. Either for hate speech, for offline behaviour, or without any public reason given:\n\nLate 2017: Baked Alaska from Twitter for hate speech.\n2018: Owen Benjamin from Twit with no reason given, Alex Jones from FB and YouTube for hate speech.\n2019: Nick Fuentes from Meta with no reason given.\n\nBetween November 2020 and January 2021 a large set of prominent figures were suspended for election-related reasons. The most suspensions were on Twitter but there were also from other platforms.\n\nSince November 2022 Twitter has unsuspended a large fraction of the suspended users that I track, probably around 1/2.\nSome people have been suspended simultaneously across multiple platforms (e.g. Alex Jones)"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#meta",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#meta",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Meta",
    "text": "Meta"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#twitter",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#twitter",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Twitter",
    "text": "Twitter\n\n\n\n\n\n\n\n\n\nThe following chart shows just accounts that were un-suspended under Musk, i.e. people with Twitter suspension that started before Oct 27 2022 and ended after that date. See below for a more fine-grained dataset of accounts unsuspended under Musk.\nYou can see that the primary original reasons for suspension were hate speech COVID misinformation. Kanye West and Nick Fuentes were re-suspended under Musk."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#youtube",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#youtube",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "YouTube",
    "text": "YouTube"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#tik-tok",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#tik-tok",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Tik Tok",
    "text": "Tik Tok"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Other Platforms",
    "text": "Other Platforms"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#hate-speech",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#hate-speech",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Hate Speech",
    "text": "Hate Speech"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#twitter-1",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#twitter-1",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Twitter",
    "text": "Twitter\nWikipedia page on Twitter Suspensions. Wikipedia has a list of around 400 Twitter suspensions. I chose not to create my own database (partly drawing from Wikipedia) for a few reasons: (1) I would want to add a lot of annotations to the Wikipedia data, e.g. about reasons for suspension or types of suspension. (2) Parsing the data is nontrivial: date ranges are given in various formats and would require some work on a regex to parse consistently. (3) There is some missing and inconsistent data, e.g. it has Trump’s suspension start-date but not end-date, and the names of people are not consistent (e.g. sometimes “Donald Trump”, sometimes “Donald J Trump”).\nThe Wikipedia dataset shows a similar basic pattern to what I document above: a dramatic increase in the rate of suspensions around mid-2017\n\n\n\n\n\nWikipedia-reported Twitter suspension by year\n\n\n\n\n\n\n\n\n\nAll Wikipedia-reported Twitter suspension, highlighting accounts with more than 1M followers (not all suspensions list the number of followers).\n\n\n\n\nTravis Brown: Twitter Watch This project appears to have data on almost all suspensions on Twitter since Feb 2022, and also tracks whether the suspension have been reversed. It does not include any suspensions which started prior to Feb 2022. There is a giant CSV file with 600K rows, suspensions.csv. Some visualizations:\n\n\n\n\n\nObservations by date of suspension\n\n\n\n\n\n\n\n\n\nObservation by date of unsuspension\n\n\n\n\n\n\n\n\n\nObservation by date of account creation\n\n\n\n\n\n\n\n\n\nSuspensions for accounts with &gt;1M followers\n\n\n\n\nTravis Brown: Twitter Unsuspensions. This is a collection of users who Twitter has un-suspended since Oct 27 2022 (when Musk took over). For some accounts there is a date of suspension but some have missing dates, I think suspension-date is only observed if after Feb 2022. (The content of this dataset is neither a subset nor a superset of the previous daatset). Unfortunately the dataset doesn’t have follower-count or twitter handle, so it’s not easy to join with other datasets or find the most prominent accounts.\n\n\n\n\n\nObservations by date of suspension\n\n\n\n\n\n\n\n\n\nObservations by date of unsuspension\n\n\n\n\nTravis Brown: Deleted Tweets / Suspended Accounts. This project scrapes profiles from the Wayback Machine, and seems to have a large set of accounts that were suspended with fairly long retention, I have not yet investigated further.\nTwitter Transparency Reports. This has data on the aggregate number of suspensions per half between July 2018 and Dec 2021. Note that the website is down but the CSV files can still be downloaded. \n\n\n\n\n\nTotal Accounts Suspended on Twitter by Reason, 2018H2-2021H2\n\n\n\n\nCounterHate list of unsuspensions. The organization CounterHate has a list of 10 large accounts reinstated by Twitter since Musk’s takeover. Note I believe they incorrectly listed Rizza Islam as an account re-activated by Twitter: I can find no evidence that the acccount @RizzaIslam was ever suspended, it seems to have been continuously tweeting from November 2022 through Feb 2023. I have added all 10 accounts to my database, and checked activity across all platforms."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#youtube-1",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#youtube-1",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "YouTube",
    "text": "YouTube\nWikipedia page on YouTube suspensions. See above for reasons why I chose not to use this dataset as the primary source.\nWikitubia: Terminated YouTubers. A list of around 2300 YouTubers that have been permanently banned, including date of ban, subscribers, reason for ban, and citation. They don’t have a date when unbanned."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#meta-facebook-instagram",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#meta-facebook-instagram",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Meta / Facebook / Instagram",
    "text": "Meta / Facebook / Instagram\nThere is no Wikipedia page of suspensions on Facebook, Instagram or WhatsApp.\nMeta’s “Community Standards Enforcement Report” is shown below. Meta’s data does not include any data on account suspensions, however there are a few other patterns of interest.\n\nContent actioned is relatively stable. There are fairly few notable upward or downward trends across the different types of content actioned: terrorism content actioned has increased significantly on both platforms, hate speech actions increased up to the end of 2020, then declined.\nThe proactive detection rate is close to 100% for most categories. there were dramatic improvements for bullying and for hate speech over 2017-2021. Note that the proactive detection rate is the share of actioned content that is automatically detected, the share of true positives that are automatically detected is surely much lower.\nThe prevalence of volations has fallen significantly. The log axis diminishes the magnitude of the decline: prevalence has fallen by a factor of 2-5 for nudity, bullying, hate speech, and graphic content. (I only show the prevalence upper bound, but the lower bound generally tracks the same course).\n\n\n\n\n\n\n\n\n\n\nFacebook’s dangerous organizations list. This list was leaked in 2021 by the Intercept. Unfortunately it does not include the dates of when each organization was added. The list is organized into the following categories:\n\nTerror Organizations (e.g. Islamic State)\nCrime Organizations (e.g. Bloods, Crips)\nHate Organizations (e.g. Aryan Nation, includes bands and websites)\nMilitarized Social Movements (e.g. United States Patrio Defense Force)\nViolent Non-State Actors (e.g. Free Syrian Army)\nHate (e.g. David Duke)\nIndividuals: Crime (e.g. Denton Suggs, Gangster Disciples)\nIndividuals: Terror (e.g. Osama bin Laden)"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#tiktok",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#tiktok",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "TikTok",
    "text": "TikTok\nCommunity Standards Report. Shows an increase in suspensions from around 1M accounts/quarter per 2020 to 6M accounts/quarter in 2023."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#twitch",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#twitch",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Twitch",
    "text": "Twitch\nStreamerBans. They seem to have a pretty comprehensive database of bans on Twitch."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms-1",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms-1",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Other Platforms",
    "text": "Other Platforms\n\nSpotify. The only unambiguous suspension from Spotify I found was Alex Jones’ podcast. Spotify removed some episodes of Joe Rogan’s podcast, and removed R Kelly and XXXtentacion’s music from playlists. They remove some white-supremacist artists and music. They removed all music from the band LostProphets after their lead singer was convicted of child sexual abuse.\nSubstack. I’m not aware of anybody who’s been kicked off Substack, they present themselves as very pro-free-speech.\nReddit. I’m not aware of any data on reddit account suspensions.\nRumble. The Rumble video-hosting platform has become quite large (they claim 70M MAU, and have a market cap of ). Their terms of service restrict content that is “abusive, inciting violence, harassing, harmful, hateful, anti-semitic, racist or threatening.” However I have not yet found a single example of a prominent user who has been suspended from Rumble."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#other-data-sources",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#other-data-sources",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Other Data Sources",
    "text": "Other Data Sources\nCan find more suspensions by searching Wikipedia for “suspended from XXX”. E.g. site:wikipedia.org \"suspended from facebook\". Possibly worth doing the same search for Google News.\nSocialBlade has data on number of followers by month since 2018, across Twitter, FB, YouTube. I’m not sure how easy it would be to scrape this data. They have a paid API, they say “up to 3 years of Historical statistics on creators.” However the website seems to have data back to at least April 2018.\nBallotpedia list of elected officials suspended from social media. It is an excellent resource, appears comprehensive and cites original reporting. I have added all of their data to the database as of January 2023.\nGlobal Internet Forum to Counter Terrorism (GIFCT). They mainly work on sharing hashes of terrorist content between platforms. They have some dicussion papers about “terror designation lists” but I don’t think they maintain any lists themselves.\nSpecially Designated National / Global Terrorist (SDN/SDGT). This is a public list maintained by the US government, and consumed by a number of tech companies. The full history is available, but it would be extremely difficult to parse.\nLumen. This has an international database of government takedown requests. They also seem to include whether the request was honored.\nCCDH Disinformation Dozen. This is a list from March 2021 of prominent accounts who were spreading anti-vax information on social media: original report, followup report from April 2021). They also have a “toxic ten” report. It’s probably worth adding both lists to the database."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#third-party-sources-on-platform-policies",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#third-party-sources-on-platform-policies",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Third-Party Sources on Platform Policies",
    "text": "Third-Party Sources on Platform Policies\nThere are a variety of third-party resources comparing policies across platforms, however none seem to have data comparable to the list above, i.e. a summary of specific content policy changes over time.\nComparisons at a single point in time.\n\nDNC (2020) Comparison of Misinformation Policies, 2020\nConsumer Reports (Aug 13 2020) Comparison of Misinformation Policies, 2020. Data as of 2020, with three levels: “allowed”, “sometimes”, and “prohibited”.\nUNC CITAP (May 22 2020) Comparison of Misinformation Policies, 2020. Has tables comparing misinfo policies as of 2020, three levels: “prohibited”, “flagged”, “allowed.”\nElection Integrity Partnership (Oct 28 2020) Comparison of Election Policies, 2020.\nCarnegie Endowment (April 1 2021) Existence of Policies, 2021. Table just marks whether a platform has a policy on some type of content, not nature of policy. They also they have a database of platform policies but it seems to only have data from February 2021.\nVirality Project Comparison of COVID Vaccine Policies in 2021.\n\nPolicies tracked over time.\n\nMchangama, Fanlo and Alkiviadou (2023) Scope Creep: An Assessment of 8 Social Media Platforms’ Hate Speech Policies. They document the hate speech policies over time for 8 platforms using a consistent rubric. They document that hate speech policies have become more broad-reaching over time. The data is available in Excel sheets here.\nKatie Harbath and Collier Fernenkes (August 2022) Election Policy Announcements, 2003-2022. Google spreadsheet with links to around 600 policy announcements, organized by platform, author, date, product-type, and country. Focussed on election-related policies, and they don’t include summaries of the policy announcement. They also wrote up analyses: (1) “A Brief History of Tech and Elections”; (2) 2022 election announcements.\nRanking Digital Rights Index, Comparison of Privacy and Transparency Policies, 2017-2022. They collect perhaps 100 different indicators across around 15 tech companies, mostly related to privacy and transparency, earliest data from 2017. All the data is available.\nGLAAD Comparison of LGBTQ user safety, 2021-2022\nCELE, Letra Chica. Tracks all public policy changes on Meta, YouTube, and Twitter. Most data from May 2020, but they go back to 2019 for Facebook by using FB’s Transparency Center. Each policy update includes a short summary of what’s changed. Tracks both Spanish and English versions. Data stored on coda.io, I think it’s queryable.\nLinterna Verdes, Circuito. Has about 15 in-depth case studies of platform moderation decisions.\nHumboldt Institute, Platform Governance Archive. Comprehensive archive of ToS, Privacy Policy, and Community Guidlines, from 2004 until late 2021, for FB, IG, Twitter, and YouTube. The data will not be updated.\nOpen Terms Archive. Started by the French Ambassador for Digital Affairs, but now a collaboration. Tracks terms for many different online services in a github repo. The Platform Governance Archive has moved to be part of this project, here.\nEFF, TOSback. Database of historical ToS documents from different services, with cross-platform comparisons. The most recent updates seem to be from May 2021, possibly was succeeded by Open Terms Archive.\nEuropean Commission, Copyright Content Moderation and Removal. This PDF report includes a lot of work which maps the copyright policies of major platforms.\n\nNarrative histories:\n\nCatherine Buni and Soraya Chemaly (2016, the Verge) History of Moderation.\nSarah Jeong (2016, Vice) The History of Twitter’s Rules\nBergen (2022) Like, Comment, Subscribe. A book on the history of YouTube, it has a lot of detail on policy changes."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-film-and-television",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-film-and-television",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Exclusions in Film and Television",
    "text": "Exclusions in Film and Television\n\n\n\n\n\n\n\n\n\n\n\nalleged crime\nresult\n\n\n\n\n1920s\nFatty Arbuckle\nrumors of immorality\nfilm industry blacklisted\n\n\n\n\n\n\n\n\n1940s\nOrson Welles\ncommunist associations\nblacklisted, moved to Switzerland\n\n\n\nDalton Trumbo\ncommunist associations\nblacklisted\n\n\n\n(around 100 people)\ncommunist associations\nblacklisted for a decade\n\n\n\n\n\n\n\n\n1950s\nCharlie Chaplin\ncommunist associations\nbanned from US\n\n\n\nElia Kazan\ntestifying before HUAC\nlost some relationships in Hollywood\n\n\n\n\n\n\n\n\n1960s\nJane Fonda\nopposition to Vietnam war\nblacklisted\n\n\n\n\n\n\n\n\n1970s\nRoman Polanski\nrape of 13yo girl\nmild disapproval from Hollywood\n\n\n\n\n\n\n\n\n1990s\nO J Simpson\nmurdered his wife\nblacklisted\n\n\n\nWoody Allen\nmolested 7yo daughter\n\n\n\n\n\n\n\n\n\n2000s\nMel Gibson\nracism & anti-semitism\n“blacklisted in Hollywood for almost a decade”\n\n\n\nMira Sorvino\nrejecting Harvey Weinstein\nblacklisted\n\n\n\nRose McGowan\nrejecting Harvey Weinstein\nblacklisted\n\n\n\nIsaiah Washington\nhomophobic remarks\nblacklisted\n\n\n\nMichael Richards\nracist remarks\nblacklisted\n\n\n\nKathy Griffin\n“told Jesus to suck it”\nbanned from talk shows and TV appearances\n\n\n\nSean Penn\nopposition to Iraq war\ndropped from movie\n\n\n\n\n\n\n\n\n2010s\nBill Cosby\nsexual assault\nblacklisted (& later imprisoned)\n\n\n\nHarvey Weinstein\nsexual assault\nblacklisted (& later imprisoned)\n\n\n\nStacy Dash\nconservative advocacy\nblacklisted\n\n\n\nKirk Camerson\ncriticism of homosexuality\nblacklisted\n\n\n\nJames Woods\nanti-Obama tweets\nblacklisted\n\n\n\nCeeLo Green\nsexual assault\nblacklisted\n\n\n\nLouis CK\nsexual harassment\nblacklisted\n\n\n\nKathy Griffin\nphoto with head of Trump\nfired by CNN, lost endorsement, cancelled tour\n\n\n\nT J Miller\nsubstance abuse, sexual assault\nblacklisted\n\n\n\nGina Carano\npolitical social media posts\nfired from TV show\n\n\n\nKevin Spacey\nsexual harassment\nlost roles in films\n\n\n\nJussie Smollett\nlied about an attack\nlost roles in TV shows\n\n\n\nNeil deGrasse Tyson\nrape, sexual harassment\ntemporarily lost roles in TV shows\n\n\n\nRoseanne Barr\nracist tweet\nlost TV show\n\n\n\n\n\n\n\n\n2020s\nWill Smith\nslapping someone at Oscars\nfilm projects put on hold\n\n\n\nJohnny Depp\ndomestic violence\nlost roles in films\n\n\n\nAmber Heard\ninvolvement in trial w Johnny Depp\nlost roles in films\n\n\n\nJustin Roiland\nsexual harassment & abuse\nlost roles in shows"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-music",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-music",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Exclusions in Music",
    "text": "Exclusions in Music\n\n\n\n\n\n\n\n\n\n\n\nalleged crime\nresult\n\n\n\n\n1940s\nPaul Robeson\ncommunist associations\nblacklist and passport revoked\n\n\n\n\n\n\n\n\n1950s\nLeonard Bernstein\ncommunist associations\nbrief blacklist\n\n\n\nLena Horne\ncommunist associations\nblacklist\n\n\n\nPete Seeger\ncommunist associations\nblacklist\n\n\n\n\n\n\n\n\n1960s\nBeatles\nsaying they’re bigger than Jesus\nconsumer boycott\n\n\n\nLovin Spoonful\ncooperating with FBI\nmusic industry boycott\n\n\n\nNina Simone\n“Mississippi Goddam”\nboycott in the South\n\n\n\nJohn Lennon\ncriticism of US and Vietnam war\nrefused entry into US\n\n\n\nEartha Kitt\ncriticism of Vietnam war\nblacklist through LBJ and CIA\n\n\n\n\n\n\n\n\n1970s\nSex Pistols\ncriticizing the Queen, swearing on TV\nbanned by the BBC, dropped by EMI\n\n\n\n\n\n\n\n\n1980s\nNWA\n“Fuck the Police” & similar songs\nradio station boycott, police boycott\n\n\n\n\n\n\n\n\n1990s\nBruce Springseen\nsong against police brutality\nbrief police boycott\n\n\n\nMarilyn Manson\ntransgressive lyrics\nbanned from performing in some states\n\n\n\nBody Count\nsong “cop killer”\nalbum withdrawn and reissued\n\n\n\n\n\n\n\n\n2000s\nDixie Chicks\nfor opposition to Iraq war\nblacklisting and consumer boycott\n\n\n\nJanet Jackson\nshowing nipple\nVH1, MTV, & Viacom radio stopped playing her music\n\n\n\nR Kelly\nsexual abuse\nbroad blacklist\n\n\n\nChris Brown\ndomestic violence\nweak boycott and blacklist\n\n\n\n\n\n\n\n\n2010s\nLostprophets\nsexual abuse\nbroad blacklist\n\n\n\nMichael Jackson\nchild molestation\nsome radio stations stop playing music\n\n\n\n\n\n\n\n\n2020s\nBeyonce\nsong against police brutality\nbrief police boycott\n\n\n\nMorgan Wallen\nusing n-word\ntemporarily dropped from radio/streaming playlists\n\n\n\nKanye West\npraise of Hitler\nlost sponsors\n\n\n\nOthers.\n\nIn radio: Father Coughlin, Rush Limbaugh, Don Imus fired from CBS for calling womens’ basketball team “nappy-headed hos”, Howard Stern fired from various radio shows for comments.\nIn sport. Colin Kapaernick blacklisted from NFL for kneeling for the anthem. Pete Rose banned from MLB for gambling.\nNazi sympathisers/collaborators. Charles Lindbergh, Henry Ford, Charles Coughlin, PG Wodehouse, Ezra Pound.\nWriters: DH Lawrence, Henry Miller, Salman Rushdie (Nicole Bonoff).\nJournalists. Jeffrey Toobin (New Yorker writer masturbated on zoom call),\nNote on R Kelly disappearing from radio"
  },
  {
    "objectID": "posts/2017-09-27-work-of-art-age-mechnical-production.html",
    "href": "posts/2017-09-27-work-of-art-age-mechnical-production.html",
    "title": "The Work of Art in the Age of Mechanical Production",
    "section": "",
    "text": "When I heard about the neural nets that copy the styles of famous painters I thought it would be the same old junk.\nAcademics have been saying forever that they were on the verge of discovering the principles of aesthetics, and that they would soon be able to automate the production of beauty – melody, harmony, proportion, plot.\nWhen I was a kid I was excited to read about this sort of thing. But they always turn out to be fatuous, catastrophically oversimplified and overconfident, written - I’m guessing - by people who are intimidated & resentful of the culture around them. Our technical understanding of what makes something look good is still weak, and I don’t think it’s improving very fast. I learned to, when I come across an article about art written by a scientist, turn the page.\nBut now I think that maybe the automatic production of beauty will arrive soon. The machine learning algorithms work by extrapolating from existing examples, which means that they can produce new examples that fit some pattern (such as the pattern of beauty) without anyone involved having any explicit understanding of what the pattern is or how it can be defined.\nThis extrapolation without understanding is what happened in the study of visual perception – i.e. making inferences from images. Our understanding of perception is slowly moving forward, as it has been for centuries, but our ability to automate perception has shot ahead. In the 15th century Leonard da Vinci studied how the light reflected by an object is related to its distance – more distant objects tend to be bluer – these are relationships that we all know unconsciously, but which take a lot of work to dig out, such that we consciously understand them. Psychologists and computer scientists are still discovering things about the physics of light which we all know unconsciously. But computer models which incorporate our explicit knowledge of the physics of light are being thrashed by pure machine-learning models, which are fed a huge databases of pictures and simply extrapolate from what they’ve already seen.[2]\nI think the same basic point is true of aesthetic things. We really struggle trying to explain why we like a picture or dislike a melody, because most of the work is done at an unconscious level. The progress in understanding those principles will probably continue to be slow.\nBut now it seems likely to me that, before long, machines will be able to do all these things on demand – play some brand new Mozart, make elegant little drawings of animals, write a pretty good pop song. And the programmer who implements them could be – probably will be – some bozo who has no clue why it works.\n\n[1] 2017-05: SIGGRAPH video with style transfer - https://www.youtube.com/watch?v=HYhzZ-Abku8\n[2] 2017-05: Michael Elad “Deep, Deep Trouble: Deep Learning’s Impact on Image Processing, Mathematics, and Humanity” https://sinews.siam.org/Details-Page/deep-deep-trouble-4"
  },
  {
    "objectID": "posts/2017-09-27-work-of-art-age-mechnical-production.html#aka-machine-learning-aesthetics-the-unconscious",
    "href": "posts/2017-09-27-work-of-art-age-mechnical-production.html#aka-machine-learning-aesthetics-the-unconscious",
    "title": "The Work of Art in the Age of Mechanical Production",
    "section": "",
    "text": "When I heard about the neural nets that copy the styles of famous painters I thought it would be the same old junk.\nAcademics have been saying forever that they were on the verge of discovering the principles of aesthetics, and that they would soon be able to automate the production of beauty – melody, harmony, proportion, plot.\nWhen I was a kid I was excited to read about this sort of thing. But they always turn out to be fatuous, catastrophically oversimplified and overconfident, written - I’m guessing - by people who are intimidated & resentful of the culture around them. Our technical understanding of what makes something look good is still weak, and I don’t think it’s improving very fast. I learned to, when I come across an article about art written by a scientist, turn the page.\nBut now I think that maybe the automatic production of beauty will arrive soon. The machine learning algorithms work by extrapolating from existing examples, which means that they can produce new examples that fit some pattern (such as the pattern of beauty) without anyone involved having any explicit understanding of what the pattern is or how it can be defined.\nThis extrapolation without understanding is what happened in the study of visual perception – i.e. making inferences from images. Our understanding of perception is slowly moving forward, as it has been for centuries, but our ability to automate perception has shot ahead. In the 15th century Leonard da Vinci studied how the light reflected by an object is related to its distance – more distant objects tend to be bluer – these are relationships that we all know unconsciously, but which take a lot of work to dig out, such that we consciously understand them. Psychologists and computer scientists are still discovering things about the physics of light which we all know unconsciously. But computer models which incorporate our explicit knowledge of the physics of light are being thrashed by pure machine-learning models, which are fed a huge databases of pictures and simply extrapolate from what they’ve already seen.[2]\nI think the same basic point is true of aesthetic things. We really struggle trying to explain why we like a picture or dislike a melody, because most of the work is done at an unconscious level. The progress in understanding those principles will probably continue to be slow.\nBut now it seems likely to me that, before long, machines will be able to do all these things on demand – play some brand new Mozart, make elegant little drawings of animals, write a pretty good pop song. And the programmer who implements them could be – probably will be – some bozo who has no clue why it works.\n\n[1] 2017-05: SIGGRAPH video with style transfer - https://www.youtube.com/watch?v=HYhzZ-Abku8\n[2] 2017-05: Michael Elad “Deep, Deep Trouble: Deep Learning’s Impact on Image Processing, Mathematics, and Humanity” https://sinews.siam.org/Details-Page/deep-deep-trouble-4"
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html",
    "title": "Sushi-Roll Model of Online Media",
    "section": "",
    "text": "\\[\n\\def\\RR{{\\bf R}}\n\\def\\bold#1{{\\bf #1}}\n\\]\nA model of internet media: the platform chooses the composition, the user chooses the quantity. I think this is a nice crisp way of modeling how media platforms (FB, YouTube, TikTok) make their decisions about content: they chooses the mix of content, i.e. the shares of each type, and then their users choose the quantity. The platform is choosing the fillings for the sushi roll and the consumer is choosing how much to eat. Their decisions jointly determine the total amount of each ingredient consumed.\nThis gives a unified model of feed ranking inclusive of ad-load, revenue-sharing, producer-side effects, and advertiser demand elasticity.\nWe can break down four different ways in which increasing the share of a given content-type \\(i\\) will affect total revenue:\nAn efficient mix of content will choose the shares such that each type of content has the same marginal value, i.e. for each the type four components of value all sum to the same number.\nRelated literature. There are some nice models of ad-media tradeoff in Anderson and Jullien (2015), but I believe they don’t consider the effects on production by producers, nor the tradeoff between different types of content (though it’s a long time since I read it).\nExpressed formally: Suppose the platform chooses \\(x_1,\\ldots,x_n\\) which represent the impression-shares of each type of content such that \\(\\sum_{i=1}^nx_i=1\\). User demand depends on the average quality of each type of content (\\(q_i\\)), and they have diminishing returns in each type of content. The platform receives \\(p_i\\) for showing an impression of type \\(i\\), but that price depends on the number of impressions-seen. We can write the maximization problem as:\n\\[\\begin{aligned}\n      \\max_{x_1,\\ldots,x_n} \\utt{\\left(\\sum_{i=1}^n q_i(x_i)x_i^\\gamma\\right)}{total}{impressions}\n                     \\utt{\\left(\\sum_{i=1}^n x_ip_i(x_i)\\right)}{avg revenue}{per impression}\n      ,\\text{ s.t. }\\sum_{i=1}^n x_i=1\n\\end{aligned}\n\\]\nThe first order condition shows us the four components of value:\n\\[\\frac{\\partial L}{\\partial x_i} =\n   \\ut{\n      (\\utt{q_i \\gamma x_i^{-(1-\\gamma)}}{incrementality}{}\n      + \\utt{q_i'(x_i)x_i^\\gamma}{effect through}{quality})\n      \\utt{\\left(\\sum_{j=1}^n p_j x_j\\right)}{avg revenue}{per impression}\n   }{effect on revenue through total impressions}\n   +\n   \\utt{\n      (\n         \\utt{p_i(x_i)}{revenue from}{additional impressions}+\n         \\utt{p'_i(x_i)x_i}{revenue from}{change in price}\n      )\n      \\utt{\\left(\\sum_{j=1}^n q_j x_j^{\\gamma}\\right)}{total}{impressions}}\n      {effect on revenue}{through impressions on $i$}\n    + \\utt{\\lambda}{avg marginal}{effect}=0\n\\]\nThe final term, \\(\\lambda\\), is the Lagrangian, representing the average marginal value of the outside option, i.e. the other types of content that are being replaced. In some cases we can simplify this model and we get a closed-form solution for the optimal content composition.\nWhat this model doesn’t include:"
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html#more-details",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html#more-details",
    "title": "Sushi-Roll Model of Online Media",
    "section": "More Details",
    "text": "More Details\nWe can walk through a series of models from simple to complicated, to build up to the full sushi-roll model:\n\nPlatform chooses share of ads. The platform chooses the share of impressions that are ads, and consumers choose how many total impressions to consume. If we assume the price of ads (CPMs) is fixed then the platform will set the ad-load to maximize the total number of ad-impressions. If the platform can influence the price of ads by their choice of quantity (i.e. they act as a monopolist) then the platform may choose to reduce ad-load to drive up CPMs.1\nIn this model we’re letting the platform set the quantity of ads, but we would get the same result if the platform instead set the price of ads, e.g. they posted a specific CPM and advertisers can buy as much as they want.\nPlatforms chooses shares of organic content. Suppose ad-load is fixed but platforms can vary the shares of different types of organic content. Users’ consumption depends on the quality of the content but they also have a taste for variety (i.e. diminishing returns in each type of content). We then get a nice closed-form solution where the share of each type of content is increasing in its relative quality. On the margin the incrementality of each type of content will be zero: i.e. increasing the share of that type of content will have no effect on total impressions.\nPlatform choses shares of ads and organic content. Now lets treat both advertisers and organic producers as the same: each producer has a quality \\(q_i\\) but they also will pay a certain price \\(p_i\\) for impressions on their content. The platform takes those prices as given. We can then distinguish between three types of producer:\n\nAdvertisers: \\(p_i&lt;0\\): the producer will pay the platform per impression.\nProfessional producers: \\(p_i&gt;0\\): the producer asks to be paid per impression.\nAmateurs: \\(p_i=0\\): there is no monetary exchange, the content is in the public domain or generated by an ordinary user.\n\nIn equilibrium the share of impressions allocated to a given producer (\\(x_i\\)) will depend both on its quality \\(q_i\\) (AKA incrementality) and the price \\(p_i\\) that the producer sets. I don’t have a closed-form solution but we can derive a first-order condition that has a straight-forward interpretation.\nThis model is easy to state but I think is primarily applicable to small platforms where they take prices as given. E.g. suppose you run an app where you (1) license certain content, or use user-generated content; (2) run ads from various different ad networks, the ads vary in CPMs but they also vary in incrementality (i.e. how obnoxious they are to your userbase).\nBecause prices are taken as given, this model doesn’t help us calculate the optimal revenue share. It will tell us the optimal ad-load, but not taking into account the elasticity of supply from the advertiser.\nNote that most platforms do not explicitly discriminate between advertisers based on their incrementality however they can implicitly discriminate by having a “quality score” or “organic bid”. This score is quite clearly designed to measure the incrementality of the advertisements, and so I think can be used to implement an efficient pricing scheme.\nPlatform choses shares of ads and organic content, prices endogenous. We can easily extend the model above to allow the price of each type of content to depend on the quantity shown (\\(p_i(x_iM)\\)). For example the price of ads will depend on the number of ad impression shown (due to advertisers’ diminishing marginal returns from ads shown). This gives platforms a reason to restrict the quantity of ad-impressions.\nPlatform chooses both shares and prices. Up to this point the platform chose only the share of each type of content.\nNow suppose the platform can set a price to pay producers (e.g. “revenue share”), and it’s a homogenous price. The price should roughly depend on (1) the producer’s elasticity of quality to price (i.e. their cost function), and (2) the incrementality of quality on the consumer side. We could set this up with a single price for all producers, or set a producer-specific price.\n\n1 In the profit-maximizing solution either the consumer-side or advertiser-side first-order-condition will be binding. It depends on the relative elasticity of the two sides of the platform."
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-1-platform-chooses-ad-load",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-1-platform-chooses-ad-load",
    "title": "Sushi-Roll Model of Online Media",
    "section": "Model 1: Platform Chooses Ad-Load",
    "text": "Model 1: Platform Chooses Ad-Load\n(see previous paper)"
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-2-platform-chooses-organic-composition",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-2-platform-chooses-organic-composition",
    "title": "Sushi-Roll Model of Online Media",
    "section": "Model 2: Platform Chooses Organic Composition",
    "text": "Model 2: Platform Chooses Organic Composition\nWe have a model where there are \\(n\\) producers, the platform assigns to each producer a share of total content \\(x_i\\), with \\(\\sum_ix_i=1\\), and the consumer will choose how many total impressions to consume (\\(M\\)) based on the average quality, but with diminishing returns in each .\n\\[\\begin{aligned}\n   q_i &\\in \\mathbb{R}^+\n      && \\text{quality of producer $i$}  \\\\\n   x_i &\\in [0,1]\n      && \\text{share of impressions on producer $i$}\\\\\n   \\sum_i x_i &= 1\n      && \\text{shares must sum to 1}\\\\\n   M  &= \\sum_{i=1}^nq_ix_i^\\gamma\n      && \\text{total impressions, diminishing returns in each producer, $0&lt;\\gamma&lt;1$} \\\\\n\\end{aligned}\\]\nThe platform wishes to maximize total impressions, \\(M\\). We want to solve for the resultant impression-share of each producer, i.e. \\(x_i\\) as a function of the qualities \\(q_1,..,q_n\\) and parameter \\(\\gamma\\). We get the following impression-maximizing shares:\n\\[x_i=\\frac{q_i^\\frac{1}{1-\\gamma}}{\\gamma\\sum_{j=1}^nq_j^\\frac{1}{1-\\gamma}}.\\]\nImplication: impression-share will be proportional to quality. Interesting the elasticity will be increasing in quality: a 1% increase in quality will get a more than 1% increase in share of impressions, because \\(\\frac{1}{1-\\gamma}&gt;1\\).\nAdding money. Suppose now that the platform gets paid for showing certain impressions. We can make various different assumption about the price paid:\n\nUniform homogenous price: the platform takes the price as given. This only makes sense if there are a subset of producers who are advertisers.\nEach producer sets a payment rate per impression.\nThe platform chooses a single price for all producers to get extra impressions."
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-3-platform-chooses-composition-prices-fixed",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-3-platform-chooses-composition-prices-fixed",
    "title": "Sushi-Roll Model of Online Media",
    "section": "Model 3: Platform Chooses Composition, Prices Fixed",
    "text": "Model 3: Platform Chooses Composition, Prices Fixed\nWe have a model with a consumer, a platform, and a set of \\(n\\) producers. The platform chooses the share of content from each producer, \\(x_i\\in[0,1]\\) with \\(\\sum_i x_i=1\\). The consumer chooses the total amount of impressions they consume, \\(M\\), based on the mixture of content and the quality of each type of content \\(q_i\\). Finally producers can set a price \\(p_i\\) for each impression that they receive from the consumer. A positive price \\(p_i&gt;0\\) means\n\\[\\begin{aligned}\n   q_i &\\in \\mathbb{R}\n      && \\text{quality of producer $i$}  \\\\\n   p_i &\\in \\mathbb{R}\n      && \\text{price offered by producer $i$}  \\\\\n   x_i &\\in [0,1]\n      && \\text{share of impressions on producer $i$}\\\\\n   \\sum_i x_i &= 1\n      && \\text{shares must sum to 1}\\\\\n   M  &= \\sum_{i=1}^nq_ix_i^\\gamma\n      && \\text{total impressions, diminishing returns in each producer, $0&lt;\\gamma&lt;1$} \\\\\n\\end{aligned}\n\\]\nIf the platform simply wanted to maximize total impressions, \\(M\\), then they can derive the optimal impression-shares as follows:\n\\[x_i^*=\\frac{q_i^\\frac{1}{1-\\gamma}}{\\gamma\\sum_{j=1}^nq_j^\\frac{1}{1-\\gamma}}.\\]\nHowever we want the platform to maximize profit, which we can write as follows:\n\\[\\begin{aligned}\n   \\text{profit} &= \\utt{\\left(\\sum_{i=1}^n q_ix_i^\\gamma\\right)}{total}{impressions}\n                  \\utt{\\left(\\sum_{i=1}^n x_ip_i\\right)}{avg revenue}{per impression}\n\\end{aligned}\n\\]\nI’m not sure if we can get a closed-form solution but we can at least get a first-order condition for each \\(x_i\\) that tells us useful stuff about comparative statics:\n\\[\\frac{\\partial L}{\\partial x_i} =\n   \\ut{\\utt{q_i \\gamma x_i^{-(1-\\gamma)}}{effect on}{total impressions}\n      \\utt{\\left(\\sum_{j=1}^n p_j x_j\\right)}{avg revenue}{per impression}\n   }{effect on revenue through total impressions}\n   +\n   \\utt{p_i\\utt{\\left(\\sum_{j=1}^n q_j x_j^{\\gamma}\\right)}{total}{impressions}}\n      {effect on revenue}{through impressions on $i$}\n    + \\utt{\\lambda}{avg marginal}{effect}=0\n\\]\nObservations:\n\nIf producers offer more, increasing \\(p_i\\), then \\(x_i\\) will go down until the marginal effect on total impression declines to balance the additional revenue.\nIf \\(p_i&lt;0\\), meaning a producer charges for impressions, then they can still have a positive number of impressions if their effect on total impressions is higher than the average of other types of content. (We could have added an additional constraint that \\(x_i\\geq 0\\).)"
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-4-platform-chooses-composition-monopolist",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-4-platform-chooses-composition-monopolist",
    "title": "Sushi-Roll Model of Online Media",
    "section": "Model 4: Platform Chooses Composition, Monopolist",
    "text": "Model 4: Platform Chooses Composition, Monopolist\nNow we allow the price of each type of content to depend on the quantity used, e.g. the price of ads will be higher when the quantity of ad-impressions is smaller (monopolist in the ad market). Strictly we should write \\(p_i(Mx_i)\\), but it’s somewhat easier to write \\(p_i(x_i)\\) and the answer should be similar for any type of content that is a small share.\n\\[\\begin{aligned}\n      \\text{profit} &= \\utt{\\left(\\sum_{i=1}^n q_ix_i^\\gamma\\right)}{total}{impressions}\n                     \\utt{\\left(\\sum_{i=1}^n x_ip_i(x_i)\\right)}{avg revenue}{per impression}\n\\end{aligned}\n\\]\nThere’s now one additional term in the first order condition:\n\\[\\frac{\\partial L}{\\partial x_i} =\n   \\ut{\\utt{q_i \\gamma x_i^{-(1-\\gamma)}}{effect on}{total impressions}\n      \\utt{\\left(\\sum_{j=1}^n p_j x_j\\right)}{avg revenue}{per impression}\n   }{effect on revenue through total impressions}\n   +\n   \\utt{\n      (\n         \\utt{p_i(x_i)}{revenue from}{additional impressions}+\n         \\utt{p'_i(x_i)x_i}{revenue from}{change in price}\n      )\n      \\utt{\\left(\\sum_{j=1}^n q_j x_j^{\\gamma}\\right)}{total}{impressions}}\n      {effect on revenue}{through impressions on $i$}\n    + \\utt{\\lambda}{avg marginal}{effect}=0\n\\]\nThe additional term represents the platform’s monopoly power with respect to the price paid. This has a natural interpretation for advertisers: showing fewer ads will drive up the price. For paid content-providers it could perhaps represent bulk discounts, I’m not sure whether this is a significant consideration."
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-2-derivation",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-2-derivation",
    "title": "Sushi-Roll Model of Online Media",
    "section": "Model 2 Derivation",
    "text": "Model 2 Derivation\nThis is derivation of model #2. (I had chatGPT help with this derivation, was very useful)\n\nSetting up the Lagrangian. The objective is to maximize the total impressions, \\(M\\), subject to the constraint that the allocated shares of impressions sum to one. We start by writing the Lagrangian: \\[\\mathcal{L} = \\sum_{i=1}^n q_i x_i^\\gamma - \\lambda \\left(\\sum_{i=1}^n x_i - 1\\right)\\]\nwhere \\(\\lambda\\) is the Lagrange multiplier associated with the constraint.\nSolving for the multiplier. To solve for the value of \\(\\lambda\\), we take the derivative of the Lagrangian with respect to \\(x_i\\) and set it equal to zero: \\[\\frac{\\partial \\mathcal{L}}{\\partial x_i} = \\gamma q_i x_i^{\\gamma - 1} - \\lambda = 0\\]\nRearranging this equation yields: \\[x_i = \\left(\\frac{\\lambda}{\\gamma q_i}\\right)^{\\frac{1}{\\gamma-1}}\\]\nTaking the sum of this expression over all producers and using the constraint that the shares of impressions must sum to one, we obtain: \\[1 = \\sum_{i=1}^n x_i = \\sum_{i=1}^n \\left(\\frac{\\lambda}{\\gamma q_i}\\right)^{\\frac{1}{\\gamma-1}}\\]\nSimplifying this equation gives:\n\\[\\lambda^{\\frac{1}{\\gamma-1}} = \\gamma \\sum_{i=1}^n q_i^{-\\frac{1}{\\gamma-1}}\\]\nSubstituting this expression for \\(\\lambda\\) back into the equation for \\(x_i\\) results in:\n\\[x_i = \\frac{q_i^{-\\frac{1}{\\gamma-1}}}{\\gamma \\sum_{j=1}^n q_j^{-\\frac{1}{\\gamma-1}}}\\]\nThis is our final expression for the share of impressions on each producer as a function of the exogenous qualities and \\(\\gamma\\).\n\nSummary:\nThe Lagrangian: \\(\\mathcal{L} = \\sum_{i=1}^n q_i x_i^\\gamma - \\lambda \\left(\\sum_{i=1}^n x_i - 1\\right)\\).\nExpression for the multiplier: \\[\\lambda^{\\frac{1}{\\gamma-1}} = \\gamma \\sum_{i=1}^n q_i^{-\\frac{1}{\\gamma-1}}\\]\nThe resultant expression for \\(x_i\\): \\[x_i = \\frac{q_i^{-\\frac{1}{\\gamma-1}}}{\\gamma \\sum_{j=1}^n q_j^{-\\frac{1}{\\gamma-1}}}.\\]\nSlightly rearranged (by me):\n\\[x_i=\\frac{q_i^\\frac{1}{1-\\gamma}}{\\gamma\\sum_{j=1}^nq_j^\\frac{1}{1-\\gamma}}.\\]"
  },
  {
    "objectID": "posts/2023-09-05-model-of-ai-imitation.html",
    "href": "posts/2023-09-05-model-of-ai-imitation.html",
    "title": "An AI Which Imitates Humans Can Beat Humans",
    "section": "",
    "text": "Thanks to comments from many, especially Giorgio Martini, Grady Ward, Rob Donnelly, Inés Moreno de Barreda, and Colin Fraser.\nIf we train AIs to imitate humans, will they ever beat humans? AI has caught up to human performance on many benchmarks, largely by learning to predict what humans would do. It seems important to know whether this is a ceiling or we should expect them to shoot out ahead of us. Will LLMs be able to write superhumanly-persuasive prose? Will image models be able to see things in photos that we cannot? There is a lot of technical literature on imitation learning in AI but I haven’t found much discussion of this point (Bowman (2023) is a notable exception).\nIn a formal model I derive five mechanisms by which imitative AI can beat humans.\nThe evidence is unclear. There are many reasons why this could theoretically occur but I couldn’t find much evidence for superhuman performance: many benchmarks which we use to evaluate ML models have human labels as the ground truth, meaning we wouldn’t know when computers do pass us by.\nThis blog post contains:"
  },
  {
    "objectID": "posts/2023-09-05-model-of-ai-imitation.html#timeline",
    "href": "posts/2023-09-05-model-of-ai-imitation.html#timeline",
    "title": "An AI Which Imitates Humans Can Beat Humans",
    "section": "Timeline",
    "text": "Timeline\n\n\n\nThe following table shows the year in which a computer (or mechanical device) could match performance with the best human:\n\n\n\narithmetic\n1642\n\n\nchess\n1997\n\n\nJeopardy\n2005\n\n\nimage recognition (ImageNet)\n2015\n\n\nhandwriting recognition (MNIST)\n2015\n\n\nquestion answering (SQuAD1.1)\n2019\n\n\ndifficult math questions (MATH)\n2023\n\n\ncoding problems (MBPP)\n(not yet)\n\n\n\n5 Kiela et al. (2021) also say that “models that achieve super-human performance on benchmark tasks (according to the narrow criteria used to define human performance) nonetheless fail on simple challenge examples and falter in real-world scenarios.”Computers have hit the ceiling on most benchmarks. Kiela et al. (2023) documents that most computer benchmarks have become “saturated,” i.e. computers get close-to-perfect performance, and that recently the speed of saturation has become quicker (see graph on right). They say identify only a single benchmark where performance is not close to the human baseline, and most of the models they discuss are imitation learning. As a consequence some work has moved to evaluating models against “adversarial” benchmarks where the problems are chosen specifically to fool computers (e.g. Dynabench, Kiela et al. (2021)).5\nOn some tasks human performance defines success. On some tasks human performance effectively is the ground truth, and so by definition computers could never beat humans. This is roughly true for text comprehension: a sentence has a given meaning if and only if the average person believes it has that meaning. When we observe computer outperformance on this type of benchmark it is because either (1) there is human variation and the computer output is more consistent; or (2) computers outperform amateur humans but the ground truth is expert humans."
  },
  {
    "objectID": "posts/2023-09-05-model-of-ai-imitation.html#performance-by-task",
    "href": "posts/2023-09-05-model-of-ai-imitation.html#performance-by-task",
    "title": "An AI Which Imitates Humans Can Beat Humans",
    "section": "Performance by Task",
    "text": "Performance by Task\nArithmetic: computers passed humans 300 years ago. Machines have been used to do calculations since the 17th century, e.g. Pascal’s calculator from 1642.\n\n\n\n\n\nBackgammon\n1979\n\n\nChess\n1997\n\n\nJeopardy\n2005\n\n\nAtari games\n2013\n\n\nGo\n2016\n\n\nStarcraft\n2019\n\n\n\n(source)\nPlaying games: computers passed humans over the last 45 years. See the table in the margin for games. I am not aware of any well-known games in which computers cannot reliably beat the best humans.\n\n\n (source)\nImage recognition: computers surpassed humans in the 2010s. With the qualifications above about the limitations of benchmark tasks.\nQuestion answering: computers surpassed humans in the 2010s. With the qualifications above about the limitations of benchmark tasks.\nFacial recognition: computers seem to be equivalent to experts. Towler et al. (2023) say “naturally skilled super-recognizers, trained forensic examiners and deep neural networks, … achiev[e] equivalent accuracy.”\nCoding: computers still below expert. See the benchmarks on PapersWithCode, also a graph on OurWorldInData, specifically APPS and MBPP. The best-performing computers are still imperfect at solving these coding challengers (which presumably can be solved by an expert programmer), but progress is rapid.\nWriting persuasive text: computer comparable to average human. A number of recent papers compare the persuasive power of LLM-generated text to human-generated text (Bai et al. (2023), Goldstein et al. (2023), Hackenburg and Margetts (2023), Matz et al. (2023), Palmer and Spirling (2023), Qin et al. (2023)). They all find that LLMs do relatively well, but none show clear signs of computer superiority.\nWriting creative blurbs: computer comparable to average human. Koivisto and Grassini (2023) compared GPT4 to online recruited humans (£2 for a 13 minute task) in giving “creative” uses for everyday items. The prompt was to “come up with original and creative uses for an object”, objects were “rope”, “box”, “pencil” and “candle.” The responses were rated by humans for their “creativity” or “originality.” GPT-4 responses were perhaps 1SD above the average human score, but the difference was smaller when choosing just the best response for each user.\nSummarizing text: computer beats average human. Two recent papers found that LLM-generated summaries, trained with feedback, were preferred by humans to human-generated summaries (Stiennon et al. (2022) using RLHF and Lee et al. (2023) using RLHF). However in both cases it wasn’t clear to me exactly how the human summarizers were incentivized, and whether they were trying to perform the same task as the LLMs.6\n6 Lee et al. (2023) say “RLAIF summaries are preferred over the reference [human-written] summaries 79% of the time, and RLHF are preferred over the reference summaries 80% of the time.”7 Hendrycks et al. (2021) says “We found that a computer science PhD student who does not especially like mathematics attained approximately 40% on MATH, while a three-time IMO gold medalist attained 90%”Doing math problems: computer comparable to expert. The latest score on the MATH benchmark is 84%, compared to 90% by a three-time IMO gold medalist. The scores have been rising very rapidly so it seems likely that computers will soon surpass humans.7"
  },
  {
    "objectID": "posts/2023-09-05-model-of-ai-imitation.html#model-implications",
    "href": "posts/2023-09-05-model-of-ai-imitation.html#model-implications",
    "title": "An AI Which Imitates Humans Can Beat Humans",
    "section": "Model Implications",
    "text": "Model Implications\n\nIf one human records all their observations then the computer will perfectly imitate them.\n\nSuppose that there is one human and they write down all of their observations, \\(\\hat{Q}=Q\\). Because the computer and human have the same priors, and observe the same data, then they will therefore end up with the same estimated weights (\\(\\hat{\\bm{w}}=\\bar{\\bm{w}}\\)), and so the computer will answer every question exactly as the human does, though neither knows the truth (\\(\\bar{\\bm{w}}\\neq\\bm{w}\\)).\n\n\nIf humans are noisy then the computer will outperform them. Suppose humans report their answers with some i.i.d. noise \\(\\epsilon\\). If the computer observes sufficiently many answers for each question then the noise will be washed out and they will outperform.\n\nIf humans record a subset of their observation then the computer will perform worse.\n\nSuppose humans only write down some of their observations, i.e. \\(\\hat{Q}\\) is a row-wise subset of \\(Q\\). Then computers and humans will give the same answers for any question in the training set, but outside of that set computers will generally do worse than humans. And so for any question \\(\\bm{q}\\not\\in\\hat{Q}\\) the computer will do worse in expectation: \\[E[\\ut{(\\bm{q}(\\bm{w}-\\bar{\\bm{w}}))^2}{computer error}]\\geq\n     E[\\ut{(\\bm{q}(\\bm{w}-\\hat{\\bm{w}}))^2}{human error}].\\] Note that we are fixing the question \\(\\bm{q}\\) and taking the expectation over all possible worlds, \\(\\bm{w}\\). I think you could probably rewrite this such that, in the world we are in, we should observe worse average performance across a set of questions, but I think you’d need to add some conditions to make sure that the questions are sufficiently independent (e.g. if there was a single weight \\(w_q\\) which dominated all the other weights then the computer might beat the human by accident).\n\nIf there are two humans then the computer will outperform them both.\n\nSuppose there are two humans who each observe answers to different question, \\(Q_A\\) and \\(Q_B\\), and they both write them all down, so \\(\\bar{Q}=(\\smallmatrix{Q_A\\\\Q_B})\\) and \\(\\bar{\\bm{a}}=(\\smallmatrix{Q_A\\bm{w}\\\\Q_B\\bm{w}})\\). Now the computer clearly has superior information to either human, and so if we let \\(\\hat{\\bm{w}}(i)\\) represent the weights of human \\(i\\in\\{A,B\\}\\), then for any question \\(\\bm{q}\\) we can write:\n\\[ E[\\ut{(\\bm{q}(\\bm{w}-\\bar{\\bm{w}}))^2}{computer error}]\\leq\n  E[\\ut{(\\bm{q}(\\bm{a}-\\hat{\\bm{w}}(i)))^2}{human error}].\n   \\]\n\n\nIf there are multiple humans then the computer can answer question no human can answer. Suppose two humans observe the answers to the following questions:\n\\[\\begin{aligned}\n      Q_A &= \\bmatrix{1 & 1 & 1 \\\\ 1 & -1 & 1} \\\\\n      Q_B &= \\bmatrix{1 & 1 & 1 \\\\ 1 & 1 & -1}\n   \\end{aligned}\\]\nThe first human will learn the exact value of \\(w_2\\) (\\(\\hat{w}_2=w_2\\)), and the second human will learn the exact value of \\(w_3\\), but neither will learn both values, and so neither could predict the answer to this question with perfect confidence:\n\\[\\begin{aligned}\n      \\tilde{q} &= \\bmatrix{1 & -1 & -1} \\\\\n   \\end{aligned}\\]\nHowever if they both recorded their observations, so the computer observes \\(\\bar{\\bm{a}}=(\\smallmatrix{Q_1\\bm{w}\\\\Q_2\\bm{w}})\\), the computer will be able to infer both \\(w_2\\) and \\(w_3\\), and thus will be able to perfectly answer \\(\\tilde{q}\\). We can see this behaviour in LLMs: they sometimes combine a pair of facts or a pair of abilities which no single human has access to, e.g. when an LLM translates between two languages, for which there exists no human speaker of both.\nIf humans write outside their expertise then the computer will do worse. In the cases above we assumed that the two humans recorded only what they directly observed, \\(\\hat{Q}\\subseteq Q\\). This means the computer essentially had a window directly to the world. However the humans could instead have written down their estimated answers to other questions. Suppose both humans wrote down answers to every possible question, \\(\\bm{q}\\in\\{-1,1\\}^p\\), then we could conjecture that the computer would learn the average of the two humans’ weights:14 \\[\\bar{\\bm{w}}=\\frac{1}{2}\\hat{\\bm{w}}_A+\\frac{1}{2}\\hat{\\bm{w}}_B.\\] Here the computer will do worse than the two humans on the original questions, \\(Q_A\\) and \\(Q_B\\). The implication is that LLMs work so well only because people tend to write about what they know. Put another way, when an LLM answers a question, it will not predict the answer given by the average person, but will predict the answer given by people who are likely to answer that question in the real world, and luckily those people tend to be people who are subject-matter experts.\n14 We would have to augment the computer’s learning rule to allow for noise in answers - I need to confirm that the weighting will be exactly 1/2.15 This is related to the “generator-discriminator” gap, but specific to knowledge rather than to logical implication.If humans have tacit knowledge, then computers can outperform in choosing a question to maximize the answer. We can model tacit knowledge with two separate sets of human weights:15\n\\[\\begin{aligned}\n      \\hat{\\bm{w}}^T   &= \\text{tacit knowledge}\\\\\n      \\hat{\\bm{w}}^E &= \\text{explicit knowledge}\\\\\n   \\end{aligned}\\]\nWhen the human encounters a new question \\(\\tilde{\\bm{q}}\\) they will use their tacit knowledge to form an estimate of the answer, \\(\\hat{a}=\\tilde{\\bm{q}}'\\hat{\\bm{w}}^T\\). But they have limited ability to introspect about that capacity, and so when asked how they make their judgments they can report only \\(\\hat{\\bm{w}}^E\\). For simplicity assume tacit knowledge is perfect (\\(\\hat{\\bm{w}}^T=\\bm{w}\\)), and explicit knowledge is imperfect (\\(\\hat{\\bm{w}}^E=\\bm{w}\\)).\nThe distinction becomes important when we want to create a new question. Here it’s useful to interpret \\(\\bm{q}\\) not as a question but as an artefact, e.g. a text or image, and interpret \\(a=\\bm{q}'\\bm{w}\\) as a property of that artefact, e.g. how persuasive is the text, or how attractive is the image. Suppose we want to choose \\(\\bm{q}\\in\\{-1,1\\}^n\\) to maximize \\(a\\). If we had perfect access to our beliefs \\(\\bm{w}^T\\) this would be simple, however if we have access only to imperfect explicit knowledge \\(\\hat{\\bm{w}}^E\\), the artefact which maximizes that function will not generally be the one which maximizes \\(a\\). This represents an asymmetry in human cognition: we can recognize certain patterns (whether text is persuasive, whether a picture is pretty), without being able to produce those patterns.\nHere the computer model is less constrained. Suppose the computer has observed sufficiently many questions such that they have perfectly learned human tacit knowledge, \\(\\bar{\\bm{w}}=\\hat{\\bm{w}}^T\\). If computation is costless we could query every single \\(\\bm{q}\\in\\{-1,1\\}^p\\) to find the highest \\(a\\). More realistically we could use a diffusion algorithm, or reinforcement learning against human or computer evaluation, to find an artefact with a high \\(a\\)."
  },
  {
    "objectID": "posts/2023-09-05-model-of-ai-imitation.html#derivation",
    "href": "posts/2023-09-05-model-of-ai-imitation.html#derivation",
    "title": "An AI Which Imitates Humans Can Beat Humans",
    "section": "Derivation",
    "text": "Derivation\nSetup.\n\\[\\begin{aligned}\n      Q &= \\bmatrix{q_1^1 & \\ldots & q^1_p \\\\ & \\ddots \\\\ q^n_1 & \\ldots & q^n_p}\n         && \\text{(matrix of $n$ questions, each with $p$ parameters)} \\\\\n      \\bm{w}'  &= \\bmatrix{w_1 \\ldots w_p}\n         && \\text{(vector of $p$ unobserved weights)}\\\\\n      \\bm{a}    &= \\bmatrix{a^1 \\\\ \\vdots \\\\ a^n}\n         = \\bmatrix{q_1^1 w_1 + \\ldots q_p^1w_p \\\\ \\vdots \\\\ q_1^n w_1 + \\ldots q_p^n w_p}\n         && \\text{(vector of $n$ observed answers)}\\\\\n   \\end{aligned}\\]\nWritten more compactly:\n\\[\\begin{aligned}\n      Q      &\\in \\{-1,1\\}^{p\\times n}\n         && \\text{($n$ questions, each has $p$ binary parameters)}\\\\\n      \\bm{w} &\\sim N(0,\\Sigma)\n         && (p\\times 1\\text{ vector of true parameters of the world)}\\\\\n      \\ut{\\bm{a}}{$n\\times1$}   &= \\ut{Q}{$n\\times p$}\\ut{\\bm{w}}{$p\\times1$}\n         && \\text{(answers provided by the world)}\\\\\n   \\end{aligned}\\]\nHuman posteriors. Given you observe a subset of a set of multivariate normal variables there is a simple expression for your posteriors over the remaining unobserved variables (e.g. see here).\n\\[\\begin{aligned}\n      \\hat{\\bm{w}} &= E[\\bm{w}|Q,\\bm{a}]\n            && \\text{(human beliefs about the world)}\\\\\n         &= \\ut{\\Sigma Q'}{$Cov(\\bm{w},\\bm{a})$}\n            (\\ut{Q\\Sigma Q'}{$Var(\\bm{a})$})^{-1}\n            \\bm{a}\n         && \\text{(from the Schur complement)}\n   \\end{aligned}\\]\nWe can use the same formula to calculate computer beliefs."
  },
  {
    "objectID": "posts/2023-09-05-model-of-ai-imitation.html#additional-observations",
    "href": "posts/2023-09-05-model-of-ai-imitation.html#additional-observations",
    "title": "An AI Which Imitates Humans Can Beat Humans",
    "section": "Additional Observations",
    "text": "Additional Observations\nThese are a few miscellaneous results additional results that helped me with intuition for the working of this model.\nWith one observation and two weights. Suppose \\(n=1, p=2\\), then we have:\n\\[\\begin{aligned}\n      Q  &= \\bmatrix{q_1 & q_2} \\\\\n      \\bm{a}'  &= \\bmatrix{a} \\\\\n      \\bm{w}'  &= \\bmatrix{w_1 & w_2 } \\\\\n      \\Sigma &= \\bmatrix{\\sigma_1^2 & \\rho \\\\ \\rho & \\sigma_2^2}\\\\\n      \\Sigma Q' &= \\bmatrix{ \\sigma_1^2q_1 + \\rho q_2 \\\\ \\rho q_1 + \\sigma_2^2 q_2 } \\\\\n      Q\\Sigma Q' &= \\bmatrix{ \\sigma_1^2q_1^2 + 2\\rho q_1q_2 + \\sigma_2^2 q_2^2} \\\\\n      \\hat{\\bm{w}}=\\Sigma Q'(Q\\Sigma Q')^{-1}\\bm{a}\n         &= \\bmatrix{ \\frac{\\sigma_1^2q_1 + \\rho q_2}{\\sigma_1^2q_1^2 + 2\\rho q_1q_2 + \\sigma_2^2 q_2^2} \\\\\n                  \\frac{\\rho q_1 + \\sigma_2^2 q_2}{\\sigma_1^2q_1^2 + 2\\rho q_1q_2 + \\sigma_2^2 q_2^2}} a\n   \\end{aligned}\\]\nWe can normalize \\(q_1=q_2=1\\), then we have: \\[\\hat{w}_1 = \\frac{\\sigma_1^2+\\rho}{\\sigma_1^2+2\\rho+\\sigma_2^2}a,\\] Here we are dividing up responsibility for the answer (\\(a\\)) into the contributions of each component, nice and simple.\nWith two observations and one weight. Here we’re over-identified.\n\\[\\begin{aligned}\n      Q  &= \\bmatrix{q^1 \\\\ q^2} \\\\\n      \\bm{a}  &= \\bmatrix{a^1 \\\\ a^2} \\\\\n      \\bm{w}  &= \\bmatrix{w } \\\\\n      \\Sigma &= \\bmatrix{\\sigma^2 }\\\\\n      \\Sigma Q' &= \\bmatrix{ \\sigma^2 q^1 & \\sigma^2 q^2 } \\\\\n      Q\\Sigma Q' &= \\bmatrix{ \\sigma^2 q^1q^1 & \\sigma^2q^1q^2 \\\\ \\sigma^2q^1q^2 & \\sigma^2q^2q^2}\n         && \\text{(this matrix doesn't have an inverse)}\n   \\end{aligned}\\]\nWith noise. Suppose we only observe the answers with random noise, then we get this:\n\\[\\begin{aligned}\n      \\ut{\\bm{a}}{$m\\times1$}   &= \\ut{Q}{$m\\times n$}\\ut{\\bm{w}}{$n\\times1$}\n         + \\ut{\\bm{e}}{$n\\times 1$} \\\\\n      \\bm{e} &\\sim N(\\bm{0},s^2I_m) && \\text{(i.i.d. noise w variance $s^2$)}\\\\\n      Cov(\\bm{w},\\bm{a})   &= \\Sigma Q' \\\\\n      Var(\\bm{a}) &= Q\\Sigma Q' + s^2I_m \\\\\n      E[\\bm{w}|Q,\\bm{q}]   &= \\Sigma Q'(Q\\Sigma Q' + s^2I_m)^{-1}\\bm{a}\n   \\end{aligned}\\]\nCompare to Bayesian linear regression. We can compare this result to Bayesian linear regression (e.g. Wikipedia):\n\\[\\begin{aligned}\n      \\bar{\\beta}  &= \\Sigma Q'(Q\\Sigma Q' + s^2I_m)^{-1}\\bm{a}\n         && \\text{(our result)} \\\\\n      \\tilde{\\beta} &= (Q'Q+s^{2}\\Sigma^{-1})^{-1}Q'\\bm{a}\n         && \\text{(Bayesian linear regression)}\\\\\n   \\end{aligned}\\]\nI believe that these can be shown to be equivalent by the matrix inversion lemma, though I haven’t confirmed this. There’s a note online that appears to show equivalence.\nExtension: quadratic forms. Instead of answers being linear in question-features (\\(a=q'w\\)) we could suppose they’re quadratic, \\(a=q'Wa\\), with \\(W\\) a matrix having dimension \\(n^2\\). I’m not sure whether we could still get an analytic solution for posteriors. Can visualize the matrix W: each bit in \\(q\\) will add an “L” overlaid on the matrix (alternatively: a row and column), and \\(a\\) will be the sum of the cells where both the row and the column are activated.\nExtension: binary answers. In some cases it is natural to think of the answer, \\(a\\), as binary instead of continuous. We might be able to reinterpret the model with \\(a\\) representing the log-odds ratio of a binary outcome. Alternatively there might be a way of having a beta-binomial conjugate prior over the probability of \\(a\\)."
  },
  {
    "objectID": "posts/2020-04-05-front-loading-restrictions.html",
    "href": "posts/2020-04-05-front-loading-restrictions.html",
    "title": "Optimal Coronavirus Policy Should be Front-Loaded",
    "section": "",
    "text": "Q: How should you sequence policies over time? E.g. suppose you want to manage the epidemic until a vaccine arrives and you have policies (lockdowns, distancing, masks) each of which is associated with a certain effect on the growth-rate of cases, but each also has some fixed social cost per day. How should you apply the policies over time?\nA: The severity of the policies should be gradually decreasing, i.e. they should gradually become less severe, as you approach the availability of a vaccine. There should not be zig-zagging between policies in this setup.\nAny justification for zig-zagging must come from some additional consideration like (a) non-separabilities in the costs, e.g. psychological/economic need for occasional respite, (b) uncertainty about the end-date, (c) uncertainty about the effect of the policies, such that there is informational-value from varying policies, or (d) desire to maintain a steady flow of cases, in order to reach herd immunity (the “mitigation” strategy)."
  },
  {
    "objectID": "posts/2020-04-05-front-loading-restrictions.html#corollary-you-should-never-expect-policy-to-get-stricter",
    "href": "posts/2020-04-05-front-loading-restrictions.html#corollary-you-should-never-expect-policy-to-get-stricter",
    "title": "Optimal Coronavirus Policy Should be Front-Loaded",
    "section": "Corollary: you should never expect policy to get stricter",
    "text": "Corollary: you should never expect policy to get stricter\nYou should never find yourself in the situation where you expect policy to get stricter in the future. If you anticipate that a stricter policy will be appropriate next week then that strict policy is appropriate this week!\nCountries in early stages of the epidemic should be doing as much or more as countries in later stages."
  },
  {
    "objectID": "posts/2020-04-05-front-loading-restrictions.html#intuition",
    "href": "posts/2020-04-05-front-loading-restrictions.html#intuition",
    "title": "Optimal Coronavirus Policy Should be Front-Loaded",
    "section": "Intuition",
    "text": "Intuition\nSuppose that there’s some tradeoff across policiers between the growth-rate and the social cost.\nThen given any fixed time-path of policies: e.g., (A,A,B,C), if it is not monotonically decreasing in severity from high-cost to low-cost, then you can do strictly better by rearranging the path of policies to be monotonically decreasing. The social cost will be identical, because the set of policies will be the same, but the number of cases will be lower at every point in time, since at any given point the cumulative growth rates, up to that point, will be lower. Thus the final cumulative number of cases will be lower."
  },
  {
    "objectID": "posts/2020-04-05-front-loading-restrictions.html#additional-reason-to-front-load-extinction",
    "href": "posts/2020-04-05-front-loading-restrictions.html#additional-reason-to-front-load-extinction",
    "title": "Optimal Coronavirus Policy Should be Front-Loaded",
    "section": "Additional Reason to Front-Load: Extinction",
    "text": "Additional Reason to Front-Load: Extinction\nAll of this is treating the number of cases as a continuous variable which means you can never completely extinguish the disease. However if that’s a possibility that’s within sight (e.g. as in NZ), then that’s a significantly stronger case for starting with very severe policies, to try to kill the disease entirely, and then you can go back to the garden of Eden."
  },
  {
    "objectID": "posts/2020-04-05-front-loading-restrictions.html#prior-discussion",
    "href": "posts/2020-04-05-front-loading-restrictions.html#prior-discussion",
    "title": "Optimal Coronavirus Policy Should be Front-Loaded",
    "section": "Prior Discussion",
    "text": "Prior Discussion\nThere’s been some discussion of zig-zagging by the Imperial group (paper) and by Timothy Gowers (twitter & post)\nGowers says the optimal policy is very short zig-zags (changing policy every other day), however I think this is misleading. It comes from fixing the lower-threshold and optimizing the upper-threshold. If instead you fixed the upper-threshold and optimized the lower-threshold, then the optimal cycle-length will be long.\nIf you choose both the upper and lower threshold (both T and S) then he notes that they’ll both be arbitarily low. However this ignores the cost of getting to zero given current cases.\nInstead a well-defined problem is to choose an optimal time-path of policy given some start-point and end-point. In that case it’ll be a path of gradually decreasing strictness (without zig-zags).\nYou can see the intuition in the diagram below: the total infections is approximately the area under the zig-zag (not quite: because the y-axis is ln(cases), but this won’t matter for the argument). Thus you can reduce the area under the line by lowering the upper threshold. However if you instead take the upper threshold as fixed, then it’s optimal to choose a lower threshold that is as low as possible, i.e. you want long cycles, not short cycles.\n\n\n\nabc"
  },
  {
    "objectID": "posts/2023-10-24-manifesto-perception-judgment-decision-making.html",
    "href": "posts/2023-10-24-manifesto-perception-judgment-decision-making.html",
    "title": "Bloodhounds and Bulldogs",
    "section": "",
    "text": "This note contains some ideas about hierarchical structure in perception, judgment and decision-making that I haven’t seen explained clearly elsewhere."
  },
  {
    "objectID": "posts/2023-10-24-manifesto-perception-judgment-decision-making.html#implications-of-encapsulated-inference",
    "href": "posts/2023-10-24-manifesto-perception-judgment-decision-making.html#implications-of-encapsulated-inference",
    "title": "Bloodhounds and Bulldogs",
    "section": "4.1 Implications of Encapsulated Inference",
    "text": "4.1 Implications of Encapsulated Inference\nHere I givae the basic predictions of the model, in subsequent sections I discuss evidence relevant to these predictions from perception, judgment, and decision-making.\n\nJudgment will degrade when presentation is unusual. We have very accurate judgments about cases which we have experience with but judgment degrades when some high-level information becomes relevant to interpretation. E.g., judgment will degrade when the same information is available but presented in an unusual way – upside-down, back to front, inverted.\nJudgment will be sensitive to irrelevant features. Judgment will be sensitive to a cue which the subjects knows to be irrelevant when (a) that cue is usually relevant in similar situations; (b) the fact that the cue is irrelevant in this situation is high-level information, not accessible to the encapsulated system.\nJudgment of hypotheticals will be poor. We will have poor ability to state how our judgment would change if one of the cues changed. E.g.: would this drawing look more like your cousin if the nostrils were more flared? Would you judge this candidate the same way if they were a woman as if they were a man? These are objective questions about your own judgments, but if judgment is encapsulated then people may give inaccurate answers to these questions.\nJudgment will be sensitive to comparisons. When someone is judging two objects at the same time the conscious brain will have access to two encapsulated judgments, and therefore will learn something about what cues influences judgment. Thus we should expect to see systematic comparison effects which will reveal the nature of the information that is encapsulated. We also should see irrelevant-influences affect judgment in between-subjects, but not in within-subjects, experiments. \nPersistence of biased intuitions. Even when we are consciously aware that an encapsulated judgment is incorrect the encapsulated system will still produce that inference, i.e. people retain a subjective perception which they know to be false (e.g., even after you learn that an illusion is misleading it still looks that way)."
  },
  {
    "objectID": "posts/2023-10-24-manifesto-perception-judgment-decision-making.html#basic-facts",
    "href": "posts/2023-10-24-manifesto-perception-judgment-decision-making.html#basic-facts",
    "title": "Bloodhounds and Bulldogs",
    "section": "5.1 Basic facts",
    "text": "5.1 Basic facts\nPeople are good judges of objects in the world.5 We are very good at judging the distance, size, or weight of an object, recognizing a face or a scene. Computers have only recently become comparable to humans after decades of dedicated work.\n5  How various visual cues affect the judgment of distance.People are poor judges of low-level perceptual stimuli. When asked to judge the magnitude of a raw sensation people are generally poor judges: e.g. comparing the brightness of two lights, the shade of two colours, the length of two lines. For computers these tasks are trivial.\nThere exist a large set of perceptual illusions. Psychologists have collected a diverse set of perceptual illusions in which people make predictable mistakes in reporting raw sensations (Muller-Lyer, Ebbinghaus, McGurk, etc.). There are still multiple active schools of thought in explaining most of these illusions."
  },
  {
    "objectID": "posts/2023-10-24-manifesto-perception-judgment-decision-making.html#predictions-of-encapsulated-inference",
    "href": "posts/2023-10-24-manifesto-perception-judgment-decision-making.html#predictions-of-encapsulated-inference",
    "title": "Bloodhounds and Bulldogs",
    "section": "5.2 Predictions of Encapsulated Inference",
    "text": "5.2 Predictions of Encapsulated Inference\nWe can give a simple model of encapsulated inference:\n\nEarly perceptual processes infer real-world values (\\(v\\)) using information about raw sensations (\\(x\\)), and about associations between sensations and value.\nBiases in judgment about the world occur because the early processes do not have access to high-level information available to the conscious brain (\\(z\\)).\nBiases in judgment about raw stimuli occur because the conscious infers the value of stimuli from the outputs of the early processes \\(\\hat{v}_1\\).\n\nIt is useful to make a distinction between raw sensations and their causes in the world. The causes can then be subdivided into value and noise, where “value” represents something of interest to the organism.\n\n\n\nsensation (\\(x\\))\nvalue (\\(v\\))\nnoise (\\(e\\))\n\n\n\n\nlight on retina\nreflectance of object\nillumination\n\n\nsize on retina\nsize of object\ndistance of object\n\n\npressure on hand\nweight of object\nsensitivity of hand\n\n\noriention on retina\norientation in world\norientation of head\n\n\nmotion on retina\nmotion of object\nmotion of eye\n\n\n\nThis gives us a series of predictions:\nSensitivity to presentation. Peoples’ ability to judge and recognize stimuli are notably worse when the stimuli are presented in a way that we’re not accustomed to: e.g. upside down, inverted, tinted, or back to front.6\n6 e.g. Valentine (1988), Galper (1970), Kemp et al (1990).Influence of high-level context. It’s useful to distinguish three different aspects of high-level context, and the predictions of double encapsulations:\n\\[\\begin{aligned}\n      \\frac{d\\hat{v}^1}{dz} &&& \\text{contextual influences on automatic judgment of world}\\\\\n      \\frac{d\\hat{v}^2}{dz} &&& \\text{contextual influences on conscious judgment of world}\\\\\n      \\frac{d\\hat{x}^2}{dz} &&& \\text{contextual influences on conscious judgment of sensations}\n   \\end{aligned}\n   \\]\nThe double-encapsulation theory predicts that automatic judgments about the world are independent of contextual influences (\\(\\frac{d\\hat{v}^1}{dz}=0\\)), while final judgments of both are dependent (\\(\\frac{d\\hat{v}^2}{dz}\\neq0\\), \\(\\frac{d\\hat{x}^2}{dz}\\neq 0\\)).\nI believe that this is broadly consistent with the empirical evidence:\n\nperceptual inferences are resistant to modulation by contextual information, except for attention effects,\nconsidered judgments of both sensations and values are commonly influenced by contextual information.7.\n\n7 Firestone & Scholl (2016, BBS)8 Cross-modal influences are often been cited as evidence against the encapsulation of perception, because they show influences across perceptual areas. However the fact that they show systematic errors in assessment of sensations demonstrates the existence of vertical encapsulation (i.e. lack of direct access to raw stimuli), at the same time as they show that the limits of horizontal encapsulation.Judgment of raw sensations. Because the conscious brain does not directly observe the raw stimuli, it must infer them from the encapsulated system’s outputs: \\(E\\hat{x}_1=[\\hat{x}_1|\\hat{v}^1]\\) and \\(\\hat{x}_2=E[\\hat{x}_2|\\hat{v}^1]\\). This predicts cross-modal effects in judgment of sensations, where one sensation influences estimate of another sensation:8\n\\[\\underbrace{\\frac{d\\hat{x}_1}{dx_2}}_{\\text{cross-modal}\\atop\\text{effect}}\n         \\propto \\underbrace{corr(x_1,v)}_{\\text{diagnostic value of $x_1$}}\n            \\Big/ \\underbrace{corr(x_2,v).}_{\\text{diagnostic value of $x_2$}}\n   \\]\nThis is consistent with many laboratory examples showing “cross-modal” effects, where one stimuli (e.g. auditory) is influenced by some other stimuli (e.g. visual).\nExamples fitting this pattern: (1) the McGurk and anti-McGurk effects; (2) the Stroop effect; (3) confusing motion of beeps and flashes; (4) confusing numerosity of beeps and flashes; (5) phoneme restoration; (6) assimilation to expectations - proofreaders’ errors; (7) Simon interference - response location & stimulus location.\n\n\n Contrast effects in shade, contrast, hue, and size. The central circle is identical in each case.\nApplication: contrast and assimilation effects. A common finding in perception is a contrast effect, where some quality appears less-intense when placed next to a more-intense neighbor, although we also observe assimilation effects (the opposite) in some circumstances. I argue that (1) contrast effects in real-world perception are due to rational inference; (2) contrast effects in raw sensory stimuli are byproducts of double-encapsulation.\nSuppose there are two neighboring objects, each with unobserved value (\\(v_1,v_2\\)) and additive noise:\n\\[\\underbrace{\\binom{x_1}{x_2}}_{observed\\atop signal}\n      =\\underbrace{\\binom{v_1}{v_2}}_{unobserved\\atop value}\n         +\\underbrace{\\binom{e_1}{e_2}}_{unobserved\\atop noise}.\n   \\]\nIf everything is normally distributed then we have the following: \\[\\underbrace{\\frac{dE[v_{1}|x_{1},x_{2}]}{dx_{2}}}_{\\text{assimilation/contrast effect}}\n      \\propto \\underbrace{\\text{corr}(v_1,v_2)}_{\\text{correlation}\\atop\\text{in values}}\n               -\\underbrace{\\text{corr}(e_1,e_2)}_{\\text{correlation}\\atop\\text{in noise}}.\n   \\]\nIn words this implies we expect an “assimilation” effect when the value is more correlated than the noise, and a “contrast” effect when the noise is more correlated than the value.\nThe equation above describes rational inferences of the encapsulated system about real-world values, but we additionally observe that contrast and assimilation effects occur in judgment of raw sensations, and for that reason they can properly be called “biases” or “illusions” relative to the full-information case. If we assume the conscious brain infers the sensations from the posteriors of the encapsulated system (\\(\\hat{x}_1\\propto \\hat{v}_1^1\\)), then we get the same comparative static:\n\\[\\frac{d\\hat{x}_1}{dx_{2}}\\propto \\text{corr}(v_1,v_2)-\\text{corr}(e_1,e_2).\\]\nI believe this gives a good account of the existence of contrast and assimilation effects across a range of cases:\n\nAdelson (1993) gives a series of examples demonstrating these effects in lightness illusions: contrast effects occur when context implies that noise is more correlated than value, and assimilation effects occur when context indicates that value is more correlated than noise.9\nWhen judgment is affected by a dissimilar stimulus (cross-modal effects), contrast effects will tend to occur when the additional cue is associated with the stimulus, assimilation effects will occur when it is associated with the value.\nThese biases will tend to be smaller for more automatic responses - e.g. grasping responses - insofar as they receive signals prior to high-level processing.\n\n9 Some papers also find Bayesian assimilation with very noisy stimuli, which I think is also consistent, but need more argument.Reproduction. People find it easy to recognize patterns but hard to reproduce them, for example to paint a picture or to transcribe a melody. This is consistent with the model above insofar as the recognition is done pre-consciously. The model also predicts certain biases in reproduction, e.g. that people will tend to fail to account for shadows when they do a drawing, because they are inferring the raw stimuli \\(x\\) from the inferred object \\(v\\), and the brain’s inference has discarded most of the information it considers to be noise, such as illumination.\nAdditional notes.\n\nDifferences in estimand. We have assumed that the encapsulated system is estimating the same quantity that the conscious brain is trying to estimate, \\(v\\). In many cases the conscious brain will want to estimate some other quantity, and this discrepancy will cause a distinct type of bias. I think this accounts for the size-weight bias in which people judge a larger object to be lighter, all else equal: if the encapsulated system is inferring the density of an object (from size and weight), and passing that estimate to the conscious brain, this would account for a negative effect of size on conscious estimates of weight.\nBimodal posteriors. When there are two plausible but distinct interpretations of a given stimulus, i.e. when posteriors are bimodal, then people often alternate between perceiving the two interpretations, e.g. the Necker cube and the black/gold dress. It’s interesting that the cognitive system seems incapable of representing bimodal posteriors directly."
  },
  {
    "objectID": "posts/2023-10-24-manifesto-perception-judgment-decision-making.html#basic-facts-1",
    "href": "posts/2023-10-24-manifesto-perception-judgment-decision-making.html#basic-facts-1",
    "title": "Bloodhounds and Bulldogs",
    "section": "6.1 Basic Facts",
    "text": "6.1 Basic Facts\nThere are a set of well-known judgment illusions. There are a large set of simple judgment questions which people give consistently wrong answers to.\n\nLogical judgment: bat-and-ball, Linda, Monty Hall, mathematics problems on exames.\nFactual judgment: anchoring, joint-separate inconsistencies.\n\nThere are many proposed explanations of these biases but there is still relatively little consensus.\nPeople are generally poor at making statistical generalizations about features. A variety of laboratory tests give people a set of cases and ask them to either (a) judge the strength of correlation between two features, or (b) predict an unobserved feature of a case given the observed features. People are often much worse than simple computer programs at performing these tasks.\nJudgment in some domains can be very accurate very accurate. People can learn extremely subtle judgments when playing chess and Go, such that computers have only been able to beat them relatively recently.10 Other domains are harder to benchmark but it seems fair to say that judgment can be finely calibrated in domains such as interpersonal emotional judgment, medical diagnosis, music or literature.11\n10 Playing a game could be seen as a combination of judgment and decision-making, but because every player shares essentially the same preferences (they prefer to win) it’s informative about judgment.11 Kahneman and Klein (2009) say that human judgment tends to be good when subjects have (i) a lot of experience, and (ii) quick feedback."
  },
  {
    "objectID": "posts/2023-10-24-manifesto-perception-judgment-decision-making.html#judgment-and-encapsulated-inference",
    "href": "posts/2023-10-24-manifesto-perception-judgment-decision-making.html#judgment-and-encapsulated-inference",
    "title": "Bloodhounds and Bulldogs",
    "section": "6.2 Judgment and Encapsulated Inference",
    "text": "6.2 Judgment and Encapsulated Inference\nThe encapsulated inference model says that judgments are largely instinctual and we have limited introspection into what determines them, e.g. when we are guessing the price of a good, judging the likelihood of an event, judging the trustworthiness of a person, we rationally trust our instincts. This has a number of consequences.\nLimited introspection. We have limited ability to explain our judgments, e.g. to formalize them in a way that a computer could reproduce them.12 The history of human sciences is consistent with this: e.g. linguists have worked for centuries on finding the rules which make a sentence grammatical, formalizing knowledge that we all posess intuitively.\n12 There is a long debate on what evidence would establish unconscious knowledge. Experimentalists have shown that subjects can learn a pattern but have trouble describing it. Critics have said (a) the subjects may have learned a different pattern but with the same extension (Quine), or (b) the questions asked of subjects are insufficiently detailed. In my view the problem is in the design of the experiments: implicit knowledge can be defined behaviorally, by observing when low-level and high-level information fail to be integrated.Hypothetical questions. The model predicts that people will be unable to accurately answer hypothetical questions about their own judgments, e.g.: Would you value this bottle of whisky the same if it had a different price on it? Would you like this house as much if it was a different colour?\nInfluence of irrelevant cues. A common finding is that judgment is influenced by irrelevant cues, i.e. when told to ignore some fact people will still be moved by it. This occurs in the model when (1) the cue is integrated into preconscious judgments, (2) the conscious brain does not know how heavily the cue is weighted, and so cannot account for it.\nEffect of comparisons. When two judgments are being made at the same time – e.g., two items are being judged – then the conscious decision-maker will learn more about the unconscious knowledge. The patterns of comparison effects will reveal the nature of implicit knowledge.\nInternal consistency. We should expect that judgment anomalies will disappear in within-subject studies because the decision-maker becomes aware of the inconsistency and adjusts their judgments.\nConsistency of biases. A lot of academic literature tries to identify biases in judgment. In our analysis biases will be caused by correlations in the environment, and so we should expect them to vary or reverse from one environment to the next, and indeed we find that many biases reverse sign:\n\nthe “contrast effect” vs the “assimilation effect”\nthe “gambler’s fallacy” vs the “hot hand fallacy”\nthe “recency effect” vs “confirmation bias”\n“overweighting of low probabilities” vs “neglect of rare events”\n\nEffect of incentives. We should see that incentives don’t materially affect biases, beyond the point where they simply get the person to pay attention. This is in contrast to inattention-based theories.\nPersistence of mistaken judgments. The encapsulated-inference theory predicts that after someone learns that a judgment is incorrect then the feeling will persist, because the encapsulated system operates independently of higher-level knowledge. Thus in many of the classic judgment illusions, even when you know the right answer the wrong answer still has some intuitive draw.13\n13 Sloman (1996) described this as Criterion S in arguing for two systems of reasoning: “a reasoning problem satisfies Criterion S if it causes people to simultaneously believe two contradictory responses.”"
  },
  {
    "objectID": "posts/2023-10-24-manifesto-perception-judgment-decision-making.html#implications-for-improving-judgments",
    "href": "posts/2023-10-24-manifesto-perception-judgment-decision-making.html#implications-for-improving-judgments",
    "title": "Bloodhounds and Bulldogs",
    "section": "6.3 Implications for Improving Judgments",
    "text": "6.3 Implications for Improving Judgments\n\nOrganize information in a way that helps your instincts to recognize patterns. For example (a) visualizing data so that visual-processing modules can be used to recognize patterns; (b) describe uncertainty in language that’s similar to the way that uncertainty is experienced: in terms of frequencies (“1 out of 5”) instead of probabilities (“20%”); (c) when looking for different interpretations of data, present it in an unfamiliar way in order to route around your unconscious inferences.\nRemove information that you know to be irrelevant. Your instincts will pick up on all available cues, even irrelevant ones. Thus attempting to ignore cues is likely to be ineffective, it’s better to physically remove irrelevant information.\nAsk yourself variations on the same question. – it will help to extract more information from your instincts."
  },
  {
    "objectID": "posts/2023-10-24-manifesto-perception-judgment-decision-making.html#background",
    "href": "posts/2023-10-24-manifesto-perception-judgment-decision-making.html#background",
    "title": "Bloodhounds and Bulldogs",
    "section": "7.1 Background",
    "text": "7.1 Background\nMotivating examples. Decision-making as a whole is an unwieldy topic, I’ll give some motivating examples focussed on economics:\n\nChoosing which wine to buy for dinner.\nChoosing how much of your income to spend vs save.\nChoosing whether to move cities for a job.\n\nWe don’t know much about what influences decisions. There are two polar schools of thought: (1) that decisions maximize some objective outcomes; (2) that decisions are buffeted by all sorts of influences – context, custom, time of day. Concretely, given variation in choices (e.g. savings rate, hours worked, charitable giving, education), we can decompose into variation in (1) budgets, (2) consequentialist preferences, (3) other non-consequentialist influences. Perhaps 1/3 each.\nI will treat decision-making as judgment of value. i.e., people judge \\(E[v|x]\\), and choice is just selecting the outcome with the highest expected-value. This allows me to use the same framework as used in perception and judgment.\nIn decision-making there’s no objective standard. When discussing perception and judgment we can determine whether people are right or wrong, but it’s not the same with decision-making. Here we never observe the true value of things, we can only talk about inconsistencies among decisions, which itself requires making assumptions over what are reasonable preferences.\nWe have limited insight into our goals. I think we have limited insight both into ordinary value judgments (why do I prefer this coffee-cup to that one?), and into overarching goals (why do I marry? have children?). Philosophy, after thousands of years, still has not given us clarity regarding our tradeoffs between different ends (pleasure, moral imperatives and religious imperatives, providing for others, receiving love & receiving esteem).\nEvolution must play some deep role in decision-making.14 As with other animals, humans’ decision-making must have been shaped to favor reproduction. However I believe there remains relatively little consensus on exactly how it affects decision-making, outside some specific areas.\n14 I think there’s a common perception that although evolution must have importantly shaped human psychology, the evolutionary perspective has made few fundamental contributions to understanding psychology.15 Some of this correlation can come from genetic effects or complementarity (when other people do X, then it’s in your interest to do X). However we see that when people migrate to a different culture in adulthood they retain a substantial part of their old preferences.We must absorb most of our preferences.15 A large share of variation in decisions must be due to the preferences that we absorb when we grow up: taste in music, religion, political beliefs, career, who we marry, how many children to have, where to live, all must be heavily influenced by early experiences.\nPreferences vs associations. Instead of estimating objective quantity, can estimate instrumental value, payoff, utility.\nThere are a set of known decision biases. There are some decision problems in which peoples’ choices consistently violate norms of rationality: Allais paradoxes, Ellsberg paradoxes, small-stakes risk-aversion, relative-thinking paradoxes, time inconsistency, anchoring effects, etc. There have been many attempts to fit these anomalies into generalizations about decision-making but there has been little consensus (e.g. with prospect theory, inattention, relative thinking)."
  },
  {
    "objectID": "posts/2023-10-24-manifesto-perception-judgment-decision-making.html#perspective-from-encapsulated-inference",
    "href": "posts/2023-10-24-manifesto-perception-judgment-decision-making.html#perspective-from-encapsulated-inference",
    "title": "Bloodhounds and Bulldogs",
    "section": "7.2 Perspective from Encapsulated Inference",
    "text": "7.2 Perspective from Encapsulated Inference\n(1) Limited introspection. The model predicts that we have poor ability to introspect, e.g. to explain how different features influenced our decisions, or to predict what we would do in a different hypothetical situation.\n(2) Sensitivity to associations. The evaluation of an outcome should be sensitive to whatever details are informative about value, and we still respond to those details when we’re aware that they are uninformative in the current situation, insofar as that awareness is not available to the encapsulated system. Thus we expect that people may have well-calibrated judgment for usual situations, but make bad or inconsistent decisions in unusual situations. This is my interpretation of many laboratory biases such as framing, anchoring, etc..\nThe decision-theory community has developed sophisticated logic to model many decision anomalies (ambiguity aversion, relative thinking, probability weighting). In my opinion most of these anomalies are due to associations which are very sensitive to context, and formal modelling is relatively unfruitful. The imperatives of publication have, I think, caused too much effort to be put in this direction: if anomalies are context-specific then research is less publishable, for that reason academics keep trying to come up with general theories, beyond the point of plausibility.16\n16 Examples: (1) people avoid ambiguity not intrinsically, but because it’s associated with bad outcomes in certain classes of situation, and they seek out ambiguity in other situations; (2) people are influenced by the choice-set because it’s often informative about relative value, but the nature of the influence varies drastically between situations (discussion here).17 More precisely: when stakes are high enough such that people no longer choose dominated alternatives, most of the between-person inconsistency in choices remains.(3) Consistency within decisions. The model says that violations of rationality are mostly inadvertent. We therefore expect decisions to be consistent within situations, though they may be inconsistent between situations. This is consistent with laboratory evidence showing that people show large framing effects, but rarely choose dominated options.17\n(4) Implicit knowledge is revealed in comparisons. 18 When two objects are evaluated side by side, that reveals some of the unconscious information to the conscious brain, and so we should expect systematic comparison effects. In choice, we can show that characteristic intransitivities reveal implicit knowledge, e.g. the intransitive cycle to the right (a “figure 8”) reveals an explicit preference for male over female, but an implicit preference for male over female. Similarly if we observe that some bundle of attributes is evaluated more highly when the comparison bundle becomes more similar in some respect, this reveals implicit knowledge about the attributes (either a positive implicit association about a shared attribute, or a negative implicit association about a non-shared attribute). Jon and I formalize this logic in our paper “Implicit Preferences”.\n18 \\[\\xymatrix{\n         \\binom{\\text{female}}{\\text{MBA}}\\ar@{-}@/_.3pc/[dr]|(.4){\\rsucc{135}}\n         & \\binom{\\text{female}}{\\text{PhD}}\\ar@{-}@/^.3pc/[dl]|(.4){\\rsucc{45}}\n         \\\\ \\binom{\\text{male}}{\\text{MBA}}\\ar@{-}@/^.3pc/[u]|{\\rsucc{270}}\n         & \\binom{\\text{male}}{\\text{PhD}}\\ar@{-}@/_.3pc/[u]|{\\rsucc{270}}\n   }\\]"
  },
  {
    "objectID": "posts/2023-10-24-manifesto-perception-judgment-decision-making.html#additional-notes-on-encapsulated-preferences",
    "href": "posts/2023-10-24-manifesto-perception-judgment-decision-making.html#additional-notes-on-encapsulated-preferences",
    "title": "Bloodhounds and Bulldogs",
    "section": "8.1 Additional Notes on Encapsulated Preferences",
    "text": "8.1 Additional Notes on Encapsulated Preferences\nPuzzle: distortion of conscious reasoning. In many cases situations which activate a visceral temptation also seems to affect our conscious thoughts: we don’t just give in to temptations, we also rationalize it. In the dog metaphor it’s like having a very persuasive bulldog who can talk you into things. You might still avoid the situation but once in the situation you choose whole-heartedly. It’s unclear why we would reason in this way.\nEvolved desires vs encapsulated desires. Should be careful not to confuse encapsulated preferences with a different relation, where evolution gives us a goal which we consciously strive for (eat sweet things, have sex) which are only proximal relative to the evolutionary goal of reproducing. Those proximal goals could still be ipmlemented in a single system which would make entirely consistent maximizing choices (but it would be maximizing an outcome that is only correlated with evolution’s goal).\nCharacteristic behaviors which reveal encapsulated preferences:\n\nPreferences are stronger when made more salient. E.g., (1) more likely to choose a croissant when you smell it; (2) you’re scared by certain things only if you can see them, when giving blood you avoid looking at the needle; (3) you’re more likely to agree to have sex if you’re aroused. Just thinking about certain things can make us aroused or happy or scared, so we can strategically choose what to think about.\nMaking personal rules. We set a certain time to start & finish working; set a goal number of words to write each day; only getting ice-cream if you’ve been to the gym; setting a rule for how much money to save. Implies you don’t trust your future self, so you make up a rule, even though there’s no way to enforce it.\nStrategic choices. We choose to avoid certain options. E.g., (1) buying a house to force yourself to save; (2) avoiding situations where you think you’ll be tempted. Formally you can call this “choice over choice sets”, and there’s a a fair amount of decision-theory on how multiple-selves can be identified if you can observe this kind of choice.\nEffect of distraction. You are more likely to indulge in proximal goals when you are distracted - e.g. manipulation of cognitive load.\nimplicit preferences. More influenced by an attribute in less-direct choice sets.\n\nOther variation in decision by context: Decisions made with salient outcomes, or abstract outcomes; decisions made under time pressure; decisions made with cognitive load; decisions made in advance; decisions over future choice sets; choices made in different moods, or after exerting willpower; decisions made directly or indirectly; from small or large choice sets; and attitudes which are expressed by involuntary responses (response time, skin conductance, pupil dilation)."
  },
  {
    "objectID": "posts/2023-10-24-manifesto-perception-judgment-decision-making.html#why-would-we-have-encapsulated-preferences",
    "href": "posts/2023-10-24-manifesto-perception-judgment-decision-making.html#why-would-we-have-encapsulated-preferences",
    "title": "Bloodhounds and Bulldogs",
    "section": "8.2 Why would we have encapsulated preferences",
    "text": "8.2 Why would we have encapsulated preferences\nTwo types: (a) evolutionary proximal goals (sugar, sex, putrid smells); (b) learned proximal goals (coffee, wine, tobacco, learned food aversions).\nProximal preferences from evolution. A lot of human preferences make sense as proximal goals for achieving a long-run evolutionary purpose: when we get a kick out of doing X, it’s because doing X was selected for in our evolutionary history. These goals are in some sense hard-wired because people still do X even when it no longer makes any evolutionary sense to do it: (1) we enjoy sex even when it doesn’t lead to reproduction; (2) we like the taste of sugar even when we know it’s bad for us; (3) we get scared of snakes, spiders, of heights, of blood even when we see them behind glass - we are even scared of pictures of these things; (4) we are disgusted by smells associated with infection, even when we know there’s no risk of infection. (There are also arguments that some emotions - such as anger, jealousy, love - are hardwired for a slightly different reason - because they serve as commitment devices in social interaction - Trivers).\nWhy would these be hardwired? Or rather, instead of hardwiring preferences, why didn’t evolution hardwire beliefs? It would seem to be more efficient to just have innate knowledge: that snakes are dangerous, that sex leads to reproduction, that rotting meat is an infection risk. Then we could treat each of these things as means, not as ends. I guess there are two reasons: (1) perhaps it’s harder to preinstall knowledge than to preinstall preferences, because each brain grows differently; (2) perhaps it’s safer to preinstall preferences, to prevent them being overridden by a malfunctioning conscious brain which thinks it knows better.\nProximal preferences from learning. We often can explain peoples’ preference for X, because X has been associated with good outcomes in the past, yet people still choose X even when they are aware it no longer has those good associations. I.e., X has turned from a means into an end in itself. This can be described as a “habit”, “learned preference.” Some examples: (1) enjoying the smell of coffee, the taste of wine, or a cigarette, when those associations must be due principally to the psychoactive drugs that have been associated with them; (2) a learned phobia; (3) a learned food aversion, even when you know it’s irrelevant (e.g. chemotherapy patients develop aversions to whatever food they were eating during the chemotherapy).\nIt’s hard to answer why learned preferences should be hard-wired.\nDifferent models which get at aspects of this type of behaviour: (1) a complementarity between a cue and consumption: the consumption becomes more valuable when exposed to an associated cue (Laibson); (2) a temptation cost - you have to pay a cost to not consume something when it is possible to choose it – (Gul and Pesendorfer) (3) preferences which depend on your state of arousal - e.g. willingness to be violent, have sex, changes preditably with context (Loewenstein on arousal)."
  },
  {
    "objectID": "posts/2023-06-06-effect-of-ai-on-communication.html",
    "href": "posts/2023-06-06-effect-of-ai-on-communication.html",
    "title": "The Influence of AI on Content Moderation and Communication",
    "section": "",
    "text": "Thanks to many comments, esp. Ravi Iyer, Sahar Massachi, Tal Yarkoni, Rafael Burde, Grady Ward, Ines Moreno de Barreda, and Daniel Quigley."
  },
  {
    "objectID": "posts/2023-06-06-effect-of-ai-on-communication.html#the-prevalence-of-policy-violating-content-will-decline",
    "href": "posts/2023-06-06-effect-of-ai-on-communication.html#the-prevalence-of-policy-violating-content-will-decline",
    "title": "The Influence of AI on Content Moderation and Communication",
    "section": "The Prevalence of Policy-Violating Content Will Decline",
    "text": "The Prevalence of Policy-Violating Content Will Decline\n\nAll large internet platforms use automated systems to detect policy-violating content.\n\nAll major platforms ban or suppress various types of content, e.g. hate speech, incitement to violence, nudity, graphic content. It has not been practical to have a human review each message because the platforms have a high volume of messages being sent with low latency. However automated systems have always been used: early systems simply checked for the appearance of prohibited words or matched against media databases, later systems used classifiers trained on human labels. See a brief history of automated content moderation here.\n\n\nSimple classifiers have high offline accuracy. Simple classifiers which just look for the appearance of specific words are often useful, e.g. certain words and phrases are highly predictive of whether text would be labelled as “toxic” or “hate speech.” However this method has many false positives (Chen (2022)) and false negatives (Heiner (2022)).\nSimple classifiers are easily evaded. It is typically easy to alter a violating message such that humans still think it is violating but the classifier does not. As a consequence the accuracy of these classifiers looks much higher offline than online, as users take steps to evade them.\n\nGröndahl et al. (2018) note that hate speech detectors can easily be fooled if you “insert typos, change word boundaries or add innocuous words.”\nHan and Tsvetkov (2020) note that simple models are poor at detecting “veiled toxicity” which they define as including “codewords, novel forms of offense, and subtle and often unintentional manifestations of social bias such as microaggressions and condescension.”\nA. Lees et al. (2021) note that simple models are poor at detecting “covert toxicity” which includes “types of toxicity that may not be immediately obvious. Covertly toxic comments may use obfuscation, code words, suggestive emojis, dark humor, or sarcasm …[and] [m]icroaggressions.” These papers evaluate models trained to identify context-independent toxicity, i.e. where the ground truth is human rating of the text alone without additional information on context or audience.\n\nLLM-based classifiers are approaching human levels of performance. In August 2023 OpenAI described using GPT-4 as a content labeler (Weng, Goel, and Vallone (2023)) and said “[l]abeling quality by GPT-4 is similar to human moderators with light training … [h]owever, both are still overperformed by experienced, well-trained human moderators.”\nLLM-based classifiers handle adversarial cases well. Google’s 2022 generation of text moderation models, which use transformer-based LLMs, are able to correctly classify many types of adversarial messages which are designed to evade simpler classifiers. A. W. Lees et al. (2022) say their classifier performs well against “code-switching, covert toxicity, emoji-based hate, human-readable obfuscation, [and] distribution shift.” Google’s 2023 generation spam classifier uses an embedding that is “robust against typos and character-level adversarial attacks” (Bursztein et al. (2023)).1\n1 Arnaud Norman writes about how algorithms to scrape email addresses are often easy to evade, by adding special characters or other obfuscations, but that ChatGPT can straight-forwardly decode most such obfuscations.Better classifiers will lower prevalence even if they are available to adversaries. Suppose an adversarial content-producer had access to the same classifier that was used by the platform. The produced could keep testing different variants of a violating post until they found a variant that was truly violating, but not identified as violating by the classifier, i.e. a false negative. However as the platform’s model becomes more accurate there will be fewer possible false positives, and so the task becomes relatively more time-consuming for the adversary, and thus we should expect prevalence to decline.\nThe prevalence of policy-violating content has declined dramatically. Meta reports that the prevalence of nudity, bullying, hate speech, and graphic content each declined by a factor of between 2 and 5 between 2017 and 2022, and that the share of identified-violating content that was first identified by an ML model (“proactive rate”) is approaching 100% for most categories. I think much of this decline can be attributed to improvements in the quality of classifiers.2 Mark Zuckerberg has been making predictions for a long time that human raters could be substituted with AI. Although he was over-optimistic about the pace, I think he has been largely correct, e.g. in late 2018 he said “through the end of 2019, we expect to have trained our systems to proactively detect the vast majority of problematic content.”3\n2 It is important to remember that the “proactive rate” is the share of detected content that is detected by AI, the share of violating content that is detected by AI will certainly be significantly lower but is not generally reported. See Meta’s Community Standards report and my visualization of this data.3 Zuckerberg, “A Blueprint for Content Governance and Enforcement”\nEmployment of human moderators will likely decline. As computer accuracy improves fewer messages will need to be escalated for human review, additionally fewer humans will be needed to label training data.\nThis prediction also applies to government monitoring and censorship. Many governments use some kind of automated scanning tools to intercept or censor messages based on their content, e.g. the US’s NSA and Cybserspace Administration of China. Better AI will allow these agencies to classify every post with reliability as high as if they had a human read each one, thus we should expect obfuscation will become a much less-effective workaround for censorship.\nThis prediction would fail if there were hard limits on the performance of AI. It’s conceivable that there are ways of obfuscating content that will remain difficult for an AI to identify for a long time. However even if LLMs cannot identify violating content in real-time it seems likely they could catch up quickly. Suppose humans invent new types of obfuscation, e.g. misspelling words in a particular way. An LLM which is continually trained on human-labeled samples could likely learn the pattern and thus force humans to continually adopt new patterns.\nPrevalence will never decline to exactly zero because it’s inherently noisy. An AI model can never perfectly predict human-rater evaluation because humans are themselves noisy: there is both between-rater variation and within-rater variation in labelling for any given piece of content. Thus if the ground truth is human judgment then even an infallible classifier could not be used to drive prevalence all the way to zero.4\n4 Strictly speaking: this will be true if no content has a probability of being labelled as positive by a human of exactly zero."
  },
  {
    "objectID": "posts/2023-06-06-effect-of-ai-on-communication.html#the-prevalence-of-context-specific-violations-will-increase",
    "href": "posts/2023-06-06-effect-of-ai-on-communication.html#the-prevalence-of-context-specific-violations-will-increase",
    "title": "The Influence of AI on Content Moderation and Communication",
    "section": "The Prevalence of Context-Specific Violations Will Increase",
    "text": "The Prevalence of Context-Specific Violations Will Increase\nSome messages have a violating significance only to their intended audience. We can define a message as violating in one of two ways: (1) has a violating significance to the average person (average citizen or average user), or (2) has a violating significance to the intended audience of that message. I will define a “contextual violation” as a message that is violating to its intended audience but not to the average person. This is stronger than just having a double meaning where both meanings are clear to all audiences. I am specifically talking about messages which are interpreted in distinct ways by different audiences. Of course contextual violations are often unstable, over time the average person will often learn the contextual meaning.\nMany messages use contextual violations.5\n5 A related phenomena is people using selective truths to give an impression that is false. E.g. it is common for anti-vaccination groups to post mainly true claims, but only those claims which reflect badly on vaccines. People with a bias against some ethnic group likewise often refrain from posting provably false claims but post only those true claims that reflect badly on the disliked group. Because the pool of claims that are true is enormous it is easy to collect a large set of true claims that collectively give a false impression.\nSaying “globalist” when your audience understands it to mean “jewish”\nSaying the opposite of what is meant, e.g. a bigot saying excessively positive things about an ethnic group, or a pro-anorexia poster making anti-anorexic statements sarcastically.\nUsing euphemisms for illegal substances or illegal acts.\nUsing emojis of eggplants and peaches with sexual connotations.\nUsing photos without explicit nudity but which can be read as pornographic.\n\nImproved detection of violations is likely to cause substitution towards contextual violations. As AI improves the ability to detect violations it seems likely that there will be at least some substitution towards context-specific violations, however as long as there is some cost to using a contextual-violation then we would expect a less than one-for-one substitution.\nPlatforms could detect contextual violations if they wanted to. When doing human evaluation then platforms could either (1) provide human raters with detail about the message’s context and audience, or (2) assign human raters to messages based on their experience with that community.6 Likewise AI models could be trained to include rich representation of the context. An additional advantage of adding context is that it can identify and exempt posts that violate the letter but not the spirit of the policy.\n6 Platforms already have some policies that include context, e.g. Facebook’s “Bullying and Harassment policy” bans “repeatedly contacting someone in a manner that is unwanted or sexually harassing.”Platforms may not want to remove contextual violations. There are reasons why platforms may be reluctant to use context in determining violations: it is more complex, and can lead to awkward PR where the platform is shown to be censoring words and images have a harmless interpretation. Additionally platforms may care more about being seen to restrict harmful content than about the actual harm prevented.\nContextual violations have long existed in broadcast media. There have been many cases where contextual violations have been tolerated: e.g. newspapers would allow classified advertisments for prostitutes if described as masseuses, vibrators if described as massage wands, contraception if described as marital aids, and abortion if described as “removal of obstructions”. Thus it seems plausible that platforms will tolerate a substantial amount of contextually-violating content to remain.7\n7 In Facebook’s Marketplace it is prohibited to list guns for sale. As a consequence people began to list gun cases, with the understanding that a case was standing in for a gun. Facebook then updated their policy to prohibit selling gun cases. In turn people began to list gun stickers as stand-ins for guns. See WSJ reports from 2020 and 2021.8 https://www.axios.com/2022/11/28/china-protests-blank-paper-covidGovernment censorship is unlikely to be constrained by context-specific violations. Once a censor discovers that a term has an anti-government significance in a certain context then they are likely to start censoring that term. E.g. China has suppressed online mentions of Winnie the Pooh because it is associated with criticism of Xi Jinping, and in 2022 Hong Kong police arrested protestors for holding blank pieces of paper.8"
  },
  {
    "objectID": "posts/2023-06-06-effect-of-ai-on-communication.html#the-prevalence-of-variants-of-known-violating-content-will-decline",
    "href": "posts/2023-06-06-effect-of-ai-on-communication.html#the-prevalence-of-variants-of-known-violating-content-will-decline",
    "title": "The Influence of AI on Content Moderation and Communication",
    "section": "The Prevalence of Variants of Known-Violating Content Will Decline",
    "text": "The Prevalence of Variants of Known-Violating Content Will Decline\nPlatforms typically check content against databases of known-violating content. In addition to running classifiers on content platforms also check content against databases of known-violating content. The databases are often shared across platforms, known as “signal sharing”, e.g. databases of illegal sexual media (PhotoDNA), IP-protected content (Content ID), or terrorist recruitment content (GIFCT).9 As a consequence sophisticated uploaders often obfuscate their content, e.g. by adding noise, and platforms expand their matching algorithms using fuzzy matching.\n9 Other signal sharing programs: National Center for Missing & Exploited Children Child Sexual Abuse Material (NCMEC-CSAM), Non-consensual Intimiate Imagery (StopNCII), ThreatExchange.Improvements to AI will help platforms relatively more. Here the ground truth is whether a piece of content is a variation of another piece of content."
  },
  {
    "objectID": "posts/2023-06-06-effect-of-ai-on-communication.html#platforms-will-not-be-able-to-identify-bots-from-their-behavior",
    "href": "posts/2023-06-06-effect-of-ai-on-communication.html#platforms-will-not-be-able-to-identify-bots-from-their-behavior",
    "title": "The Influence of AI on Content Moderation and Communication",
    "section": "Platforms Will Not Be Able to Identify Bots from Their Behavior",
    "text": "Platforms Will Not Be Able to Identify Bots from Their Behavior\nMost online platforms struggle with automated users (bots) who are disruptive in a variety of ways. One way of protecting against bots is with behavioral tests, e.g. a CAPTCHA test asking users an image-recognition task10, or by using on-platform behavior to detect whether a user is human. However improvements in AI mean that computers have human-level performance on image-recognition tasks, and can learn to imitate human-style behavior patterns, thus it seems likely these behavioral tests will become ineffective against sophisticated actors. Searles et al. (2023) finds that most contemporary CAPTCHAs can be solved by computers with higher-than-human accuracy (p10).11\n10 CAPTCHA stands for Completely Automated Public Turing test to tell Computers and Humans Apart.11 Similarly behavioural fingerprinting will become ineffective against advanced actors, e.g. using voice recognition to verify identity.12 The 3rd-party identity providers will themselves have to rely on some other ground truth when accepting signups.This does not imply that the prevalence of bots will increase. All platforms need some defense against bots so they will have to rely relatively more on other forms of authentication, such as monetary payment, offline identity credentials (government ID, credit card number), hard-to-fake metadata (unique IP address, device ID), or 3rd-party identity provider (Sign in with Google, OpenID).12 Thus the barriers to signing up for a service, and especially posting on it, will become higher, but the effect on equilibrium prevalence of bots is ambiguous."
  },
  {
    "objectID": "posts/2023-06-06-effect-of-ai-on-communication.html#platforms-will-find-it-hard-to-discriminate-between-real-and-fake-media",
    "href": "posts/2023-06-06-effect-of-ai-on-communication.html#platforms-will-find-it-hard-to-discriminate-between-real-and-fake-media",
    "title": "The Influence of AI on Content Moderation and Communication",
    "section": "Platforms Will Find It Hard to Discriminate between Real and Fake Media",
    "text": "Platforms Will Find It Hard to Discriminate between Real and Fake Media\nIn some cases the ground truth depends on properties outside the content. I will refer to these properties as “external” in contrast to “internal” properties which depend only on the content such as whether a picture depicts nudity. I discuss the distinction at greater length below. Some examples of external properties:\n\nWhether a piece of media was generated in the traditional way (photographing a scene, recording a sound), or has been manipulated or synthesized.\nWhether text was written by a human.\nWhether text was written by a specific person, e.g. by Shakespeare.\n\nAdvances in AI will help with both forgery-detection and forgery-creation. It is clear that a better statistical model of genuine artefacts will help detect forgeries but it will also help create convincing forgeries.\nDetermined forgers will be able to fool humans. It seems likely that the latter effect will dominate: it will gradually become possible to camouflage computer-generated content such that neither a computer nor a human could tell them apart. If the content-producer has access to the platforms’ model then they can keep perturbing their fake media until it is labelled as non-fake.\nWe cannot reliably discriminate between real and AI-generated media. As of late 2023, programs to detect synthetically generated media have relatively poor accuracy: OpenAI announced a model to detect LLM-created text in January 2023 but then shut it down in July because of poor performance. In June 2023 the NY Times compared a variety of tools to detect computer-generated images and found that with minimal effort they could all be reliably fooled.\nThe prevalence of synthetic media will increase on unmoderated platforms. The major platforms have incentives to limit the prevalence of fake media,13 and can control the prevalence even without reliable classifiers. E.g. Meta and YouTube dramatically decreased the prevalence of misinformation over 2016-2020 not primarily through real-time detection of whether a given claim is false, but by (1) adjusting ranking to penalize publishers who tend to circulate false claims; (2) punishing publishers who circulate proven-false claims. Thus I do not expect overall prevalence of fake factual media to substantially increase on the major platforms.\n13 The goals of platforms in content moderation are discussed in my note on ranking, Cunningham (2023)."
  },
  {
    "objectID": "posts/2023-06-06-effect-of-ai-on-communication.html#fake-media-deepfakes-will-not-have-a-substantial-influence-on-politics",
    "href": "posts/2023-06-06-effect-of-ai-on-communication.html#fake-media-deepfakes-will-not-have-a-substantial-influence-on-politics",
    "title": "The Influence of AI on Content Moderation and Communication",
    "section": "Fake Media (Deepfakes) Will Not Have a Substantial Influence on Politics",
    "text": "Fake Media (Deepfakes) Will Not Have a Substantial Influence on Politics\nAs synthetic media becomes common people will rely more on provenance. As it becomes cheaper to manipulate and synthesize media then people are likely to become more skeptical and rely relatively more on the provenance of information. Thus although synthetic media will likely circulate I do not think it will have a substantial influence on beliefs in equilibrium.\nIt has always been easy to create misleading documents.It is not difficult to forge or alter documents, or edit video in a misleading way. As a consequence mainstream media organizations typically do not publish leaked materials unless they have either a chain or provenance for the leaks or independent confirmation of their content.\nInfluential forgeries of documents have been historically rare. In an Appendix below I compile a simple dataset of politically influential document leaks in the US over the past 25 years and estimate around 10% of them were based on forged materials.1415\n14 I know of two forged documents that were widely taken as true in the last 25 years, from around 15 substantial leaks that I could find: (1) the “yellowcake” letters from Iraq to Niger, cited in the 2002 US case for war against Iraq; (2) a fake G W Bush military transcript reported on by CBS and Dan Rather in 2004. It’s notable both that these cases are somewhat rare, and that each was passed through a chain of apparently reputable parties.15 This argument implies that, prior to AI, anonymously leaked video would be more likely to be published and circulated than anonymously leaked documents, because video is harder to fake. In fact I cannot think of many cases of influential anonymous leaks of videos. When Trump’s “Access Hollywood” tape was leaked to the Washington Post they got confirmation before publishing it. In fact maybe leaked video has always been untrustworthy because it has always been easy to make deceptive edits.16 Snopes.com has an enormous database of both false claims and misleadingly manipulated media that has circulated since 1994. A typical recent example is an edit of a Bill Gates interview to make it appear he wants to use vaccination to reduce population growth.The quantity of false claims circulating on the internet is not primarily constrained by the quality of their content. A great deal of false claims already circulate on the internet, especially in loosely moderated parts: e.g. by email, on Telegram, 4chan, Truth Social, WhatsApp, Twitter. It’s not clear that the quality of the faked media is an important constraint on the volume that circulates. It’s not uncommon to find a clip of an interview with a politician edited to make it appear that they are admitting to a crime or secret agenda.16 If people already take what they see at face value then adding deepfakes seems unlikely to change their opinions substantially. Alternatively if people are skeptical and look for corroborating sources then, again, deepfakes would be unpersuasive. It seems that deepfakes would only be influential if there are a significant population who are exposed to many lies but are not haded because the documentary evidence is not sufficiently strong."
  },
  {
    "objectID": "posts/2023-06-06-effect-of-ai-on-communication.html#communication-will-migrate-towards-large-closed-platforms",
    "href": "posts/2023-06-06-effect-of-ai-on-communication.html#communication-will-migrate-towards-large-closed-platforms",
    "title": "The Influence of AI on Content Moderation and Communication",
    "section": "Communication Will Migrate Towards Large Closed Platforms",
    "text": "Communication Will Migrate Towards Large Closed Platforms\nSmall platforms will be overrun with AI-created content. In particular, AI-created bots, spam, obfuscated violating content, and fake media. This would imply that consumers will tend to migrate to larger closed platforms with more effective defences, and which have more restriction on participation. This continues a general movement over the last 20 years of communication moving from small open platforms (independent email, small forums, mailing lists, independent websites) to large closed platforms (large email providers, large social media platforms).\nPeople will rely more on established sources of truth. E.g. they will rely relatively more on Wikipedia, Community Notes, and mainstream recognized media sources. The ordinary content-based signs of trustworthiness will become less reliable: having a professional website, well-edited text, well-argued reasoning, and documentary evidence.\nPeople will rely more on cryptographic signing to verify authenticity. I am not sure how strong this effect will be: it is generally more efficient for an intermediary to verify authenticity of senders than for users to do it themselves. I think we’ve seen that in other domains: (1) PGP signing of email has been less important than email providers filtering spam and phishing; (2) SSL certificates in browsers have been less important than browsers giving warnings for suspected phishing sites (e.g. Google’s safe browsing database of sites with phishing or malware is used to give warnings in Chrome and Safari).\n\n\n\nPedigree will become more important in publication. As an editor accepting submissions (e.g. an academic journal, a literary magazine, a newspaper letters page) the quality of the work submitted is typically correlated with more superficial features such as the grammaticallity and the length. As it becomes easy to synthesize text then those superficial features will become less informative about quality and editors are likely to rely relatively more on hard-to-fake signals like the pedigree of authors: what have they published before, and which college the author went to."
  },
  {
    "objectID": "posts/2023-06-06-effect-of-ai-on-communication.html#entertainment-will-become-largely-synthetic",
    "href": "posts/2023-06-06-effect-of-ai-on-communication.html#entertainment-will-become-largely-synthetic",
    "title": "The Influence of AI on Content Moderation and Communication",
    "section": "Entertainment will Become Largely Synthetic",
    "text": "Entertainment will Become Largely Synthetic\nA classifier that can detect whether a photo is pretty can also generate a synthetic photo that is pretty, and a classifier that can detect whether a joke is funny should also be able to generate funny jokes.17 On average people spend around 3 hours per day watching entertainment (TV, YouTube, TikTok, Instagram). It seems likely that trained models will be able to synthesize content that is highly engaging though it’s hard to anticipate what it will look like.\n17 I think language models haven’t yet been very good at jokes because they generate one word at a time (autoregressive), while jokes typically have a logical structure such that the setup is probable given the punchline, but not the other way around. When we get language models which generate text using different statistical algorithms (e.g. diffusion instead of autoregressive generation) then it seems likely they’ll be able to create good jokes."
  },
  {
    "objectID": "posts/2023-06-06-effect-of-ai-on-communication.html#things-will-get-weird",
    "href": "posts/2023-06-06-effect-of-ai-on-communication.html#things-will-get-weird",
    "title": "The Influence of AI on Content Moderation and Communication",
    "section": "Things Will Get Weird",
    "text": "Things Will Get Weird\nMuch of our common-sense understanding of media will be violated when we routinely use AI models to manipulate and synthesize artefacts. Some examples:\n\nPeople will synthesize completely new violating images/videos. Thiel, Stroebel, and Portnoff (2023) say that, as of early 2023, less than 1% of child sexual abuse media (CSAM) appears to be synthetically generated. However the ability to synthesize has been advancing rapidly, “to the point that some images are only distinguishable from reality if the viewer is very familiar with photography, lighting and the characteristics of diffusion model outputs … it is likely that in under a year it will become significantly easier to generate adult images that are indistinguishable from actual images.”\nProducers will synthesize content to sit on the edge of a category. If platforms take action whenever content passes some threshold then adversarial actors will generate or perturb content such that it sits right below the threshold. If a platform removes a photo whenever more than 50% of raters would say it depicts nudity then producers would upload photos which 49% of raters would say depicts nudity. People would upload movies which almost look like an existing IP-protected movie, and students might submit essays that are close to existing sources but don’t quite trigger the plagiarism detector."
  },
  {
    "objectID": "posts/2017-04-15-the-mechanical-and-the-rational.html",
    "href": "posts/2017-04-15-the-mechanical-and-the-rational.html",
    "title": "The Repeated Failure of Laws of Behaviour",
    "section": "",
    "text": "krazy kat\n\n\n\nNutshell\n\nIn retrospect a lot of behaviour that was studied in the lab, which we thought was telling us about the wiring of animals, actually was telling us about the world outside the animal..\nIt has turned out, over and over again, that an animal’s response to a stimulus reflects the animal’s beliefs about what that stimulus represents in the world. So the “laws” of behaviour that we discovered are actually just describing, at a remove, regularities in the world.\n\n\n\n\n\n\n\n“law” of behaviour\ntruth about the environment\n\n\n\n\nAnimals will tend to repeat whichever action is rewarded\nActions which have been rewarded in the past tend to be rewarded in the future\n\n\nObjects appear darker to people when neighboring objects are brighter\nObjects are darker when neighboring objects are brighter\n\n\nBlue objects appear more distant to people\nBlue objects are more distant\n\n\nPeople brake when the rate of change of the angle of an approaching object (\\(\\frac{\\dot{\\theta}}{\\theta}\\)) exceeds some threshold\n\\[\\frac{\\dot{\\theta}}{\\theta}\\] determines the time to impact of an approaching object in typical circumstances, and so the optimal time to brake\n\n\nConsumers’ expenditure increases less than proportionally with changes in income\nChanges in income are typically temporary, and so imply a less-than-proportional response to maintain stable long-run expenditure\n\n\n\nHere’s the point expressed formally. Some scientist observes response \\(r\\) and stimulus \\(s\\), and proposes a law of behaviour, a simple function from \\(s\\) to \\(r\\). But for any such law, \\(r(s)\\) there exists at least one rationalization, under which the organism has beliefs about an unobserved variable \\(x\\), and they choose \\(r\\) optimally given what they infer about \\(x\\) from observing \\(s\\), i.e.,\n\\[r(s) = \\arg \\max_{r} \\int u(s,r,x) f(x|s)\\]\nGiven some pattern of behaviour \\(r(s)\\), we can back out the beliefs that would justify that behaviour, \\(f(x\\|s)\\), and we’ve seen – many times repeated – that those beliefs turn out to be accurate – as in the cases above, even when the scientist wasn’t aware of that truth.\nYou could reply that, sure, it’s optimal in the typical situation, but animals keep applying the same behaviours in cases where it’s not optimal, and that’s why they are laws of behaviour. There are some cases like that, but it seems to me that there are many more cases which go in the other direction: when the situation is changed, the behaviour changes, and it turns out the animal does what’s optimal, not what the law implies.\nThe biggest example of the failure of behavioural laws is the theory of conditioning and associative learning. Psychologists started with proposing a simple function that governs behaviour – the more often you are rewarded for doing something, the more often you do it – but then were forced to add a long list of qualifications and special cases (context-dependence, blocking, intermittent reinforcement, extinction, matching). It gradually became clear that the complications were not arbitrary, but made sense from the animal’s point of view: they are sensible strategies based on what an animal should expect in a typical environment. So the complex rules that we had been mapping out were not telling us about the animal’s wiring, they were instead telling us about the world that the animal lives in. Animals tend to repeat actions that are rewarded (i.e. obey the laws of reinforcement learning) only when they have reason to believe that rewards will be positively correlated across time. When they are in a situation where they don’t expect that correlation, then they no longer obey the rules of reinforcement learning. Mitchell et al. (2009) cite a lot of evidence about human associative learning and say:\n\n“we reconsider (and reject) one of the oldest and most deeply entrenched dual-system theories in the behavioral sciences, namely the traditional view of associative learning as an unconscious, automatic process that is divorced from higher-order cognition.”\n\nTheir rejection is based on evidence that, when people learn associations, they only follow them insofar as those associations are good guides to achieving their goals.\nThere’s a very similar case in perception, where psychologists have been trying to learn the function from sensation to perception. A famous observation was of lateral inhibition: a stimulus seems less bright when the neighbouring stimulus gets brighter. In the 1950s this was thought to be due to wiring of neurons in the eye, but gradually it became clear that the effect only occurs in certain cases, and in other cases the opposite effect is observed. And then people realized that the cases in which it works are exactly the cases where it would be a reasonable inference in a typical environment. Adelson (1993, Science):\n\n“All of the phenomena discussed above lead to the same conclusion: Brightness judgments cannot be simply explained with low-level mechanisms. Geometrical changes that should be inconsequential for low-level mechanisms can cause dramatic changes in the brightness report. It is as if the visual system automatically estimates the reflectances of surfaces in the world[.]”\n\nA similar thing has happened in the study of control laws, or invariants, simple principles which map stimulus to response. For example Lee’s (1976) tau-dot model of braking: you brake when \\(\\dot{\\tau}\\) is above some threshold, where \\(\\tau=\\frac{\\theta}{\\dot{\\theta}}\\), and \\(\\theta\\) is the angle of an approaching object. Many people spent a lot of time proposing control laws for different domains, and testing control laws against each other, but the field (I believe) has now mostly given up the hope of finding simple laws to model behaviour. Weber and Fajen (2014) say:\n\n“numerous studies have demonstrated that observers often rely on non-invariants and that the particular optical variables upon which they rely to guide action can change—as a consequence of practice, as a function of the range of conditions that are encountered, and as a function of the dynamics of the controlled system.”\n\nSlightly more of a stretch - there are some similar episodes in the study of economic decision-making. In the 1950s economists had established various laws about how expenditure related to a person’s income. Milton Friedman, in 1957, showed that many properties and puzzles of the expenditure function could be understood as byproducts of a person rationally planning to spread their income over time. Likewise Lucas (1976) argued that, in three different cases, where a statistical regularity in decisions was observed, you could explain why they occur if you model people as making tradeoffs given sensible beliefs about their economic prospects.\nI’m not trying to argue that all behaviour is rational and that the brain optimally combines all available information. But looking at the track record of psychologists they have systematically underestimated the brain – they keep proposing simple behavioural rules – response as a function of stimulus – which later turn out to be only true insofar as they reflect some deeper organizing principle.\nI think the same is true for a lot of behavioural biases. Economists sometimes treat “loss aversion”, “probability weighting”, etc., as if they are hard-wired, and scratch their heads when an experiment finds behaviour going in the opposite direction. But almost certainly these regularities are just local manifestations of some deeper - as yet unknown - principles. (An earlier post makes this point about “relative thinking” effects).\nThe rest of this note gives some more detail about how this pattern played out in the history of reinforcement learning.\n\n\n\n\nKrazy Kat\n\n\n\n\nReinforcement Learning\nFor a long time I’d been confused about the status of reinforcement learning theory – the theory was massively popular in the first half of the 20th century, but these days psychologists seem to treat it as defunct, an ex-theory. And yet it’s still being used: people who train animals still talk a lot about conditioning, neuroscientists are crazy about reinforcement learning, as are people who work in artificial intelligence.\nFrom having read a lot of this stuff my basic understanding is now this: The generalizations about how animals and humans learn are valid only in specific contexts, they are not deep and fundamental laws of behaviour, as had been once believed. However the theory remains a very useful simple model of decision-making because it’s a good model of the environment in which decision-makers typically work. I.e., the theory doesn’t tell us much about how the brain works, it tells us mostly about the environment which the brain was designed for.\n\n\nClassic Examples of Reinforcement Learning\nIn 1905 Thorndike proposed a basic principle of animal learning: “Of several responses … those which are accompanied or closely followed by satisfaction … will be more likely to recur.” The same basic idea was later described as “instrumental conditioning,” “operant conditioning”, or “reinforcement learning.” The term ‘behaviorism’ referred to a school of psychology, popular in the middle of the 20th century, based on this principle or variants.\nRats and pigeons in boxes will pretty quickly learn to tap a button, when tapping the button is followed by receiving a food pellet. They can also learn much more complex functions through reinforcement, like learning to tap on the left button when they see a square, and on the right button when they see a circle. (B F Skinner worked on using pigeons to guide missiles, by training them to tap on pictures of military targets.)\nIn the 1950s there were some influential studies in which human behaviour was manipulated through conditioning. In Greenspoon (1955) a subject would be asked to say aloud a sequence of words, whichever came to mind, and the experimenter would give subtle positive cues when he heard certain types of word - for example plural nouns. Gradually subjects would start saying plural nouns more often, without being aware of this. The implication drawn was that many of our everyday decisions, which we think of as conscious and deliberate, are actually just imprints of previous patterns of reinforcement.\nI believe that lots of animal training still uses principles of conditioning: you give the animal a small reward whenever it does something you want it to, and you gradually build up more complicated behaviours. They also use other concepts from conditioning like secondary reinforcers and intermittent reinforcement.\nIn 1992 IBM built a backgammon-playing neural net that used a kind of reinforcement learning – in short the computer would be more likely to make the same move again if it had a good outcome in previous cases. The ultimate reinforcement was from winning the game, and expectations propagated back from that, to learn the value of positions in the middle of the game. The program was a great success – it was trained against itself, and quickly became good enough to beat most human players.\nIn the 1990s there was a lot of excitement when some neuroscientists discovered that levels of dopamine in the mid-brain responded to rewards in a way consistent with a reinforcement-learning model. In particular, dopamine didn’t correlate with the level of reward, but correlated with the level of unexpected reward: i.e., if you receive a reward in a situation where you wouldn’t normally. This is the kind of calculation which an algorithm would do if it was implementing reinforcement learning: it would update weights when a rewards is different from the expected level of reward.\n\n\nAdditional Laws\nThere are some interesting additional laws that were discovered about conditioning.\nblocking. The order in which associations are learned is important. Suppose a pigeon learns to press a lever whenever she hears a beep. Subsequently, the beep is always accompanied by a flash. When the flash appears by itself, the pigeon won’t have learned to peck. But if the beep and flash were paired right from the beginning, then both the beep or flash would, by themselves, be sufficient for the pigeon to peck. So learning one association can “block” another association from being learned. (“Reverse blocking” is sometimes, but not always, also observed: the pigeon learns to associate A and B with a reward, but then when she finds that B predicts the reward by itself, she subsequently ignores A.).\nintermittent reinforcement. Intermittent reinforcement is often found to create more robust associations than unvarying reinforcement. If you give a pigeon a pellet every time she pecks the button, then when you stop giving her pellets she’ll stop pecking the button. If you only give her a pellet occasionally, then the behaviour will take much longer to die out.\nmatching. If you give a pigeon two different levers, each of which will release a pellet with a fixed probability, then the bird will learn to peck preferentially on the lever with the higher probability. However it will still occasionally peck on the lever with the lower probability, roughly in proportion to the ratio of probabilities. This seems to be a violation of rationality because if the pigeon had learned the probabilities, and was maximizing expected value, then it should peck constantly at the high-value lever.\n(also: secondary reinforcement; overtraining & extinction; superstition.)\nAt the height of the enthusiasm for conditioning many people thought these laws gave insight into all aspects of human behaviour - mental illness, adolescent delinquency, sexual behaviour, language.\n\n\nDifficult Cases\nIn the first few decades of reinforcement learning many confirmations of the basic theory were published but, as often happens, the published evidence became less coherent as time went on. Many of the laws of reinforcement learning turn out to apply only in a subset of situations, or the parameters varied widely, and in other situations the effects seem to reverse.\nreverse reinforcement. An old finding, regarding rats running mazes, is that when the rat finds a piece of cheese down one passage, then they were less likely to go down that passage the next time they were in the maze. According to reinforcement learning they should be more likely to go down that passage.\ncontext specificity. The speed of learning associations between stimuli and responses is very different depending on the stimulus and the response. Some associations can be learned firmly with just a single experience, for example a rat refusing to eat red pellets after getting nauseous after eating a red pellet. Others associations take far longer, e.g. a rat learning to associate a sound with getting nauseous.\nawareness. In humans, despite many attempts, very few cases have been found in which reinforcement can affect behaviour without people being consciously aware of it. Some of the classic findings have been reconsidered: in the study which manipulated peoples’ choice of words, it was found that the effect only occurred among subjects who were consciously aware of the association. Additionally, many learned responses can be turned on or off by simply telling the person. Colgan (1970) told subjects, after they learned an association, that the association is no longer valid (“from now on the bell will not signal an electric shock”) and he found that, although this didn’t entirely extinguish the flinching, it was much less pronounced after the instruction.\n\n\nPutting it Back Together\nAs I said, the laws of reinforcement learning turn out to apply only in a subset of situations.\nOne interpretation is that there do exist laws of learning, but that they are more complex. However the agreements and deviations from reinforcement-learning are not at random: in many cases they can be understood: the theory works in just those contexts where past associations tend to be a good guide to future associations; and it fails in contexts where that’s not true. E.g., a rat knows that the taste of food is a good predictor of whether it’ll make you sick, but doesn’t have reason to believe that the sound you hear when you eat food is a very good predictor.\nConsider the rat who is less likely to run down a passage when they were previously rewarded for running down that passage. This makes sense if the rat remembers that he just ate the food down that passage, and so wants to look elsewhere. In this situation the rat expects the future payoff of an action to be negatively correlated with the past payoff, rather than positively correlated, and so we get the opposite effect than that predictedb by reinforcement learning.\nThe basic law of reinforcement learning can be recast in terms of beliefs: if you expect the future payoff of an action to be positively correlated with its past payoff, then it is rational to perform whichever act was rewarded in a similar situation in the past.\nThe other laws of reinforcement can also be recast in terms of beliefs. And the situations in which those laws are violated are often exactly the situations where such beliefs would not apply.\nintermittent reinforcement. Suppose an act was only occasionally reinforced. This means that on previous occasions when rewards stopped they resumed again later. So it’s not surprising that, having experienced rewards stop and resume once before, when they stop again you expect them to resume again.\nmatching. There’s an obvious argument that probability matching is rational – in your usual environment the probability of reward changes over time, so it makes sense to continue monitoring each action, to see if the payoff has changed (see Estes, 1976).\nblocking. Blocking can be explained by a learning model, given a prior that either A or B is predictive of the reward, but not both. (See “Explaining away in Weight Space” by Dayan and Kakade, and good summary at http://www.cs.cmu.edu/~ggordon/conditioning-slides.pdf ).\nSometimes we observe that forward blocking occurs but not backward blocking. This can be explained by a mechnical prediction model (a Kalman filter), where you update weights only when unexpected things happen. So, you learn that A & B are both associated with reward, then you are exposed to cases with just A and reward. Can this model be rationalized? It’s not clear to me. (AKA, what priors would justify a Kalman filter, given that it depends on the order of presentation?)\nMitchell (2009) also notes that there is less blocking when you introduce cognitive load, implying that it’s not an automatic or mechanical effect (subjects “showed blocking of skin conductance CRs only when blocking was a valid inference.”)\ntransfer learning. There are some examples where organisms can transfer patterns they have learned across quite different stimuli, e.g. learning patterns of complementarity/substitutability (Mitchell (2009) p190). This would require an elaborate reinforcement learning model to rationalize, but is simple with a rational model.\ncontext specificity. De Houwer, Vandorpe and Beckers (2005) say (in “Why have associative models fared so well?”)1\n1 Also Seligman (1970) On the Generality of Laws of Learning, “That all events are equally associable and obey common laws is a central assumption of general process learning theory … A review of data from the traditional learning paradigms shows that the assumption of equivalent associability is false … it is speculated that the laws of learning themselves may vary with the preparedness of the organism for the associa- tion and that different physiological and cognitive mechanisms may covary with the dimension.”\nThe two types of models can be differentiated … by manipulating variables that influence the likelihood that people will reason in a certain manner but that should have no impact on the operation of the associative model. We have seen that such variables (e.g., instructions, secondary tasks, ceiling effects, nature of the cues and outcomes) do indeed have a huge effect. Given these results, it is justified to entertain the belief that participants are using controlled processes such as reasoning and to look for new ways to model and understand these processes.\n\n\n\nConclusion\nI guess there are two natural followup questions to this argument:\n(1) If you looked in the history of psychology, could you find as many examples of making the opposite mistake?\nIn other words, how often have we have over-estimated the rationality of human behaviour. Yeah, maybe you’re right. It’s not hard to find economists who will insist that, whatever people do, it’s in their best interest. I just think that psychologists tend to make the other mistake.\n(2) If you’re so down on it, then why is reinforcement learning useful to dog trainers and to computer programmers?\n\n\nMisc Notes\n\neconomic applications of association-based decision-making. Gilboa & Schmeidler: case-based decision-making; Camerer: experience-weighted attraction learning. The NYU guy has a paper. Erev & Roth (1998) say that reinforcement learning does a good job predicting behaviour in some games, better than equilibrium play. I wouldn’t defend completely rational behaviour, but on the other hand I wouldn’t expect RL behaviour to be stable: probably behaviour approximates RL in some contexts, and does the opposite in others. It’s not obvious that the RL model is a very good level of abstraction to describe behaviour at. Charness & Levin (2005) run an experiment where reinforcement & Bayesian updating give different predictions: you choose between urns, one more sensitive to state, one less sensitive. If you draw from the less-sensitive urn, and you receive a positive outcome, then you update about the state, and the Bayesian prediction is that you should switch urns, while reinforcement learning says you’ll stay with the same urn. They find that people largely stay with the same urn.\n\nthe gambler’s fallacy goes in the opposite direction to reinforcement learning. The gambler’s fallacy: winning a gamble at time \\(t\\) makes you decrease the expectation of winning at \\(t+1\\), i.e. the opposite prediction of a simple reinforcement model. However there’s a heuristic rationalization similar to the rationalization of rats in a maze: caveman Ug is shaking trees to get coconuts out. If there’s no coconut at time t, then there’s an increased probability of a coconut at t+1.\nPoggio and visual perception. In 1983 Poggio found that he could reinterpret prior findings in perception as implementation of Bayesian inference:\n\n“All problems in vision and more general perception were inverse problems, going back from the image to 3-D properties of objects and scenes. They were also, as typical for inverse problems, ill-posed. We used regularization techniques to “solve” specific vision problems such as edge detection and motion computation. In the process, we found that some of the existing algorithms for shape-from-shading, optical flow, and surface interpolation were a form of regularization. Our main contribution was to recognize illposedness as the main characteristic of vision problems and regularization as the set of techniques to be used for solving them.”\n\nShephard’s theory of generalization. Shepard (1987) and Tenenbaum and Griffiths (2001) give a persuasive argument that apparent laws governing generalization between stimuli are context-dependent, in a way that is consistent with Bayesian inference."
  },
  {
    "objectID": "posts/2017-02-25-weber-fechner-law.html",
    "href": "posts/2017-02-25-weber-fechner-law.html",
    "title": "Weber’s Law Doesn’t Imply Concave Representations or Concave Judgments",
    "section": "",
    "text": "runningman\nNutshell."
  },
  {
    "objectID": "posts/2017-02-25-weber-fechner-law.html#linear-representation-multiplicative-noise",
    "href": "posts/2017-02-25-weber-fechner-law.html#linear-representation-multiplicative-noise",
    "title": "Weber’s Law Doesn’t Imply Concave Representations or Concave Judgments",
    "section": "Linear Representation & Multiplicative Noise",
    "text": "Linear Representation & Multiplicative Noise\nAssume people get signals about underlying value with multiplicative noise, \\(s=v\\cdot e\\), with \\(e\\) lognormal. For conciseness let \\(\\delta=JND(v_{1},p)\\), then \\(\\delta\\) can be implicitly defined as:\n\\[\n\\begin{aligned}\np   =&  P(E[v_{1}+\\delta|s_{2}]&gt;E[v_{1}|s_{1}]) \\\\\n    =&  P((v_{1}+\\delta)e_{2}&gt;v_{1}e_{1}) \\\\\n    =&  P(\\ln(v_{1}+\\delta)+\\ln e_{2}&gt;\\ln v_{1}+\\ln e_{1}) \\\\\n    =&  \\Phi\\left(\\frac{\\ln(v_{1}+\\delta)-\\ln v_{1}}{\\sigma_{e}^{2}+\\sigma_{e}^{2}}\\right)\n\\end{aligned}\n\\]\nwhere \\(\\Phi\\) is the CDF of a standard normal distribution. Then,\n\\[\n\\begin{aligned}\n\\ln(v_{1}+\\delta)-\\ln v_{1} =&   2\\sigma_{e}^{2}\\Phi^{-1}(p) \\\\\n\\frac{v_{1}+\\delta}{v_{1}}  =&   \\exp(2\\sigma_{e}^{2}\\Phi^{-1}(p))\\\\\nJND(v_{1},p)=\\delta         =&   v_{1}\\left[\\exp(2\\sigma_{e}^{2}\\Phi^{-1}(p))-1\\right]\n\\end{aligned}\n\\]\nIn other words, the just noticeable difference is proportional to the value, \\(v_{1}\\), as found by Weber."
  },
  {
    "objectID": "posts/2017-02-25-weber-fechner-law.html#a-concave-representation-additive-noise",
    "href": "posts/2017-02-25-weber-fechner-law.html#a-concave-representation-additive-noise",
    "title": "Weber’s Law Doesn’t Imply Concave Representations or Concave Judgments",
    "section": "A Concave Representation & Additive Noise",
    "text": "A Concave Representation & Additive Noise\nSuppose that the decision-maker receives a concave signal of value with additive noise, i.e. \\(s=\\ln v+e\\), with Gaussian \\(e\\). Then the derivation is very similar:\n\\[\n\\begin{aligned}\n  p =& P(E[v_{1}|s_{2}]&gt;E[v_{1}|s_{1}]) \\\\\n    =& P(\\ln(v_{1}+\\delta)+e_{2}&gt;\\ln v_{1}+e_{1}).\n\\end{aligned}\n\\]\nThe rest of the derivation is the same: i.e., the JND in the neighborhood of \\(v_{1}\\) will be proportional to \\(v_{1}\\)."
  },
  {
    "objectID": "posts/2017-02-25-weber-fechner-law.html#multiplicative-noise-posteriors-are-concave-in-v",
    "href": "posts/2017-02-25-weber-fechner-law.html#multiplicative-noise-posteriors-are-concave-in-v",
    "title": "Weber’s Law Doesn’t Imply Concave Representations or Concave Judgments",
    "section": "Multiplicative Noise => Posteriors are Concave in \\(v\\)",
    "text": "Multiplicative Noise =&gt; Posteriors are Concave in \\(v\\)\nSuppose we have lognormal priors for both \\(v\\) and \\(e\\):\n\\[\n\\begin{eqnarray*}\n\\ln v & \\sim & N(\\mu_{v},\\sigma_{v}^{2})\\\\\n\\ln e & \\sim & N(\\mu_{e},\\sigma_{e}^{2}),\n\\end{eqnarray*}\n\\]\nand \\(s=v\\cdot e\\), then we will have posteriors like:\n\\[\n\\begin{eqnarray*}\nf(\\ln v|s) & \\sim & N(\\frac{\\sigma_{v}^{2}}{\\sigma_{e}^{2}+\\sigma_{v}^{2}}\\ln s,\\left(\\sigma_{v}^{-2}+\\sigma_{e}^{-2}\\right)^{-1})\\\\\nE[v|s] & = & \\exp\\left(\\frac{\\sigma_{v}^{2}}{\\sigma_{e}^{2}+\\sigma_{v}^{2}}\\ln s+\\frac{1}{2}\\left(\\sigma_{v}^{-2}+\\sigma_{e}^{-2}\\right)^{-1}\\right)\\\\\n& = & s^{\\frac{\\sigma_{v}^{2}}{\\sigma_{e}^{2}+\\sigma_{v}^{2}}}e^{\\frac{1}{2}(\\sigma_{v}^{-2}+\\sigma_{e}^{-2})^{-1}}\n\\end{eqnarray*}\n\\]\nThis means that the expected \\(v\\) is concave in the signal \\(s\\) (because the exponent is less than one). Intuitively: a doubling of the value, which causes a doubling of the stimulus, will cause a less than doubling of the expected value conditional on that stimulus, because it will cause us to revise upwards our beliefs about both \\(v\\) and \\(e\\).\nFinally, we are also interested in the average posterior for a given \\(v\\). This will also be concave (abbreviating \\(\\alpha=\\frac{\\sigma_{v}^{2}}{\\sigma_{e}^{2}+\\sigma_{v}^{2}}\\), and dropping the constant coefficient in \\(E[v|s]\\)):\n\\[\n\\begin{eqnarray*}\nE[E[v|s]|v] & = & \\int(v\\cdot e)^{\\alpha}f(e)de\\\\\n& = & v^{\\alpha}\\int e^{\\alpha}f(e)de.\n\\end{eqnarray*}\n\\]"
  },
  {
    "objectID": "posts/2017-02-25-weber-fechner-law.html#additive-noise-posteriors-are-linear-in-v",
    "href": "posts/2017-02-25-weber-fechner-law.html#additive-noise-posteriors-are-linear-in-v",
    "title": "Weber’s Law Doesn’t Imply Concave Representations or Concave Judgments",
    "section": "Additive Noise => Posteriors are Linear in \\(v\\)",
    "text": "Additive Noise =&gt; Posteriors are Linear in \\(v\\)\nSuppose again that the decision-maker receives a logarithmic signal with additive noise: \\(s=\\ln v+u\\), and let \\(u\\) be Gaussian. (I changed notation from \\(e\\) to \\(u\\) because I use a lot of exponential functions in the derivation.) Now assume that, in addition, \\(v\\) is drawn from an improper uniform \\((0,\\infty)\\). Consider the expected value of \\(v\\) given the signal \\(s\\) (I drop the constant term from the Gaussian distribution for conciseness):\n\\[\n\\begin{eqnarray*}\nE[v|s] & = & \\frac{\\int_{0}^{\\infty}ve^{-\\left(s-\\ln v\\right)^{2}}dv}{\\int_{0}^{\\infty}e^{-\\left(s-\\ln v\\right)^{2}}dv}.\n\\end{eqnarray*}\n\\]\nNow exchange variables, so that \\(v=e^{z}\\):\n\\[\n\\begin{eqnarray*}\nE[v|s] & = & \\frac{\\int_{-\\infty}^{\\infty}e^{z}e^{-(s-z)^{2}}e^{z}dz}{\\int_{-\\infty}^{\\infty}e^{-(s-z)^{2}}e^{z}dz}\\\\\n& = & \\frac{\\int_{-\\infty}^{\\infty}e^{-s^{2}+2(1+s)z-z^{2}}dz}{\\int_{-\\infty}^{\\infty}e^{z-(s-z)^{2}}dz}\\\\\n& = & \\frac{\\int_{-\\infty}^{\\infty}e^{-(z-1-s)^{2}}e^{1+2s}dz}{\\int_{-\\infty}^{\\infty}e^{s+\\frac{1}{4}}e^{-((s+\\frac{1}{2})-z)^{2}}dz}\\\\\n& = & e^{s}e^{3/4}\\frac{\\int_{-\\infty}^{\\infty}e^{-(z-1-s)^{2}}dz}{\\int_{-\\infty}^{\\infty}e^{-((s+\\frac{1}{2})-z)^{2}}dz}\n\\end{eqnarray*}\n\\]\nNote that both of the integrals are independent of \\(s\\) (because the integration is between \\(-\\infty\\) and \\(\\infty\\)), so there exists some \\(\\kappa\\) such that:\n\\[\nE[x|s]=\\text{e}^{s}\\kappa.\n\\]\nFinally we are interested in the average posterior for a given \\(v\\) (here I’m again ignoring all constant terms):\n\\[\n\\begin{eqnarray*}\nE[E[v|s]|v] & = & \\int_{-\\infty}^{\\infty}E[v|s=\\ln v+u]\\text{e}^{-u^{2}}du\\\\\n& = & \\int_{-\\infty}^{\\infty}\\text{e}^{(\\ln v+u)}\\kappa\\text{e}^{-u^{2}}du\\\\\n& = & v\\int_{-\\infty}^{\\infty}\\kappa\\text{e}^{u-u^{2}}du.\n\\end{eqnarray*}\n\\]\nI.e., despite the logarithmic internal representation, the average posterior is linear in the value."
  },
  {
    "objectID": "posts/2023-11-18-history-automated-text-moderation.html",
    "href": "posts/2023-11-18-history-automated-text-moderation.html",
    "title": "The History of Automated Text Moderation",
    "section": "",
    "text": "This document describes five technologies for automated text moderation, each roughly correspond to an historical phase.\nAs a working example we will use the detection of “toxic” comments. In practice many different definitions of “toxic” have been used in the industry, and there are a variety of related concepts, e.g. “hate speech” and “offensive”.\n\n(1) Keywords\nThe simplest technology is to hard-code a list of words which are considered “toxic”, e.g. a list of curse words. This can be implemented with regular expression. This has obvious limits on the accuracy and cannot be easily maintained, however many platforms still maintain a keyword block list for some sensitive terms.\n\n\n(2) Simple classifier (“Bag of words”)\nWe can collect a large set of human-labeled data on whether individual messages are toxic, and then predict toxicity from the appearance of individual words e.g. using logistic regression or naive Bayes. These classifiers will find that certain words are highly predictive of toxicity. Simple classifiers often have reasonable accuracy but will have many important false positives and false negatives, and they are easy to evade by rewording or misspelling text.\n\n1961: Maron (1961) proposes the Naive Bayes classifier\n\n\n\n(3) Embedding-based classifier (2013-2018)\nThese models have two stages:\n\nPretrain: for each word calculate an embedding (a vector of numbers) which predicts its likelihood of co-occurring with other words. Pairs of words which are nearby in embedding-space typically have similar meanings.\nTrain: train a model to predict toxicity of a comment using the embedding of the words in a message (e.g. the average embedding).\n\nAn advantage over simple classifiers is that these models require much less labeled data for an equal performance, because the pre-training stage has already learned (crudely) the meanings of different words. Thus these models can identify words that are diagnostic of toxicity even if they never appeared in the toxicity training set.\nHowever embedding-based classifiers are still bad at edge cases, e.g. when a word is used inside a negation (“is an idiot” vs “is not an idiot”), or if a word is mis-spelt, or if harmless words are used to express an meaning that is toxic (“your brain is a bowl of jello”).\n\n2013: Word2Vec: a word embedding using a 2-layer neural network, (Mikolov et al. (2013))\n2014: GloVe: Global Vectors for Word Representation. They say “training is performed on aggregated global word-word co-occurrence statistics from a corpus” (Pennington, Socher, and Manning (2014)).\n2015: fastText: word embedding from FAIR. They released pre-trained models for 294 languages (Joulin et al. (2016))\n2017: Jigsaw Perspective Toxicity API v1 from Google.1\n\n1 I couldn’t find any authoritative documentation on the architecture of this classifier: I found one reference to it using the GloVe embeddings.\n\n(4) LLM-based classifiers (2018-2023)\nThese models have three stages:\n\nEmbedding: Compute embedding of each token (a token is roughly equal to a word).\nPretrain: Train a deep neural net to predict a token from surrounding tokens (or prior tokens), using attention (i.e. don’t weight all words equally) on an enormous training set of text from books and the internet.\nTrain: Train a model to predict toxicity from labeled data using the top-level neurons in the net as features.\n\nConceptually these are similar to embeddings but (1) they can represent the meaning of entire sentences instead of just words, (2) have more layers so tend to have more sophisticated representations of meaning.\n\n2017: Transformer architecture (Vaswani et al. (2017))\n2018: BERT transformer LLM, this model has been widely used as base model for a variety of natural language tasks, including content moderation (Devlin et al. (2018))\n\n\n\n(5) Zero-shot LLMs (2023-)\nThese models have three stages:\n\nEmbedding: Compute the embedding of each token.\nPretrain: Train a deep net to predict the next token from previous tokens, as above.\nDirectly ask the model whether a given message violates a given policy, e.g. “is the following sentence toxic? ___”\n\nNotably this method does not use any human-labeled data, it only needs to be told what type of text it is looking for. This is referred to as “zero shot”, meaning it needs zero training data. These models can also use “few shot” learning, where a small number of examples are given instead of the thousands of examples that had ordinarily been used.\nThis has big benefits: it allows you to very quickly refine policy, and the LLM can generate explanations for why it made a decision.\n\n2020: GPT-3: reasonable zero-shot performance (Brown et al. (2020))\n2022: ChatGPT published: very good zero-shot performance on many tasks.\n2023: OpenAI provides GPT-4-based content moderation tools (Weng, Goel, and Vallone (2023))\n2023: Startups providing LLM-based content moderation: SafetyKit, CheckStep, Hive, Cove.\n2023: Stanford CoPE:an open-source LLM for moderation.\n\n\n\nDiscussion\nQ: now that we can use LLMs for arbitrary labeling, will we change policies?\n\nProposals are coming out of Michael Bernstein’s lab, e.g. Jia et al. (2023), in using LLMs to substantially change how content is ranked.\nDave Wilner has argued that because LLMs offer much greater flexibility then platforms will find it easier to write more complex policies and update them more frequently.\n\nQ: what do we know about degree of accuracy across languages?\n\nAI typically has a strong anglophone bias. Performance in non-English languages tends to be proportional to the distance from English, e.g. European languages tend to be worse. However many also noted that there is typically a large anglophone bias in human moderation. \nSome literature shows that LLMs have good performance in languages with relatively little training data, e.g. Armengol-Estapé, Gibert Bonet, and Melero (2021). \n\nQ: will censorship change when using LLMs instead of humans?\n\nJeff noted that an advantage of human censors over machine censors is that humans might exercise their judgment to refuse to censor while machines will not.\n\n\n\n\n\n\n\nReferences\n\nArmengol-Estapé, Jordi, Ona de Gibert Bonet, and Maite Melero. 2021. “On the Multilingual Capabilities of Very Large-Scale English Language Models.” https://arxiv.org/abs/2108.13349.\n\n\nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, et al. 2020. “Language Models Are Few-Shot Learners.” https://arxiv.org/abs/2005.14165.\n\n\nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. “Bert: Pre-Training of Deep Bidirectional Transformers for Language Understanding.” arXiv Preprint arXiv:1810.04805.\n\n\nJia, Chenyan, Michelle S Lam, Minh Chau Mai, Jeff Hancock, and Michael S Bernstein. 2023. “Embedding Democratic Values into Social Media AIs via Societal Objective Functions.” arXiv Preprint arXiv:2307.13912.\n\n\nJoulin, Armand, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2016. “Bag of Tricks for Efficient Text Classification.” https://arxiv.org/abs/1607.01759.\n\n\nMikolov, Tomas, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of Word Representations in Vector Space.” https://arxiv.org/abs/1301.3781.\n\n\nPennington, Jeffrey, Richard Socher, and Christopher D. Manning. 2014. “GloVe: Global Vectors for Word Representation.” In Empirical Methods in Natural Language Processing (EMNLP), 1532–43. http://www.aclweb.org/anthology/D14-1162.\n\n\nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” Advances in Neural Information Processing Systems 30.\n\n\nWeng, Lilian, Vik Goel, and Andrea Vallone. 2023. “Using GPT-4 for Content Moderation.” https://openai.com/blog/using-gpt-4-for-content-moderation."
  },
  {
    "objectID": "posts/2023-08-02-small-effects.html",
    "href": "posts/2023-08-02-small-effects.html",
    "title": "The Paradox of Small Effects",
    "section": "",
    "text": "In summary:\n\nAttitudes are hard to change. Many fields in social science have adopted a doctrine of “small effects”: high quality studies tend to show that peoples’ attitudes are not very sensitive to exposure to media, or to their peers’ attitudes.\nYet attitudes do change. We see very wide society-level variation in attitudes, which are hard to explain without peer or media effects.\nResolution of the paradox: each effect is small, but there are a lot of them.\n\n(see an earlier Facebook post)\n\n(1) Attitudes are Hard to Change\nMany fields in social science tend to say that attitudes show little influence from either peer effects or from media exposure:\n\nAngrist (2014) says studies of peer effects “have mostly uncovered little in the way of socially significant causal effects.”\nPolitical scientists talk about “the paradox of minimal effects”, Ansolabehere (2006) says that election campaigns “seem to be inessential to understanding who wins and who loses.”\nDavid Stromberg says “the lesson from the last 50 years of media research is that it is very hard to manipulate voters … evidence of [supply side bias] effects is weak or non-existent”\n\nThere are many studies which find large effects but they tend to be treated with extreme skepticism by the methodologists: they are overwhelmingly from lab experiments or observational data and so can be very biased.\n\n\n(2) Attitudes do Change\nAttitudes vary a huge amount across time and space:\n\nVariation in political and religious attitudes.\nVariation in attitudes towards other races, sexes, sexualities, religions.\nVariation in preferences over food, e.g. for rice vs wheat vs corn.\nVariation in preferences over how many children to have.\n\nIt is hard to explain this variation with individual economic circumstances: when someone migrates to another country they face different economic circumstances (different prices and income) but they typically maintain their attitudes for decades.\nIt is hard to explain this variation with genetic variation, because attitudes vary so much over time, while genes move very slowly.\nSo it seems like peer and media effects must be substantial proximal determinants of attitudes.\n\n\n(3) Resolution: Each Effect is Small, but There are Many\nHow can we resolve small treatment effects with big variation in outcomes? It makes sense if we’ve only been testing very small treatments. Each individual effect is small but there are millions of them, so collectively the effects are large.\nPeer effect studies tend to find small effects when looking at random assignment of peers, e.g. random assignment of roommates, but this may be because time with your roommate constitutes only a very small share of your overall exposure to other people and ideas.1 Collectively that exposure must be hugely important in your attitudes.\n1 Kremer and Levy (2008) say “Most studies do not find effects of these predetermined characteristics on the whole sample of students … conventional peer effects on academic achievement … are not estimated to be particularly important.”Media studies tend to find small effects from exposure to social media or to television, but in most cases the media exposure is only a single-digit percentage-point share of their lifetime exposure to media. So the aggregate effect can be far larger than that measured in any credible experiment or natural experiment (in addition, much of the effect likely propagates through peer effects).\nAn individual campaign advertisement might have very small effects on voting intention, but an individual campaign advertisement is only a tiny share of your lifetime exposure to political communication. Small individual effects are consistent with political attitudes being overwhelmingly determined by exposure and persuasion.\nMore technically: we can reconcile small effects with big variations if:\n\nEffects have a long half-life, e.g. exposure in childhood can affect your attitudes as an adult.\nPeer effects are propagated through many weak links instead of a few strong links: i.e. there are substantial influences from all of society, not just closest friends and family.\nPersuasion works even with indirect channels, e.g. your political views aren’t just affected by campaign ads, but also by the implicit attitudes to politics reflected in all the media you’re exposed to.\nAttitudes are sensitive to the average rather than the total amount of persuasive material you’re exposed to, thus marginal effects can be small while total effects are large.\n\n(As a footnote: from my time in social media companies I learned that individual peer effects are tiny, yet we also know that social media demand is entirely peer effects, i.e. people only use Facebook because other people use Facebook.)\n\n\nOther Notes\nThe paradox of large effects. Tosh et al. (2021) discuss an opposite problem: in some fields there are many claims of large effects, but it is not possible to reconcile the aggregate variance in the data with so many large effects. E.g. they discuss a paper claiming to show that exposure to age-related words tends to lower a subject’s subsequent walking speed by 13%. If people are exposed to many such primes, and they are uncorrelated, then we should expect huge and implausible variation in peoples’ day-to-day walking speed.\nTheir problem is somewhat the opposite: they are talking about a literature which has many non-credible effects from lab experiments or observational data. Instead I’m talking about literature which has credible but small effects.\n\n\n\n\n\n\n\n\n\nReferences\n\nAngrist, Joshua D. 2014. “The Perils of Peer Effects.” Labour Economics 30: 98–108. https://doi.org/https://doi.org/10.1016/j.labeco.2014.05.008.\n\n\nAnsolabehere, Stephen. 2006. “The Paradox of Minimal Effects.” Capturing Campaign Effects, 29–44.\n\n\nTosh, Christopher, Philip Greengard, Ben Goodrich, Andrew Gelman, Aki Vehtari, and Daniel Hsu. 2021. “The Piranha Problem: Large Effects Swimming in a Small Pond.” arXiv Preprint arXiv:2105.13445."
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html",
    "href": "posts/2017-12-10-unconscious-influences.html",
    "title": "On Unconscious Influences (Part 1)",
    "section": "",
    "text": "Over a couple of years I spent a lot of time in offices looking out the window, thinking about decision-making & the unconscious, scribbling little bits & pieces in a notebook.\n\n\n\nNBER\n\n\nI ended up writing two papers - “Hierarchical Aggregation of Information and Decision-Making” by myself and “Implicit Preferences Inferred from Choice” with Jon de Quidt. The papers are fairly technical, and this post is going to be a layperson’s guide to the background, what’s known about unconscious knowledge, and a tiny bit about the ideas in those papers.\nHere is the argument in a nutshell:\n\nThere are plenty of reasons to think that unconscious influences are strong – in other words, that people have limited insight into what factors influence their decisions.\nThe idea of unconscious influences has been in and out of the mainstream of psychology for the last 200 years, but always hounded by arguments over what it means, i.e. over what evidence would be sufficient to show that a decision was influenced by an unconscious factor. The battle has had many reversals: a new types of evidence has been proposed which is thought to reveal unconscious influences, and then later the technique or interpretation is shown to have substantial flaws and the line of inquiry fizzles out. A couple of decades pass and a different approach becomes popular.\nTwo broad classes of evidence are the following: (A) people reveal their unconscious preoccupations in their involuntary responses – in how their pupils dilate, how quickly they respond to a stimulus, in their word associations, dreams, slips of the tongue; (B) people reveal unconscious influences in discrepancies between how they act and how they explain their behaviour. Both sources of evidence have got tangled in debates about interpretation, and there are substantial camps on either side with not much agreement on what constitutes sufficient evidence for unconscious influences.\nA third type of evidence is less common but, I think, more powerful: evidence from inconsistencies in decision-making. The idea being that unconscious factors are by their nature isolated from conscious factors, i.e. they don’t interact with conscious beliefs and desires, and this isolation will cause certain characteristic inconsistencies among decisions.\nThis can be made precise with an analogy: the relationship between the conscious and unconscious brain is like the relationship between a blind man and his guide dog. The blind man makes decisions based, in part, on which direction the guide dog is pulling towards, so the guide dog’s beliefs and desires influence the man’s decisions, but without the man knowing exactly what those beliefs and desires are, and so he couldn’t tell you how much any particular factor contributed to his decision. Testing for unconscious influences in behaviour is just testing the degree to which your brain is being led by a guide dog.\nThe internal-consistency definition of unconscious influences implies two ways of looking for them: (1) testing whether people can accurately answer hypothetical questions about decisions they would make if factors changed - i.e. navigating without your guide dog; and (2) testing whether people make consistent judgments when judging two outcomes at a time.\nFirst, hypothetical questions. We can ask people, how would your judgment change if this factor changed? Would you still like this painting if the name of the artist was different? Would this drawing look more like your cousin if the nostrils were bigger? Unconscious influences imply that people will not be able to give accurate answers to these hypothetical questions because if the description of the situation is abstract then their unconscious brain won’t be able to evaluate it (AKA, they don’t know which direction they would go in without knowing what their guide dog will say).\nThe second way of testing for unconscious influences is what my paper with Jon is about: unconscious influences particularly leave their mark in comparisons, where you evaluate two outcomes simultaneously or consecutively, or when you choose between two outcomes. When confronted with two outcomes you surface two unconscious judgments and that gives you some insight into what is affecting those judgments, which in turn will affect your conscious decision.\nSuppose you had an unconscious preference for men over women, but a conscious preference to be indifferent, this will manifest in the following: (A) when you see two CVs which are identical, except that one is a man and one is a woman, then you’re indifferent between them; (B) when you see two CVs which differ in some other respect (e.g. one has a PhD, the other has an MBA), then you consistently have a preference for the CV belonging to the man. Your guide dog has a bias towards men, which you’re not aware of: the bias will only sway your decision in the second case because, in the second case, when your guide dog pulls you towards the man with a PhD, you cannot figure out how much of that pull is due to his being a man, and how much is due to his PhD.\nIn the end I think that our brains are full of guide dogs all pulling in different directions. If we had the stomach for it we could plot out our decisions all on a map – measure how each factor influences our judgment – and we would be able to see both the surface influences and the deeper latent influences.\n\n\n\n\nMotivating examples\nSome definitions & theory\nWays of measuring implicit preferences\nThe proposal\n\n\n\n\nLittauer"
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html#contents",
    "href": "posts/2017-12-10-unconscious-influences.html#contents",
    "title": "On Unconscious Influences (Part 1)",
    "section": "",
    "text": "Motivating examples\nSome definitions & theory\nWays of measuring implicit preferences\nThe proposal\n\n\n\n\nLittauer"
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html#involuntary-responses",
    "href": "posts/2017-12-10-unconscious-influences.html#involuntary-responses",
    "title": "On Unconscious Influences (Part 1)",
    "section": "Involuntary responses",
    "text": "Involuntary responses\nFreud is the most famous theorist of extracting unconscious factors from involuntary responses – he wrote three books on different methods: one on dreams, one on jokes, one on mistakes (mis-reading, mis-hearing, mis-speaking). An example from the last book: “A woman who is very anxious to get children always reads ‘storks’ instead of ‘stocks’.” Most of Freud’s examples of unconscious influences are much more complex than this one, and more often the hidden factor influencing behaviour is something unpleasant or shameful.\nAnother way of measuring unconscious cognition is through measuring arousal. Most famous is the “Iowa card task” from Bechara et al. (1996). They had their subjects choose among playing cards, and receive rewards if they chose certain cards. They found that people gradually learned which types of cards were rewarded, but they also found that the subjects’ automatic responses (measured by skin conductance, i.e. sweating) would show an awareness of the pattern more quickly than the subjects’ choices would: after a while, when the subject’s hand hovered over one of the cards which was rewarded, the subject would sweat a little more, even though the subject wasn’t any more likely to choose that card. They said that this showed that unconscious learning was outpacing conscious learning. Antonio Damasio, one of the authors of this study, went on to write Descartes’ Error which accused Descartes’ of starting the great misapprehension that emotions and reason are in competition – Damasio said that his experiments show how emotions inform reason and improve decision-making. A lot of subsequent papers tried to show that snap decisions, which avoid conscious processing, can produce better outcomes than slow considered decisions.\nEven more famous is the “Implicit Association Test” (IAT) (Greenwald, McGhee and Schwartz (1998)). Subjects are told to press a button whenever they see something from either of two different categories of stimuli, e.g. press the button if you see either a black face or a word with a positive association. Their finding, much-replicated, was that people are relatively quicker at tasks (meaning they have shorter response times) when they are asked to identify a set such as “black face or negative word” or “white face or positive word” than to identify a set like “black face or positive word” or “white face or negative word.” They find that this occurs even among people who report no conscious negative feelings towards black people, and they interpret this as revealing an unconscious association between black people and negative feelings, and they argue that this association could affect your decision-making without you being aware of it.\nMany other measures of automatic responses have been popular at different times: hypnosis and word association (Freud used both of these before moving to talking therapy); Rorsach blots (AKA inkblot tests); thematic apperception test (interpret an ambiguous drawing, still widely used); lie detectors AKA polygraphs (they measure autonomic responses - blood pressure, pulse, respiration, and skin conductivity - as you are asked different questions).\nUnfortunately a great deal of this research turns out to be both hard to replicate, and reliant on strong assumptions in order to interpret as surfacing unconscious associations. Newell and Shanks (2014) give strong arguments for both of these points, covering many of the methods I mentioned here.\nIt is worth mentioning that, although Freud’s more elaborate theories died off, his idea that psychosomatic illness is an indirect expression of a psychological stress, especially about something shameful, I believe remains one of the standard theories of modern neurology (O’Sullivan, 2015).\nHowever even if we had solid evidence for unconscious influences on involuntary responses, this still stops short of unconscious influences on decision-making. It’s possible that our associations show up in sweating, response time, and dreams, but have little effect on decision-making, and if that’s so then unconscious associations are not terribly important for social science. Most of the authors in this literature have assumed that the unconscious factors they identify affect real decisions but have left that extrapolation untested. Blanton et al. (2009) say that there’s no persuasive evidence that implicit racial bias, as measured by the IAT, predicts peoples’ decision-making, once you control for measures of explicit racial bias, i.e. when you just ask people how they feel about black people. (Singal (2016) has a long discussion on this point)."
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html#ability-to-describe-the-influence",
    "href": "posts/2017-12-10-unconscious-influences.html#ability-to-describe-the-influence",
    "title": "On Unconscious Influences (Part 1)",
    "section": "Ability to Describe the Influence",
    "text": "Ability to Describe the Influence\nA second type of evidence is to compare self-reported influences on behaviour with actual influences on behaviour. Here are some examples:\n\nIn the mid 20th-century behaviourists found that they could shape their subjects choices through conditioning with rewards and punishments, and the subjects seemed to remain ignorant of this shaping. For example if you say ‘mm-hmm’ whenever someone uses a plural noun, then after a while that person ends up using plural nouns more often, apparently unaware of the influence (Thorndike and Rock (1934); Greenspoon (1955)).\nSince the 1970s social psychologists have published all sorts of experiments in which they vary an apparently irrelevant factor and find that this can affect peoples’ decision-making. Nisbett and Wilson (1977) summarize a lot of experiments and say “subjects are sometimes (a) unaware of the existence of a stimulus that importantly influenced a response, (b) unaware of the existence of the response, and (c) unaware that the stimulus has affected the response.”\nAnother paradigm from the 1970s asks people to make a judgement - e.g. which stock to pick - and also to rate the importance of factors which contributed to their decision. Slovic et al. (1972) find a low correlation (0.34) between the ratings that a stockbrokers put on factors, and the actual influence of these factors on their decisions. There is a small literature with similar findings across a variety of tasks.\nFinally, since the 1970s a smaller group of psychologists have been running experiments in which people learn a complicated pattern, and then are asked about their insight into it. E.g. in Arthur Reber’s “artificial grammar” experiments subjects learn, through trial and error, to discriminate between two categories of words. After some time they become very good at the task, but when asked to explain how they are making decisions they often say they don’t know, or they come up with rules that do not match their actual performance.\n\nAs in the previous category, a lot of this evidence is very fragile: either hard to replicate, or based on delicate interpretations of what is happening in the experiment. Newell and Shanks (2014) again give a good summary.\nAn additional problem is that these findings could reflect knowledge being difficult to articulate, without it being unconscious. And this literature is full of reversals which bear this out: when experiments are repeated it has often turned out that the subjects do report awareness of the pattern that they have learned if they are asked the question in a different way. Mitchell et al. (2009) say “[i]t is very difficult to provide a satisfactory demonstration of unaware conditioning simply by showing conditioning in the absence of awareness. This is because it is very difficult to be sure that the awareness measure and the conditioning measure are equally sensitive.”\n\n\n\nIIES"
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html#isolation-of-unconscious-influences",
    "href": "posts/2017-12-10-unconscious-influences.html#isolation-of-unconscious-influences",
    "title": "On Unconscious Influences (Part 1)",
    "section": "Isolation of Unconscious Influences",
    "text": "Isolation of Unconscious Influences\nFinally there’s a third type of evidence which is more strictly behavioural: an unconscious factor is one which is isolated from your other conscious beliefs and desires – i.e. it does not interact with conscious factors – and that isolation will be reflected in your behaviour. This isolation criterion has been given various names, but I don’t think it’s ever been explained as clearly as it could be.\nTo be precise think of the blind man (the conscious part of the brain) and the guide dog (the unconscious). The guide dog can know something – e.g. she knows when the crossing light is flashing – which the man does not know, and her knowledge will influence the man’s decisions through her recommendation of when to cross. However the guide dog’s knowledge is isolated from the man’s knowledge: it only influences his decisions through the narrow channel of pulling on the leash. Suppose you tell the man that the crossing lights are not working properly, and so whatever color they show is entirely at random and uninformative. The man and dog, considered as a system, has two pieces of information: (1) the light is green (i.e. indicating ready to cross); and (2) the color is uninformative. However the two pieces of information are known by different actors, implying that they will not be integrated, because neither the man or dog knows both. This will be reflected in the man’s behaviour: he will be influenced by the guide-dog’s recommendation, because the dog sees other things in addition to the crossing-light, such as oncoming traffic. And so the man’s behaviour will still be influenced by the color of the light, even though he knows that the color is irrelevant.\nIf information is separated in the brain, we ought to see characteristic patterns of that in behavior. I know of just a few cases where the isolation of knowledge has come up clearly in trying to define or measure unconscious influences.\nStich (1978) said that certain mental states are “inferentially unintegrated”: \n\n“[unconscious beliefs are] largely inferentially isolated from the large body of inferentially integrated beliefs to which a subject has access”\n\nHe gives an example: suppose Noam Chomsky has a theory of grammar, and that there exists some grammatical rule which is a counterexample to that theory. If a linguist knows that rule consciously, then the linguist will immediately infer that Chomsky’s theory is false. But if the linguist only knows the rule unconsciously, then they won’t be able to make that inference, because the knowledge is “inferentially unintegrated” – i.e. the knowledge is isolated from the knowledge regarding Chomsky’s theory. 1\n1 Quine says you shouldn’t call this type of thing unconscious knowledge – your linguistic practice may obey some rule, but you can’t say that you unconsciously know that rule, because there are infinitely many different rules that would imply that pattern of behavior. But this skeptical objection is too tough: Quine would deny that a cow can have a belief about where a water trough is, & instead admit only that the cow’s behavior is consistent with a particular belief among infinitely many others.A separate place where this separation has come up is in the work of Zenon Pylyshyn and Jerry Fodor since the 1980s regarding perception being “cognitively impenetrable,” or “informationally encapsulated.” They mean that perceptual processes often make inferences without taking into account all the information that is available, i.e. by drawing only on a subset of information. Their principal argument was from perceptual illusions: they argue that illusions can typically be understood as rational inferences from a subset of the information available. Helmholtz had a nice example: if you close one eye and press with your finger on the edge of your eyelid then you’ll perceive a point of light, but the light will be coming from the opposite side of your field of vision from where your finger is. This is because the left side of your retina receives light from the right side of your visual field and vice versa. So when your retina receives some stimulation on the left-hand side your brain makes infers that light is coming from the right-hand side. This is a sensible inference given only the information that your eye has, i.e. just the information from the retina. In this case there is additional information - the fact your finger is pressing on your eyelid - which should give a different interpretation to the stimulation, but your visual cortex is not wired up to incorporate that information, and so it misinterpret the signals it receives.\nThe Helmholtz-Fodor-Pylyshyn model of encapsulated inference isn’t quite the same as the case of the blind man and the guide dog. In their examples the pre-conscious process have a strict subset of the information available to the conscious brain. In other words the man isn’t blind, it’s just a case where the dog leads in a different direction than the man would. Fodor (1983) does have a brief discussion on whether early perceptual processes have access to information not available to the conscious brain, which would imply unconscious influences, in my sense.\nFinally the isolation argument has appeared in the literature on human “associative learning,” in testing whether or not the associations that we learn through conditioning are conscious. A typical experiment involves ringing a bell and then giving subjects a small electric shock. After a while people learn to flinch when they hear the bell. For a long time psychologists tried to map out the logic of how such associations would form, trying to figure out the rule which governed learning of associations. However in the last few decades an argument has been made that these learned associations are not in fact mechanical - there is no simple rule - instead they are more-or-less optimal responses to the environment based on the entirety of the information available, i.e. they are not isolated from other knowledge, though the argument isn’t usually put in terms of conscious vs unconscious knowledge. For example Colgan (1970) told subjects, after they learned an association, that the association is no longer valid (“from now on the bell will not signal an electric shock”) and he found that, although this didn’t entirely extinguish the flinching, it did cause it to markedly decrease. This implies the flinching is not isolated from your conscious knowledge: the association, at least to some degree, interacts with more abstract knowledge. There are many other circumstances where rule-based theories of association-learning have foundered because it turns out that peoples’ responses respond to outside considerations. De Houwer, Vandorpe and Beckers (2005) summarize the evidence against associative models (which can be interpreted as models with unconscious knowledge):\n\nThe two types of models can be differentiated … by manipulating variables that influence the likelihood that people will reason in a certain manner but that should have no impact on the operation of the associative model. We have seen that such variables (e.g., instructions, secondary tasks, ceiling effects, nature of the cues and outcomes) do indeed have a huge effect. Given these results, it is justified to entertain the belief that participants are using controlled processes such as reasoning and to look for new ways to model and understand these processes.\n\nMitchell says:\n\n“The results consistently show evidence for skin conductance [effects] only in participants who are aware of the [relationship] … [a]lthough there are many papers arguing for unaware conditioning, close inspection reveals, in almost all cases, that the measure of conditioning was most likely more sensitive than that of awareness.”\n\nIn retrospect a lot of behavior that was studied in the lab, which was thought to be telling us about the wiring of the animals, actually was telling us about the world outside the animal, because it has turned out that the animals’ response is the optimal response to the typical circumstances it faces in the world. (See my other post The Repeated Failure of Laws of Behaviour , and also Mitchell et al. (2009) section 4.3)\nIf this line of thought were entirely correct – if all information was integrated and fed into every decision – then there would be no unconscious influences in my sense. However I do think that there’s plenty of evidence that remains for a lack of integratation between cognitive processes.\nIn Part 2 of this essay I will give a more formal statement of how decisions can reveal unconscious knowledge (and unconscious motivations), and a survey what I think is the strength of the evidence.\n\n\n\nCaltech"
  },
  {
    "objectID": "posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html",
    "href": "posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html",
    "title": "Peer Effects, Culture, and Taxes",
    "section": "",
    "text": "In short:1"
  },
  {
    "objectID": "posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html#relation-to-literature-on-health-taxes",
    "href": "posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html#relation-to-literature-on-health-taxes",
    "title": "Peer Effects, Culture, and Taxes",
    "section": "Relation to Literature on Health Taxes",
    "text": "Relation to Literature on Health Taxes\nThe key point of this note is that the existence of peer effects would justify a tax on unhealthy goods, independent of any direct externalities or internalities from consumption, along a transition path when we learn new information about that good. I haven’t found discussion of this point in the economics literature on health taxation. The literature in public health often seems to make this argument implicitly but I think it deserves to be made much more clearly.\nMost countries have substantial restriction on unhealthy goods (“sin taxes”). Most countries have substantial taxes on tobacco and alcohol, outlaw many drugs, carcinogens, trans fats, and many countries are planning to introduce a tax on sugar.\nThe economics literature is not in consensus about the appropriate taxation of unhealthy goods. E.g. DeCicca (2022) reviews the evidence on tobacco taxes and conclude that they are higher than would be justified by the purely economic externalities, e.g. through healthcare (“evidence on the magnitude of the externalities does not support current tax levels.”) They then consider the evidence for “internalities,” i.e. myopic decision-making by consumers, and say that the evidence and theoretical framework is too sparse to draw a conclusion (“the empirical evidence on the magnitudes of the internalities from smoking is surprisingly thin.”)\nDiscussion of optimal taxation rarely discusses peer effects. DeCicca et al. (2022) mention the existence of an empirical literature on the peer effects on smoking, but don’t discuss the findings, or whether it would have implications for setting taxes.\nKenkel et al. (2002) calibrate a model of rational addiction with peer effects. They show that peer effects will tend to magnify distortions due to other externalities, and so increase the size of the efficient tax for a given individual elasticity. In their model peer effects only magnify existing distortions, they do not justify a tax by themselves. However that paper does not discuss the role of adjustment to new information (the focus of this note), a case in which peer effects will cause inefficiencies even without other externalities.\nAllcott et al. (2020) argue for a 2c/oz tax on soda based on (1) a 1c/oz externality due to healthcare costs, and (2) a 1c/oz “internality” due to peoples’ myopic decision-making. They do not (I believe) discuss a peer-effect justification for taxation, which would constitute an additional separate justification for a soda or sugar tax.\nThe Public Health literature discusses peer effects, but it’s unclear what role they play in setting policy. Since the 1990s the Public Health literature has put a lot of emphasis on “community health” or “population health” interventions, in part due to the perceived importance of social norms, AKA peer effects. However this literature rarely includes an explicit calculation of costs and benefits to justify a given magnitude of intervention. As a consequence it’s often unclear whether peer effects are thought to be relevant due to magnifying some other distortion (e.g. a fiscal externality or myopic decision-making), or due to retarding the aggregate adjustment to new information. However I believe the informal reasoning used in public health essentially appeals to the argument I am making: they argue that peoples’ health would be substantially better off with small lifestyle changes, but they are held back by social norms.2\n2 A highly-cited paper by epidemiologist Geoffrey Rose (2001) argues that public health interventions should focus on community-wide interventions rather than just on high-risk individuals, in part because of peer influences. He says “Eating, smoking, exercise and all our other life-style characteristics are constrained by social norms. If we try to eat differently from our friends it will not only be inconvenient, but we risk being regarded as cranks or hypochondriacs.”"
  },
  {
    "objectID": "posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html#application-to-tobacco-taxes",
    "href": "posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html#application-to-tobacco-taxes",
    "title": "Peer Effects, Culture, and Taxes",
    "section": "Application to Tobacco Taxes",
    "text": "Application to Tobacco Taxes\nConsumption of tobacco declined very slowly after the health effects were discovered. Deaths from lung cancer increased by a factor of 10 between 1925 and 1950, and by 1950 it was fairly clear that smoking was the overwhelming cause.3 Rates of smoking peaked around 1960 and have been declining by about 5%/year ever since, and are now about 1/3 of their peak. Thus most of the deaths from smoking have been among people who started smoking after it became clear that smoking caused cancer.\n3 Doll and Bradford Hill (1950)The decline in smoking was primarily due to a change in beliefs. DeCicca et al. (2022) document that in the US the tax rates on cigarettes stayed relatively constant between 1960 and 2022, and they say most studies estimate that cigarette demand is fairly insensitive to price (estimates of elasticity between -0.05 and -0.35). They also find relatively weak effects from other regulations, e.g. bans on advertising, bans on smoking in public places.\nThus the large aggregate decline in smoking seems to have been driven by a change in tastes, which presumably was downstream from a change in beliefs about health effects. That change in tastes has propagated very slowly presumably due to peer effects (AKA “norms” or “culture”).\nIn retrospect the decline was inefficiently slow: multiple generations got a taste for tobacco from their peers and their parents, and then found the habit difficult to shake. This implies an externality from smoking. In retrospect welfare would’ve been higher if we’d had high taxes in the 1960s to hasten the decline of smoking, and indeed in a simplified model it could’ve been Pareto improving: each person would lose utility from smoking less, but offset by a gain from their peers smoking less.\nSuppose we come to believe that saturated fat or sugar has a comparable health effect to tobacco. Then in the long-run we expect their use will decline, but it might take 50 years for that to happen. Thus it would be efficient to apply a temporary tax to correct the peer-effect externality."
  },
  {
    "objectID": "posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html#long-run-effects-on-culture",
    "href": "posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html#long-run-effects-on-culture",
    "title": "Peer Effects, Culture, and Taxes",
    "section": "Long-run effects on culture",
    "text": "Long-run effects on culture\nNunn (2020) gives many examples of contemporary cultural traits which appear to reflect differences in historical environments that no longer exist today.\n\nBecker (2020): There are large contemporary differences in culture between societies which live in lands well-suited for pastoralism vs agriculture.\nAlesina et al. (2011): “the descendants of societies that traditionally practiced plough agriculture, today have lower rates of female participation in the workplace, in politics, and in entrepreneurial activities, as well as a greater prevalence of attitudes favoring gender inequality.”\n\nHowever these types of argument are notoriously difficult to evaluate: identification requires assuming many other causal channels are zero, and researchers typically examine a lot of data before finding an association that could be publishable."
  },
  {
    "objectID": "posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html#evidence-from-migration",
    "href": "posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html#evidence-from-migration",
    "title": "Peer Effects, Culture, and Taxes",
    "section": "Evidence from Migration",
    "text": "Evidence from Migration\nAlesina and Guiliano (2013) say\n\n“when immigrants move to a place with different institutions, overwhelmingly their cultural values change gradually, if ever, but rarely within two generations."
  },
  {
    "objectID": "posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html#evidence-from-heritability",
    "href": "posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html#evidence-from-heritability",
    "title": "Peer Effects, Culture, and Taxes",
    "section": "Evidence from Heritability",
    "text": "Evidence from Heritability\nTwin studies show fairly high heritability of health behaviors. I give below some very rough estimates for the contribution of shared environment to adult health behaviors from twin studies. The shared environment estimate comes from the degree of correlation between fraternal twins (precisely: the excess relative to 50% of the correlation between identical twins).5\n5 Vink et al. (2015) Heritability of smoking initiation and nicotine dependence – study of Dutch twins. They cite other studies with similar decomposition for smoking inititation. “individual differences in smoking initiation were explained by genetic (44%), shared environmental (51%) and unique environmental (5%).”  Verhulst (2015) The heritability of alcohol use disorders: a meta-analysis of twin and adoption studies: “The best-fit estimate of the heritability of AUD was 0.49 [95% confidence interval (CI) 0.43–0.53], and the proportion of shared environmental variance was 0.10 (95% CI 0.03–0.16).”  Maes (1997) Genetic and Environmental Factors in Relative Body Weight and Human Adiposity: “genetic factors explain 50 to 90% of the variance in BMI”. They don’t seem to give a preferred estimate for shared environment contribution, I’m going to say 10% based on skimming this and other papers.\n\n\n\n\n\n\n\n\n\nheritability (\\(h^2\\))\nshared env (\\(c^2\\))\nunique env (\\(e^2\\))\n\n\n\n\nsmoking initiation\n45%\n50%\n5%\n\n\nalcohol use disorder\n50%\n10%\n40%\n\n\nobesity\n70%\n10%\n20%\n\n\n\nHowever twin studies under-state the importance of peer effects. The “shared environment” will pick up the influence of parents and mutual friends of siblings, however the “unique environment” will also include the peer effects from each sibling’s idiosyncratic friendships.\nMore importantly, twin studies decompose the variance in behaviours for a given population (usually a single country) at a particular time. However for most of these behaviors the within-society variance is small relative to the between-society variance. Thus twin decompositions of variance will only pick up the contribution of local peer effects (your family or neighborhood), not society-wide peer effects."
  },
  {
    "objectID": "posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html#summary-of-models",
    "href": "posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html#summary-of-models",
    "title": "Peer Effects, Culture, and Taxes",
    "section": "Summary of Models",
    "text": "Summary of Models\nI describe three models of peer effects, each an extension of the last.\n\nStatic model. Implies (1) aggregate elasticity greater than individual elasticity; (2) decentralized decision-making will cause too much variation in behaviour relative to optimal.\nTwo-generation model. Suppose the second generation wishes to stay close to the first generation’s consumption. Then the first generation imposes an externality on the second generation, and they will fail to respond to anticipated future changes in price (or information).\nInfinite generation model. We can compare the rate of adjustment chosen by agents (ignoring their externality on future generations) and the efficient rate of adjustment, and therefore the appropriate tax to restore efficiency.\n\nTo add:\n\nAn overlapping generations model.\nA fixed-cost model. I believe that this peer-effects model is isomorphic to a model with spillovers due to production with fixed costs, i.e. a Hotelling monopolist who must choose a point on a line, as in Anderson and Waldfogel (2005). However I would like to confirm this.\nMatrix of effects. I think you can characterize the theory with two derivatives: the externality and the strategic complementarity. I think it would be useful to give examples of the 9 permutations (pos/zero/negative; pos/zero/negative)."
  },
  {
    "objectID": "posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html#model-static-peer-effects",
    "href": "posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html#model-static-peer-effects",
    "title": "Peer Effects, Culture, and Taxes",
    "section": "Model: Static Peer Effects",
    "text": "Model: Static Peer Effects\nSummary. Suppose you buy \\(x_i\\) at price \\(p\\), and utility depends both on distance from ideal-point, \\(\\hat{x}_i\\) and distance from avg level of consumption, \\(\\bar{x}\\). Then we’ll see that:\n\nAggregate price elasticity will be higher than individual price elasticity because when one person moves it gives everyone else a reason to move.\nAverage consumption is the same comparing social planner to decentralized.\nWelfare-optimal consumption would be more compressed then decentralized consumption, because when we inflict an externality whenever we depart from the mean.\nAverage consumption moves slowly. Suppose everyone’s ideal points changes due to new information about health but they take last period’s mean consumption as the reference point. Then the society’s consumption will not adjust all the way to the new equilibrium.\n\n\n\n\n\n\n\nEach person will choose a level of consumption between their exogenous preference and the population-average consumption (represented by the dashed line).\n\n\n\nConsumption with spillovers. Suppose you choose your level of consumption, \\(x_i\\), and utility depends both on distance from ideal-point and distance from avg level of consumption: \\[u_i(x_i)\n      =  - \\frac{\\alpha}{2}{\\utt{(x_i-\\hat{x}_i)}{distance from}{ideal point}}^2\n         -\\frac{\\gamma}{2}{\\utt{(x_i-\\bar{x})}{distance from}{popn mean}}^2   - px_i.\n         \\]\nWe get optimal consumption, a function of ideal-point, avg consumption, and price: \\[\\utt{x_i^*}{utility-maximizing}{consumption} =\n      \\frac{\\alpha}{\\alpha+\\gamma}\\hat{x}_i +\n      \\frac{\\gamma}{\\alpha+\\gamma}\\bar{x} -\n      \\frac{1}{\\alpha+\\gamma}p.\n      \\]\nWe get average consumption (\\(\\bar{x}=\\frac{1}{n}\\sum x_i^*\\)): \\[\\begin{aligned}\n      \\bar{x} &= \\frac{\\alpha}{\\alpha+\\gamma}\\frac{1}{n}\\sum \\hat{x}_i + \\frac{\\gamma}{\\alpha+\\gamma}\\bar{x} - \\frac{1}{\\alpha+\\gamma}p \\\\\n               &= \\utt{\\frac{1}{n}\\sum \\hat{x}_i}{avg ideal}{point}- \\frac{1}{\\alpha}p.\n   \\end{aligned}\n   \\]\nObservations:\n\n\n\n\n\n\n\n\n\n\nAverage consumption will be more price-sensitive than individual consumption (\\(\\frac{1}{\\alpha}&gt;\\frac{1}{\\alpha+\\gamma}\\)).\nThe average level of consumption is independent of the strength of spillovers (\\(\\gamma\\)): spillovers just compress the variance, they don’t change the average level of consumption.\nSuppose your utility depends on prior aggregate consumption, then you get slow convergence to equilibrium (I derive a dynamic model below).\n\nFinally we derive welfare-maximizing consumption: \\[U = \\sum_i u_i(x_i)\n      = \\sum_i\\left( - \\frac{\\alpha}{2}(x_i-\\hat{x}_i)^2\n         -\\frac{\\gamma}{2}(x_i-\\frac{1}{n}\\sum_jx_j)^2   - px_i\\right)\n   \\]\nThe first-order condition for \\(x_i\\) wil be: \\[\\frac{dU}{dx_i}\n      = - \\utt{\\alpha(x_i-\\hat{x}_i)}{MC of departing}{from ideal point}\n         -\\utt{\\gamma(x_i-\\bar{x})}{MC of departing}{from avg}\n         - p\n         +\\utt{\\sum_j\\frac{1}{n}\\gamma(x_j-\\bar{x})}{marginal peer externality}{on others}=0\n   \\]\nSo we have: \\[\\begin{aligned}\n   \\utt{x_i^*}{welfare-maximizing}{consumption for $i$} &=\n      \\frac{\\alpha}{\\alpha+2\\gamma}\\hat{x}_i +\n      \\frac{2\\gamma}{\\alpha+2\\gamma}\\bar{x} -\n      \\frac{1}{\\alpha+2\\gamma}p.\\\\\n   \\utt{\\bar{x}^*}{avg welfare-max}{consumption}\n      &= \\frac{1}{n}\\sum_i\\hat{x}_i-\\frac{1}{\\alpha}p.\n   \\end{aligned}\n   \\]\nObservations:\n\nThe average consumption is the same, whether decentralized or welfare-maximizing, but with welfare-maximizing consumption everyone has moved closer to the mean.\n\nQ: adding direct externalities? Suppose each unit of consumption inflicts $1 non-peer externality on others, e.g. smog, second-hand smoke, congestion, etc. Without peer effects then a $1 tax would appropriately correct for this. Would the efficient tax also be $1 in this model?"
  },
  {
    "objectID": "posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html#extension-direct-externalities",
    "href": "posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html#extension-direct-externalities",
    "title": "Peer Effects, Culture, and Taxes",
    "section": "Extension: Direct Externalities",
    "text": "Extension: Direct Externalities\nNow we will add a direct externality cost, e.g. representing second-hand smoke or fiscal externality (I believe the results would be the same for an internality, e.g. myopic consumption choice). We will simplify the utility function in some other ways: everyone has the same ideal-point (at zero), and we ignore price:\n\\[u_i = -\\utt{x_i^2}{everyone's ideal}{point is zero}\n            - \\utt{\\frac{\\gamma}{2}(x_i-\\bar{x})^2}{peer}{effects}\n            - \\utt{\\theta \\bar{x}}{direct}{externality}.\n      \\]\nIf there were no peer effects then everyone would simply set \\(x_i=0\\), but the welfare-maximizing choice would be to set \\(x_i=\\theta\\), at which point the marginal personal cost of increasing consumption is equal to the marginal externality.\nWith peer effects the decentralized choice of \\(x_i\\) will ignore the externality and will simply bias their choice of \\(x_i\\) towards the population mean: \\[x_i^* = \\frac{\\gamma}{\\gamma+1}\\bar{x}.\\]\nThe unique equilibrium will be at \\(\\bar{x}=0\\), independent of the strength of peer effects.\nNow we can derive the welfare-maximizing choice of \\(x_i\\) with peer effects: \\[\n      \\begin{aligned}\n         U  &= \\frac{1}{2}\\sum_i x_i^2 - \\sum_i \\frac{\\gamma}{2}(x_i-\\bar{x})^2 - \\theta \\sum_i x_i \\\\\n         \\frac{dU}{dx_i}   &=\n                     \\utt{x_i}{marginal cost}{of departing from 0}\n                  - \\utt{\\gamma (x_i-\\bar{x})}{marginal cost}{of departing from avg}\n                  + \\utt{\\sum_j \\frac{1}{n}\\gamma (x_j-\\bar{x})}{marginal peer harm}{to others}\n      \\end{aligned}\n   \\] The third term above (“marginal peer harm to others”) will always be equal to zero (it simplifies to \\(\\gamma(\\bar{x}-\\bar{x})\\)). T\n\\[\\begin{aligned}\n                  - \\utt{\\theta}{marginal}{externality} =0 \\\\\n            &= x_i - \\gamma (x_i-\\bar{x}) - \\theta = 0 \\\\\n         x_i &= \\frac{-\\gamma\\bar{x}+\\theta}{1-\\gamma} \\\\\n         x_i &= \\ut{\\theta}{externality} + \\utt{(\\frac{\\gamma}{1-\\gamma})(\\theta-\\bar{x})}{pull up}{rest of population} \\\\\n      \\end{aligned}\n   \\]\nObservations:\n\nIn equilibrium we can set \\(x_i=\\bar{x}\\) and we will have \\(x_i=\\theta\\), i.e. the welfare-maximizing choice is independent of the strength of peer effects, because they’ll all be zero whatever value you choose.\nHowever if we hold fixed everyone else’s choice (take \\(\\bar{x}\\) as given), then your choice effectively has two externalities: (1) the direct externality \\(\\theta\\), (2) the ."
  },
  {
    "objectID": "posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html#model-peer-effects-with-two-generations",
    "href": "posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html#model-peer-effects-with-two-generations",
    "title": "Peer Effects, Culture, and Taxes",
    "section": "Model: peer effects with two generations",
    "text": "Model: peer effects with two generations\nFor the dynamic models I assume peer effects operate solely through the prior generation’s actions: i.e. there’s some cost of departing from what your ancestors do. I assume there are no peer effects within a generation, and so can treat each generation as having just one agent.\nTwo generation model. Suppose the second generation gets disutility from departing from the first generation’s level of consumption (“habituation”): \\[\n   \\begin{aligned}\n      U(x_1,x_2)     &= u_1 + \\beta u_2 \\\\\n      u_1(x_1)       &= \\utt{(x_1-\\hat{x}_1)^2}{deviation}{from ideal} \\\\\n      u_2(x_1,x_2)   &= \\utt{(x_2-\\hat{x}_2)^2}{deviation}{from ideal}\n                        + \\gamma \\ut{(x_2-x_1)^2}{habitutation}\\\\\n   \\end{aligned}\n\\]\nResults:\n\nThe decentralized solution is inefficient. The first generation doesn’t care about the second generation, & so hurts the second generation.\nThe centralized solution would alter first generation’s consumption. We push the first generation a bit towards the second generation’s ideal-point. If we have a perfect ability to tax and rebate then we can implement the centralized solution with a tax.\nRoughly: efficient tax would bring you 1/3 of the way towards long-run ideal point. Suppose the half-life of adjustment is 1 generation (\\(\\gamma=1\\)) and there is no discounting (\\(\\beta=1\\)). Then the first generation consumption should be 1/3 of the way towards the long-term ideal point.\n\nDecentralized solution. The first generation just chooses their ideal point, the second generation is somewhere between the habituation point and their own ideal point. If \\(\\gamma=1\\) then they’re half-way, meaning the half-life of adjustment is one generation.\n\\[\\begin{aligned}\n   x_1  &= \\hat{x}_1\\\\\n   x_2  &= \\hat{x}_1\\frac{\\gamma}{1+\\gamma}+ \\hat{x}_2\\frac{1}{1+\\gamma}\n\\end{aligned}\\]\nCentralized solution. We now adjust the first-generation consumption towards the second-generation’s ideal point (derivation below):\n\\[\\begin{aligned}\n   x_1  &= \\hat{x}_1\\frac{1+\\gamma}{1+\\gamma+\\gamma\\beta}+ \\hat{x}_2 \\frac{\\gamma\\beta}{1+\\gamma+\\gamma\\beta}\\\\\n   x_2  &= \\hat{x}_1\\frac{\\gamma}{1+\\gamma+\\gamma\\beta}+ \\hat{x}_2\\frac{1+\\gamma\\beta}{1+\\gamma+\\gamma\\beta}\n\\end{aligned}\\]\nObservations:\n\nIf no spillovers (\\(\\gamma=0\\)) then \\(x_1=\\hat{x}_1\\), \\(x_2=\\hat{x}_2\\).\nIf we don’t care about the future (\\(\\beta=0\\)) then we get the decentralized solution again.\n\nDerivation:\n\\[\n   \\begin{aligned}\n      \\frac{dU}{dx_1}   &= (x_1 - \\hat{x}_1) - \\gamma\\beta(x_2-x_1) = 0 \\\\\n                  x_1   &= \\frac{\\hat{x}_1+\\gamma\\beta x_2}{1+\\gamma\\beta} \\\\\n      \\frac{dU}{dx_2}   &= (x_2 - \\hat{x}_2) + \\gamma(x_2-x_1) = 0 \\\\\n                  x_2   &= \\frac{\\hat{x}_2+\\gamma x_1}{1+\\gamma} \\\\\n                        &= \\frac{\\hat{x}_2+\\gamma \\frac{\\hat{x}_1+\\gamma\\beta x_2}{1+\\gamma\\beta}}{1+\\gamma} \\\\\n                        &= \\hat{x}_2\\frac{1}{1+\\gamma} + \\hat{x_1}\\frac{\\gamma}{(1+\\gamma)(1+\\gamma\\beta)}\n                              + x_2\\frac{\\gamma^2\\beta}{(1+\\gamma)(1+\\gamma\\beta)} \\\\\n      x_2\\frac{(1+\\gamma)(1+\\gamma\\beta)-\\gamma^2\\beta}{(1+\\gamma)(1+\\gamma\\beta)}\n                        &= \\hat{x}_2\\frac{1}{1+\\gamma} + \\hat{x_1}\\frac{\\gamma}{(1+\\gamma)(1+\\gamma\\beta)} \\\\\n      x_2 (1+\\gamma+\\gamma\\beta)\n                        &= \\hat{x}_2(1+\\gamma\\beta)+\\hat{x}_1\\gamma \\\\\n      x_2               &= \\hat{x}_2\\frac{1+\\gamma\\beta}{1+\\gamma+\\gamma\\beta} + \\hat{x}_1\\frac{\\gamma}{1+\\gamma+\\gamma\\beta} \\\\\n      x_1      &= \\hat{x}_1\\left(\\frac{1}{1+\\gamma\\beta}+\\frac{\\gamma^2\\beta}{(1+\\gamma\\beta)(1+\\gamma+\\gamma\\beta)}\\right)\n                  + \\hat{x}_2 \\frac{\\gamma\\beta}{1+\\gamma\\beta}\\frac{1+\\gamma\\beta}{1+\\gamma+\\gamma\\beta}\\\\\n               &= \\hat{x}_1\\frac{1+\\gamma}{1+\\gamma+\\gamma\\beta}+ \\hat{x}_2 \\frac{\\gamma\\beta}{1+\\gamma+\\gamma\\beta}\n   \\end{aligned}\n\\]"
  },
  {
    "objectID": "posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html#model-peer-effects-with-infinite-generations",
    "href": "posts/2023-01-23-peer-effects-norms-culture-sin-taxes.html#model-peer-effects-with-infinite-generations",
    "title": "Peer Effects, Culture, and Taxes",
    "section": "Model: peer effects with infinite generations",
    "text": "Model: peer effects with infinite generations\n\n\n\n\n\n\nPeer effects will cause inefficiently slow adjustment to price changes.\n\n\n\nWe model an infinite series of agents. Each has an ideal-point of zero, but pays some adjustment cost for departing from the prior level of consumption (\\(x_{t-1}\\)). Given some starting point (\\(x_0&gt;0\\)) we wish to know how rapidly consumption will converge to its long-run equilibrium.\n\\[\\begin{aligned}\n   \\utt{u_t}{agent $t$}{utility} &= -\\ut{x_{t}^2}{cost} - \\utt{\\gamma(x_{t}-x_{t-1})^2}{adjustment}{cost}  \\\\\n   \\utt{U}{social}{welfare} &= -\\sum_{t=1}^\\infty \\beta^t u_t\n\\end{aligned}\\]\nEach agent individually will simply bias their consumption towards last period’s consumption, and so the convergence factor will be simply \\(\\frac{\\gamma}{\\gamma+1}\\): \\[x_t=\\frac{\\gamma}{\\gamma+1}x_{t-1}.\\]\nHowever when maximizing social welfare we also need to take into account the effect on future utility. By taking the first-order condition with respect to \\(x_t\\) we can derive a Euler-equation relationship between \\(x_{t-1}\\), \\(x_t\\), and \\(x_{t+1}\\):\n\\[\\begin{aligned}\n      \\frac{dU}{dx_t}   \n         &= \\gamma\\beta^t (x_t-x_{t-1}) - \\gamma\\beta^{t+1}(x_{t+1}-x_t)+\\beta^t x_t =0 \\\\\n         &= (\\gamma+\\gamma\\beta+1)x_t -\\gamma x_{t-1}-\\gamma\\beta x_{t+1}\\\\\n     x_t &= x_{t-1}\\frac{\\gamma}{\\gamma+\\gamma\\beta+1}\n            + x_{t+1}\\frac{\\gamma\\beta}{\\gamma+\\gamma\\beta+1}\n\\end{aligned}\\]\nIf we conjecture exponential convergence, meaning \\(x_{t+1}=\\theta x_t\\), we can solve for the convergence factor \\(\\theta\\): \\[\n   \\begin{aligned}\n      x_t &= \\theta^{-1}x_t \\frac{\\gamma}{\\gamma+\\gamma\\beta+1}\n            + \\theta x_t\\frac{\\gamma\\beta}{\\gamma+\\gamma\\beta+1} \\\\\n    \\frac{\\gamma+\\gamma\\beta+1}{\\gamma} &= \\theta^{-1}+\\theta\\beta \\\\\n    \\theta^2\\ut{\\beta}{a} -\\theta \\ut{\\frac{\\gamma+\\gamma\\beta+1}{\\gamma}}{b} +\\ut{1}{c} &= 0 \\\\\n   \\end{aligned}\n\\]\nApplying the quadratic formula we get a solution for \\(\\theta\\): \\[\\begin{aligned}\n      \\theta   &=\\frac{-b\\pm\\sqrt{b^2-4ac}}{2a}\\\\\n               &= \\frac{-\\frac{\\gamma+\\gamma\\beta+1}{\\gamma}\\pm\\sqrt{\\frac{\\gamma+\\gamma\\beta+1}{\\gamma}^2-4\\beta}}{2\\beta}\n   \\end{aligned}\n\\]\nObservations:\n\n\n\n\n\n\nSimulation to confirm the solution. We let \\(\\beta=\\gamma=1\\), implying a law of motion of \\(x_{t+1}=3x_t-x_{t-1}\\), and as predicted \\(x\\) declines by a factor of 0.38 each generation.\n\n\n\n\nThe solution seems to be irreducibly quadratic. If \\(\\beta=1\\) then we can get a slightly simpler expression for the relationship between \\(\\theta\\) and \\(\\gamma\\): \\[\\theta^2 -\\theta\\frac{2\\gamma+1}{\\gamma}+1=0\\] \\[\\theta = \\frac{\\frac{2\\gamma+1}{\\gamma}\\pm\\sqrt{\\left(\\frac{2\\gamma+1}{\\gamma}\\right)^2-4}}{2}\\]\nIf \\(\\beta=1\\) (no discounting) and \\(\\gamma=1\\) (habituation and ideal-point are equally strong):\n\nDecentralized convergence: will not be forward-looking, and so each period will be half-way between prior and the long-run equilibrium: \\(\\frac{x_{t+1}}{x_t}=0.5\\).\nCentralized convergence: \\[\\theta = \\frac{3\\pm\\sqrt{3^2-4}}{2}=\\frac{3}{2}\\pm\\frac{\\sqrt{5}}{2}\n    = (2.618, 0.381966)\n\\]\nWe choose the root that is below 1 (the other root would be explosive), and so \\(\\frac{x_{t+1}}{x_t}=\\theta\\simeq 0.38\\), i.e. socially optimal convergence will be only somewhat faster than decentralized convergence in this case. If we add time-discounting I believe the difference between the two rates of convergence will get smaller.\n\nIf \\(\\beta=0\\) (infinite discounting) then \\(\\theta=\\frac{\\gamma}{\\gamma+1}\\), i.e. the centralized solution is the same as the decentralized solution, because we do not care about future generations.\nIf \\(\\beta=1\\) (no discounting) and \\(\\gamma=2\\) (adjustment costs strong) then decentralized convergence will have \\(\\frac{x_{t+1}}{x_t}=\\frac{2}{3}\\) and centralized convergence will have \\(\\frac{x_{t+1}}{x_t}=\\theta=\\frac{1}{2}\\)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "On Deriving Things\n\n\n\n\n\n\n\n\n\n\n\nNov 28, 2024\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nPremature Optimization and the Valley of Confusion\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2024\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nPeer Effects, Culture, and Taxes\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2024\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nBloodhounds and Bulldogs\n\n\nOn Perception, Judgment, & Decision-Making\n\n\n\n\n\n\n\n\nApr 27, 2024\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nThe Influence of AI on Content Moderation and Communication\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2023\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nThe History of Automated Text Moderation\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2023\n\n\nIntegrity Institute collaborators: Alex Rosenblatt, Jeff Allen, Ejona Varangu, Dave Sullivan, Tom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nThinking About Tradeoffs? Draw an Ellipse\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\nTom Cunningham, OpenAI.\n\n\n\n\n\n\n\n\n\n\n\nExperiment Interpretation and Extrapolation\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n\n\n\n\n\nAn AI Which Imitates Humans Can Beat Humans\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n\n\n\n\n\nSushi-Roll Model of Online Media\n\n\nPreviously: “pizza model”, “salami model”\n\n\n\n\n\n\n\n\nSep 8, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n\n\n\n\n\nHow Much has Social Media affected Polarization?\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n\n\n\n\n\nThe Paradox of Small Effects\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n\n\n\n\n\nRanking by Engagement\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n\n\n\n\n\nSocial Media Suspensions of Prominent Accounts\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2023\n\n\n\n\n\n\n\n\n\n\n\nOptimal Coronavirus Policy Should be Front-Loaded\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2020\n\n\n\n\n\n\n\n\n\n\n\nOn Unconscious Influences (Part 1)\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2017\n\n\n\n\n\n\n\n\n\n\n\nThe Work of Art in the Age of Mechanical Production\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2017\n\n\n\n\n\n\n\n\n\n\n\nRepulsion from the Prior\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2017\n\n\n\n\n\n\n\n\n\n\n\nThe Repeated Failure of Laws of Behaviour\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2017\n\n\n\n\n\n\n\n\n\n\n\nSamuelson & Expected Utility\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2017\n\n\n\n\n\n\n\n\n\n\n\nEconomist Explorers\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2017\n\n\n\n\n\n\n\n\n\n\n\nWeber’s Law Doesn’t Imply Concave Representations or Concave Judgments\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2017\n\n\n\n\n\n\n\n\n\n\n\nRelative Thinking\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2016\n\n\n\n\n\n\nNo matching items"
  }
]