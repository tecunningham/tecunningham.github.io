[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Tech Progress Dashboard ingestion log\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nKnowledge-Creating LLMs\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2026\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nLLM verification\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2025\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nForecasts of AI & Economic Growth\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2025\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nEconomics and Transformative AI\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2025\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nOn Deriving Things\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2025\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nToo Much Good News is Bad News\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2024\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nPremature Optimization and the Valley of Confusion\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2024\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nPeer Effects, Culture, and Taxes\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2024\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nBloodhounds and Bulldogs\n\n\nOn Perception, Judgment, & Decision-Making\n\n\n\n\n\n\n\n\nApr 27, 2024\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nThe Influence of AI on Content Moderation and Communication\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2023\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nThe History of Automated Text Moderation\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2023\n\n\nIntegrity Institute collaborators: Alex Rosenblatt, Jeff Allen, Ejona Varangu, Dave Sullivan, Tom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nThinking About Tradeoffs? Draw an Ellipse\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\nTom Cunningham, OpenAI.\n\n\n\n\n\n\n\n\n\n\n\nExperiment Interpretation and Extrapolation\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nAn AI Which Imitates Humans Can Beat Humans\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nSushi-Roll Model of Online Media\n\n\nPreviously: “pizza model”, “salami model”\n\n\n\n\n\n\n\n\nSep 8, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n\n\n\n\n\nHow Much has Social Media affected Polarization?\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n\n\n\n\n\nThe Paradox of Small Effects\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n\n\n\n\n\nRanking by Engagement\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nSocial Media Suspensions of Prominent Accounts\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2023\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nOptimal Coronavirus Policy Should be Front-Loaded\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2020\n\n\n\n\n\n\n\n\n\n\n\nOn Unconscious Influences (Part 1)\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2017\n\n\n\n\n\n\n\n\n\n\n\nThe Work of Art in the Age of Mechanical Production\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2017\n\n\n\n\n\n\n\n\n\n\n\nRepulsion from the Prior\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2017\n\n\n\n\n\n\n\n\n\n\n\nThe Repeated Failure of Laws of Behaviour\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2017\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nEconomist Explorers\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2017\n\n\n\n\n\n\n\n\n\n\n\nSamuelson & Expected Utility\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2017\n\n\n\n\n\n\n\n\n\n\n\nWeber’s Law Doesn’t Imply Concave Representations or Concave Judgments\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2017\n\n\n\n\n\n\n\n\n\n\n\nRelative Thinking\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2016\n\n\nTom Cunningham\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2025-10-19-forecasts-of-AI-growth.html",
    "href": "posts/2025-10-19-forecasts-of-AI-growth.html",
    "title": "Forecasts of AI & Economic Growth",
    "section": "",
    "text": "Validation Checks\n\nOverall: ⚠️ Warning\n\n✅ [35/35] Cited sources exist in posts/ai.bib (programmatic)\n✅ [25/25] Table rows have required fields (programmatic)\n✅ [25/25] QMD quotes match posts/ai.bib (programmatic)\n✅ [25/25] QMD growth values match posts/ai.bib (programmatic)\n⚠️ [33/35] Abstracts present for all cited sources (programmatic)\n❌ [15/18] Bib quotes present in local fulltext version (programmatic)\n\nLast checked: 2026-02-20"
  },
  {
    "objectID": "posts/2025-10-19-forecasts-of-AI-growth.html#footnotes",
    "href": "posts/2025-10-19-forecasts-of-AI-growth.html#footnotes",
    "title": "Forecasts of AI & Economic Growth",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI classified Epoch’s GATE model (Erdil et al. (2025)) as by “AI people”, though the authors are a mixture of academic economists and people who work in AI.↩︎\nIt seems to me quite plausible that these papers over-estimate the productivity impact of existing LLMs: (1) the AB tests showing productivity improvements are on unrepresentatively self-contained tasks and are likely distorted by publication selection; (2) the Eloundou et al. (2023) estimates of very large time-savings from GPT-4 are based just on intuitions.↩︎\nComin and Mestieri (2014) say “the average adoption lag across all technologies (and countries) is 44 years,” but since the 1950s it has been 7-18 years.↩︎\n“Between 1 and 5% of all work hours are currently assisted by generative AI, and respondents report time savings equivalent to 1.4% of total work hours. … implies a potential productivity gain of 1.1%.”↩︎\nSuppose the total valuation of AI-related companies is $10T, which is perhaps around 10% of all capital stock. Using P/E of 15, a $10T valuation implies a stream of $600B in earnings/year, which is 2% of GDP.↩︎"
  },
  {
    "objectID": "posts/data/tech-progress-dashboard.llm/ingest-log.llm.html",
    "href": "posts/data/tech-progress-dashboard.llm/ingest-log.llm.html",
    "title": "Tech Progress Dashboard ingestion log",
    "section": "",
    "text": "Source post: posts/2026-02-14-tech-progress-dashboard.llm.qmd\nSource seed CSV: posts/data/tech-progress-dashboard.llm/seed-series.llm.csv\nSeed rows parsed from CSV: 283\nSeries replaced with fresh OWID downloads: (none)\nDerived AI/ML trend rows added: 46\nFinal normalized rows: 329\nFinal unique series: 16\nJS bundle written: posts/data/tech-progress-dashboard.llm/tpd-data.llm.js\n\n\n\n\nIf OWID download fails, seed rows are retained for those series.\nDerived AI/ML trend indexes are formula-based from Epoch growth-rate statements and are not raw point extracts.\nDashboard reads window.TPD_DATA from the generated JS bundle."
  },
  {
    "objectID": "posts/data/tech-progress-dashboard.llm/ingest-log.llm.html#notes",
    "href": "posts/data/tech-progress-dashboard.llm/ingest-log.llm.html#notes",
    "title": "Tech Progress Dashboard ingestion log",
    "section": "",
    "text": "If OWID download fails, seed rows are retained for those series.\nDerived AI/ML trend indexes are formula-based from Epoch growth-rate statements and are not raw point extracts.\nDashboard reads window.TPD_DATA from the generated JS bundle."
  },
  {
    "objectID": "posts/2025-12-30-llm-verification.html",
    "href": "posts/2025-12-30-llm-verification.html",
    "title": "LLM verification",
    "section": "",
    "text": "A prediction: people will move towards producing documents that are machine-verified. A document will come with a checklist so you can see that it satisfies certain properties, as verified by LLMs:\n\n\n\n\n\nClaude\nGemini\nGPT\n\n\n\n\nFactual claims are accurate\n✅\n✅\n✅\n\n\nLogically consistent\n✅\n✅\n✅\n\n\nCentral idea is novel\n✅\n✅\n✅\n\n\nThe writing is readable\n✅\n✅\n✅\n\n\n\n\nIf your blog post starts with this checklist I’ll be more likely to read it.\nThis is already happening for mathematicians and programmers: they verify LLM-produced proofs with a formal verification tool (e.g. Lean), and LLM-produced code with unit tests. I’m predicting that this pattern will spread to all other areas of knowledge work, as LLMs get better at verifying correctness.\n\nNotes\n\nA corollary: there’s a magic prompt.\n\nInstead of saying “answer question Q”, it’s better to say “answer question Q, and give me a way of verifying that the answer is correct.”\nYou want the LLM to give you a checklist like the one above, decomposing the verification into many subproblems. Programmers have learnt to prompt “write a program to do P, and a set of tests to verify that it does P.”\n\nExamples of criteria you want to check:\n\n\nAn infection prevention plan: verify that the plan is consistent with the relevant protocols.\nA tax return: verify that each of the IRS rules are satisfied.\nA legal memo: verify that citations are accurate.\nAn insurance claim: verify that the claim answers all relevant questions.\nAn insurance decision: verify that the decision is consistent with close precedents.\n\n\nAn analogy with two friends.\n\nYou have one friend who is full of new ideas, you have another friend who can tell whether an idea is good or bad. Each friend is somewhat useful, but when you combine them they’re amazing. (I think this is the case for mathematicians: LLMs will produce a fountain of proofs, and Lean can distinguish which are sound or unsound).\n\nImplication: credentials become less important.\n\nMany people are saying that LLMs will make credentials more important, because they make it harder to superficially distinguish high-quality and low-quality work. Ryan Briggs says:\n\n“Prediction: in the short-to-medium term LLMs will make the reputation of the researcher matter more for whether or not we view results as credible because it will become too hard to read everything and people will want shortcuts for filtering. Again, this hits juniors hardest.”\n\nIt’s possible this is true but there’s a countervailing force: LLMs are better paper-writers, but also better referees. In fact they may be relatively better referees than they are authors, which would shift balance in favor of the less-credentialed. If we had a perfect test for the quality of work, we wouldn’t need to rely on reputation at all.\nIf someone entirely unqualified makes a breakthrough in ML or mathematics they can verify it. Historically this has been much harder in soft disciplines like economics, but if the cost of verification falls to zero.\nA related point: in an old post on AI and communication I argued that with LLMs reputation will become less important for internal properties (where the ground truth is human judgment, i.e. verification is cheap), more important for external properties (where the ground truth is in the world, i.e. verification is expensive).\n\n\n\n\nA More Precise Story\n\nDifferent domains have different costs of verification:\n\nCheap to verify: whether an image looks good, whether a joke is funny, whether a sudoku solution is valid, whether a formalized proof is sound, whether code passes a specific test.\nCostly to verify: whether a medical paper works, whether an academic paper is high quality, whether a human-written proof is sound, whether code fulfills a specification.\n\nLLM-verification will be a big benefit in domains where it’s costly to verify.\nThere is a complementarity between LLM-generation and LLM-verification, the value of both is more than the sum of the value of each.\nWhen doing LLM-generation it’s useful to ask the LLM to self-verify. E.g. by (1) generating a Lean proof and validating it; (2) generating unit tests and running them; (3) generating a checklist and asking an independent LLM to check each box.\nLLM-generation can hurt communication equilibria where verification is costly, when LLM generation lowers the cost of accidental attributes (not essential attributes). E.g. if LLMs make it cheap to fix spelling errors, or to adopt idioms of the discipline, then there will be less separation in equilibrium.\n\n\n\nFormal Models\nA couple of very hasty models to sketch how to formalize this. It would be nice to have a single model which incorporates all the mechanisms above.\n\nModel 1: quality vs polish.\n\nSuppose you care just about intrinsic quality \\(q\\), but your signal is \\[s=q+p\\] where \\(p\\) is polish. You know that \\(q\\) and \\(p\\) are positively correlated (better books have better covers), so \\(s\\) is a highly reliable signa of quality.\nSuppose LLMs lower the cost of polish, so now everyone has high \\(p\\). This makes the signal-extraction problem worse, and you’ll rely relatively more on another signal, e.g. the author’s reputation (assuming it’s another signal correlated with \\(q\\)).\nSuppose instead that LLMs lower the cost of directly observing quality \\(q\\). This will then imply putting relatively less weight on the author’s reputation.\nImplications:\n\nLLMs lowering the cost of polish will cause more weight to be put on reputation.\nLLMs lowering the cost of verification will cause less weight to be put on reputation.\n\n\nModel 2: search.\n\nYou have \\(N\\) ideas with unobserved iid payoffs, and you can pay cost \\(c\\) to find the true payoff (AKA Weitzmann’s Pandora’s box problem).\nClaim: there’s a complementarity between the number of ideas you have (\\(N\\)) and the cheapness of verification (inverse of \\(c\\)). Formally:\n\\[\\begin{aligned}\n  V_n(c)   & =\\int_{0}^{\\sigma(c)} \\big(1-F(t)^n\\big)dt,\n     && \\text{(expected value from optimal strategy)}\\\\\n  E[(X-\\sigma)^+] &= c && \\text{(implicit definition of $\\sigma(c)$)}\n\\end{aligned}\\]\nFrom inspection we can see that the expression has a complementarity between \\(c\\) and \\(n\\)."
  },
  {
    "objectID": "posts/2026-01-29-knowledge-creating-llms.html",
    "href": "posts/2026-01-29-knowledge-creating-llms.html",
    "title": "Knowledge-Creating LLMs",
    "section": "",
    "text": "Thanks to Zoë Hitzig & Parker Whitfill, among others, for helpful comments."
  },
  {
    "objectID": "posts/2026-01-29-knowledge-creating-llms.html#footnotes",
    "href": "posts/2026-01-29-knowledge-creating-llms.html#footnotes",
    "title": "Knowledge-Creating LLMs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMany other technologies share knowledge – speaking, writing, printing, the internet – LLMs just continue this progression but further lower the costs of sharing.↩︎"
  },
  {
    "objectID": "posts/2025-09-13-recursive-self-improvement-explosion.html",
    "href": "posts/2025-09-13-recursive-self-improvement-explosion.html",
    "title": "Recursive Self-Improvement",
    "section": "",
    "text": "TL;DR: an explosion is AI capabilities is possible but it’s intrinsically difficult to forecasat.\n\nDefine an explosion as a significant acceleration of the historical rates of progress in AI, e.g. measured by effective compute required for a given level of accuracy (e.g. perplexity).\n\nProgress in AI is rapid but smooth.\n\nWe can measure progress in various ways, algorithmic progress has accelerated over the last 10-15 years. Epoch estimate that algorithmic efficiency has been growing at 3X/year between 2012-2024, measured by effective compute.\n\nForecasters expect an 8-20% chance of an explosion.\n\nThey define an explosion as a 3X speedup in the historical rate of progress by some measure, from the METR/FRI survey.\n\nAI has already been accelerating AI research.\n\nAI has been self-accelerating for decades: (1) automatic differentiation; (2) bayesian optimization of hyperparameters; (3) neural architecture search; (4) LLM coding autocomplete and chatbots; (5) LLM coding agents.\n\nThere’s good theoretical reason to expect an explosion.\n\n(…)\n\nAI usefulness for optimization depends on features of the setup.\n\nHassabis says AI will make progress wherever there’s (1) combinatorial search space; (2) clear feedback; (3) lots of data or an automatic validator.\nHowever there’s clearly a fourth condition: that the data has some lower-dimensional latent structure. There are many problems that satisfy the first 3 but where we don’t expect substantial progress from autonomous NNs: (A) the telephone book; (B) mapping out stars in the sky; (C) documenting the genome.\n\nAI speedups to discovery depends on the shape of the underlying landscape.\n\n(abc)"
  },
  {
    "objectID": "posts/2025-09-13-recursive-self-improvement-explosion.html#summary-1",
    "href": "posts/2025-09-13-recursive-self-improvement-explosion.html#summary-1",
    "title": "Recursive Self-Improvement",
    "section": "summary",
    "text": "summary\n\nGoal: forecast when we’re likely to get explosive intelligence growth.\nSummary: once you automate a sufficient number of tasks (do them with capital) then it’ll kick off an intelligence explosion.\nBasic model:\n\nIf \\(\\dot{S}_t=S_t^{1-\\beta}\\), so you get explosion if \\(\\beta&lt;0\\), steady-state growth if \\(\\beta=0\\), and gradual slowing if \\(\\beta&gt;1\\).\nNow if you start automating inputs, the effective exponent gradually gets higher.\n\nMulti-sector version.\n\nYou gradually automate some of the things, so they can be done with capital.\nYou have spillovers between different processes.\n\nEstimates of \\(\\beta\\):\n\nBloom overall \\(\\beta=3\\)\nFor software R&D they estimate \\(\\beta=3\\)\nthey say in software \\(\\beta=0.1\\)\n\nNOTE: \\(\\beta\\)\nQuestions:\n\nAny historical domain where we’ve seen regimes of \\(\\beta&lt;0\\)?"
  },
  {
    "objectID": "posts/2025-09-13-recursive-self-improvement-explosion.html#my-observations",
    "href": "posts/2025-09-13-recursive-self-improvement-explosion.html#my-observations",
    "title": "Recursive Self-Improvement",
    "section": "my observations",
    "text": "my observations\n\nQ: how to think about the growth effect of uplift vs automation?\nAutomation makes things free, rather than being produced by capital."
  },
  {
    "objectID": "posts/2025-09-13-recursive-self-improvement-explosion.html#tom-houlden-questions",
    "href": "posts/2025-09-13-recursive-self-improvement-explosion.html#tom-houlden-questions",
    "title": "Recursive Self-Improvement",
    "section": "tom houlden questions",
    "text": "tom houlden questions\nQ: is this graphical system novel?\nQ: is there a nice metaphor/analogy to think about automation/explosion?"
  },
  {
    "objectID": "posts/2025-09-13-recursive-self-improvement-explosion.html#metaphors",
    "href": "posts/2025-09-13-recursive-self-improvement-explosion.html#metaphors",
    "title": "Recursive Self-Improvement",
    "section": "metaphors",
    "text": "metaphors\n\nDrawing balls from an urn.\n\nThere’s some distribution of values, then you get a very nice expression. The expected value of \\(N\\) draws just depends on the extreme value distribution of \\(f(v)\\). This is exactly Kortum (1997).\n\nA tool factory makes better tools.\n\nI think this is Jones’ metaphor. Distinct from Kortum, because the returns to search now depends on the stock of ideas.\n\nMachine tools and regular tools\n\n\nThe Dutch make machine tools.\nThe machine tools make regular tools.\nThe regular tools make products.\n\nAt some point the machine tools become good enough to make themselves, but it’s a discrete jump.\n\nLego - combining ideas.\n\nWeitzmann’s recombinant search is like this: you combine ideas to make new ideas, now you have a larger stock of ideas to combine.\n\nRecipes.\n\nYou try out recipes, which are combinations of prior recipes. Jones (2023) – something like a reconciliation of Weitzman & Kortum.\n\nBlacksmith.\n\nYou have a hammer and you spend time making horseshoes, or working on a new hammer.\nA harder hammer both (1) makes horseshoes faster, or (2) makes your hammer still harder."
  }
]