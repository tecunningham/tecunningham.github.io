[
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html",
    "href": "posts/2023-01-31-social-media-suspensions-data.html",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "",
    "text": "Tom Cunningham. (@testingham) First version Jan 31 2023, last updated April 12 2023.11 Thanks to comments from Sahar Massachi, Katie Harbath, Nichole Sessego, and many others.\nThis note describes the suspension practices of the major social media platforms. I have collected a dataset of around 200 suspensions of prominent people across 12 platforms, stored in a google spreadsheet. The chart below summarizes the full dataset:\nThe data helps illuminate what platforms are doing. It is very difficult for an outside observer to see how a platform moderates their content. The advantages of studying the suspension of prominent users are that (1) the data is public and (2) the outcomes are comparable across platforms.\nKey findings.\nI am working on a separate essay about why platforms suspend users. It is difficult to give clear reasons why platforms suspend users. In a separate essay I try to break down how much their action can be attributed to influence from owners, from employees, from users, from advertisers, or from governments. Having this dataset of suspensions is very useful to be able to make generalizations about platform behavior."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#politicians",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#politicians",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Politicians",
    "text": "Politicians\nAmong US Federal politicians only Republicans have been suspended. In the US 8 Republicans have had one or more suspension, but no Democrats. Among the Republicans the suspensions were for a variety of reasons: related to the Jan 6 riots (Trump, Barry Moore, MTG), related to COVID (Ron Johnson, Rand Paul, MTG), for misgendering (Jim Banks), for tweeting a threat (Briscoe Cain), for animal blood on a profile photo (Steve Daines), one by a rogue employee (Trump).\nIt seems to me that the asymmetry in suspensions is primarily due to Republicans being more likely to violate the policies, rather than asymmetric enforcement of existing policies. I am not aware of any cases where a Democratic politician violated one of these policies but was not suspended.\n\n\n\n\n\nSuspension of national politicians outside the US has been relatively rare. My dataset contains 13 national politicians who were suspended in the world outside the US, compared to 8 in the US. This is a big asymmetry, and something of a puzzle. I have discussed this with a number of people who worked in enforcement and they attribute to a mixture of (1) less policy-violating behaviour from non-US politicians; (2) looser enforcement against non-US politicians; (3) lower overall social media usage outside the US; and (4) lower coverage of non-US politicians in my dataset."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#us-prominent-figures",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#us-prominent-figures",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "US Prominent Figures",
    "text": "US Prominent Figures\nThis shows all suspensions of US “notable people”:\n\n\n\n\n\nBetween 2015 and 2017 there were a series of alt-right personalities suspended from Twitter. The suspensions were often not for their views but their behaviour:\n\n2015: Charles Johnson from Twitter for a threat.\n2016: Milo Yiannopoulos from Twitter for harassment, Richard Spencer from Twitter for manipulation.\n2017: Roger Stone from Twitter for abuse.\n2018: Alex Jones from Twitter for incitement and abuse.\n\nBeginning in late 2017 more alt-right accounts were suspended. Either for hate speech, for offline behaviour, or without any public reason given:\n\nLate 2017: Baked Alaska from Twitter for hate speech.\n2018: Owen Benjamin from Twit with no reason given, Alex Jones from FB and YouTube for hate speech.\n2019: Nick Fuentes from Meta with no reason given.\n\nBetween November 2020 and January 2021 a large set of prominent figures were suspended for election-related reasons. The most suspensions were on Twitter but there were also from other platforms.\n\nSince November 2022 Twitter has unsuspended a large fraction of the suspended users that I track, probably around 1/2.\nSome people have been suspended simultaneously across multiple platforms (e.g. Alex Jones)"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#meta",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#meta",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Meta",
    "text": "Meta"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#twitter",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#twitter",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Twitter",
    "text": "Twitter\n\n\n\n\n\nThe following chart shows just accounts that were un-suspended under Musk, i.e. people with Twitter suspension that started before Oct 27 2022 and ended after that date. See below for a more fine-grained dataset of accounts unsuspended under Musk.\nYou can see that the primary original reasons for suspension were hate speech COVID misinformation. Kanye West and Nick Fuentes were re-suspended under Musk."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#youtube",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#youtube",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "YouTube",
    "text": "YouTube"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#tik-tok",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#tik-tok",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Tik Tok",
    "text": "Tik Tok"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Other Platforms",
    "text": "Other Platforms"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#hate-speech",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#hate-speech",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Hate Speech",
    "text": "Hate Speech"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#twitter-1",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#twitter-1",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Twitter",
    "text": "Twitter\nWikipedia page on Twitter Suspensions. Wikipedia has a list of around 400 Twitter suspensions. I chose not to create my own database (partly drawing from Wikipedia) for a few reasons: (1) I would want to add a lot of annotations to the Wikipedia data, e.g. about reasons for suspension or types of suspension. (2) Parsing the data is nontrivial: date ranges are given in various formats and would require some work on a regex to parse consistently. (3) There is some missing and inconsistent data, e.g. it has Trump’s suspension start-date but not end-date, and the names of people are not consistent (e.g. sometimes “Donald Trump”, sometimes “Donald J Trump”).\nThe Wikipedia dataset shows a similar basic pattern to what I document above: a dramatic increase in the rate of suspensions around mid-2017\n\n\n\n\n\nWikipedia-reported Twitter suspension by year\n\n\n\n\n\n\n\n\n\nAll Wikipedia-reported Twitter suspension, highlighting accounts with more than 1M followers (not all suspensions list the number of followers).\n\n\n\n\nTravis Brown: Twitter Watch This project appears to have data on almost all suspensions on Twitter since Feb 2022, and also tracks whether the suspension have been reversed. It does not include any suspensions which started prior to Feb 2022. There is a giant CSV file with 600K rows, suspensions.csv. Some visualizations:\n\n\n\n\n\nObservations by date of suspension\n\n\n\n\n\n\n\n\n\nObservation by date of unsuspension\n\n\n\n\n\n\n\n\n\nObservation by date of account creation\n\n\n\n\n\n\n\n\n\nSuspensions for accounts with &gt;1M followers\n\n\n\n\nTravis Brown: Twitter Unsuspensions. This is a collection of users who Twitter has un-suspended since Oct 27 2022 (when Musk took over). For some accounts there is a date of suspension but some have missing dates, I think suspension-date is only observed if after Feb 2022. (The content of this dataset is neither a subset nor a superset of the previous daatset). Unfortunately the dataset doesn’t have follower-count or twitter handle, so it’s not easy to join with other datasets or find the most prominent accounts.\n\n\n\n\n\nObservations by date of suspension\n\n\n\n\n\n\n\n\n\nObservations by date of unsuspension\n\n\n\n\nTravis Brown: Deleted Tweets / Suspended Accounts. This project scrapes profiles from the Wayback Machine, and seems to have a large set of accounts that were suspended with fairly long retention, I have not yet investigated further.\nTwitter Transparency Reports. This has data on the aggregate number of suspensions per half between July 2018 and Dec 2021. Note that the website is down but the CSV files can still be downloaded. \n\n\n\n\n\nTotal Accounts Suspended on Twitter by Reason, 2018H2-2021H2\n\n\n\n\nCounterHate list of unsuspensions. The organization CounterHate has a list of 10 large accounts reinstated by Twitter since Musk’s takeover. Note I believe they incorrectly listed Rizza Islam as an account re-activated by Twitter: I can find no evidence that the acccount @RizzaIslam was ever suspended, it seems to have been continuously tweeting from November 2022 through Feb 2023. I have added all 10 accounts to my database, and checked activity across all platforms."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#youtube-1",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#youtube-1",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "YouTube",
    "text": "YouTube\nWikipedia page on YouTube suspensions. See above for reasons why I chose not to use this dataset as the primary source.\nWikitubia: Terminated YouTubers. A list of around 2300 YouTubers that have been permanently banned, including date of ban, subscribers, reason for ban, and citation. They don’t have a date when unbanned."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#meta-facebook-instagram",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#meta-facebook-instagram",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Meta / Facebook / Instagram",
    "text": "Meta / Facebook / Instagram\nThere is no Wikipedia page of suspensions on Facebook, Instagram or WhatsApp.\nMeta’s “Community Standards Enforcement Report” is shown below. Meta’s data does not include any data on account suspensions, however there are a few other patterns of interest.\n\nContent actioned is relatively stable. There are fairly few notable upward or downward trends across the different types of content actioned: terrorism content actioned has increased significantly on both platforms, hate speech actions increased up to the end of 2020, then declined.\nThe proactive detection rate is close to 100% for most categories. there were dramatic improvements for bullying and for hate speech over 2017-2021. Note that the proactive detection rate is the share of actioned content that is automatically detected, the share of true positives that are automatically detected is surely much lower.\nThe prevalence of volations has fallen significantly. The log axis diminishes the magnitude of the decline: prevalence has fallen by a factor of 2-5 for nudity, bullying, hate speech, and graphic content. (I only show the prevalence upper bound, but the lower bound generally tracks the same course).\n\n\n\n\n\n\nFacebook’s dangerous organizations list. This list was leaked in 2021 by the Intercept. Unfortunately it does not include the dates of when each organization was added. The list is organized into the following categories:\n\nTerror Organizations (e.g. Islamic State)\nCrime Organizations (e.g. Bloods, Crips)\nHate Organizations (e.g. Aryan Nation, includes bands and websites)\nMilitarized Social Movements (e.g. United States Patrio Defense Force)\nViolent Non-State Actors (e.g. Free Syrian Army)\nHate (e.g. David Duke)\nIndividuals: Crime (e.g. Denton Suggs, Gangster Disciples)\nIndividuals: Terror (e.g. Osama bin Laden)"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#tiktok",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#tiktok",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "TikTok",
    "text": "TikTok\nCommunity Standards Report. Shows an increase in suspensions from around 1M accounts/quarter per 2020 to 6M accounts/quarter in 2023."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#twitch",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#twitch",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Twitch",
    "text": "Twitch\nStreamerBans. They seem to have a pretty comprehensive database of bans on Twitch."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms-1",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms-1",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Other Platforms",
    "text": "Other Platforms\n\nSpotify. The only unambiguous suspension from Spotify I found was Alex Jones’ podcast. Spotify removed some episodes of Joe Rogan’s podcast, and removed R Kelly and XXXtentacion’s music from playlists. They remove some white-supremacist artists and music. They removed all music from the band LostProphets after their lead singer was convicted of child sexual abuse.\nSubstack. I’m not aware of anybody who’s been kicked off Substack, they present themselves as very pro-free-speech.\nReddit. I’m not aware of any data on reddit account suspensions.\nRumble. The Rumble video-hosting platform has become quite large (they claim 70M MAU, and have a market cap of ). Their terms of service restrict content that is “abusive, inciting violence, harassing, harmful, hateful, anti-semitic, racist or threatening.” However I have not yet found a single example of a prominent user who has been suspended from Rumble."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#other-data-sources",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#other-data-sources",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Other Data Sources",
    "text": "Other Data Sources\nCan find more suspensions by searching Wikipedia for “suspended from XXX”. E.g. site:wikipedia.org \"suspended from facebook\". Possibly worth doing the same search for Google News.\nSocialBlade has data on number of followers by month since 2018, across Twitter, FB, YouTube. I’m not sure how easy it would be to scrape this data. They have a paid API, they say “up to 3 years of Historical statistics on creators.” However the website seems to have data back to at least April 2018.\nBallotpedia list of elected officials suspended from social media. It is an excellent resource, appears comprehensive and cites original reporting. I have added all of their data to the database as of January 2023.\nGlobal Internet Forum to Counter Terrorism (GIFCT). They mainly work on sharing hashes of terrorist content between platforms. They have some dicussion papers about “terror designation lists” but I don’t think they maintain any lists themselves.\nSpecially Designated National / Global Terrorist (SDN/SDGT). This is a public list maintained by the US government, and consumed by a number of tech companies. The full history is available, but it would be extremely difficult to parse.\nLumen. This has an international database of government takedown requests. They also seem to include whether the request was honored.\nCCDH Disinformation Dozen. This is a list from March 2021 of prominent accounts who were spreading anti-vax information on social media: original report, followup report from April 2021). They also have a “toxic ten” report. It’s probably worth adding both lists to the database."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#third-party-sources-on-platform-policies",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#third-party-sources-on-platform-policies",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Third-Party Sources on Platform Policies",
    "text": "Third-Party Sources on Platform Policies\nThere are a variety of third-party resources comparing policies across platforms, however none seem to have data comparable to the list above, i.e. a summary of specific content policy changes over time.\nComparisons at a single point in time.\n\nDNC (2020) Comparison of Misinformation Policies, 2020\nConsumer Reports (Aug 13 2020) Comparison of Misinformation Policies, 2020. Data as of 2020, with three levels: “allowed”, “sometimes”, and “prohibited”.\nUNC CITAP (May 22 2020) Comparison of Misinformation Policies, 2020. Has tables comparing misinfo policies as of 2020, three levels: “prohibited”, “flagged”, “allowed.”\nElection Integrity Partnership (Oct 28 2020) Comparison of Election Policies, 2020.\nCarnegie Endowment (April 1 2021) Existence of Policies, 2021. Table just marks whether a platform has a policy on some type of content, not nature of policy. They also they have a database of platform policies but it seems to only have data from February 2021.\nVirality Project Comparison of COVID Vaccine Policies in 2021.\n\nPolicies tracked over time.\n\nMchangama, Fanlo and Alkiviadou (2023) Scope Creep: An Assessment of 8 Social Media Platforms’ Hate Speech Policies. They document the hate speech policies over time for 8 platforms using a consistent rubric. They document that hate speech policies have become more broad-reaching over time. The data is available in Excel sheets here.\nKatie Harbath and Collier Fernenkes (August 2022) Election Policy Announcements, 2003-2022. Google spreadsheet with links to around 600 policy announcements, organized by platform, author, date, product-type, and country. Focussed on election-related policies, and they don’t include summaries of the policy announcement. They also wrote up analyses: (1) “A Brief History of Tech and Elections”; (2) 2022 election announcements.\nRanking Digital Rights Index, Comparison of Privacy and Transparency Policies, 2017-2022. They collect perhaps 100 different indicators across around 15 tech companies, mostly related to privacy and transparency, earliest data from 2017. All the data is available.\nGLAAD Comparison of LGBTQ user safety, 2021-2022\nCELE, Letra Chica. Tracks all public policy changes on Meta, YouTube, and Twitter. Most data from May 2020, but they go back to 2019 for Facebook by using FB’s Transparency Center. Each policy update includes a short summary of what’s changed. Tracks both Spanish and English versions. Data stored on coda.io, I think it’s queryable.\nLinterna Verdes, Circuito. Has about 15 in-depth case studies of platform moderation decisions.\nHumboldt Institute, Platform Governance Archive. Comprehensive archive of ToS, Privacy Policy, and Community Guidlines, from 2004 until late 2021, for FB, IG, Twitter, and YouTube. The data will not be updated.\nOpen Terms Archive. Started by the French Ambassador for Digital Affairs, but now a collaboration. Tracks terms for many different online services in a github repo. The Platform Governance Archive has moved to be part of this project, here.\nEFF, TOSback. Database of historical ToS documents from different services, with cross-platform comparisons. The most recent updates seem to be from May 2021, possibly was succeeded by Open Terms Archive.\nEuropean Commission, Copyright Content Moderation and Removal. This PDF report includes a lot of work which maps the copyright policies of major platforms.\n\nNarrative histories:\n\nCatherine Buni and Soraya Chemaly (2016, the Verge) History of Moderation.\nSarah Jeong (2016, Vice) The History of Twitter’s Rules\nBergen (2022) Like, Comment, Subscribe. A book on the history of YouTube, it has a lot of detail on policy changes."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-film-and-television",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-film-and-television",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Exclusions in Film and Television",
    "text": "Exclusions in Film and Television\n\n\n\n\n\n\n\n\n\n\n\nalleged crime\nresult\n\n\n\n\n1920s\nFatty Arbuckle\nrumors of immorality\nfilm industry blacklisted\n\n\n\n\n\n\n\n\n1940s\nOrson Welles\ncommunist associations\nblacklisted, moved to Switzerland\n\n\n\nDalton Trumbo\ncommunist associations\nblacklisted\n\n\n\n(around 100 people)\ncommunist associations\nblacklisted for a decade\n\n\n\n\n\n\n\n\n1950s\nCharlie Chaplin\ncommunist associations\nbanned from US\n\n\n\nElia Kazan\ntestifying before HUAC\nlost some relationships in Hollywood\n\n\n\n\n\n\n\n\n1960s\nJane Fonda\nopposition to Vietnam war\nblacklisted\n\n\n\n\n\n\n\n\n1970s\nRoman Polanski\nrape of 13yo girl\nmild disapproval from Hollywood\n\n\n\n\n\n\n\n\n1990s\nO J Simpson\nmurdered his wife\nblacklisted\n\n\n\nWoody Allen\nmolested 7yo daughter\n\n\n\n\n\n\n\n\n\n2000s\nMel Gibson\nracism & anti-semitism\n“blacklisted in Hollywood for almost a decade”\n\n\n\nMira Sorvino\nrejecting Harvey Weinstein\nblacklisted\n\n\n\nRose McGowan\nrejecting Harvey Weinstein\nblacklisted\n\n\n\nIsaiah Washington\nhomophobic remarks\nblacklisted\n\n\n\nMichael Richards\nracist remarks\nblacklisted\n\n\n\nKathy Griffin\n“told Jesus to suck it”\nbanned from talk shows and TV appearances\n\n\n\nSean Penn\nopposition to Iraq war\ndropped from movie\n\n\n\n\n\n\n\n\n2010s\nBill Cosby\nsexual assault\nblacklisted (& later imprisoned)\n\n\n\nHarvey Weinstein\nsexual assault\nblacklisted (& later imprisoned)\n\n\n\nStacy Dash\nconservative advocacy\nblacklisted\n\n\n\nKirk Camerson\ncriticism of homosexuality\nblacklisted\n\n\n\nJames Woods\nanti-Obama tweets\nblacklisted\n\n\n\nCeeLo Green\nsexual assault\nblacklisted\n\n\n\nLouis CK\nsexual harassment\nblacklisted\n\n\n\nKathy Griffin\nphoto with head of Trump\nfired by CNN, lost endorsement, cancelled tour\n\n\n\nT J Miller\nsubstance abuse, sexual assault\nblacklisted\n\n\n\nGina Carano\npolitical social media posts\nfired from TV show\n\n\n\nKevin Spacey\nsexual harassment\nlost roles in films\n\n\n\nJussie Smollett\nlied about an attack\nlost roles in TV shows\n\n\n\nNeil deGrasse Tyson\nrape, sexual harassment\ntemporarily lost roles in TV shows\n\n\n\nRoseanne Barr\nracist tweet\nlost TV show\n\n\n\n\n\n\n\n\n2020s\nWill Smith\nslapping someone at Oscars\nfilm projects put on hold\n\n\n\nJohnny Depp\ndomestic violence\nlost roles in films\n\n\n\nAmber Heard\ninvolvement in trial w Johnny Depp\nlost roles in films\n\n\n\nJustin Roiland\nsexual harassment & abuse\nlost roles in shows"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-music",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-music",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Exclusions in Music",
    "text": "Exclusions in Music\n\n\n\n\n\n\n\n\n\n\n\nalleged crime\nresult\n\n\n\n\n1940s\nPaul Robeson\ncommunist associations\nblacklist and passport revoked\n\n\n\n\n\n\n\n\n1950s\nLeonard Bernstein\ncommunist associations\nbrief blacklist\n\n\n\nLena Horne\ncommunist associations\nblacklist\n\n\n\nPete Seeger\ncommunist associations\nblacklist\n\n\n\n\n\n\n\n\n1960s\nBeatles\nsaying they’re bigger than Jesus\nconsumer boycott\n\n\n\nLovin Spoonful\ncooperating with FBI\nmusic industry boycott\n\n\n\nNina Simone\n“Mississippi Goddam”\nboycott in the South\n\n\n\nJohn Lennon\ncriticism of US and Vietnam war\nrefused entry into US\n\n\n\nEartha Kitt\ncriticism of Vietnam war\nblacklist through LBJ and CIA\n\n\n\n\n\n\n\n\n1970s\nSex Pistols\ncriticizing the Queen, swearing on TV\nbanned by the BBC, dropped by EMI\n\n\n\n\n\n\n\n\n1980s\nNWA\n“Fuck the Police” & similar songs\nradio station boycott, police boycott\n\n\n\n\n\n\n\n\n1990s\nBruce Springseen\nsong against police brutality\nbrief police boycott\n\n\n\nMarilyn Manson\ntransgressive lyrics\nbanned from performing in some states\n\n\n\nBody Count\nsong “cop killer”\nalbum withdrawn and reissued\n\n\n\n\n\n\n\n\n2000s\nDixie Chicks\nfor opposition to Iraq war\nblacklisting and consumer boycott\n\n\n\nJanet Jackson\nshowing nipple\nVH1, MTV, & Viacom radio stopped playing her music\n\n\n\nR Kelly\nsexual abuse\nbroad blacklist\n\n\n\nChris Brown\ndomestic violence\nweak boycott and blacklist\n\n\n\n\n\n\n\n\n2010s\nLostprophets\nsexual abuse\nbroad blacklist\n\n\n\nMichael Jackson\nchild molestation\nsome radio stations stop playing music\n\n\n\n\n\n\n\n\n2020s\nBeyonce\nsong against police brutality\nbrief police boycott\n\n\n\nMorgan Wallen\nusing n-word\ntemporarily dropped from radio/streaming playlists\n\n\n\nKanye West\npraise of Hitler\nlost sponsors\n\n\n\nOthers.\n\nIn radio: Father Coughlin, Rush Limbaugh, Don Imus fired from CBS for calling womens’ basketball team “nappy-headed hos”, Howard Stern fired from various radio shows for comments.\nIn sport. Colin Kapaernick blacklisted from NFL for kneeling for the anthem. Pete Rose banned from MLB for gambling.\nNazi sympathisers/collaborators. Charles Lindbergh, Henry Ford, Charles Coughlin, PG Wodehouse, Ezra Pound.\nWriters: DH Lawrence, Henry Miller, Salman Rushdie (Nicole Bonoff).\nJournalists. Jeffrey Toobin (New Yorker writer masturbated on zoom call),\nNote on R Kelly disappearing from radio"
  },
  {
    "objectID": "posts/2017-09-27-work-of-art-age-mechnical-production.html",
    "href": "posts/2017-09-27-work-of-art-age-mechnical-production.html",
    "title": "The Work of Art in the Age of Mechanical Production",
    "section": "",
    "text": "When I heard about the neural nets that copy the styles of famous painters I thought it would be the same old junk.\nAcademics have been saying forever that they were on the verge of discovering the principles of aesthetics, and that they would soon be able to automate the production of beauty – melody, harmony, proportion, plot.\nWhen I was a kid I was excited to read about this sort of thing. But they always turn out to be fatuous, catastrophically oversimplified and overconfident, written - I’m guessing - by people who are intimidated & resentful of the culture around them. Our technical understanding of what makes something look good is still weak, and I don’t think it’s improving very fast. I learned to, when I come across an article about art written by a scientist, turn the page.\nBut now I think that maybe the automatic production of beauty will arrive soon. The machine learning algorithms work by extrapolating from existing examples, which means that they can produce new examples that fit some pattern (such as the pattern of beauty) without anyone involved having any explicit understanding of what the pattern is or how it can be defined.\nThis extrapolation without understanding is what happened in the study of visual perception – i.e. making inferences from images. Our understanding of perception is slowly moving forward, as it has been for centuries, but our ability to automate perception has shot ahead. In the 15th century Leonard da Vinci studied how the light reflected by an object is related to its distance – more distant objects tend to be bluer – these are relationships that we all know unconsciously, but which take a lot of work to dig out, such that we consciously understand them. Psychologists and computer scientists are still discovering things about the physics of light which we all know unconsciously. But computer models which incorporate our explicit knowledge of the physics of light are being thrashed by pure machine-learning models, which are fed a huge databases of pictures and simply extrapolate from what they’ve already seen.[2]\nI think the same basic point is true of aesthetic things. We really struggle trying to explain why we like a picture or dislike a melody, because most of the work is done at an unconscious level. The progress in understanding those principles will probably continue to be slow.\nBut now it seems likely to me that, before long, machines will be able to do all these things on demand – play some brand new Mozart, make elegant little drawings of animals, write a pretty good pop song. And the programmer who implements them could be – probably will be – some bozo who has no clue why it works.\n\n[1] 2017-05: SIGGRAPH video with style transfer - https://www.youtube.com/watch?v=HYhzZ-Abku8\n[2] 2017-05: Michael Elad “Deep, Deep Trouble: Deep Learning’s Impact on Image Processing, Mathematics, and Humanity” https://sinews.siam.org/Details-Page/deep-deep-trouble-4"
  },
  {
    "objectID": "posts/2017-09-27-work-of-art-age-mechnical-production.html#aka-machine-learning-aesthetics-the-unconscious",
    "href": "posts/2017-09-27-work-of-art-age-mechnical-production.html#aka-machine-learning-aesthetics-the-unconscious",
    "title": "The Work of Art in the Age of Mechanical Production",
    "section": "",
    "text": "When I heard about the neural nets that copy the styles of famous painters I thought it would be the same old junk.\nAcademics have been saying forever that they were on the verge of discovering the principles of aesthetics, and that they would soon be able to automate the production of beauty – melody, harmony, proportion, plot.\nWhen I was a kid I was excited to read about this sort of thing. But they always turn out to be fatuous, catastrophically oversimplified and overconfident, written - I’m guessing - by people who are intimidated & resentful of the culture around them. Our technical understanding of what makes something look good is still weak, and I don’t think it’s improving very fast. I learned to, when I come across an article about art written by a scientist, turn the page.\nBut now I think that maybe the automatic production of beauty will arrive soon. The machine learning algorithms work by extrapolating from existing examples, which means that they can produce new examples that fit some pattern (such as the pattern of beauty) without anyone involved having any explicit understanding of what the pattern is or how it can be defined.\nThis extrapolation without understanding is what happened in the study of visual perception – i.e. making inferences from images. Our understanding of perception is slowly moving forward, as it has been for centuries, but our ability to automate perception has shot ahead. In the 15th century Leonard da Vinci studied how the light reflected by an object is related to its distance – more distant objects tend to be bluer – these are relationships that we all know unconsciously, but which take a lot of work to dig out, such that we consciously understand them. Psychologists and computer scientists are still discovering things about the physics of light which we all know unconsciously. But computer models which incorporate our explicit knowledge of the physics of light are being thrashed by pure machine-learning models, which are fed a huge databases of pictures and simply extrapolate from what they’ve already seen.[2]\nI think the same basic point is true of aesthetic things. We really struggle trying to explain why we like a picture or dislike a melody, because most of the work is done at an unconscious level. The progress in understanding those principles will probably continue to be slow.\nBut now it seems likely to me that, before long, machines will be able to do all these things on demand – play some brand new Mozart, make elegant little drawings of animals, write a pretty good pop song. And the programmer who implements them could be – probably will be – some bozo who has no clue why it works.\n\n[1] 2017-05: SIGGRAPH video with style transfer - https://www.youtube.com/watch?v=HYhzZ-Abku8\n[2] 2017-05: Michael Elad “Deep, Deep Trouble: Deep Learning’s Impact on Image Processing, Mathematics, and Humanity” https://sinews.siam.org/Details-Page/deep-deep-trouble-4"
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html",
    "title": "Sushi-Roll Model of Online Media",
    "section": "",
    "text": "\\[\n\\def\\RR{{\\bf R}}\n\\def\\bold#1{{\\bf #1}}\n\\]\nA model of internet media: the platform chooses the composition, the user chooses the quantity. I think this is a nice crisp way of modeling how media platforms (FB, YouTube, TikTok) make their decisions about content: they chooses the mix of content, i.e. the shares of each type, and then their users choose the quantity. The platform is choosing the fillings for the sushi roll and the consumer is choosing how much to eat. Their decisions jointly determine the total amount of each ingredient consumed.\nThis gives a unified model of feed ranking inclusive of ad-load, revenue-sharing, producer-side effects, and advertiser demand elasticity.\nWe can break down four different ways in which increasing the share of a given content-type \\(i\\) will affect total revenue:\nAn efficient mix of content will choose the shares such that each type of content has the same marginal value, i.e. for each the type four components of value all sum to the same number.\nRelated literature. There are some nice models of ad-media tradeoff in Anderson and Jullien (2015), but I believe they don’t consider the effects on production by producers, nor the tradeoff between different types of content (though it’s a long time since I read it).\nExpressed formally: Suppose the platform chooses \\(x_1,\\ldots,x_n\\) which represent the impression-shares of each type of content such that \\(\\sum_{i=1}^nx_i=1\\). User demand depends on the average quality of each type of content (\\(q_i\\)), and they have diminishing returns in each type of content. The platform receives \\(p_i\\) for showing an impression of type \\(i\\), but that price depends on the number of impressions-seen. We can write the maximization problem as:\n\\[\\begin{aligned}\n      \\max_{x_1,\\ldots,x_n} \\utt{\\left(\\sum_{i=1}^n q_i(x_i)x_i^\\gamma\\right)}{total}{impressions}\n                     \\utt{\\left(\\sum_{i=1}^n x_ip_i(x_i)\\right)}{avg revenue}{per impression}\n      ,\\text{ s.t. }\\sum_{i=1}^n x_i=1\n\\end{aligned}\n\\]\nThe first order condition shows us the four components of value:\n\\[\\frac{\\partial L}{\\partial x_i} =\n   \\ut{\n      (\\utt{q_i \\gamma x_i^{-(1-\\gamma)}}{incrementality}{}\n      + \\utt{q_i'(x_i)x_i^\\gamma}{effect through}{quality})\n      \\utt{\\left(\\sum_{j=1}^n p_j x_j\\right)}{avg revenue}{per impression}\n   }{effect on revenue through total impressions}\n   +\n   \\utt{\n      (\n         \\utt{p_i(x_i)}{revenue from}{additional impressions}+\n         \\utt{p'_i(x_i)x_i}{revenue from}{change in price}\n      )\n      \\utt{\\left(\\sum_{j=1}^n q_j x_j^{\\gamma}\\right)}{total}{impressions}}\n      {effect on revenue}{through impressions on $i$}\n    + \\utt{\\lambda}{avg marginal}{effect}=0\n\\]\nThe final term, \\(\\lambda\\), is the Lagrangian, representing the average marginal value of the outside option, i.e. the other types of content that are being replaced. In some cases we can simplify this model and we get a closed-form solution for the optimal content composition.\nWhat this model doesn’t include:"
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html#more-details",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html#more-details",
    "title": "Sushi-Roll Model of Online Media",
    "section": "More Details",
    "text": "More Details\nWe can walk through a series of models from simple to complicated, to build up to the full sushi-roll model:\n\nPlatform chooses share of ads. The platform chooses the share of impressions that are ads, and consumers choose how many total impressions to consume. If we assume the price of ads (CPMs) is fixed then the platform will set the ad-load to maximize the total number of ad-impressions. If the platform can influence the price of ads by their choice of quantity (i.e. they act as a monopolist) then the platform may choose to reduce ad-load to drive up CPMs.1\nIn this model we’re letting the platform set the quantity of ads, but we would get the same result if the platform instead set the price of ads, e.g. they posted a specific CPM and advertisers can buy as much as they want.\nPlatforms chooses shares of organic content. Suppose ad-load is fixed but platforms can vary the shares of different types of organic content. Users’ consumption depends on the quality of the content but they also have a taste for variety (i.e. diminishing returns in each type of content). We then get a nice closed-form solution where the share of each type of content is increasing in its relative quality. On the margin the incrementality of each type of content will be zero: i.e. increasing the share of that type of content will have no effect on total impressions.\nPlatform choses shares of ads and organic content. Now lets treat both advertisers and organic producers as the same: each producer has a quality \\(q_i\\) but they also will pay a certain price \\(p_i\\) for impressions on their content. The platform takes those prices as given. We can then distinguish between three types of producer:\n\nAdvertisers: \\(p_i&lt;0\\): the producer will pay the platform per impression.\nProfessional producers: \\(p_i&gt;0\\): the producer asks to be paid per impression.\nAmateurs: \\(p_i=0\\): there is no monetary exchange, the content is in the public domain or generated by an ordinary user.\n\nIn equilibrium the share of impressions allocated to a given producer (\\(x_i\\)) will depend both on its quality \\(q_i\\) (AKA incrementality) and the price \\(p_i\\) that the producer sets. I don’t have a closed-form solution but we can derive a first-order condition that has a straight-forward interpretation.\nThis model is easy to state but I think is primarily applicable to small platforms where they take prices as given. E.g. suppose you run an app where you (1) license certain content, or use user-generated content; (2) run ads from various different ad networks, the ads vary in CPMs but they also vary in incrementality (i.e. how obnoxious they are to your userbase).\nBecause prices are taken as given, this model doesn’t help us calculate the optimal revenue share. It will tell us the optimal ad-load, but not taking into account the elasticity of supply from the advertiser.\nNote that most platforms do not explicitly discriminate between advertisers based on their incrementality however they can implicitly discriminate by having a “quality score” or “organic bid”. This score is quite clearly designed to measure the incrementality of the advertisements, and so I think can be used to implement an efficient pricing scheme.\nPlatform choses shares of ads and organic content, prices endogenous. We can easily extend the model above to allow the price of each type of content to depend on the quantity shown (\\(p_i(x_iM)\\)). For example the price of ads will depend on the number of ad impression shown (due to advertisers’ diminishing marginal returns from ads shown). This gives platforms a reason to restrict the quantity of ad-impressions.\nPlatform chooses both shares and prices. Up to this point the platform chose only the share of each type of content.\nNow suppose the platform can set a price to pay producers (e.g. “revenue share”), and it’s a homogenous price. The price should roughly depend on (1) the producer’s elasticity of quality to price (i.e. their cost function), and (2) the incrementality of quality on the consumer side. We could set this up with a single price for all producers, or set a producer-specific price."
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-1-platform-chooses-ad-load",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-1-platform-chooses-ad-load",
    "title": "Sushi-Roll Model of Online Media",
    "section": "Model 1: Platform Chooses Ad-Load",
    "text": "Model 1: Platform Chooses Ad-Load\n(see previous paper)"
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-2-platform-chooses-organic-composition",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-2-platform-chooses-organic-composition",
    "title": "Sushi-Roll Model of Online Media",
    "section": "Model 2: Platform Chooses Organic Composition",
    "text": "Model 2: Platform Chooses Organic Composition\nWe have a model where there are \\(n\\) producers, the platform assigns to each producer a share of total content \\(x_i\\), with \\(\\sum_ix_i=1\\), and the consumer will choose how many total impressions to consume (\\(M\\)) based on the average quality, but with diminishing returns in each .\n\\[\\begin{aligned}\n   q_i &\\in \\mathbb{R}^+\n      && \\text{quality of producer $i$}  \\\\\n   x_i &\\in [0,1]\n      && \\text{share of impressions on producer $i$}\\\\\n   \\sum_i x_i &= 1\n      && \\text{shares must sum to 1}\\\\\n   M  &= \\sum_{i=1}^nq_ix_i^\\gamma\n      && \\text{total impressions, diminishing returns in each producer, $0&lt;\\gamma&lt;1$} \\\\\n\\end{aligned}\\]\nThe platform wishes to maximize total impressions, \\(M\\). We want to solve for the resultant impression-share of each producer, i.e. \\(x_i\\) as a function of the qualities \\(q_1,..,q_n\\) and parameter \\(\\gamma\\). We get the following impression-maximizing shares:\n\\[x_i=\\frac{q_i^\\frac{1}{1-\\gamma}}{\\gamma\\sum_{j=1}^nq_j^\\frac{1}{1-\\gamma}}.\\]\nImplication: impression-share will be proportional to quality. Interesting the elasticity will be increasing in quality: a 1% increase in quality will get a more than 1% increase in share of impressions, because \\(\\frac{1}{1-\\gamma}&gt;1\\).\nAdding money. Suppose now that the platform gets paid for showing certain impressions. We can make various different assumption about the price paid:\n\nUniform homogenous price: the platform takes the price as given. This only makes sense if there are a subset of producers who are advertisers.\nEach producer sets a payment rate per impression.\nThe platform chooses a single price for all producers to get extra impressions."
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-3-platform-chooses-composition-prices-fixed",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-3-platform-chooses-composition-prices-fixed",
    "title": "Sushi-Roll Model of Online Media",
    "section": "Model 3: Platform Chooses Composition, Prices Fixed",
    "text": "Model 3: Platform Chooses Composition, Prices Fixed\nWe have a model with a consumer, a platform, and a set of \\(n\\) producers. The platform chooses the share of content from each producer, \\(x_i\\in[0,1]\\) with \\(\\sum_i x_i=1\\). The consumer chooses the total amount of impressions they consume, \\(M\\), based on the mixture of content and the quality of each type of content \\(q_i\\). Finally producers can set a price \\(p_i\\) for each impression that they receive from the consumer. A positive price \\(p_i&gt;0\\) means\n\\[\\begin{aligned}\n   q_i &\\in \\mathbb{R}\n      && \\text{quality of producer $i$}  \\\\\n   p_i &\\in \\mathbb{R}\n      && \\text{price offered by producer $i$}  \\\\\n   x_i &\\in [0,1]\n      && \\text{share of impressions on producer $i$}\\\\\n   \\sum_i x_i &= 1\n      && \\text{shares must sum to 1}\\\\\n   M  &= \\sum_{i=1}^nq_ix_i^\\gamma\n      && \\text{total impressions, diminishing returns in each producer, $0&lt;\\gamma&lt;1$} \\\\\n\\end{aligned}\n\\]\nIf the platform simply wanted to maximize total impressions, \\(M\\), then they can derive the optimal impression-shares as follows:\n\\[x_i^*=\\frac{q_i^\\frac{1}{1-\\gamma}}{\\gamma\\sum_{j=1}^nq_j^\\frac{1}{1-\\gamma}}.\\]\nHowever we want the platform to maximize profit, which we can write as follows:\n\\[\\begin{aligned}\n   \\text{profit} &= \\utt{\\left(\\sum_{i=1}^n q_ix_i^\\gamma\\right)}{total}{impressions}\n                  \\utt{\\left(\\sum_{i=1}^n x_ip_i\\right)}{avg revenue}{per impression}\n\\end{aligned}\n\\]\nI’m not sure if we can get a closed-form solution but we can at least get a first-order condition for each \\(x_i\\) that tells us useful stuff about comparative statics:\n\\[\\frac{\\partial L}{\\partial x_i} =\n   \\ut{\\utt{q_i \\gamma x_i^{-(1-\\gamma)}}{effect on}{total impressions}\n      \\utt{\\left(\\sum_{j=1}^n p_j x_j\\right)}{avg revenue}{per impression}\n   }{effect on revenue through total impressions}\n   +\n   \\utt{p_i\\utt{\\left(\\sum_{j=1}^n q_j x_j^{\\gamma}\\right)}{total}{impressions}}\n      {effect on revenue}{through impressions on $i$}\n    + \\utt{\\lambda}{avg marginal}{effect}=0\n\\]\nObservations:\n\nIf producers offer more, increasing \\(p_i\\), then \\(x_i\\) will go down until the marginal effect on total impression declines to balance the additional revenue.\nIf \\(p_i&lt;0\\), meaning a producer charges for impressions, then they can still have a positive number of impressions if their effect on total impressions is higher than the average of other types of content. (We could have added an additional constraint that \\(x_i\\geq 0\\).)"
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-4-platform-chooses-composition-monopolist",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-4-platform-chooses-composition-monopolist",
    "title": "Sushi-Roll Model of Online Media",
    "section": "Model 4: Platform Chooses Composition, Monopolist",
    "text": "Model 4: Platform Chooses Composition, Monopolist\nNow we allow the price of each type of content to depend on the quantity used, e.g. the price of ads will be higher when the quantity of ad-impressions is smaller (monopolist in the ad market). Strictly we should write \\(p_i(Mx_i)\\), but it’s somewhat easier to write \\(p_i(x_i)\\) and the answer should be similar for any type of content that is a small share.\n\\[\\begin{aligned}\n      \\text{profit} &= \\utt{\\left(\\sum_{i=1}^n q_ix_i^\\gamma\\right)}{total}{impressions}\n                     \\utt{\\left(\\sum_{i=1}^n x_ip_i(x_i)\\right)}{avg revenue}{per impression}\n\\end{aligned}\n\\]\nThere’s now one additional term in the first order condition:\n\\[\\frac{\\partial L}{\\partial x_i} =\n   \\ut{\\utt{q_i \\gamma x_i^{-(1-\\gamma)}}{effect on}{total impressions}\n      \\utt{\\left(\\sum_{j=1}^n p_j x_j\\right)}{avg revenue}{per impression}\n   }{effect on revenue through total impressions}\n   +\n   \\utt{\n      (\n         \\utt{p_i(x_i)}{revenue from}{additional impressions}+\n         \\utt{p'_i(x_i)x_i}{revenue from}{change in price}\n      )\n      \\utt{\\left(\\sum_{j=1}^n q_j x_j^{\\gamma}\\right)}{total}{impressions}}\n      {effect on revenue}{through impressions on $i$}\n    + \\utt{\\lambda}{avg marginal}{effect}=0\n\\]\nThe additional term represents the platform’s monopoly power with respect to the price paid. This has a natural interpretation for advertisers: showing fewer ads will drive up the price. For paid content-providers it could perhaps represent bulk discounts, I’m not sure whether this is a significant consideration."
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-2-derivation",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-2-derivation",
    "title": "Sushi-Roll Model of Online Media",
    "section": "Model 2 Derivation",
    "text": "Model 2 Derivation\nThis is derivation of model #2. (I had chatGPT help with this derivation, was very useful)\n\nSetting up the Lagrangian. The objective is to maximize the total impressions, \\(M\\), subject to the constraint that the allocated shares of impressions sum to one. We start by writing the Lagrangian: \\[\\mathcal{L} = \\sum_{i=1}^n q_i x_i^\\gamma - \\lambda \\left(\\sum_{i=1}^n x_i - 1\\right)\\]\nwhere \\(\\lambda\\) is the Lagrange multiplier associated with the constraint.\nSolving for the multiplier. To solve for the value of \\(\\lambda\\), we take the derivative of the Lagrangian with respect to \\(x_i\\) and set it equal to zero: \\[\\frac{\\partial \\mathcal{L}}{\\partial x_i} = \\gamma q_i x_i^{\\gamma - 1} - \\lambda = 0\\]\nRearranging this equation yields: \\[x_i = \\left(\\frac{\\lambda}{\\gamma q_i}\\right)^{\\frac{1}{\\gamma-1}}\\]\nTaking the sum of this expression over all producers and using the constraint that the shares of impressions must sum to one, we obtain: \\[1 = \\sum_{i=1}^n x_i = \\sum_{i=1}^n \\left(\\frac{\\lambda}{\\gamma q_i}\\right)^{\\frac{1}{\\gamma-1}}\\]\nSimplifying this equation gives:\n\\[\\lambda^{\\frac{1}{\\gamma-1}} = \\gamma \\sum_{i=1}^n q_i^{-\\frac{1}{\\gamma-1}}\\]\nSubstituting this expression for \\(\\lambda\\) back into the equation for \\(x_i\\) results in:\n\\[x_i = \\frac{q_i^{-\\frac{1}{\\gamma-1}}}{\\gamma \\sum_{j=1}^n q_j^{-\\frac{1}{\\gamma-1}}}\\]\nThis is our final expression for the share of impressions on each producer as a function of the exogenous qualities and \\(\\gamma\\).\n\nSummary:\nThe Lagrangian: \\(\\mathcal{L} = \\sum_{i=1}^n q_i x_i^\\gamma - \\lambda \\left(\\sum_{i=1}^n x_i - 1\\right)\\).\nExpression for the multiplier: \\[\\lambda^{\\frac{1}{\\gamma-1}} = \\gamma \\sum_{i=1}^n q_i^{-\\frac{1}{\\gamma-1}}\\]\nThe resultant expression for \\(x_i\\): \\[x_i = \\frac{q_i^{-\\frac{1}{\\gamma-1}}}{\\gamma \\sum_{j=1}^n q_j^{-\\frac{1}{\\gamma-1}}}.\\]\nSlightly rearranged (by me):\n\\[x_i=\\frac{q_i^\\frac{1}{1-\\gamma}}{\\gamma\\sum_{j=1}^nq_j^\\frac{1}{1-\\gamma}}.\\]"
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html#footnotes",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html#footnotes",
    "title": "Sushi-Roll Model of Online Media",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn the profit-maximizing solution either the consumer-side or advertiser-side first-order-condition will be binding. It depends on the relative elasticity of the two sides of the platform.↩︎"
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html",
    "title": "Repulsion from the Prior",
    "section": "",
    "text": "the shortest version: contrary to recent reports, I do not think it’s possible for you to be a Bayesian and consistently exaggerate things.\n{: .center-image }"
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#short-version",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#short-version",
    "title": "Repulsion from the Prior",
    "section": "Short Version",
    "text": "Short Version\n\nIf we think of perception as inference, it has implications about the types of biases we would have.\nYet many biases and illusions seems to go in the exact opposite direction – sometimes called “anti-Bayesian” biases – in particular there are ubiquitous contrast effects, while Bayesian inference seems to imply assimilation effects.\nWei and Stocker (2015) say they can rationalize these contrast effects, under the assumption that our sensory mechanisms are tuned to the environment, such that they are relatively more sensitive to more likely signals. They say that this will imply contrast effects (that the bias is inversely proportional to the slope of the prior).\nYet their results contradict some simple laws of Bayesian inference – the law of iterated expectations, and law of total variance – so there is something odd going on.\n(If this explanation doesn’t work, then why do we get repulsion? I think that Ted Adelson explained the basic reason in the 70s. Will write another post on this.)"
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#shortish-version",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#shortish-version",
    "title": "Repulsion from the Prior",
    "section": "Shortish Version",
    "text": "Shortish Version\n\nHere’s a nice crisp problem: in what cases does inference attract towards the prior, and in what cases does it repulse away from it?\nGiven an unknown variable \\(x\\) and a signal \\(s\\), let’s say that there’s “attraction” at a given value of \\(x\\) if the average inferred value of \\(x\\) is closer to the prior than \\(x\\) itself is –\n\\[|E[E[x|s]|x]-\\mu|&lt;|x-\\mu|\\]\nAttraction effects are typically treated as the norm. For example if \\(x\\) is drawn from a normal distribution and if \\(s\\) is equal to \\(x\\) plus normal noise, then you’ll always get attraction to the prior. I.e., if \\(x\\) is above the mean, then it’ll be, on average, estimated to be closer to the mean than it actually is.\nHowever this has sometimes been treated as a puzzle in studies of perception: perception seems like inference, but we also find what look like repulsion effects. For example “contrast” effects, in which an object seems less dark when you put it next to another, darker, object. If we assume that the colour of the neighboring objects affects your prior about the target object, then this would imply an attraction effect. Yet repulsion effects seems to be the norm across all sorts of judgments (lightness, colour, volume, orientation, size), and similar contrast effects occur in time as well as in space (i.e., something seems less dark if it is preceded by something darker) – though of course there are exceptions. These types of illusion are sometimes called “anti-Bayesian.”\nA common explanation of these contrast effects is that we ‘code for differences’ – i.e. that something about our neural wiring causes us to encode differences, rather than levels, and this causes us to exaggerate differences, i.e. get contrast effects.\nBut this assumes that we encode the difference and then forget to decode (AKA coding catastrophe, AKA the el Greco fallacy). If you write down a Bayesian model, which makes its best effort to infer the level from the difference, you typically do not find the desired contrast effects (Schwartz, Hsu & Dayan (2007)).\nWei and Stocker (2015) announce that they have made a breakthrough – a fully Bayesian model which generates contrast/repulsion effects generically. They say that the key assumption is that we are more sensitive to differences in areas where signals are more likely to fall – i.e., sensitivity is proportional to the density of the prior.\nFormally, let \\(x\\sim f\\), and \\[s=F(x)+\\varepsilon.\\] This means that sensitivity is proportional to the density of the prior – and it implies that \\(s\\) will be roughly uniformly distributed – so in some sense it’s an efficient use of signal capacity. Given this setup, and some simplifications, they find that the bias is proportional to the slope of the prior – so if the prior is symmetric & single-peaked then for values above the mean, the bias will be positive, and vice versa – i.e. repulsion away from the prior everywhere.\nIn the note below I give a proof that implies that it is impossible to have repulsion effects everywhere – which seems to contradict the results of Wei & Stocker.\nI’m not sure what the source of the contradiction is – it could be either (a) Wei & Stocker’s results are true locally, but do not apply at the tails of the distribution, and so things balance out that way; (b) there is a difference in the implicit assumption used when taking conditional expectations (AKA the Borel-Kolmogorov paradox); or (c) I made a mistake.\nI also mention below a related result, that there cannot be a consistent upward or downward bias (i.e., it cannot be that \\(E[\\hat{x}\\|x]&gt;x\\) for all \\(x\\)). This is relevant for Wei & Stocker’s result applied to asymmetric priors – e.g. if the prior is everywhere decreasing – where the result seems to imply a consistent upward bias. \n\n{: .center-image }"
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#summary-of-proof",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#summary-of-proof",
    "title": "Repulsion from the Prior",
    "section": "Summary of proof",
    "text": "Summary of proof\n\nSuppose that there is repulsion from the prior everywhere, i.e. for all \\(x\\), \\(\\|E[\\hat{x}\\|x]-\\mu\\|&gt;\\|x-\\mu\\|\\).\nThis implies that \\(Var[\\hat{x}]&gt;Var[x]\\).\nBut this contradicts the law of total variance, which says that \\(Var[E[A\\|B]]\\leq Var[A]\\)."
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#detail",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#detail",
    "title": "Repulsion from the Prior",
    "section": "Detail:",
    "text": "Detail:\nSuppose there are two random variables \\(x\\) and \\(s\\), and let \\(\\hat{x}=E[x\\|s]\\). Let \\(x\\) be mean-zero, and let’s assume repulsion from the prior everywhere, i.e. for all \\(x\\):\n\\[\nE[\\hat{x}|x]|&gt;|x|\n\\]\nFrom this repulsion assumption I think it’s clear that there’s more variance in \\(E[\\hat{x}\\|x]\\) than in \\(x\\):\n\\[Var[E[\\hat{x}|x]]&gt;Var[x]\\]\nNow let’s apply the law of total variance:\n\\[\n\\begin{aligned}\nVar[A]=& E[Var[A|B]]+Var[E[A|B]] \\\\\\\\\nVar[\\hat{x}]=& E[Var[\\hat{x}|x]]+Var[E[\\hat{x}|x]]\n\\end{aligned}\n\\]\nThus implying that:\n\\[Var[\\hat{x}]\\equiv Var[E[x|s]]&gt;Var[x]\\]\nApplying the law of total variance again we get:\n\\[\\begin{aligned}\nVar[x]=& E[Var[x|s]]+Var[E[x|s]] \\\\\\\\\n      &gt;& Var[x]\n\\end{aligned}\\]\nA contradiction."
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#no-consistent-upwarddownward-bias",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#no-consistent-upwarddownward-bias",
    "title": "Repulsion from the Prior",
    "section": "No consistent upward/downward bias",
    "text": "No consistent upward/downward bias\nThe law of iterated expectations states that, for any \\(A\\) and \\(B\\):\n\\[E[E[A|B]]=E[A]\\]\nThis implies that there cannot be a consistent upward or downward bias, i.e. it cannot be true that:\n\\[E[\\hat{x}|x]&gt;x, \\forall x\\]"
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#references",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#references",
    "title": "Repulsion from the Prior",
    "section": "References",
    "text": "References\n\nSchwartz, Hsu & Dayan (2007, Nature Review Neuro) “Space and Time in Visual Context”\nWei & Stocker (2015, Nature Neuroscience) “A Bayesian observer model constrained by efficient coding can explain ‘anti-Bayesian’ percepts”"
  },
  {
    "objectID": "posts/2023-09-05-model-of-ai-imitation.html",
    "href": "posts/2023-09-05-model-of-ai-imitation.html",
    "title": "An AI Which Imitates Humans Can Beat Humans",
    "section": "",
    "text": "Thanks to comments from many, especially Giorgio Martini, Grady Ward, Rob Donnelly, Inés Moreno de Barreda, and Colin Fraser.\nIf we train AIs to imitate humans, will they ever beat humans? AI has caught up to human performance on many benchmarks, largely by learning to predict what humans would do. It seems important to know whether this is a ceiling or we should expect them to shoot out ahead of us. Will LLMs be able to write superhumanly-persuasive prose? Will image models be able to see things in photos that we cannot? There is a lot of technical literature on imitation learning in AI but I haven’t found much discussion of this point (Bowman (2023) is a notable exception).\nIn a formal model I derive five mechanisms by which imitative AI can beat humans.\nThe evidence is unclear. There are many reasons why this could theoretically occur but I couldn’t find much evidence for superhuman performance: many benchmarks which we use to evaluate ML models have human labels as the ground truth, meaning we wouldn’t know when computers do pass us by.\nThis blog post contains:"
  },
  {
    "objectID": "posts/2023-09-05-model-of-ai-imitation.html#timeline",
    "href": "posts/2023-09-05-model-of-ai-imitation.html#timeline",
    "title": "An AI Which Imitates Humans Can Beat Humans",
    "section": "Timeline",
    "text": "Timeline\n\n\n\nThe following table shows the year in which a computer (or mechanical device) could match performance with the best human:\n\n\n\narithmetic\n1642\n\n\nchess\n1997\n\n\nJeopardy\n2005\n\n\nimage recognition (ImageNet)\n2015\n\n\nhandwriting recognition (MNIST)\n2015\n\n\nquestion answering (SQuAD1.1)\n2019\n\n\ndifficult math questions (MATH)\n2023\n\n\ncoding problems (MBPP)\n(not yet)\n\n\n\nComputers have hit the ceiling on most benchmarks. Kiela et al. (2023) documents that most computer benchmarks have become “saturated,” i.e. computers get close-to-perfect performance, and that recently the speed of saturation has become quicker (see graph on right). They say identify only a single benchmark where performance is not close to the human baseline, and most of the models they discuss are imitation learning. As a consequence some work has moved to evaluating models against “adversarial” benchmarks where the problems are chosen specifically to fool computers (e.g. Dynabench, Kiela et al. (2021)).55 Kiela et al. (2021) also say that “models that achieve super-human performance on benchmark tasks (according to the narrow criteria used to define human performance) nonetheless fail on simple challenge examples and falter in real-world scenarios.”\nOn some tasks human performance defines success. On some tasks human performance effectively is the ground truth, and so by definition computers could never beat humans. This is roughly true for text comprehension: a sentence has a given meaning if and only if the average person believes it has that meaning. When we observe computer outperformance on this type of benchmark it is because either (1) there is human variation and the computer output is more consistent; or (2) computers outperform amateur humans but the ground truth is expert humans."
  },
  {
    "objectID": "posts/2023-09-05-model-of-ai-imitation.html#performance-by-task",
    "href": "posts/2023-09-05-model-of-ai-imitation.html#performance-by-task",
    "title": "An AI Which Imitates Humans Can Beat Humans",
    "section": "Performance by Task",
    "text": "Performance by Task\nArithmetic: computers passed humans 300 years ago. Machines have been used to do calculations since the 17th century, e.g. Pascal’s calculator from 1642.\n\n\n\n\n\nBackgammon\n1979\n\n\nChess\n1997\n\n\nJeopardy\n2005\n\n\nAtari games\n2013\n\n\nGo\n2016\n\n\nStarcraft\n2019\n\n\n\n(source)\nPlaying games: computers passed humans over the last 45 years. See the table in the margin for games. I am not aware of any well-known games in which computers cannot reliably beat the best humans.\n\n\n (source)\nImage recognition: computers surpassed humans in the 2010s. With the qualifications above about the limitations of benchmark tasks.\nQuestion answering: computers surpassed humans in the 2010s. With the qualifications above about the limitations of benchmark tasks.\nFacial recognition: computers seem to be equivalent to experts. Towler et al. (2023) say “naturally skilled super-recognizers, trained forensic examiners and deep neural networks, … achiev[e] equivalent accuracy.”\nCoding: computers still below expert. See the benchmarks on PapersWithCode, also a graph on OurWorldInData, specifically APPS and MBPP. The best-performing computers are still imperfect at solving these coding challengers (which presumably can be solved by an expert programmer), but progress is rapid.\nWriting persuasive text: computer comparable to average human. A number of recent papers compare the persuasive power of LLM-generated text to human-generated text (Bai et al. (2023), Goldstein et al. (2023), Hackenburg and Margetts (2023), Matz et al. (2023), Palmer and Spirling (2023), Qin et al. (2023)). They all find that LLMs do relatively well, but none show clear signs of computer superiority.\nWriting creative blurbs: computer comparable to average human. Koivisto and Grassini (2023) compared GPT4 to online recruited humans (£2 for a 13 minute task) in giving “creative” uses for everyday items. The prompt was to “come up with original and creative uses for an object”, objects were “rope”, “box”, “pencil” and “candle.” The responses were rated by humans for their “creativity” or “originality.” GPT-4 responses were perhaps 1SD above the average human score, but the difference was smaller when choosing just the best response for each user.\nSummarizing text: computer beats average human. Two recent papers found that LLM-generated summaries, trained with feedback, were preferred by humans to human-generated summaries (Stiennon et al. (2022) using RLHF and Lee et al. (2023) using RLHF). However in both cases it wasn’t clear to me exactly how the human summarizers were incentivized, and whether they were trying to perform the same task as the LLMs.66 Lee et al. (2023) say “RLAIF summaries are preferred over the reference [human-written] summaries 79% of the time, and RLHF are preferred over the reference summaries 80% of the time.”\nDoing math problems: computer comparable to expert. The latest score on the MATH benchmark is 84%, compared to 90% by a three-time IMO gold medalist. The scores have been rising very rapidly so it seems likely that computers will soon surpass humans.77 Hendrycks et al. (2021) says “We found that a computer science PhD student who does not especially like mathematics attained approximately 40% on MATH, while a three-time IMO gold medalist attained 90%”"
  },
  {
    "objectID": "posts/2023-09-05-model-of-ai-imitation.html#model-implications",
    "href": "posts/2023-09-05-model-of-ai-imitation.html#model-implications",
    "title": "An AI Which Imitates Humans Can Beat Humans",
    "section": "Model Implications",
    "text": "Model Implications\n\nIf one human records all their observations then the computer will perfectly imitate them.\n\nSuppose that there is one human and they write down all of their observations, \\(\\hat{Q}=Q\\). Because the computer and human have the same priors, and observe the same data, then they will therefore end up with the same estimated weights (\\(\\hat{\\bm{w}}=\\bar{\\bm{w}}\\)), and so the computer will answer every question exactly as the human does, though neither knows the truth (\\(\\bar{\\bm{w}}\\neq\\bm{w}\\)).\n\n\nIf humans are noisy then the computer will outperform them. Suppose humans report their answers with some i.i.d. noise \\(\\epsilon\\). If the computer observes sufficiently many answers for each question then the noise will be washed out and they will outperform.\n\nIf humans record a subset of their observation then the computer will perform worse.\n\nSuppose humans only write down some of their observations, i.e. \\(\\hat{Q}\\) is a row-wise subset of \\(Q\\). Then computers and humans will give the same answers for any question in the training set, but outside of that set computers will generally do worse than humans. And so for any question \\(\\bm{q}\\not\\in\\hat{Q}\\) the computer will do worse in expectation: \\[E[\\ut{(\\bm{q}(\\bm{w}-\\bar{\\bm{w}}))^2}{computer error}]\\geq\n     E[\\ut{(\\bm{q}(\\bm{w}-\\hat{\\bm{w}}))^2}{human error}].\\] Note that we are fixing the question \\(\\bm{q}\\) and taking the expectation over all possible worlds, \\(\\bm{w}\\). I think you could probably rewrite this such that, in the world we are in, we should observe worse average performance across a set of questions, but I think you’d need to add some conditions to make sure that the questions are sufficiently independent (e.g. if there was a single weight \\(w_q\\) which dominated all the other weights then the computer might beat the human by accident).\n\nIf there are two humans then the computer will outperform them both.\n\nSuppose there are two humans who each observe answers to different question, \\(Q_A\\) and \\(Q_B\\), and they both write them all down, so \\(\\bar{Q}=(\\smallmatrix{Q_A\\\\Q_B})\\) and \\(\\bar{\\bm{a}}=(\\smallmatrix{Q_A\\bm{w}\\\\Q_B\\bm{w}})\\). Now the computer clearly has superior information to either human, and so if we let \\(\\hat{\\bm{w}}(i)\\) represent the weights of human \\(i\\in\\{A,B\\}\\), then for any question \\(\\bm{q}\\) we can write:\n\\[ E[\\ut{(\\bm{q}(\\bm{w}-\\bar{\\bm{w}}))^2}{computer error}]\\leq\n  E[\\ut{(\\bm{q}(\\bm{a}-\\hat{\\bm{w}}(i)))^2}{human error}].\n   \\]\n\n\nIf there are multiple humans then the computer can answer question no human can answer. Suppose two humans observe the answers to the following questions:\n\\[\\begin{aligned}\n      Q_A &= \\bmatrix{1 & 1 & 1 \\\\ 1 & -1 & 1} \\\\\n      Q_B &= \\bmatrix{1 & 1 & 1 \\\\ 1 & 1 & -1}\n   \\end{aligned}\\]\nThe first human will learn the exact value of \\(w_2\\) (\\(\\hat{w}_2=w_2\\)), and the second human will learn the exact value of \\(w_3\\), but neither will learn both values, and so neither could predict the answer to this question with perfect confidence:\n\\[\\begin{aligned}\n      \\tilde{q} &= \\bmatrix{1 & -1 & -1} \\\\\n   \\end{aligned}\\]\nHowever if they both recorded their observations, so the computer observes \\(\\bar{\\bm{a}}=(\\smallmatrix{Q_1\\bm{w}\\\\Q_2\\bm{w}})\\), the computer will be able to infer both \\(w_2\\) and \\(w_3\\), and thus will be able to perfectly answer \\(\\tilde{q}\\). We can see this behaviour in LLMs: they sometimes combine a pair of facts or a pair of abilities which no single human has access to, e.g. when an LLM translates between two languages, for which there exists no human speaker of both.\nIf humans write outside their expertise then the computer will do worse. In the cases above we assumed that the two humans recorded only what they directly observed, \\(\\hat{Q}\\subseteq Q\\). This means the computer essentially had a window directly to the world. However the humans could instead have written down their estimated answers to other questions. Suppose both humans wrote down answers to every possible question, \\(\\bm{q}\\in\\{-1,1\\}^p\\), then we could conjecture that the computer would learn the average of the two humans’ weights:14 \\[\\bar{\\bm{w}}=\\frac{1}{2}\\hat{\\bm{w}}_A+\\frac{1}{2}\\hat{\\bm{w}}_B.\\] Here the computer will do worse than the two humans on the original questions, \\(Q_A\\) and \\(Q_B\\). The implication is that LLMs work so well only because people tend to write about what they know. Put another way, when an LLM answers a question, it will not predict the answer given by the average person, but will predict the answer given by people who are likely to answer that question in the real world, and luckily those people tend to be people who are subject-matter experts.14 We would have to augment the computer’s learning rule to allow for noise in answers - I need to confirm that the weighting will be exactly 1/2.\nIf humans have tacit knowledge, then computers can outperform in choosing a question to maximize the answer. We can model tacit knowledge with two separate sets of human weights:\n\\[\\begin{aligned}\n      \\hat{\\bm{w}}^T   &= \\text{tacit knowledge}\\\\\n      \\hat{\\bm{w}}^E &= \\text{explicit knowledge}\\\\\n   \\end{aligned}\\]\nWhen the human encounters a new question \\(\\tilde{\\bm{q}}\\) they will use their tacit knowledge to form an estimate of the answer, \\(\\hat{a}=\\tilde{\\bm{q}}'\\hat{\\bm{w}}^T\\). But they have limited ability to introspect about that capacity, and so when asked how they make their judgments they can report only \\(\\hat{\\bm{w}}^E\\). For simplicity assume tacit knowledge is perfect (\\(\\hat{\\bm{w}}^T=\\bm{w}\\)), and explicit knowledge is imperfect (\\(\\hat{\\bm{w}}^E=\\bm{w}\\)).\nThe distinction becomes important when we want to create a new question. Here it’s useful to interpret \\(\\bm{q}\\) not as a question but as an artefact, e.g. a text or image, and interpret \\(a=\\bm{q}'\\bm{w}\\) as a property of that artefact, e.g. how persuasive is the text, or how attractive is the image. Suppose we want to choose \\(\\bm{q}\\in\\{-1,1\\}^n\\) to maximize \\(a\\). If we had perfect access to our beliefs \\(\\bm{w}^T\\) this would be simple, however if we have access only to imperfect explicit knowledge \\(\\hat{\\bm{w}}^E\\), the artefact which maximizes that function will not generally be the one which maximizes \\(a\\). This represents an asymmetry in human cognition: we can recognize certain patterns (whether text is persuasive, whether a picture is pretty), without being able to produce those patterns.\nHere the computer model is less constrained. Suppose the computer has observed sufficiently many questions such that they have perfectly learned human tacit knowledge, \\(\\bar{\\bm{w}}=\\hat{\\bm{w}}^T\\). If computation is costless we could query every single \\(\\bm{q}\\in\\{-1,1\\}^p\\) to find the highest \\(a\\). More realistically we could use a diffusion algorithm, or reinforcement learning against human or computer evaluation, to find an artefact with a high \\(a\\)."
  },
  {
    "objectID": "posts/2023-09-05-model-of-ai-imitation.html#derivation",
    "href": "posts/2023-09-05-model-of-ai-imitation.html#derivation",
    "title": "An AI Which Imitates Humans Can Beat Humans",
    "section": "Derivation",
    "text": "Derivation\nSetup.\n\\[\\begin{aligned}\n      Q &= \\bmatrix{q_1^1 & \\ldots & q^1_p \\\\ & \\ddots \\\\ q^n_1 & \\ldots & q^n_p}\n         && \\text{(matrix of $n$ questions, each with $p$ parameters)} \\\\\n      \\bm{w}'  &= \\bmatrix{w_1 \\ldots w_p}\n         && \\text{(vector of $p$ unobserved weights)}\\\\\n      \\bm{a}    &= \\bmatrix{a^1 \\\\ \\vdots \\\\ a^n}\n         = \\bmatrix{q_1^1 w_1 + \\ldots q_p^1w_p \\\\ \\vdots \\\\ q_1^n w_1 + \\ldots q_p^n w_p}\n         && \\text{(vector of $n$ observed answers)}\\\\\n   \\end{aligned}\\]\nWritten more compactly:\n\\[\\begin{aligned}\n      Q      &\\in \\{-1,1\\}^{p\\times n}\n         && \\text{($n$ questions, each has $p$ binary parameters)}\\\\\n      \\bm{w} &\\sim N(0,\\Sigma)\n         && (p\\times 1\\text{ vector of true parameters of the world)}\\\\\n      \\ut{\\bm{a}}{$n\\times1$}   &= \\ut{Q}{$n\\times p$}\\ut{\\bm{w}}{$p\\times1$}\n         && \\text{(answers provided by the world)}\\\\\n   \\end{aligned}\\]\nHuman posteriors. Given you observe a subset of a set of multivariate normal variables there is a simple expression for your posteriors over the remaining unobserved variables (e.g. see here).\n\\[\\begin{aligned}\n      \\hat{\\bm{w}} &= E[\\bm{w}|Q,\\bm{a}]\n            && \\text{(human beliefs about the world)}\\\\\n         &= \\ut{\\Sigma Q'}{$Cov(\\bm{w},\\bm{a})$}\n            (\\ut{Q\\Sigma Q'}{$Var(\\bm{a})$})^{-1}\n            \\bm{a}\n         && \\text{(from the Schur complement)}\n   \\end{aligned}\\]\nWe can use the same formula to calculate computer beliefs."
  },
  {
    "objectID": "posts/2023-09-05-model-of-ai-imitation.html#additional-observations",
    "href": "posts/2023-09-05-model-of-ai-imitation.html#additional-observations",
    "title": "An AI Which Imitates Humans Can Beat Humans",
    "section": "Additional Observations",
    "text": "Additional Observations\nThese are a few miscellaneous results additional results that helped me with intuition for the working of this model.\nWith one observation and two weights. Suppose \\(n=1, p=2\\), then we have:\n\\[\\begin{aligned}\n      Q  &= \\bmatrix{q_1 & q_2} \\\\\n      \\bm{a}'  &= \\bmatrix{a} \\\\\n      \\bm{w}'  &= \\bmatrix{w_1 & w_2 } \\\\\n      \\Sigma &= \\bmatrix{\\sigma_1^2 & \\rho \\\\ \\rho & \\sigma_2^2}\\\\\n      \\Sigma Q' &= \\bmatrix{ \\sigma_1^2q_1 + \\rho q_2 \\\\ \\rho q_1 + \\sigma_2^2 q_2 } \\\\\n      Q\\Sigma Q' &= \\bmatrix{ \\sigma_1^2q_1^2 + 2\\rho q_1q_2 + \\sigma_2^2 q_2^2} \\\\\n      \\hat{\\bm{w}}=\\Sigma Q'(Q\\Sigma Q')^{-1}\\bm{a}\n         &= \\bmatrix{ \\frac{\\sigma_1^2q_1 + \\rho q_2}{\\sigma_1^2q_1^2 + 2\\rho q_1q_2 + \\sigma_2^2 q_2^2} \\\\\n                  \\frac{\\rho q_1 + \\sigma_2^2 q_2}{\\sigma_1^2q_1^2 + 2\\rho q_1q_2 + \\sigma_2^2 q_2^2}} a\n   \\end{aligned}\\]\nWe can normalize \\(q_1=q_2=1\\), then we have: \\[\\hat{w}_1 = \\frac{\\sigma_1^2+\\rho}{\\sigma_1^2+2\\rho+\\sigma_2^2}a,\\] Here we are dividing up responsibility for the answer (\\(a\\)) into the contributions of each component, nice and simple.\nWith two observations and one weight. Here we’re over-identified.\n\\[\\begin{aligned}\n      Q  &= \\bmatrix{q^1 \\\\ q^2} \\\\\n      \\bm{a}  &= \\bmatrix{a^1 \\\\ a^2} \\\\\n      \\bm{w}  &= \\bmatrix{w } \\\\\n      \\Sigma &= \\bmatrix{\\sigma^2 }\\\\\n      \\Sigma Q' &= \\bmatrix{ \\sigma^2 q^1 & \\sigma^2 q^2 } \\\\\n      Q\\Sigma Q' &= \\bmatrix{ \\sigma^2 q^1q^1 & \\sigma^2q^1q^2 \\\\ \\sigma^2q^1q^2 & \\sigma^2q^2q^2}\n         && \\text{(this matrix doesn't have an inverse)}\n   \\end{aligned}\\]\nWith noise. Suppose we only observe the answers with random noise, then we get this:\n\\[\\begin{aligned}\n      \\ut{\\bm{a}}{$m\\times1$}   &= \\ut{Q}{$m\\times n$}\\ut{\\bm{w}}{$n\\times1$}\n         + \\ut{\\bm{e}}{$n\\times 1$} \\\\\n      \\bm{e} &\\sim N(\\bm{0},s^2I_m) && \\text{(i.i.d. noise w variance $s^2$)}\\\\\n      Cov(\\bm{w},\\bm{a})   &= \\Sigma Q' \\\\\n      Var(\\bm{a}) &= Q\\Sigma Q' + s^2I_m \\\\\n      E[\\bm{w}|Q,\\bm{q}]   &= \\Sigma Q'(Q\\Sigma Q' + s^2I_m)^{-1}\\bm{a}\n   \\end{aligned}\\]\nCompare to Bayesian linear regression. We can compare this result to Bayesian linear regression (e.g. Wikipedia):\n\\[\\begin{aligned}\n      \\bar{\\beta}  &= \\Sigma Q'(Q\\Sigma Q' + s^2I_m)^{-1}\\bm{a}\n         && \\text{(our result)} \\\\\n      \\tilde{\\beta} &= (Q'Q+s^{2}\\Sigma^{-1})^{-1}Q'\\bm{a}\n         && \\text{(Bayesian linear regression)}\\\\\n   \\end{aligned}\\]\nI believe that these can be shown to be equivalent by the matrix inversion lemma, though I haven’t confirmed this. There’s a note online that appears to show equivalence.\nExtension: quadratic forms. Instead of answers being linear in question-features (\\(a=q'w\\)) we could suppose they’re quadratic, \\(a=q'Wa\\), with \\(W\\) a matrix having dimension \\(n^2\\). I’m not sure whether we could still get an analytic solution for posteriors. Can visualize the matrix W: each bit in \\(q\\) will add an “L” overlaid on the matrix (alternatively: a row and column), and \\(a\\) will be the sum of the cells where both the row and the column are activated.\nExtension: binary answers. In some cases it is natural to think of the answer, \\(a\\), as binary instead of continuous. We might be able to reinterpret the model with \\(a\\) representing the log-odds ratio of a binary outcome. Alternatively there might be a way of having a beta-binomial conjugate prior over the probability of \\(a\\)."
  },
  {
    "objectID": "posts/2020-04-05-front-loading-restrictions.html",
    "href": "posts/2020-04-05-front-loading-restrictions.html",
    "title": "Optimal Coronavirus Policy Should be Front-Loaded",
    "section": "",
    "text": "Q: How should you sequence policies over time? E.g. suppose you want to manage the epidemic until a vaccine arrives and you have policies (lockdowns, distancing, masks) each of which is associated with a certain effect on the growth-rate of cases, but each also has some fixed social cost per day. How should you apply the policies over time?\nA: The severity of the policies should be gradually decreasing, i.e. they should gradually become less severe, as you approach the availability of a vaccine. There should not be zig-zagging between policies in this setup.\nAny justification for zig-zagging must come from some additional consideration like (a) non-separabilities in the costs, e.g. psychological/economic need for occasional respite, (b) uncertainty about the end-date, (c) uncertainty about the effect of the policies, such that there is informational-value from varying policies, or (d) desire to maintain a steady flow of cases, in order to reach herd immunity (the “mitigation” strategy)."
  },
  {
    "objectID": "posts/2020-04-05-front-loading-restrictions.html#corollary-you-should-never-expect-policy-to-get-stricter",
    "href": "posts/2020-04-05-front-loading-restrictions.html#corollary-you-should-never-expect-policy-to-get-stricter",
    "title": "Optimal Coronavirus Policy Should be Front-Loaded",
    "section": "Corollary: you should never expect policy to get stricter",
    "text": "Corollary: you should never expect policy to get stricter\nYou should never find yourself in the situation where you expect policy to get stricter in the future. If you anticipate that a stricter policy will be appropriate next week then that strict policy is appropriate this week!\nCountries in early stages of the epidemic should be doing as much or more as countries in later stages."
  },
  {
    "objectID": "posts/2020-04-05-front-loading-restrictions.html#intuition",
    "href": "posts/2020-04-05-front-loading-restrictions.html#intuition",
    "title": "Optimal Coronavirus Policy Should be Front-Loaded",
    "section": "Intuition",
    "text": "Intuition\nSuppose that there’s some tradeoff across policiers between the growth-rate and the social cost.\nThen given any fixed time-path of policies: e.g., (A,A,B,C), if it is not monotonically decreasing in severity from high-cost to low-cost, then you can do strictly better by rearranging the path of policies to be monotonically decreasing. The social cost will be identical, because the set of policies will be the same, but the number of cases will be lower at every point in time, since at any given point the cumulative growth rates, up to that point, will be lower. Thus the final cumulative number of cases will be lower."
  },
  {
    "objectID": "posts/2020-04-05-front-loading-restrictions.html#additional-reason-to-front-load-extinction",
    "href": "posts/2020-04-05-front-loading-restrictions.html#additional-reason-to-front-load-extinction",
    "title": "Optimal Coronavirus Policy Should be Front-Loaded",
    "section": "Additional Reason to Front-Load: Extinction",
    "text": "Additional Reason to Front-Load: Extinction\nAll of this is treating the number of cases as a continuous variable which means you can never completely extinguish the disease. However if that’s a possibility that’s within sight (e.g. as in NZ), then that’s a significantly stronger case for starting with very severe policies, to try to kill the disease entirely, and then you can go back to the garden of Eden."
  },
  {
    "objectID": "posts/2020-04-05-front-loading-restrictions.html#prior-discussion",
    "href": "posts/2020-04-05-front-loading-restrictions.html#prior-discussion",
    "title": "Optimal Coronavirus Policy Should be Front-Loaded",
    "section": "Prior Discussion",
    "text": "Prior Discussion\nThere’s been some discussion of zig-zagging by the Imperial group (paper) and by Timothy Gowers (twitter & post)\nGowers says the optimal policy is very short zig-zags (changing policy every other day), however I think this is misleading. It comes from fixing the lower-threshold and optimizing the upper-threshold. If instead you fixed the upper-threshold and optimized the lower-threshold, then the optimal cycle-length will be long.\nIf you choose both the upper and lower threshold (both T and S) then he notes that they’ll both be arbitarily low. However this ignores the cost of getting to zero given current cases.\nInstead a well-defined problem is to choose an optimal time-path of policy given some start-point and end-point. In that case it’ll be a path of gradually decreasing strictness (without zig-zags).\nYou can see the intuition in the diagram below: the total infections is approximately the area under the zig-zag (not quite: because the y-axis is ln(cases), but this won’t matter for the argument). Thus you can reduce the area under the line by lowering the upper threshold. However if you instead take the upper threshold as fixed, then it’s optimal to choose a lower threshold that is as low as possible, i.e. you want long cycles, not short cycles.\n\n\n\nabc"
  },
  {
    "objectID": "posts/2017-02-25-samuelson-expected-utility.html",
    "href": "posts/2017-02-25-samuelson-expected-utility.html",
    "title": "Samuelson & Expected Utility",
    "section": "",
    "text": "clown\n\n\n\n“If in every event which can possibly occur the consequence of action I is not preferred to that of action II, and if in some possible event the consequence of II is preferred to that of I, then any sane preferer would prefer II to I.”\n\nThis sentence, in a letter from Savage in 1950, finally persuaded Samuelson that rational choices must obey expected utility. He had been skeptical – thinking that expected utility was just a simple approximation, like exponential discounting or like separability. Marschack and Savage and Friedman all wrote letters trying to persuade him, and though they were right, they kept using bad arguments, and Samuelson disposed of them.\nSavage and Friedman wrote a paper saying that, because people buy both insurance and lottery tickets, expected utility implies that the utility-of-money must be concave then convex. Samuelson saw that this was ridiculous, & said “there’s as much to be learned about gambling from Dostoyevsky as from Pascal.”\nBut the sentence from the letter above finally persuaded Samuelson.\nThis is all from Ivan Moscati’s “How Economists Came to Accept Expected Utility Theory”."
  },
  {
    "objectID": "posts/2017-02-25-economist-explorers.html",
    "href": "posts/2017-02-25-economist-explorers.html",
    "title": "Economist Explorers",
    "section": "",
    "text": "explorers\n\n\nImagine a group of adventurers set out to explore a new continent, each one choosing a different valley to map. And suppose that, instead of sending back reports of what they found (“a forest, a swamp, mosquitoes”), each one wrote reports calculated to appear as exciting as possible (“a forest, abundant water, rich fauna, couldn’t disconfirm rumors of a city of gold”).\nThis is how I feel when I’m refereeing economics papers: everyone’s trying to tell an exciting story, and it takes a great deal of work to figure out what actual novel facts they have discovered. It’s frustrating because it’s such an inefficient way to explore the territory: so many people have spent so much time on this, and we have so little to show. What have we discovered about decision-making in the last 50 years after proposing thousands of different models, running tens of thousands of experiments, and regressing millions of variables? Couldn’t we have accumulated more knowledge if we’d organized things differently?\n(Although I have to admit that many of my own papers do include some speculation about cities of gold. But I think the most useful thing a referee can do is to suggest changes to the title and abstract, to make it more transparent exactly what the paper has found.)\nMy original post on Facebook\n–\nFollowup: what have we discovered about decision-making in the last 50 years?\nOnce, when I taught a graduate class in behavioural economics, a couple of students came from civil engineering & dentistry, hoping to learn something useful that they could use in modelling decision-making - e.g. in modelling how people make decisions about commuting. I was embarrassed in how little I could help them. I was able to think of a lot of useful stuff about decision making that’s ~50 years or older: utility & expected utility, exponential (& hyperbolic) discounting, Engel curves, responses to permanent & temporary income, tractable demand estimation, the offsetting income & substitution effects of wages. All of this is immediately useful in quantifying decision-making. But I can think of few recent examples of quantitatively useful findings. Special mention to kahneman & tversky. If you ask, say, about attitudes to uncertainty, to time, to temptation, to intertemporal complementarities, I would begin my answer with “there are various schools of thought…”."
  },
  {
    "objectID": "posts/2017-04-15-the-mechanical-and-the-rational.html",
    "href": "posts/2017-04-15-the-mechanical-and-the-rational.html",
    "title": "The Repeated Failure of Laws of Behaviour",
    "section": "",
    "text": "krazy kat"
  },
  {
    "objectID": "posts/2017-04-15-the-mechanical-and-the-rational.html#footnotes",
    "href": "posts/2017-04-15-the-mechanical-and-the-rational.html#footnotes",
    "title": "The Repeated Failure of Laws of Behaviour",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlso Seligman (1970) On the Generality of Laws of Learning, “That all events are equally associable and obey common laws is a central assumption of general process learning theory … A review of data from the traditional learning paradigms shows that the assumption of equivalent associability is false … it is speculated that the laws of learning themselves may vary with the preparedness of the organism for the associa- tion and that different physiological and cognitive mechanisms may covary with the dimension.”↩︎"
  },
  {
    "objectID": "posts/2017-02-25-weber-fechner-law.html",
    "href": "posts/2017-02-25-weber-fechner-law.html",
    "title": "Weber’s Law Doesn’t Imply Concave Representations or Concave Judgments",
    "section": "",
    "text": "runningman\nNutshell."
  },
  {
    "objectID": "posts/2017-02-25-weber-fechner-law.html#linear-representation-multiplicative-noise",
    "href": "posts/2017-02-25-weber-fechner-law.html#linear-representation-multiplicative-noise",
    "title": "Weber’s Law Doesn’t Imply Concave Representations or Concave Judgments",
    "section": "Linear Representation & Multiplicative Noise",
    "text": "Linear Representation & Multiplicative Noise\nAssume people get signals about underlying value with multiplicative noise, \\(s=v\\cdot e\\), with \\(e\\) lognormal. For conciseness let \\(\\delta=JND(v_{1},p)\\), then \\(\\delta\\) can be implicitly defined as:\n\\[\n\\begin{aligned}\np   =&  P(E[v_{1}+\\delta|s_{2}]&gt;E[v_{1}|s_{1}]) \\\\\n    =&  P((v_{1}+\\delta)e_{2}&gt;v_{1}e_{1}) \\\\\n    =&  P(\\ln(v_{1}+\\delta)+\\ln e_{2}&gt;\\ln v_{1}+\\ln e_{1}) \\\\\n    =&  \\Phi\\left(\\frac{\\ln(v_{1}+\\delta)-\\ln v_{1}}{\\sigma_{e}^{2}+\\sigma_{e}^{2}}\\right)\n\\end{aligned}\n\\]\nwhere \\(\\Phi\\) is the CDF of a standard normal distribution. Then,\n\\[\n\\begin{aligned}\n\\ln(v_{1}+\\delta)-\\ln v_{1} =&   2\\sigma_{e}^{2}\\Phi^{-1}(p) \\\\\n\\frac{v_{1}+\\delta}{v_{1}}  =&   \\exp(2\\sigma_{e}^{2}\\Phi^{-1}(p))\\\\\nJND(v_{1},p)=\\delta         =&   v_{1}\\left[\\exp(2\\sigma_{e}^{2}\\Phi^{-1}(p))-1\\right]\n\\end{aligned}\n\\]\nIn other words, the just noticeable difference is proportional to the value, \\(v_{1}\\), as found by Weber."
  },
  {
    "objectID": "posts/2017-02-25-weber-fechner-law.html#a-concave-representation-additive-noise",
    "href": "posts/2017-02-25-weber-fechner-law.html#a-concave-representation-additive-noise",
    "title": "Weber’s Law Doesn’t Imply Concave Representations or Concave Judgments",
    "section": "A Concave Representation & Additive Noise",
    "text": "A Concave Representation & Additive Noise\nSuppose that the decision-maker receives a concave signal of value with additive noise, i.e. \\(s=\\ln v+e\\), with Gaussian \\(e\\). Then the derivation is very similar:\n\\[\n\\begin{aligned}\n  p =& P(E[v_{1}|s_{2}]&gt;E[v_{1}|s_{1}]) \\\\\n    =& P(\\ln(v_{1}+\\delta)+e_{2}&gt;\\ln v_{1}+e_{1}).\n\\end{aligned}\n\\]\nThe rest of the derivation is the same: i.e., the JND in the neighborhood of \\(v_{1}\\) will be proportional to \\(v_{1}\\)."
  },
  {
    "objectID": "posts/2017-02-25-weber-fechner-law.html#multiplicative-noise-posteriors-are-concave-in-v",
    "href": "posts/2017-02-25-weber-fechner-law.html#multiplicative-noise-posteriors-are-concave-in-v",
    "title": "Weber’s Law Doesn’t Imply Concave Representations or Concave Judgments",
    "section": "Multiplicative Noise => Posteriors are Concave in \\(v\\)",
    "text": "Multiplicative Noise =&gt; Posteriors are Concave in \\(v\\)\nSuppose we have lognormal priors for both \\(v\\) and \\(e\\):\n\\[\n\\begin{eqnarray*}\n\\ln v & \\sim & N(\\mu_{v},\\sigma_{v}^{2})\\\\\n\\ln e & \\sim & N(\\mu_{e},\\sigma_{e}^{2}),\n\\end{eqnarray*}\n\\]\nand \\(s=v\\cdot e\\), then we will have posteriors like:\n\\[\n\\begin{eqnarray*}\nf(\\ln v|s) & \\sim & N(\\frac{\\sigma_{v}^{2}}{\\sigma_{e}^{2}+\\sigma_{v}^{2}}\\ln s,\\left(\\sigma_{v}^{-2}+\\sigma_{e}^{-2}\\right)^{-1})\\\\\nE[v|s] & = & \\exp\\left(\\frac{\\sigma_{v}^{2}}{\\sigma_{e}^{2}+\\sigma_{v}^{2}}\\ln s+\\frac{1}{2}\\left(\\sigma_{v}^{-2}+\\sigma_{e}^{-2}\\right)^{-1}\\right)\\\\\n& = & s^{\\frac{\\sigma_{v}^{2}}{\\sigma_{e}^{2}+\\sigma_{v}^{2}}}e^{\\frac{1}{2}(\\sigma_{v}^{-2}+\\sigma_{e}^{-2})^{-1}}\n\\end{eqnarray*}\n\\]\nThis means that the expected \\(v\\) is concave in the signal \\(s\\) (because the exponent is less than one). Intuitively: a doubling of the value, which causes a doubling of the stimulus, will cause a less than doubling of the expected value conditional on that stimulus, because it will cause us to revise upwards our beliefs about both \\(v\\) and \\(e\\).\nFinally, we are also interested in the average posterior for a given \\(v\\). This will also be concave (abbreviating \\(\\alpha=\\frac{\\sigma_{v}^{2}}{\\sigma_{e}^{2}+\\sigma_{v}^{2}}\\), and dropping the constant coefficient in \\(E[v|s]\\)):\n\\[\n\\begin{eqnarray*}\nE[E[v|s]|v] & = & \\int(v\\cdot e)^{\\alpha}f(e)de\\\\\n& = & v^{\\alpha}\\int e^{\\alpha}f(e)de.\n\\end{eqnarray*}\n\\]"
  },
  {
    "objectID": "posts/2017-02-25-weber-fechner-law.html#additive-noise-posteriors-are-linear-in-v",
    "href": "posts/2017-02-25-weber-fechner-law.html#additive-noise-posteriors-are-linear-in-v",
    "title": "Weber’s Law Doesn’t Imply Concave Representations or Concave Judgments",
    "section": "Additive Noise => Posteriors are Linear in \\(v\\)",
    "text": "Additive Noise =&gt; Posteriors are Linear in \\(v\\)\nSuppose again that the decision-maker receives a logarithmic signal with additive noise: \\(s=\\ln v+u\\), and let \\(u\\) be Gaussian. (I changed notation from \\(e\\) to \\(u\\) because I use a lot of exponential functions in the derivation.) Now assume that, in addition, \\(v\\) is drawn from an improper uniform \\((0,\\infty)\\). Consider the expected value of \\(v\\) given the signal \\(s\\) (I drop the constant term from the Gaussian distribution for conciseness):\n\\[\n\\begin{eqnarray*}\nE[v|s] & = & \\frac{\\int_{0}^{\\infty}ve^{-\\left(s-\\ln v\\right)^{2}}dv}{\\int_{0}^{\\infty}e^{-\\left(s-\\ln v\\right)^{2}}dv}.\n\\end{eqnarray*}\n\\]\nNow exchange variables, so that \\(v=e^{z}\\):\n\\[\n\\begin{eqnarray*}\nE[v|s] & = & \\frac{\\int_{-\\infty}^{\\infty}e^{z}e^{-(s-z)^{2}}e^{z}dz}{\\int_{-\\infty}^{\\infty}e^{-(s-z)^{2}}e^{z}dz}\\\\\n& = & \\frac{\\int_{-\\infty}^{\\infty}e^{-s^{2}+2(1+s)z-z^{2}}dz}{\\int_{-\\infty}^{\\infty}e^{z-(s-z)^{2}}dz}\\\\\n& = & \\frac{\\int_{-\\infty}^{\\infty}e^{-(z-1-s)^{2}}e^{1+2s}dz}{\\int_{-\\infty}^{\\infty}e^{s+\\frac{1}{4}}e^{-((s+\\frac{1}{2})-z)^{2}}dz}\\\\\n& = & e^{s}e^{3/4}\\frac{\\int_{-\\infty}^{\\infty}e^{-(z-1-s)^{2}}dz}{\\int_{-\\infty}^{\\infty}e^{-((s+\\frac{1}{2})-z)^{2}}dz}\n\\end{eqnarray*}\n\\]\nNote that both of the integrals are independent of \\(s\\) (because the integration is between \\(-\\infty\\) and \\(\\infty\\)), so there exists some \\(\\kappa\\) such that:\n\\[\nE[x|s]=\\text{e}^{s}\\kappa.\n\\]\nFinally we are interested in the average posterior for a given \\(v\\) (here I’m again ignoring all constant terms):\n\\[\n\\begin{eqnarray*}\nE[E[v|s]|v] & = & \\int_{-\\infty}^{\\infty}E[v|s=\\ln v+u]\\text{e}^{-u^{2}}du\\\\\n& = & \\int_{-\\infty}^{\\infty}\\text{e}^{(\\ln v+u)}\\kappa\\text{e}^{-u^{2}}du\\\\\n& = & v\\int_{-\\infty}^{\\infty}\\kappa\\text{e}^{u-u^{2}}du.\n\\end{eqnarray*}\n\\]\nI.e., despite the logarithmic internal representation, the average posterior is linear in the value."
  },
  {
    "objectID": "posts/2023-07-27-meta-2020-elections-experiments.html",
    "href": "posts/2023-07-27-meta-2020-elections-experiments.html",
    "title": "How Much has Social Media affected Polarization?",
    "section": "",
    "text": "TL;DR: The experiments run by Meta during the 2020 elections were not big enough to test the theory that social media has made a substantial contribution to polarization in the US. Nevertheless there are other reasons to doubt it."
  },
  {
    "objectID": "posts/2023-07-27-meta-2020-elections-experiments.html#discussion",
    "href": "posts/2023-07-27-meta-2020-elections-experiments.html#discussion",
    "title": "How Much has Social Media affected Polarization?",
    "section": "Discussion",
    "text": "Discussion\nTrends in affective polarization. Boxell et al. (2022) document affective polarization across a dozen countries, 1978-2020:\n\n\nIn the US affective polarization index increased from around 25 to 50, “an increase of 1.08 standard deviations as measured in the 1978 distribution.” (I’m not sure if the SD increased).\nAcross the world there’s no clear trend: some countries increased, other countries decreased. This weakens the simple argument that polarization has increased at the same time as social media use.\nIn the US the trend seems to be almost entirely due to increasing negative feelings about the opposing party:\n\n\nThe US timeseries can be seen online from the ANES.\nObservational data finds that much of the growth in polarization in the US was among people who were not online. Boxell et al. (2017) say\n\n“the growth in polarization in recent years [1996-2012] is largest for the demographic groups least likely to use the internet and social media”\n\nContent on Meta platforms. Guess et al. (2023b) has data from the control group in their 2020 experiments:\n\n\n\nShare of Impressions\nFacebook\nInstagram\n\n\n\n\nPolitical content\n14%\n5%\n\n\nPolitical news content\n6%\n-\n\n\nContent from untrustworthy sources\n3%\n1%\n\n\nUncivil content\n3%\n2%\n\n\n\nPew 2022 has data on where people get their news from:\n\n\n\n\npct adults regularly get news from\n\n\n\n\ntelevision\n65%\n\n\nnews websites\n63%\n\n\nsearch\n60%\n\n\nsocial media\n50%\n\n\nradio\n47%\n\n\nprint\n33%\n\n\npodcasts\n23%\n\n\n\nRadio show popularity. Around half of the top 20 most-listened radio shows in the US are conservative talk, with around 90M weekly listeners (this is double-counting overlapping users). Data from 2021.\nTelevision. Fox News is Cable TV’s most-watched network with around 5M regular viewers. (source from 2016).\nTime spent on social media. Statista: Average time-spent 150 minutes/day/person on social networks\nThe academic literature has identified other possible causes of polarization. Some potential causes: southern realignment, 1968 changes to the primary system, the Obama presidency, the tea party movement (though each of these could be in part proximal causes). Martin & Yurcoglu (2017) argue that a large part of recent growth is due to cable news: &gt; “the cable news channels can explain an increase in political polarization of similar size to that observed in the US population over [2000-2008]. … In absolute terms, however, this increase is fairly small.”\nSee also Haidt and Bail’s long document Social Media and Political Dysfunction: A Collaborative Review\nDoes Allcott et al. (2020) find that Facebook use increases polarization? This paper reports on an experiment paying people to stop using Facebook for a month. They find an effect of -0.16 SDs (\\(\\pm\\) 0.08) on a measure they describe as “political polarization,” however there are some subtleties:\n\n\nUnlike the questions used in typical population surveys the questions were explicitly about their feelings during the period of the experiment, e.g. “Thinking back over the last 4 weeks, how warm or cold did you feel towards the parties and the president on the feeling thermometer?”\nPolarization is measured by a composite of different measures. By far the largest effect was on the “congenial news exposure” question: “over the last 4 weeks how often did you see news that made you better understand the point of view of the Democrat (Republican) party?” The score was the difference between the answer for their own party vs the other-side party. It seems to me that it’s not surprising that deactivating Facebook would affect one’s exposure to such news, but that this wouldn’t normally be called a measure of “polarization” in the literature. The paper mentions in a footnote that “the effect on the political polarization index is robust to excluding each of the seven individual component variables,” but it turns out that removing “congenial news exposure” halves the effect-size and shifts the p-value from 0.00 to 0.09 (i.e. from very significant to non-significant). I’m not sure I would describe this as a finding that is “robust”.\nThe paper finds no significant effect on their two “affective polarization” measures (-0.08 \\(\\pm\\) 0.08 SD, and 0 \\(\\pm\\) 0.04 SD), however the 2020 papers which cite Allcott et al. (2020) seem to treat it as finding that Facebook has a positive effect on “polarization” without noting that it has a null effect on affective polarization."
  },
  {
    "objectID": "posts/2023-08-02-small-effects.html",
    "href": "posts/2023-08-02-small-effects.html",
    "title": "The Paradox of Small Effects",
    "section": "",
    "text": "In summary:\n\nAttitudes are hard to change. Many fields in social science have adopted a doctrine of “small effects”: high quality studies tend to show that peoples’ attitudes are not very sensitive to exposure to media, or to their peers’ attitudes.\nYet attitudes do change. We see very wide society-level variation in attitudes, which are hard to explain without peer or media effects.\nResolution of the paradox: each effect is small, but there are a lot of them.\n\n(see an earlier Facebook post)\n\n(1) Attitudes are Hard to Change\nMany fields in social science tend to say that attitudes show little influence from either peer effects or from media exposure:\n\nAngrist (2014) says studies of peer effects “have mostly uncovered little in the way of socially significant causal effects.”\nPolitical scientists talk about “the paradox of minimal effects”, Ansolabehere (2006) says that election campaigns “seem to be inessential to understanding who wins and who loses.”\nDavid Stromberg says “the lesson from the last 50 years of media research is that it is very hard to manipulate voters … evidence of [supply side bias] effects is weak or non-existent”\n\nThere are many studies which find large effects but they tend to be treated with extreme skepticism by the methodologists: they are overwhelmingly from lab experiments or observational data and so can be very biased.\n\n\n(2) Attitudes do Change\nAttitudes vary a huge amount across time and space:\n\nVariation in political and religious attitudes.\nVariation in attitudes towards other races, sexes, sexualities, religions.\nVariation in preferences over food, e.g. for rice vs wheat vs corn.\nVariation in preferences over how many children to have.\n\nIt is hard to explain this variation with individual economic circumstances: when someone migrates to another country they face different economic circumstances (different prices and income) but they typically maintain their attitudes for decades.\nIt is hard to explain this variation with genetic variation, because attitudes vary so much over time, while genes move very slowly.\nSo it seems like peer and media effects must be substantial proximal determinants of attitudes.\n\n\n(3) Resolution: Each Effect is Small, but There are Many\nHow can we resolve small treatment effects with big variation in outcomes? It makes sense if we’ve only been testing very small treatments. Each individual effect is small but there are millions of them, so collectively the effects are large.\nPeer effect studies tend to find small effects when looking at random assignment of peers, e.g. random assignment of roommates, but this may be because time with your roommate constitutes only a very small share of your overall exposure to other people and ideas.1 Collectively that exposure must be hugely important in your attitudes.1 Kremer and Levy (2008) say “Most studies do not find effects of these predetermined characteristics on the whole sample of students … conventional peer effects on academic achievement … are not estimated to be particularly important.”\nMedia studies tend to find small effects from exposure to social media or to television, but in most cases the media exposure is only a single-digit percentage-point share of their lifetime exposure to media. So the aggregate effect can be far larger than that measured in any credible experiment or natural experiment (in addition, much of the effect likely propagates through peer effects).\nAn individual campaign advertisement might have very small effects on voting intention, but an individual campaign advertisement is only a tiny tiny share of your lifetime exposure to political communication. Small individual effects are consistent with peoples’ political attitudes being overwhelmingly determined by exposure and persuasion.\nMore technically: we can reconcile small effects with big variations if:\n\nEffects have a long half-life, e.g. exposure in childhood can affect your attitudes as an adult.\nPeer effects are propagated through many weak links, instead of a few strong links: i.e. there are substantial influences from all of society in addition to your family.\nPersuasion works even with indirect channels, e.g. your political views aren’t just affected by campaign ads, but also by the implicit attitudes to politics reflected in all the media you’re exposed to\nAttitudes are sensitive to the average rather than the total amount of persuasive material you’re exposed to, thus marginal effects can be small while total effects are large.\n\n(As a footnote: from my time in social media companies I learned that individual peer effects are tiny, yet we also know that social media demand is entirely peer effects, i.e. people only use Facebook because other people use Facebook.)\n\n\nOther Notes\nThe paradox of large effects. Tosh et al. (2021) discuss an opposite problem: in some fields there are many claims of large effects, but it is not possible to reconcile the aggregate variance in the data with so many large effects. E.g. they discuss a paper claiming to show that exposure to age-related words tends to lower a subject’s subsequent walking speed by 13%. If people are exposed to many such primes, and they are uncorrelated, then we should expect huge and implausible variation in peoples’ day-to-day walking speed.\nTheir problem is somewhat the opposite: they are talking about a literature which has many non-credible effects from lab experiments or observational data. Instead I’m talking about literature which has credible but small effects.\n\n\n\n\n\n\n\n\n\nReferences\n\nAngrist, Joshua D. 2014. “The Perils of Peer Effects.” Labour Economics 30: 98–108. https://doi.org/https://doi.org/10.1016/j.labeco.2014.05.008.\n\n\nAnsolabehere, Stephen. 2006. “The Paradox of Minimal Effects.” Capturing Campaign Effects, 29–44.\n\n\nTosh, Christopher, Philip Greengard, Ben Goodrich, Andrew Gelman, Aki Vehtari, and Daniel Hsu. 2021. “The Piranha Problem: Large Effects Swimming in a Small Pond.” arXiv Preprint arXiv:2105.13445."
  },
  {
    "objectID": "posts/2023-04-18-netflix-cider-experimentation-note.html",
    "href": "posts/2023-04-18-netflix-cider-experimentation-note.html",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "",
    "text": "I give a simple Bayesian way of thinking about experiments, and implications for interpretation and extrapolation.\n\n\n Thanks to J. Mark Hou for comments. \nSetup: The canonical tech problem is to choose a policy to maximize long-run user retention. Because the policy space is high-dimensional it’s not feasible to run experiments on every alternative (there are trillions), instead most of the decision-making is done with human intuition based on observational data, and experiments are run to confirm those intuitions.\n\nThe inference problem. The basic problem of experimentation is to estimate the true effect given the observed effect. The problem can become complicated when we have a set of different observed effects, e.g. across experiments, across metrics, across subgroups, or across time. \nTwo common approaches are: (1) adjust confidence intervals (e.g. Bonferroni, always-valid, FDR-adjusted); (2) adjust point estimates based on the distribution (empirical Bayes). Both have significant drawbacks: my suggested approach is to let decision-makers make their own best-estimates of the true effects but provide them with an informative set of benchmark statistics so they can compare the results of any given experiment to the results from a reference group.1\nThe extrapolation problem. Given an effect on metric A what’s our best estimate of the effect on metric B? This problem is common to observational inference, proximal goals, and extrapolation.\nThere are three approaches to solving this: (1) using raw priors; (2) using correlation across units (surrogacy); (3) using correlation across experiments (meta-analysis). I argue that approach #3 is generally the best option but reasonable care needs to be taken in interpreting the results.\n\n1 If the decision-maker is not technical then a data scientist or engineer can summarize for the decision-maker their best-estimate of the true impact on long-run outcomes, taking into account the evidence from the experiment and other sources of evidence, including the distribution of effects from other experiments.I also briefly discuss two additional problems:\n\nThe explore-exploit problem. We would like to choose which experiments to run in an efficient and automated way. I think the technical solution is relatively clear but tech companies have struggled to implement it because good execution requires some discipline. I describe a simple algorithm that is not optimal but very simple and robust.\nThe culture problem. Inside tech companies people keep misusing experiments and misinterpreting the results, especially (1) running under-powered experiments, (2) selectively choosing results, and (3) looking at correlations without thinking about identification.\nA common response is to restrict access to only a subset of experiment resuts. However this often backfires because (1) it is difficult to formally specify the right subset; (2) it reinforces a perception that experimental results can be interpreted as best-estimates of true treatment effects; (3) it reinforces a norm of selecting experimental results as arguments for a desired outcome. I think a better alternative is to explicitly frame the problem as one of predicting the true effect given imperfect evidence, and benchmark peoples’ prior performance in predicting the true effect of an intervention. (This section is unfinished, I hope to add more)."
  },
  {
    "objectID": "posts/2023-04-18-netflix-cider-experimentation-note.html#strategic-problems",
    "href": "posts/2023-04-18-netflix-cider-experimentation-note.html#strategic-problems",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "1.1 Strategic Problems",
    "text": "1.1 Strategic Problems\nThere are additionally some strategic problems in experiment interpretation.\n\nStrategic stopping (“peeking”). An engineer will wait until an experiment has a high estimated impact, or low p-value, before presenting it for launch review. A common proposed remedy is that all experiments should be evaluated after the same length of time, or that engineers should pre-specify the length of experiments.\nSelection of treatments (“winners curse”). An engineer will run a dozen variants and only present for launch review the best-performing one. A common proposed remedy is that every variant should be officially presented in launch reviews, even the poorly-performing ones.\nSelection of metrics (“cherry picking”). An engineer will choose to show the experiment results on the metrics that are favorable, not those that are unfavorable. A common proposed remedy is that the set of metrics should be standardized for all launches, or that the set of evaluation metrics should be pre-specified by the engineer (AKA a pre-analysis plan).\n\nI will argue that the commonly proposed remedies are highly imperfect fixes. These are complicated things to think about because the mix together issues of statistical inference and of strategic behaviour. In the discussion that follows I try to separate those out as clearly as possible."
  },
  {
    "objectID": "posts/2023-04-18-netflix-cider-experimentation-note.html#strategic-stopping",
    "href": "posts/2023-04-18-netflix-cider-experimentation-note.html#strategic-stopping",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "1.2 Strategic Stopping",
    "text": "1.2 Strategic Stopping\nI will ignore dynamic effects. For simplicity assume that all effects are constant, so the length of an experiment effectively determines just the sample size of that experiment. I.e. I will ignore time-dependent and exposure-dependent effects.\nStopping rules are irrelevant to expected effect sizes. Suppose an experiment has a given estimate. Does it matter to your estimate of the true causal effect if you learn that the experimenter chose the sample size \\(N\\) by a data-dependent rule, e.g. continuing to collect data until the estimate was statistically significant? If you are estimating the true causal effect, \\(E[t|\\hat{t}]\\) then it doesn’t matter, your posterior will be identical either way.6 A simple proof: suppose we observe two noisy signals, \\(x_1\\) and \\(x_2\\): \\[\\begin{aligned}\n         x_1 &= v + e_1 \\\\\n         x_2 &= v + e_2 \\\\\n         v,e_1,e_2 &\\sim  N(0,1)\n      \\end{aligned}\\] Suppose a peeker will report \\(x_1\\) only if \\(x_1&gt;0\\), otherwise they will report \\(x_1+x_2\\). We can compare the expectation of \\(v\\) given the sum, depending on whther the engineer peeked: \\[\\utt{E[v|x_1+x_2]}{estimate}{without peeking} =\n         \\utt{E[v|x_1+x_2|x_1&lt;0]}{estimate}{with peeking}\\]6 This argument holds if the engineer always has to report the most-recent estimate. If they can choose to ignore later datapoints, and report an earlier result, this is essentially a “selection of metrics” case as below, and so the selection rule is relevant for interpretation.\nThis holds because \\(x_1+x_2\\) is a sufficient statistic for the distribution, i.e. \\(x_1&lt;0\\) does not tell us any additional information. Note that peeking is not irrelevant to interpretation of a result if (1) the engineer can choose to report either \\(x_1\\) or \\(x_2\\), (2) the engineer can choose to report \\(x_1\\) alone after observing \\(x_2\\).\nStopping rules would be relevant if we made decisions based on statistical-significance. A stopping rule would be relevant if we conditioned only on statistical-signficance instead of the full estimate. In other words the expected true effect, conditioning only on whether or not the estimated effect is statistically significant, will depend on the stopping rule. For example if people kept running experiments until they were significant then significant experiments would tend to have small effect sizes. However it is clearly bad practice to condition only on this binary piece of information when you have the full estimate, and if you have the full estimate then the stopping rule becomes irrelevant.\nThe optimal stopping rule is data-dependent. The discussion above took a stopping rule as given, we can also ask what’s the efficient stopping rule. It’s clear that a fixed length is inefficient: we should stop an experiment sooner if it does unexpectedly well or unexpectedly badly, in both of those cases the value of collecting more information has decreased because it’s less likely to change our mind about a launch decision. Thus enforcing a static or pre-specific experiment length will lead to inefficient decision-making.\nConsidering engineers’ incentives. Now consider the launch process as a game, with the engineers trying to persuade the director to launch their feature. Suppose the director’s ex post optimal strategy is to launch if \\(E[t|\\hat{t}]&gt;0\\), and suppose the engineers get a bonus whenever their feature is launched. In equilibrium the engineers will keep their experiments running until \\(E[t|\\hat{t}]&gt;0\\), which will cause a skew distribution: the distribution of posteriors will show a cluster just above the threshold. The director’s strategy is ex post optimal but it’s not an efficient use of experimentation resources. In this game the director would likely wish to pre-commit to a different threshold which induces more efficient effort by engineers. However a more direct solution would be to align engineers’ incentives with those of the director by rewarding them for their true impact, i.e. setting their bonuses proportional to \\(\\max\\{E[t|\\hat{t}],0\\}\\), instead of discontinuously rewarding them for whether or not they launched."
  },
  {
    "objectID": "posts/2023-04-18-netflix-cider-experimentation-note.html#selection-of-treatments",
    "href": "posts/2023-04-18-netflix-cider-experimentation-note.html#selection-of-treatments",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "1.3 Selection of Treatments",
    "text": "1.3 Selection of Treatments\nIf you learn an experiment is the top-performing variant it should change your asssessment. Suppose we have a result \\(\\hat{t}_1\\), and we are estimating the true treatment effect, \\(t_1\\). If we learn that another variant has a lower treatment effect, \\(\\hat{t_1}&gt;\\hat{t}_2\\), then it is rational to update our assessment of \\(t_1\\):\n\\[\\utt{E[t_1|\\hat{t}_1,\\hat{t}_1&gt;\\hat{t}_2]}{assessment knowing}{it's winner}&lt;\n      \\utt{E[t_1|\\hat{t}_1]}{assessment}{given outcome}\n      \\]\nThis will hold whenever \\(Cov(t_1,t_2)&gt;0\\), i.e. when we have some shared source of uncertainty about the two treatment effects.7 We can write a model for this, however conditioning on this binary information (whether a variant is the winner) is not an efficient way of using the information at your disposal.7 Because \\(t_1\\) and \\(t_2\\) represent independent experiments we’ll have \\(cov(e_1,e_2)=0\\).\nIt’s better to condition on the whole distribution. In almost all cases we know much more than whether \\(\\hat{t}_1\\) is the winner, we also know the value of \\(\\hat{t}_2\\), and then this reduces simply to the empirical Bayes problem, i.e. we simply wish to estimate: \\[E[t_1|\\hat{t}_1,\\ldots,\\hat{t}_n],\\] and we can do that in the usual way.8 E.g. if we have a Normal prior over treatment effects then we can estimate \\(\\sigma_t^2\\) from \\(Var(\\hat{t})\\) and \\(\\sigma_e^2\\). Once we have conditioned on \\(\\sigma_t^2\\) then it becomes irrelevant whether variant 1 is the winner or not, i.e.: \\[E[t_1|\\hat{t}_1,\\sigma_t^2]=E[t_1|\\hat{t}_1,\\sigma_t^2,\\hat{t}_1&gt;\\hat{t}_2].\\]8 Andrews et al. (2019) describes some unbiased estimates for treatment effects conditional on them being winners. In general I would say this is an inefficient use of information, because we know much more about the distribution of treatment effects than just whether a specific variant is the winner. However that paper does argue that empirical Bayes estimates struggle when the sample-size is small or when we are estimating the tails of when variants are non-exchangeable, and in those cases the unbiased estimators may be useful.\nPut another way: the selection rule is irrelevant (just as the stopping rule is irrelevant) once we condition on the distribution of observed outcomes.\nImplication: show the distribution. If we are worried that engineers are selecting variants based on their outcomes then the simplest and cleanest fix is to calculate the distribution of variants and use that to discount any experiment results, either explicitly with an empirical Bayes estimator, or implicitly by showing the decision-maker the distribution."
  },
  {
    "objectID": "posts/2023-04-18-netflix-cider-experimentation-note.html#selection-of-metrics",
    "href": "posts/2023-04-18-netflix-cider-experimentation-note.html#selection-of-metrics",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "1.4 Selection of Metrics",
    "text": "1.4 Selection of Metrics\nSuppose engineers are selectively presenting the most favorable metrics. Suppose there are two outcome metrics from a single experiment, and the engineer will present whichever is the most favorable. Knowing this fact should rationally affect your judgment of the treatment effect on the presented metric: \\[\\utt{E[t_1|\\hat{t}_1]}{assessment knowing}{only metric 1} &gt;\n      \\utt{E[t_1|\\hat{t}_1,\\hat{t}_1&gt;\\hat{t}_2]}{assessment knowing}{metric 1 beats metric 2}\\]\nImplication: engineers should present all outcome metrics."
  },
  {
    "objectID": "posts/2023-04-18-netflix-cider-experimentation-note.html#on-launch-criteria",
    "href": "posts/2023-04-18-netflix-cider-experimentation-note.html#on-launch-criteria",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "1.5 On Launch Criteria",
    "text": "1.5 On Launch Criteria\nChoosing weights on metrics for a launch decisions involves many considerations: network effects, noise, cross-metric proxy effects, and dynamic effects. In addition launch rules serve a bureaucratic role, and engineers will often want the launch rule to be public and without discretion. To make clear decisions it’s important to peel apart these layers, I recommend these steps:\n\nChoose a set of final metrics. These are the metrics we would care about if we had perfect knowledge of the experimental effect. We can define tradeoffs between them, it’s convenient to express those tradeoffs in terms of percentage changes, e.g. we might be indifferent between 1% DAU, 2% time/DAU, and 5% prevalence of bad content.9\nChoose a set of proximal metrics. These are the metrics on which we are confident we can detect our experiment’s effect, meaning the measured impact will be close to the true impact on these metrics (i.e. has a high signal-noise ratio). To determine whether a metric is moved we can use the fraction of a given class of experiments that have a statistically-significant effect on that metric: if the share is greater than 50% then we can be confident that the estimated effect is close to the true effect.\nIdentify conversion factors between proximal and final metrics. These tell us the best-estimate impact on final metrics given the impact on proximal metrics. Conversion factors can be estimated either from (a) long-running tuning experiments; (b) a meta-analysis of prior experiments with similar designs.\nA final linear launch criteria can then be expressed as a set of conversion-factor weights applied to each of the proximal metrics.10\n\n9 Arguably revenue or profit is a more truly final metric, and these are just proxies, but these are probably close enough to final for most purposes.10 For derivation see Cunningham and Kim (2019)."
  },
  {
    "objectID": "posts/2023-04-18-netflix-cider-experimentation-note.html#comparing-launch-rules",
    "href": "posts/2023-04-18-netflix-cider-experimentation-note.html#comparing-launch-rules",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "1.6 Comparing Launch Rules",
    "text": "1.6 Comparing Launch Rules\n\n\n\n\n\n\n\n\n\n\n\nShip if sum is positive\n\n\n\n\n\n\n\n\nI find it useful to visualize different launch rules. For simplicity suppose our utility function is linear: we have two metrics, 1 and 2, and we care about them equally: \\[U(t_1,t_2)=t_1+t_2.\\] But we only observe noisy estimates \\(\\hat{t}_1,\\hat{t}_2\\).\nKohavi et al. (2020) recommend a stat-sig shipping rule. They say (p105):\n\nIf no metrics are positive-significant then do not ship\nIf some are positive-significant and none are negative-significant then ship\nIf some are positive-significant and some are negative-significant then “decide based on the tradeoffs.\n\nI represent this in the first diagram (but I treat condition 3 as a non-ship). The dotted line represents \\(\\hat{t}_1+\\hat{t}_2=0\\).\nThe stat-sig shipping rule has strange consequences. You can see that this rule will recommend shipping things even with negative face-value utility (\\(U(\\hat{t}_1,\\hat{t}_2)&lt;0\\)), when there’s a negative outcome on the relatively noisier metric. This will still hold if we evaluate utility with shrunk estimates, when there’s equal proportional shrinkage on the two metrics, but if there’s greater shrinkage on the noisier metric it will not hold.\nLinear shipping rules are better. In the margin I illustrate (1) a rule to ship wherever the sum is positive; (2) a rule to ship wherever the sum is stat-sig positive. I have drawn the second assuming that \\(cov(\\hat{t}_1,\\hat{t}_2)=0\\). With a positive covariance the threshold would be higher.\n\n\n\n\n\nThe Leontief sandwich. I assumed above that our true utility function is linear. In fact tech companies often explicitly give nonlinear objective functions to teams, e.g.: \\[\\begin{aligned}\n      \\max_k &\\ A(k)\n         && \\text{(goal)} \\\\\n      \\text{s.t.} &\\ B(k)\\leq \\bar{B}\n         && \\text{(guardrail)}\n   \\end{aligned}\\]\nThis is illustrated at right, the indifference curves are L-shaped so I’ll call it a Leontief utility. Having Leontief preferences can cause some unintuitive decision-making, in particular the tradeoff between \\(A\\) and \\(B\\) will varies drastically depending on your location. One important observation is that if your goal is assessed at the end of some time-point (e.g. at the end of the half) then optimal launch decisions will depend on your future expectations, e.g. you’d be willing to launch a feature that boosts A at the cost of B only if you expect a future launch to make up that deficit in B.\nIn practice I think it’s useful to think of this nonlinear objective function as sitting in the middle of the hierarchy of an organization, with approximately linear objective functions above and below it, i.e. a “Leontief sandwich.”\nAt the highest layer the CEO (or shareholders) care about all the metrics in way that is locally linear, i.e. they do not have sharp discontinuities in how they assess the company’s health. At the lowest layer engineers and data scientists are trying to make individual changes that achieve the Org’s overall goals, but because they only account for a small share of the overall org’s impact they can treat their objectives as locally linear (& likewise in a value function we make linear tradeoffs between objectives because we’re in such a small region). Finally even for orgs which have nonlinear objective functions it’s often reasonable to think of the nonlinearities as “soft”, e.g. if an org comes in slightly below a guardrail the punishment is slight, and if they come in above the guardrail then they will be rewarded. This softening makes the effective objective function much closer to linear, and so I think for many practical purposes it’s reasonable to start with a linear objective function."
  },
  {
    "objectID": "posts/2023-04-18-netflix-cider-experimentation-note.html#with-meta-analysis",
    "href": "posts/2023-04-18-netflix-cider-experimentation-note.html#with-meta-analysis",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "2.1 With Meta-Analysis",
    "text": "2.1 With Meta-Analysis\nWith \\(n\\) metrics we can write the underlying model as: \\[\\utt{\\pmatrix{\\hat{t}_1\\\\\\vdots\\\\\\hat{t}_n}}{observed}{effects}\n      = \\utt{\\pmatrix{t_1\\\\\\vdots\\\\t_n}}{true}{effects}\n         +\\utt{\\pmatrix{e_1\\\\\\vdots\\\\e_n}}{noise}{(=user variation)}\\]\nHere we are treating \\(\\Delta \\text{DAU}_{SR}\\) and \\(\\Delta \\text{DAU}_{LR}\\) as two different metrics, but for some experiments we only observe the first. We thus want to estimate the effect on long-run retention (DAU\\(_{LR}\\)) given short-run metrics. \\[E[\\Delta\\text{DAU}_{LR} |\n       \\Delta \\widehat{\\text{DAU}}_{SR}, \\ldots, \\Delta\\widehat{\\text{engagement}}_{SR}]\\]\nwhere \\[\\begin{aligned}\n      \\Delta\\text{DAU}_{LR}   &= \\textit{true}\\text{ effect on long-run daily active users (AKA retention)}\\\\\n      \\Delta\\widehat{\\text{DAU}}_{SR} &= \\textit{estimated}\\text{ effect on short-run daily active users} \\\\\n      \\Delta\\widehat{\\text{engagement}}_{SR} &= \\textit{estimated}\\text{ effect on short-run engagement}\n   \\end{aligned}\\]\nRunning a Regression will be Biased. The obvious thing to do is run a regression across experiments: \\[\\Delta\\widehat{\\text{DAU}}_{LR} \\sim\n      \\Delta \\widehat{\\text{DAU}}_{SR} + \\ldots + \\Delta\\widehat{\\text{engagement}}_{SR}\\]\nHowever this will be biased. The simplest way to demonstrate the bias is to show that even with AA tests (where there is zero treatment effect on either metric) we will still get a strong predictive relationship between the observed treatment effects on each of the two metrics (see figure).\n\n\n\n\n\nA simulated scatter-plot showing 20 experiments, with N=1,000,000, \\(\\sigma_{e1}^2=\\sigma_{e2}^2=1\\), with correlation 0.8. The experiments are all AA-tests, i.e. there are no true treatment effects, yet a regression of \\(\\hat{t}_2\\) on \\(\\hat{t}_1\\) will consistently yield statistically-significant coefficients of around 0.8.\n\n\nThe bias is because in the regression our LHS variable is estimated retention (\\(\\Delta\\widehat{\\text{DAU}}_{LR}\\) instead of \\(\\Delta\\text{DAU}_{LR}\\)), and the noise in that estimate will be correlated with the noise in the estimates of short-run metrics. In the linear bivariate case (where we have just one RHS variable) then we can write: \\[\\begin{aligned}\n      \\ut{\\frac{cov(\\hat{t}_2,\\hat{t}_1)}{var(\\hat{t}_1)}}{regression}\n      = \\utt{\\frac{cov(t_2,\\hat{t}_1)}{var(\\hat{t_1})}}{what we}{want to know}\n         + \\ut{\\frac{cov(e_2,e_1)}{var(\\hat{t}_1)}}{bias}\n   \\end{aligned}\\]\nThe bias will be small if the short-run metrics have high signal-noise ratios (SNR), \\(\\frac{var(t_1)}{var(e_1)}\\gg 0\\). A simple test for SNR ratio is the distribution of p-values: if most experiments are significant then the SNR is high. However in the typical case (1) \\(\\Delta \\widehat{\\text{DAU}}_{SR}\\) is the best predictor of \\(\\Delta \\widehat{\\text{DAU}}_{LR}\\); and (2) \\(\\Delta \\widehat{\\text{DAU}}_{SR}\\) has a low signal-noise ratio (i.e. few outcomes are stat-sig). This means the bias is large, and so results are hard to interpret.\n\n2.1.1 Adjusting for the Bias\nHere are some alternatives:\n\nRun a regression just using the high-SNR metrics. We could just drop \\(\\Delta\\widehat{\\text{DAU}}_{SR}\\) as a regressor because of the bias, but we lose predictive power (\\(R^2\\)) so it’s hard to know when this will be a good idea without an explicit model.\nAdjust for bias in linear estimator. If we want a linear estimator then we can estimate and adjust for the bias. \\[\\begin{aligned}\n   \\utt{\\frac{cov(t_2,\\hat{t}_1)}{var(\\hat{t_1})}}{BLUE for}{$t_2$ given $\\hat{t}_1$}\n      &= \\frac{cov(t_2,t_1)}{var(\\hat{t}_1)}\n      = \\ut{\\frac{cov(\\hat{t}_2,\\hat{t}_1)}{var(\\hat{t}_1)}}{regression result}\n         - \\utt{\\frac{cov(e_2,e_1)}{var(\\hat{t}_1)}}{observable}{variables}\n\\end{aligned}\\]\nIf everything is joint normal then the expectation is itself linear, and so this will be optimal. In practice the true distribution of effect-sizes is somewhat fat-tailed, which imply that the conditional expectation will be nonlinear in the observables. Nevertheless I think this is a good start. (One other complication is that the SNR is more complicated to calculate when experiments vary in their sample size).11\nUse experiment splitting. You can randomly assign users in each experiment to one or other sub-experiments. You now effectively have a set of pairs of experiments, each of which has experiments with identical treatment effects (\\(\\Delta \\text{DAU}_{LR}\\)) but independent noise. Thus you can run a regression with LHS from one split, and RHS from other split, and you’ll get an unbiased estimate. Additionally you can easily fit a nonlinear model (Coey and Cunningham (2019) has details of how to do an experiment-splitting).\nRun a regression just using the strongest experiments. If the distribution of experiments is fat-tailed then the strongest experiments will have higher SNR, and so lower bias. A worry about this is that you’re only estimating the relationship from outliers, so nonlinearities are more of a worry. At the same time the assumption of fat-tailed treatment-effects gives reason to believe the expectation will be nonlinear. (This is roughly how I interpret the Peysakhovich and Eckles (2018) experiments-as-instruments paper. They propose using L0 regularization and experiment-splitting cross-validations, which I think effectively selects the strongest experiments.)\n\n11 See Cunningham and Kim (2019), and see Tripuraneni et al. (2023) for a slightly different setup with weaker assumptions.Choosing a Reference Class. It is important to think about the reference-class of experiments which we use to calibrate our estimates. The long-run DAU prediction can be though of as an empirical-bayes estimate, which is our best estimate conditional on the experiment being a random draw from this class of experiments.\nIn many cases a company’s experiments will naturally fall into different classes: e.g. some have a very steep relationship between engagement and DAU, others have a very flat. It’s important to both (1) visualize all the experiments, so that a reference-class can be chosen sensibly; (2) calculate the \\(R^2\\) across experiments, so we can have some sense of confidence in our extrapolation."
  },
  {
    "objectID": "posts/2023-04-18-netflix-cider-experimentation-note.html#observational-inference",
    "href": "posts/2023-04-18-netflix-cider-experimentation-note.html#observational-inference",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "2.2 Observational Inference",
    "text": "2.2 Observational Inference\nWhat we want to know: Given the short-run effect of a content experiment on engagement we want to predict the long-run effect on DAU. We can start with a simple regression along these lines: \\[\\utt{\\text{DAU}_{u,t+1}}{long-run}{retention} \\sim \\utt{\\text{engagement}_{u,t}}{short-run}{engagement}\\]\nWe could set up a DAG and discuss the surrogacy conditions. The condition are that (1) all effects of an experiment on DAU are via short-run engagement; and (2) there is no unobserved factor which affects both SR engagement and LR DAU:\n\\[\\xymatrix{\n      &  *+[F-:&lt;6pt&gt;]\\txt{unobserved}\\ar@{.&gt;}[d] \\ar@{.&gt;}[dr] \\\\\n         *+[F]{\\text{experiment}} \\ar[r] \\ar@{.&gt;}@/_1pc/[rr]\n         & *+[F]{\\text{SR engagement}}\\ar[r]\n         & *+[F]{\\text{LR DAU}}\n      }\\]\nIn fact we know that engagement doesn’t literally lie on the causal chain, instead we think engagement is a good proxy for content which might lie on the causal chain.\nIn any case I find the following setup an easier way to think about the assumptions necessary for identification:\nWe can write it out a simple structural model as follows (for compactness I leave out coefficients):\n\\[\\begin{array}{rcccccccc}\n   \\text{engagement}_{u,t}\n      &=& \\utt{\\text{temperament}_{u}}{user-specific}{propensity to engage}  \n      &+& \\utt{\\text{mood}_{u,t}}{time-varying}{mood/holiday/etc.}\n      &+& \\utt{\\text{content}_{u,t}}{content seen}{on platform}\n      &+& \\utt{\\text{distractions}_{u,t}}{other platform effects}{e.g. messages, notifs}\\\\\n   \\text{DAU}_{u,t}\n      &=& \\text{temperament}_{u}\n      &+&\\text{mood}_{u,t}\n      &+&\\utt{\\sum_{s=1}^\\infty\\beta^s\\text{content}_{u,t-s}}{prior experience}{w content}\n      &+&\\text{distractions}_{u,t}\\\\\n\\end{array}\\]\nSome general observations:\n\nWe would get a more credible estimate if we could directly measure content quality. E.g. if we could use the quality of the content available to the user on the RHS, instead of just their engagement on that content. This wouldn’t get perfect identification but it would help.\nThe relative shares of variation in the RHS is important. If most of the variation in engagement is due to variation in content (i.e. high \\(R^2\\) from content), then we don’t need to worry much about confounding from other effects. We can think of introducing control variables as a way of increasing the share of varation in engagement due to content.\nWe should control for distractions. If we have measures of app-related events that don’t affect content-seen but do affect engagement, e.g. notifications, messages, then we should use those as controls. This will increase the relative share of variation in engagement due to content.\nControlling for pre-treatment outcomes changes variation used. If we control for engagement\\(_{t-1}\\) this will change the relative contribution of each factor in the variation of engagement. Specifically it will reduce the share of the terms with higher autocorrelation. Thus by definition temperament will reduce its contribution. However it’s unclear whether mood or content has higher autocorrelation, and so controlling for pre-treatment could either increase or decrease the relative contribution of content. It’s probably worth doing some simple decomposition of variation in engagement into (1) user, (2) content, and (3) mood (the residual), both statically and over time.\nUnivariate linear prediction is usually pretty good. In my experience you can get a fairly good prediction of most user-level metrics with a linear function of the lagged values. If you use a multivariate or nonlinear function you’ll get a better fit but only by a small amount (one exception: when predicting discrete variables like DAU it’s useful to use a continuous lagged variable like time-spent). So I’m skeptical that adding more regressors or adding nonlinearity will significantly change the estimates or the credibility of the estimates.\nEstimand is not \\(\\beta\\) but \\(\\frac{1}{1-\\beta}\\). Suppose we see that 1 unit of engagement causes a certain increase in DAU over the following weeks. We then want to apply that estimate to an experiment which permanently increases engagement by 1 unit. We thus should take the integral over all the subsequent DAU effects. In the simple exponential case the effect of a shock at period \\(t\\) on DAU at period \\(t+s\\) will be \\(\\beta^s\\), and so the cumulative effect on all subsequent periods will be \\(1+\\beta+\\beta^2+\\ldots=\\frac{1}{1-\\beta}\\).\nAutocorrelation in content makes things messier. If there is significant autocorrelation in content then the interpretation of DAU~engagement is more difficult. E.g. if we see that engagement on \\(t\\) is correlated with DAU on \\(t+1\\) this could be because either (1) content on \\(t\\) content caused the DAU on \\(t+1\\), or (2) good content on \\(t\\) is correlated with good content on \\(t+1\\), which in turn causes DAU on \\(t+1\\). I don’t think controlling for pre-treatment levels or trends solves this."
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html",
    "href": "posts/2017-12-10-unconscious-influences.html",
    "title": "On Unconscious Influences (Part 1)",
    "section": "",
    "text": "Over a couple of years I spent a lot of time in offices looking out the window, thinking about decision-making & the unconscious, scribbling little bits & pieces in a notebook.\n\n\n\nNBER\n\n\nI ended up writing two papers - “Hierarchical Aggregation of Information and Decision-Making” by myself and “Implicit Preferences Inferred from Choice” with Jon de Quidt. The papers are fairly technical, and this post is going to be a layperson’s guide to the background, what’s known about unconscious knowledge, and a tiny bit about the ideas in those papers.\nHere is the argument in a nutshell:\n\nThere are plenty of reasons to think that unconscious influences are strong – in other words, that people have limited insight into what factors influence their decisions.\nThe idea of unconscious influences has been in and out of the mainstream of psychology for the last 200 years, but always hounded by arguments over what it means, i.e. over what evidence would be sufficient to show that a decision was influenced by an unconscious factor. The battle has had many reversals: a new types of evidence has been proposed which is thought to reveal unconscious influences, and then later the technique or interpretation is shown to have substantial flaws and the line of inquiry fizzles out. A couple of decades pass and a different approach becomes popular.\nTwo broad classes of evidence are the following: (A) people reveal their unconscious preoccupations in their involuntary responses – in how their pupils dilate, how quickly they respond to a stimulus, in their word associations, dreams, slips of the tongue; (B) people reveal unconscious influences in discrepancies between how they act and how they explain their behaviour. Both sources of evidence have got tangled in debates about interpretation, and there are substantial camps on either side with not much agreement on what constitutes sufficient evidence for unconscious influences.\nA third type of evidence is less common but, I think, more powerful: evidence from inconsistencies in decision-making. The idea being that unconscious factors are by their nature isolated from conscious factors, i.e. they don’t interact with conscious beliefs and desires, and this isolation will cause certain characteristic inconsistencies among decisions.\nThis can be made precise with an analogy: the relationship between the conscious and unconscious brain is like the relationship between a blind man and his guide dog. The blind man makes decisions based, in part, on which direction the guide dog is pulling towards, so the guide dog’s beliefs and desires influence the man’s decisions, but without the man knowing exactly what those beliefs and desires are, and so he couldn’t tell you how much any particular factor contributed to his decision. Testing for unconscious influences in behaviour is just testing the degree to which your brain is being led by a guide dog.\nThe internal-consistency definition of unconscious influences implies two ways of looking for them: (1) testing whether people can accurately answer hypothetical questions about decisions they would make if factors changed - i.e. navigating without your guide dog; and (2) testing whether people make consistent judgments when judging two outcomes at a time.\nFirst, hypothetical questions. We can ask people, how would your judgment change if this factor changed? Would you still like this painting if the name of the artist was different? Would this drawing look more like your cousin if the nostrils were bigger? Unconscious influences imply that people will not be able to give accurate answers to these hypothetical questions because if the description of the situation is abstract then their unconscious brain won’t be able to evaluate it (AKA, they don’t know which direction they would go in without knowing what their guide dog will say).\nThe second way of testing for unconscious influences is what my paper with Jon is about: unconscious influences particularly leave their mark in comparisons, where you evaluate two outcomes simultaneously or consecutively, or when you choose between two outcomes. When confronted with two outcomes you surface two unconscious judgments and that gives you some insight into what is affecting those judgments, which in turn will affect your conscious decision.\nSuppose you had an unconscious preference for men over women, but a conscious preference to be indifferent, this will manifest in the following: (A) when you see two CVs which are identical, except that one is a man and one is a woman, then you’re indifferent between them; (B) when you see two CVs which differ in some other respect (e.g. one has a PhD, the other has an MBA), then you consistently have a preference for the CV belonging to the man. Your guide dog has a bias towards men, which you’re not aware of: the bias will only sway your decision in the second case because, in the second case, when your guide dog pulls you towards the man with a PhD, you cannot figure out how much of that pull is due to his being a man, and how much is due to his PhD.\nIn the end I think that our brains are full of guide dogs all pulling in different directions. If we had the stomach for it we could plot out our decisions all on a map – measure how each factor influences our judgment – and we would be able to see both the surface influences and the deeper latent influences.\n\n\n\n\nMotivating examples\nSome definitions & theory\nWays of measuring implicit preferences\nThe proposal\n\n\n\n\nLittauer"
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html#contents",
    "href": "posts/2017-12-10-unconscious-influences.html#contents",
    "title": "On Unconscious Influences (Part 1)",
    "section": "",
    "text": "Motivating examples\nSome definitions & theory\nWays of measuring implicit preferences\nThe proposal\n\n\n\n\nLittauer"
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html#involuntary-responses",
    "href": "posts/2017-12-10-unconscious-influences.html#involuntary-responses",
    "title": "On Unconscious Influences (Part 1)",
    "section": "Involuntary responses",
    "text": "Involuntary responses\nFreud is the most famous theorist of extracting unconscious factors from involuntary responses – he wrote three books on different methods: one on dreams, one on jokes, one on mistakes (mis-reading, mis-hearing, mis-speaking). An example from the last book: “A woman who is very anxious to get children always reads ‘storks’ instead of ‘stocks’.” Most of Freud’s examples of unconscious influences are much more complex than this one, and more often the hidden factor influencing behaviour is something unpleasant or shameful.\nAnother way of measuring unconscious cognition is through measuring arousal. Most famous is the “Iowa card task” from Bechara et al. (1996). They had their subjects choose among playing cards, and receive rewards if they chose certain cards. They found that people gradually learned which types of cards were rewarded, but they also found that the subjects’ automatic responses (measured by skin conductance, i.e. sweating) would show an awareness of the pattern more quickly than the subjects’ choices would: after a while, when the subject’s hand hovered over one of the cards which was rewarded, the subject would sweat a little more, even though the subject wasn’t any more likely to choose that card. They said that this showed that unconscious learning was outpacing conscious learning. Antonio Damasio, one of the authors of this study, went on to write Descartes’ Error which accused Descartes’ of starting the great misapprehension that emotions and reason are in competition – Damasio said that his experiments show how emotions inform reason and improve decision-making. A lot of subsequent papers tried to show that snap decisions, which avoid conscious processing, can produce better outcomes than slow considered decisions.\nEven more famous is the “Implicit Association Test” (IAT) (Greenwald, McGhee and Schwartz (1998)). Subjects are told to press a button whenever they see something from either of two different categories of stimuli, e.g. press the button if you see either a black face or a word with a positive association. Their finding, much-replicated, was that people are relatively quicker at tasks (meaning they have shorter response times) when they are asked to identify a set such as “black face or negative word” or “white face or positive word” than to identify a set like “black face or positive word” or “white face or negative word.” They find that this occurs even among people who report no conscious negative feelings towards black people, and they interpret this as revealing an unconscious association between black people and negative feelings, and they argue that this association could affect your decision-making without you being aware of it.\nMany other measures of automatic responses have been popular at different times: hypnosis and word association (Freud used both of these before moving to talking therapy); Rorsach blots (AKA inkblot tests); thematic apperception test (interpret an ambiguous drawing, still widely used); lie detectors AKA polygraphs (they measure autonomic responses - blood pressure, pulse, respiration, and skin conductivity - as you are asked different questions).\nUnfortunately a great deal of this research turns out to be both hard to replicate, and reliant on strong assumptions in order to interpret as surfacing unconscious associations. Newell and Shanks (2014) give strong arguments for both of these points, covering many of the methods I mentioned here.\nIt is worth mentioning that, although Freud’s more elaborate theories died off, his idea that psychosomatic illness is an indirect expression of a psychological stress, especially about something shameful, I believe remains one of the standard theories of modern neurology (O’Sullivan, 2015).\nHowever even if we had solid evidence for unconscious influences on involuntary responses, this still stops short of unconscious influences on decision-making. It’s possible that our associations show up in sweating, response time, and dreams, but have little effect on decision-making, and if that’s so then unconscious associations are not terribly important for social science. Most of the authors in this literature have assumed that the unconscious factors they identify affect real decisions but have left that extrapolation untested. Blanton et al. (2009) say that there’s no persuasive evidence that implicit racial bias, as measured by the IAT, predicts peoples’ decision-making, once you control for measures of explicit racial bias, i.e. when you just ask people how they feel about black people. (Singal (2016) has a long discussion on this point)."
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html#ability-to-describe-the-influence",
    "href": "posts/2017-12-10-unconscious-influences.html#ability-to-describe-the-influence",
    "title": "On Unconscious Influences (Part 1)",
    "section": "Ability to Describe the Influence",
    "text": "Ability to Describe the Influence\nA second type of evidence is to compare self-reported influences on behaviour with actual influences on behaviour. Here are some examples:\n\nIn the mid 20th-century behaviourists found that they could shape their subjects choices through conditioning with rewards and punishments, and the subjects seemed to remain ignorant of this shaping. For example if you say ‘mm-hmm’ whenever someone uses a plural noun, then after a while that person ends up using plural nouns more often, apparently unaware of the influence (Thorndike and Rock (1934); Greenspoon (1955)).\nSince the 1970s social psychologists have published all sorts of experiments in which they vary an apparently irrelevant factor and find that this can affect peoples’ decision-making. Nisbett and Wilson (1977) summarize a lot of experiments and say “subjects are sometimes (a) unaware of the existence of a stimulus that importantly influenced a response, (b) unaware of the existence of the response, and (c) unaware that the stimulus has affected the response.”\nAnother paradigm from the 1970s asks people to make a judgement - e.g. which stock to pick - and also to rate the importance of factors which contributed to their decision. Slovic et al. (1972) find a low correlation (0.34) between the ratings that a stockbrokers put on factors, and the actual influence of these factors on their decisions. There is a small literature with similar findings across a variety of tasks.\nFinally, since the 1970s a smaller group of psychologists have been running experiments in which people learn a complicated pattern, and then are asked about their insight into it. E.g. in Arthur Reber’s “artificial grammar” experiments subjects learn, through trial and error, to discriminate between two categories of words. After some time they become very good at the task, but when asked to explain how they are making decisions they often say they don’t know, or they come up with rules that do not match their actual performance.\n\nAs in the previous category, a lot of this evidence is very fragile: either hard to replicate, or based on delicate interpretations of what is happening in the experiment. Newell and Shanks (2014) again give a good summary.\nAn additional problem is that these findings could reflect knowledge being difficult to articulate, without it being unconscious. And this literature is full of reversals which bear this out: when experiments are repeated it has often turned out that the subjects do report awareness of the pattern that they have learned if they are asked the question in a different way. Mitchell et al. (2009) say “[i]t is very difficult to provide a satisfactory demonstration of unaware conditioning simply by showing conditioning in the absence of awareness. This is because it is very difficult to be sure that the awareness measure and the conditioning measure are equally sensitive.”\n\n\n\nIIES"
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html#isolation-of-unconscious-influences",
    "href": "posts/2017-12-10-unconscious-influences.html#isolation-of-unconscious-influences",
    "title": "On Unconscious Influences (Part 1)",
    "section": "Isolation of Unconscious Influences",
    "text": "Isolation of Unconscious Influences\nFinally there’s a third type of evidence which is more strictly behavioural: an unconscious factor is one which is isolated from your other conscious beliefs and desires – i.e. it does not interact with conscious factors – and that isolation will be reflected in your behaviour. This isolation criterion has been given various names, but I don’t think it’s ever been explained as clearly as it could be.\nTo be precise think of the blind man (the conscious part of the brain) and the guide dog (the unconscious). The guide dog can know something – e.g. she knows when the crossing light is flashing – which the man does not know, and her knowledge will influence the man’s decisions through her recommendation of when to cross. However the guide dog’s knowledge is isolated from the man’s knowledge: it only influences his decisions through the narrow channel of pulling on the leash. Suppose you tell the man that the crossing lights are not working properly, and so whatever color they show is entirely at random and uninformative. The man and dog, considered as a system, has two pieces of information: (1) the light is green (i.e. indicating ready to cross); and (2) the color is uninformative. However the two pieces of information are known by different actors, implying that they will not be integrated, because neither the man or dog knows both. This will be reflected in the man’s behaviour: he will be influenced by the guide-dog’s recommendation, because the dog sees other things in addition to the crossing-light, such as oncoming traffic. And so the man’s behaviour will still be influenced by the color of the light, even though he knows that the color is irrelevant.\nIf information is separated in the brain, we ought to see characteristic patterns of that in behavior. I know of just a few cases where the isolation of knowledge has come up clearly in trying to define or measure unconscious influences.\nStich (1978) said that certain mental states are “inferentially unintegrated”: \n\n“[unconscious beliefs are] largely inferentially isolated from the large body of inferentially integrated beliefs to which a subject has access”\n\nHe gives an example: suppose Noam Chomsky has a theory of grammar, and that there exists some grammatical rule which is a counterexample to that theory. If a linguist knows that rule consciously, then the linguist will immediately infer that Chomsky’s theory is false. But if the linguist only knows the rule unconsciously, then they won’t be able to make that inference, because the knowledge is “inferentially unintegrated” – i.e. the knowledge is isolated from the knowledge regarding Chomsky’s theory. 1\nA separate place where this separation has come up is in the work of Zenon Pylyshyn and Jerry Fodor since the 1980s regarding perception being “cognitively impenetrable,” or “informationally encapsulated.” They mean that perceptual processes often make inferences without taking into account all the information that is available, i.e. by drawing only on a subset of information. Their principal argument was from perceptual illusions: they argue that illusions can typically be understood as rational inferences from a subset of the information available. Helmholtz had a nice example: if you close one eye and press with your finger on the edge of your eyelid then you’ll perceive a point of light, but the light will be coming from the opposite side of your field of vision from where your finger is. This is because the left side of your retina receives light from the right side of your visual field and vice versa. So when your retina receives some stimulation on the left-hand side your brain makes infers that light is coming from the right-hand side. This is a sensible inference given only the information that your eye has, i.e. just the information from the retina. In this case there is additional information - the fact your finger is pressing on your eyelid - which should give a different interpretation to the stimulation, but your visual cortex is not wired up to incorporate that information, and so it misinterpret the signals it receives.\nThe Helmholtz-Fodor-Pylyshyn model of encapsulated inference isn’t quite the same as the case of the blind man and the guide dog. In their examples the pre-conscious process have a strict subset of the information available to the conscious brain. In other words the man isn’t blind, it’s just a case where the dog leads in a different direction than the man would. Fodor (1983) does have a brief discussion on whether early perceptual processes have access to information not available to the conscious brain, which would imply unconscious influences, in my sense.\nFinally the isolation argument has appeared in the literature on human “associative learning,” in testing whether or not the associations that we learn through conditioning are conscious. A typical experiment involves ringing a bell and then giving subjects a small electric shock. After a while people learn to flinch when they hear the bell. For a long time psychologists tried to map out the logic of how such associations would form, trying to figure out the rule which governed learning of associations. However in the last few decades an argument has been made that these learned associations are not in fact mechanical - there is no simple rule - instead they are more-or-less optimal responses to the environment based on the entirety of the information available, i.e. they are not isolated from other knowledge, though the argument isn’t usually put in terms of conscious vs unconscious knowledge. For example Colgan (1970) told subjects, after they learned an association, that the association is no longer valid (“from now on the bell will not signal an electric shock”) and he found that, although this didn’t entirely extinguish the flinching, it did cause it to markedly decrease. This implies the flinching is not isolated from your conscious knowledge: the association, at least to some degree, interacts with more abstract knowledge. There are many other circumstances where rule-based theories of association-learning have foundered because it turns out that peoples’ responses respond to outside considerations. De Houwer, Vandorpe and Beckers (2005) summarize the evidence against associative models (which can be interpreted as models with unconscious knowledge):\n\nThe two types of models can be differentiated … by manipulating variables that influence the likelihood that people will reason in a certain manner but that should have no impact on the operation of the associative model. We have seen that such variables (e.g., instructions, secondary tasks, ceiling effects, nature of the cues and outcomes) do indeed have a huge effect. Given these results, it is justified to entertain the belief that participants are using controlled processes such as reasoning and to look for new ways to model and understand these processes.\n\nMitchell says:\n\n“The results consistently show evidence for skin conductance [effects] only in participants who are aware of the [relationship] … [a]lthough there are many papers arguing for unaware conditioning, close inspection reveals, in almost all cases, that the measure of conditioning was most likely more sensitive than that of awareness.”\n\nIn retrospect a lot of behavior that was studied in the lab, which was thought to be telling us about the wiring of the animals, actually was telling us about the world outside the animal, because it has turned out that the animals’ response is the optimal response to the typical circumstances it faces in the world. (See my other post The Repeated Failure of Laws of Behaviour , and also Mitchell et al. (2009) section 4.3)\nIf this line of thought were entirely correct – if all information was integrated and fed into every decision – then there would be no unconscious influences in my sense. However I do think that there’s plenty of evidence that remains for a lack of integratation between cognitive processes.\nIn Part 2 of this essay I will give a more formal statement of how decisions can reveal unconscious knowledge (and unconscious motivations), and a survey what I think is the strength of the evidence.\n\n\n\nCaltech"
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html#footnotes",
    "href": "posts/2017-12-10-unconscious-influences.html#footnotes",
    "title": "On Unconscious Influences (Part 1)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nQuine says you shouldn’t call this type of thing unconscious knowledge – your linguistic practice may obey some rule, but you can’t say that you unconsciously know that rule, because there are infinitely many different rules that would imply that pattern of behavior. But this skeptical objection is too tough: Quine would deny that a cow can have a belief about where a water trough is, & instead admit only that the cow’s behavior is consistent with a particular belief among infinitely many others.↩︎"
  },
  {
    "objectID": "posts/2023-04-28-ranking-by-engagement.html",
    "href": "posts/2023-04-28-ranking-by-engagement.html",
    "title": "Ranking by Engagement",
    "section": "",
    "text": "Thanks to comments from Jeff Allen, Jacquelyn Zehner, David Evan Harris, Jonathan Stray, and others. If you find this note useful for your work send me an email and tell me :).\nSix observations on ranking by engagement on social media platforms:\nIn an appendix I formalize the argument. I show that all these observations can be expressed as covariances between different properties of content, e.g. between the retentiveness, predicted engagement rates, and other measures of content quality. From those covariances we can derive Pareto frontiers and visualize how platforms are trading-off between different outcomes."
  },
  {
    "objectID": "posts/2023-04-28-ranking-by-engagement.html#formal-observations",
    "href": "posts/2023-04-28-ranking-by-engagement.html#formal-observations",
    "title": "Ranking by Engagement",
    "section": "Formal Observations",
    "text": "Formal Observations\nHere I describe a few formal properties of a model of ranking based on a joint-normal distribution of attributes. I have a longer writeup with proofs of these results which I hope to publish soon, I am happy to share a draft on request.\n\nThe covariance between item attributes will determine a Pareto frontier among outcomes. Suppose we know the joint distribution of attributes and we can choose a subset with share \\(p\\) of the distribution (e.g. a fixed number of impressions given a pool of possible stories to show), and we want to calculate the average value of each attribute in the subset of content shown to the user. Then we can describe the Pareto frontier over subsets, i.e. the set of realized average outcomes, and it will be a function of the covariances among attributes over pieces of content. With 2 attributes the Pareto frontier will be an ellipse with shape exactly equal to an isoprobability curve from the joint density.\nThe shape of the ellipse has a simple interpretation. If two attributes are positively correlated then the Pareto frontier will be tight meaning there is little tradeoff, i.e. we will have similar aggregate outcomes independent of the relative weights put on each outcome in ranking. If instead two attributes are negatively correlated then the Pareto frontier will be loose meaning outcomes will vary a lot with the relative weights used in ranking.\nOur assumption that the share \\(p\\) is fixed is equivalent to assuming that any ranking rule will get the same number of impressions. This assumption obviously has some tension with retentiveness being an outcome variable: if some ranking rule has low retentiveness, then we would expect lower impressions. Accounting for this would make the Pareto frontier significantly more complicated to model, for simplicity we can interpret every attribute except retentiveness as a short-run outcome. Alternatively we could interpret them as relative instead of absolute outcomes, e.g. as engagement/impression or engagement/DAU.\nImproving a classifiers will stretch the Pareto frontier. As a classifier gets better the average prediction will stay the same but the variance will increase, meaning the Pareto frontier will stretch out, and given a linear indifference curve we can derive the effect on outcomes.\nThe joint distribution plus utility weights will determine ranking weights. If we observe only some outcomes then we can calculate the conditional expectation for other outcomes. Typically we want to know retentiveness, and we can write the conditional expectation as follows: \\[E[\\text{retentiveness}|\n   \\text{engagement},\\ldots,\\text{user preference}].\\] This expectation has a closed-form solution when the covariance matrix is joint normal. When we have just two signals, for example engagement and quality, we can write:\n\\[\\begin{aligned}\n   E[r|e,q] &= \\frac{1}{1-\\gamma^2}(\\rho_e-\\gamma\\rho_q)e +\n               \\frac{1}{1-\\gamma^2}(\\rho_q-\\gamma\\rho_e)q\\\\\n   r     &= \\text{retentiveness}\\\\\n   e     &= \\text{engagement (predicted)}\\\\\n   q     &= \\text{quality (predicted)}\\\\\n   \\rho_{e}     &= \\text{covariance of engagement and retentiveness}\\\\\n   \\rho_{q}     &= \\text{covariance of quality and retentiveness}\\\\\n   \\gamma     &= \\text{covariance of engagement and quality}\n\\end{aligned}\\]\nNote that the slope of the iso-retentiveness line in \\((e,q)\\)-space will be \\(-\\frac{\\rho_e-\\gamma\\rho_q}{\\rho_q-\\gamma\\rho_e}\\).\nExperiments which vary ranking weights tell us about covariances. We can write findings from experiments as follows. First, suppose we find that retention is higher when ranked by engagement than when unranked, this can be written:\n\\[\\begin{aligned}\n      \\utt{E[r|e&gt;e^*]}{ranked by}{engagement} &&gt; \\ut{E[r]}{unranked}\n   \\end{aligned}\\]\nHere \\(e^*\\) is chosen such that \\(P(e&gt;e^*)=p\\) for some \\(p\\), representing the share of potential inventory that the user consumes. This implies that engagement must positively correlate with retentiveness, \\(\\rho_e&gt;0\\).\nNext we can express that retention is higher when we put some weight \\(\\beta\\) on quality:\n\\[\\begin{aligned}\n   \\utt{E[r|e+\\beta q&gt;\\kappa^*]}{ranked by}{engagement and quality} &&gt; \\utt{E[r|e&gt;e^*]}{ranked by}{engagement}\n\\end{aligned}\\]\nHere \\(\\kappa^*\\) is chosen such that \\(P(e+\\beta q &gt; \\kappa^*)=P(e&gt;e^*)=p\\). If \\(\\beta\\) is fairly small then we can infer that the iso-retentiveness line is downward-sloping, implying: \\[\\frac{\\rho_e-\\gamma\\rho_q}{\\rho_q-\\gamma\\rho_e}&gt;0.\\]\nThis implies that both engagement and quality have the same sign. I don’t think they both can be negative, so they both must be positive:\n\\[\\begin{aligned}\n      \\rho_e - \\gamma \\rho_q &&gt; 0 \\\\\n      \\rho_q - \\gamma \\rho_e &&gt; 0.\n   \\end{aligned}\\]\nI think it’s reasonable to treat preferences as locally linear. To have a well-defined maximization problem (with an interior solution) we need either nonlinear preferences or a nonlinear Pareto frontier. It’s always easier to treat things as linear when you can, so a relevant question is which of these two is closer to linear? Internally companies often treat their preferences as nonlinear, e.g. setting specific goals and guardrails, but those are always flexible and often have justifications as incentive devices. Typical metric changes are small, only single-digit percentage points, over that range the Pareto frontier does show significant diminishing returns while (it seems to me) value to the company does not."
  },
  {
    "objectID": "posts/2023-04-28-ranking-by-engagement.html#overviews-of-recommender-systems-chronological",
    "href": "posts/2023-04-28-ranking-by-engagement.html#overviews-of-recommender-systems-chronological",
    "title": "Ranking by Engagement",
    "section": "Overviews of Recommender Systems (chronological)",
    "text": "Overviews of Recommender Systems (chronological)\n\nAdomavicius and Tuzhilin (2005) “Toward the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible Extensions”\n\nAn influential overview of recommender systems (14,000 citations!). The canonical example is recommending movies to get the highest predicted rating. They use “rating” as similar to “engagement”. A more recent survey is Roy and Dutta (2022).\n\nDavidson et al. (2010) “The YouTube Video Recommendation System”\n\n\n“[videos] are scored and ranked using … signals [which] can be broadly categorized into three groups corresponding to three different stages of ranking: 1) video quality, 2) user specificity and 3) diversification.””\n\n\n\n\n“The primary metrics we consider include click through rate (CTR), long CTR (only counting clicks that led to watches of a substantial fraction of the video), session length, time until first long watch, and recommendation coverage (the fraction of logged in users with recommendations).”\n\n\n\nThey say recommendations are good because they have high click-through rate. - A blog post from 2012 discusses a switch from views to watch time: “Our video discovery features were previously designed to drive views. This rewarded videos that were successful at attracting clicks, rather than the videos that actually kept viewers engaged. (Cleavage thumbnails, anyone?)”\n\nGomez-Uribe and Hunt (2015) “The Netflix Recommender System: Algorithms, Business Value, and Innovation”\n\nClearly states that they evaluate AB tests using engagement, but it is regarded as an imperfect proxy for retention:\n\n\n\n“we have observed that improving engagement—the time that our members spend viewing Netflix content—is strongly correlated with improving retention. Accordingly, we design randomized, controlled experiments … to compare the medium-term engagement with Netflix along with member cancellation rates across algorithm variants. Algorithms that improve these A/B test metrics are considered better.”\n\n\n\nCovington et al. (2016) “Deep Neural Networks for YouTube Recommendations”\nThis paper proposed a very influential architecture for content recommendation (the paper has 3000 citations). They say:\n\n“Our final ranking objective is constantly being tuned based on live A/B testing results but is generally a simple function of expected watch time per impression. Ranking by click-through rate often promotes deceptive videos that the user does not complete (“clickbait”) whereas watch time better captures engagement”\n\nLada, Wang, & Yan (2021, FB Blog) How does news feed predict what you want to see?\n\nThorburn, Bengani, & Stray (2022, Understanding Recommenders) “How Platform Recommenders Work”\n\nThis is an excellent short article with description and illustration of the stages in building a slate of content: moderation, candidate generation, ranking, and reranking. Includes links to many posts from platforms describing their systems.\n\n\n\n\n\n\n\nArvin Narayanan (2023) “Understanding Social Media Recommendation Algorithms”\n\nA good overview of recommendation algorithms, with an in-depth discussion of Facebook’s MSI.\n\n\nCriticisms of social media recommendation: (1) harm users because “implicit-feedback-based feeds cater to our basest impulses,” (2) harm creators because “engagement optimization … is a fickle overlord,” (3) harms society because “social media platforms are weakening institutions by undermining their quality standards and making them less trustworthy. While this has been widely observed in the case of news … my claim is that every other institution is being affected, even if not to the same degree.”\n\n\nThe technical part of the essay is excellent but I found some of the arguments about harm and social effects hard to follow.\n\n\nKleinberg, Mullainathan, and Raghavan (2022)"
  },
  {
    "objectID": "posts/2023-04-28-ranking-by-engagement.html#proposals-for-change-chronological",
    "href": "posts/2023-04-28-ranking-by-engagement.html#proposals-for-change-chronological",
    "title": "Ranking by Engagement",
    "section": "Proposals for Change (chronological)",
    "text": "Proposals for Change (chronological)\n\nAndrew Mauboussin (2022, SurgeAI) “Moving Beyond Engagement: Optimizing Facebook’s Algorithms for Human Values”\n\nSays that the problem is “the most engaging content is often the most toxic.” They propose using human raters, e.g. ask people “did this post make you feel closer to your friends and family on a 1-5 scale?” They label a small set of FB posts as a proof of concept.\n\nBengani, Stray, & Thorburn (2022,Medium) “What’s Right and What’s Wrong with Optimizing for Engagement”\n\nThey define engagement as “a set of user behaviors, generated in the normal course of interaction with the platform, which are thought to correlate with value to the user, the platform, or other stakeholders.” Reviews evidence for good and bad effects of ranking by engagement.\n\n\nOvadya & Thorburn (2023). Bridging Systems: Open Problems for Countering Destructive Divisiveness across Ranking, Recommenders, and Governance\n\nStray, Iyer, Larrauri (2023) “The Algorithmic Management of Polarization and Violence on Social Media”\n\n\nOur overall goal should be to minimize “destructive conflict”.\n\n\n\n\nThe major lever used has been content moderation: changing the visibility of content based on semantic criteria (e.g. downranking toxic, disallowing hate speech).\n\n\n\n\nHowever we should put relatively more work on system design, e.g. adding friction or changing the mechanics of sharing or engagement-based ranking. In part because there’s a robust correlation between content that causes destructive conflict and content that is engaging.\n\n\n\nMilli, Belli, and Hardt (2021) (2021) “From Optimizing Engagement to Measuring Value”\n\nMilli, Pierson and Garg (2023) Choosing the Right Weights: Balancing Value, Strategy, and Noise in Recommender Systems\n\nI find the model a little hard to follow. \n\n\n\n\nLubin & Gilbert (2023) “Accountability Infrastructure: How to implement limits on platform optimization to protect population health”\n\nA very wide-ranging and loose discussion of issues related to ranking content. Makes an analogy with 19th century measures to control public health. I think the main proposal is that firms come up with metrics to measure their effect on social problems such as mental health, and regularly report on how they’re doing. They suggest requirements for platforms of different sizes:\n\n\n\n\n\n\n\n\n\n1M+\nSubmitted plan for metrics and methods for evaluation of potential structural harms\n\n\n10M+\nConsistent data collection on potential structural harms\n\n\n50M+\nQuarterly, enforceable assessments on product aggregate effects on structural harms, with breakouts for key subgroups\n\n\n100M+\nMonthly, enforceable assessments on product aggregate effects as well as targeted assessments of specific product rollouts for any subproduct used by at least 50 million users, with breakouts for key subgroups"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html",
    "href": "posts/2016-04-30-relative-thinking.html",
    "title": "Relative Thinking",
    "section": "",
    "text": "peaches\nThere are a lot of cute thought experiments where the apparent value of something depends on what it’s compared to:\nMany people have felt that there’s a common principle at work, in particular: that the sensitivity to an attribute (price, probability, square feet) depends on the set of quantities that you’re considering. But different people have proposed different principles:\nAll of these models can be thought about as indifference curves that change slope as you change the elements in the choice set, e.g. below adding option C makes the indifference curves rotate clockwise, and so makes you prefer B to A:\n\\[\n   \\xymatrix{\\, &  &  &  &  & . & \\,\\, &  & \\,\\\\\n   \\, & A &  &  &  &  & A\\\\\n   \\, &  &  & B & \\ar@{-}[uullll] &  &  &  & B & \\ar@{-}[uul]\\\\\n   \\, &  &  &  &  &  &  &  &  &  C \\\\\n   \\ar[uuuu]\\ar[rrrr] &  &  &  & \\ar@{-}[uullll] & \\ar[uuuu]\\ar[rrrr] &  &  & \\ar@{-}[uuuull] & \\,\n   }\n\\]\nBut each theory has different assumption about how the slope of the indifference curves depends on the placement of the options.\nI’m going to try to make the following points:\nA few other points that I’ll leave for later:"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#digression-perception",
    "href": "posts/2016-04-30-relative-thinking.html#digression-perception",
    "title": "Relative Thinking",
    "section": "DIGRESSION: PERCEPTION",
    "text": "DIGRESSION: PERCEPTION\nComparison effects have been studied in perception for a long time, and the same points I make here also apply there. At first people proposed that perceptual comparison effects were hardwired things, mechanical effects, but on further study it turned out that they were context-dependent, in a way that makes them look like sensible inferences.\nHere’s a classic contrast effect:\n\nthe same shade of grey looks darker when surrounded by white, than when surrounded by black.\nFor a long time psychology textbooks gave this as an example of a hardwired contrast effect – i.e. this is caused by the basic wiring of neurons in our eyes. (Some still do).\nBut take a look at this (White’s illusion):\n\nhere the same shade of grey looks lighter on the left than on the right, despite the surroundings being relatively lighter on the left than on the right. This is exactly the opposite of what’s predicted by a hardwired contrast effect in perception.\n(An even cleaner example would be ceteris paribus, where making some part of the background lighter, all else held equal, makes the foreground appear lighter. The figure above does imply that there must exist at least one such case: imagine a third set of grey rectangles which are surrounded only by white. That third case must serve as a ceteris paribus case for one or other of the two cases above, probably both.)\nThe general point is this: There is not a stable relationship between the perceived-lightness of an object and the lightness of surrounding objects. There isn’t even an all-else-equal relationship. The relationship can run in either direction depending on the context.\nBut that’s not the last word. The set of contexts where it goes one way or the other way aren’t arbitrary (“contrast effects” and “assimilation effects”). Adelson (2001) shows that you can usually predict when you’ll observe one effect or the other: roughly, whether or not the surrounding lightness is a positive or negative ecological cue for illumination. I.e., in typical circumstances, is the surrounding lightness positively or negatively associated with illumination? (See an example at the bottom of this post.)"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#digression-2",
    "href": "posts/2016-04-30-relative-thinking.html#digression-2",
    "title": "Relative Thinking",
    "section": "DIGRESSION 2",
    "text": "DIGRESSION 2\nI would write out lists of comparison-effect examples over and over while working on my PhD. My train of thought would get detached and I’d end up asking myself questions like: What are you doing sitting in this office, a continent away from your friends and an ocean and a continent away from your family? Why do you spend your weekends in this sad building, where people stare at the carpet when they pass each other? What rock did you hit in adolescence that knocked you out of orbit, and sent you here? Are you trying to make your mother proud? Avenge your father? Do these professors you work with look like the kind of man you want to be? Did you stumble into one of those academic fields that people snigger about? Why, when you talk about your work, do the people you admire glaze over, and the people who bore you perk up? Do you think that giving your life to intellectual things makes you better than other people? Do you look down on people who don’t think so clearly? What are you doing on a Friday night at the NBER eating a tuna subway sandwich and reading reddit? If it takes you 5 years to get straight one point about relative thinking – one corner of one shelf in one cupboard – then how long is it going to take to tidy up the whole house? When an undergraduate corners you, asking earnest & tedious questions, doesn’t it remind you of yourself?\n\n\n\ndesk"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#alternative-heuristics",
    "href": "posts/2016-04-30-relative-thinking.html#alternative-heuristics",
    "title": "Relative Thinking",
    "section": "ALTERNATIVE HEURISTICS",
    "text": "ALTERNATIVE HEURISTICS\nOK. Well, here’s the body of the argument: I’m going to discuss a few perfectly reasonable reasons why we might infer the value of different attributes from the choice set, and each reason will imply one of the above laws in a subset of cases. However I also show that the same reason can imply the exact opposite of that law, for cases outside that subset.\n\n(1) MRS shifting towards MRT.\nA choice set which varies in different attributes has an implicit rate of tradeoff between the attributes – i.e. the marginal rate of transformation (MRT) – and it is easy to think of cases where our preferences would naturally adapt to the implicit tradeoff, i.e., where our relative value, AKA our marginal rate of substitution (MRS), would rotate towards the tradeoff that is implicit in the choice set (i.e. the MRT).\nThe MRS shifting towards the MRT could be rationalized in two separate ways. First, suppose you believe the social MRS to be informative about your own MRS, and you believe that the choice set reflects the market price, and finally that the price reflects the social MRS (as it would in a competitive equilibrium). Second, suppose you believe the person who constructed the choice set to be cooperative (in the sense of Grice) - i.e., they only include things in the choice set which they think you might want: this implies that the MRT in the choice set reflects their beliefs about your MRS, which is itself informative. If I’m staying in your spare room and and you ask me “would you prefer a poached egg or gruel for breakfast?” then I will figure that your gruel must be pretty good.\nIf there are just two attributes (i,j) and two alternatives (a,b) then the implicit tradeoff is \\(MRT_{i,j}=\\frac{\\|a_{i}-b_{i}\\|}{\\|a_{j}-b_{j}\\|}\\). If the MRS rotates to meet this MRT then the sensitivity to attribute \\(i\\) will be decreasing in the range observed along that dimension, exactly as implied by the range-sensitivity theory (i.e., V, M&C, BR&S).\nHowever the MRS-MRT effect implies range sensitivity only for a 2-attribute, 2-alternative case. Outside of that case the intuitions depart.\nA marginal rate of transformation can only be directly identifed from a menu if the number of alternatives is equal to the number of attributes. I.e., to define a plane in \\(n\\) dimensions from a set of points, you’ll need exactly \\(n\\) points. If you have fewer then it becomes the statistical problem of fitting a line to a set of points. Here is a simple example where the MRT theory and other theories (e.g. range-sensitivity) give qualitatively different answers, and in which the MRS-MRT theory seems more faithful to the intuition. Suppose we have the following three options:\n\\[\n\\xymatrix@C=1em@R=1em{\n& \\binom{\\mbox{100K salary}}{\\mbox{199 days off}}\\\\\n\\\\\n&  & \\binom{\\mbox{105K salary}}{\\mbox{189 days off}} & \\binom{\\mbox{110K salary}}{\\mbox{189 days off}}\\\\\n\\\\\n\\ar[rrrr]\\ar[uuuu] & & &  &  & \\, \\\\\n}\n\\]\nThe intermediate option is dominated by the by the option on the right, and intuitively - to me - the existence of the intermediate option makes the rightmost option more desirable - because the choice set makes 10-days-off seem to be worth between \\$5K and \\$10K, meaning the higher salary seems to come at a low cost in terms of days-off. This intuition is not captured by range sensitivity, because the intermediate option does not change the range in either dimension. However the intermediate option does change the implicit MRS, in the sense of the best-fitting line (e.g. orthogonal regression), and the change will be in favor of the rightmost option – fitting my own intuition.\nEven in the 3-attribute 3-alternative case, it is no longer true that \\(MRT_{i,j}\\) is equal to the ratio of ranges on dimension i and j, it’s now a more complicated function.\nWhen one attribute is a good and the other is a bad (e.g. price and quality; salary and hours) it is sometimes reasonable to think that choosing neither alternative is an additional implicit element of the choice set, i.e. the point (0,0). In these cases the ratio of the ranges reduces to the ratio of the maximum values (\\(\\frac{\\max_{c\\in C}c_{i}}{\\max_{c\\in C}c_{j}}\\)), which has similar comparative statics to the theory in (C) - which depends on the ratio of average values - than the theory in (V,M&C,BR&S) - which depends on the ratio of the ranges.\nAn interesting fact: the effects of this MRS-MRT theory will not be detectable when the choice set is binary: suppose your MRS shifts towards the MRT implicit in the choice set, then although your final MRS will be closer to the MRT, it will not cross the MRT, i.e. the shift in MRS will not alter which of the two elements you prefer. This implies that the MRS-MRT theory cannot rationalize the existence of cycles in binary choice, and so cannot explain evidence for ‘subadditivity’ of different dimensions, such as probability, money, or delay (see Read (2001) for citations). For example, if your have a prior belief that \\(a\\) is better than \\(b\\), and then you observe a choice set containing \\(a\\) and \\(b\\), then you may revise upward your valuation of \\(b\\) relative to \\(a\\), but this observation wouldn’t cause you to switch preference, i.e. to think that \\(b\\) is now better than \\(a\\). (This could be violated under some unusual priors, e.g. if you had bimodal beliefs about the value of \\(b\\)).\n\n\n(2) MRS shifting towards demand.\nThere is a second strong intuition for choice sets influencing preferences: combinations offered often reflect combinations desired, so a relative increase in attribute 1 could be interpreted as a positive signal about the value of attribute 1. Suppose we manipulate the choice set, while keeping the relative price fixed, for example consider these two choice sets, trading off the price and quantity of some good:\n\\[\n\\xymatrix@C=.5em@R=.5em{\\ar[rrrrr]\\ar[ddddd] & & &  &  & \\text{apples}\\\\\n& \\binom{\\text{1 apple}}{\\$1}\\\\\n&  & \\binom{\\text{2 apples}}{\\$2}\\\\\n&  & & \\binom{\\text{3 apples}}{\\$3}\\\\\n\\\\\n\\\\\n\\$ }\n\\]\n\\[\n\\xymatrix@C=.5em@R=.5em{\\ar[rrrrr]\\ar[ddddd] & & & &  & \\text{apples}\\\\\n& \\,\\,\\,\\,\\,\\, & \\\\\n& & \\binom{\\text{2 apples}}{\\$2}\\\\\n& & & \\binom{\\text{3 apples}}{\\$3}\\\\\n& & & & \\binom{\\text{4 apples}}{\\$4}\\\\\n\\\\\n\\$ }\n\\]\nA natural intuition is that people will tend to switch from \\(\\binom{2}{\\$2}\\) to \\(\\binom{3}{\\$3}\\), when going from the first to the second choice set. None of the theories discussed above gives an unambiguous prediction about the change in MRS between these two choice sets - because both the range and magnitude change by the same amount on each dimension - yet the intuition remains quite clear (I think).\nThis idea could be easily formalized - suppose people know the supply curve but are uncertain about the demand curve - then when they observe an increase in quantity they attribute this to a higher demand, and so they infer an increase in value of the good. I think this is similar to the intuition given in Kamenica (2008) - when you observe a higher price/quantity combination, you infer that demand is higher, and so you infer that the value of each marginal good must be higher. I think that a similar foundation is used in Drolet, Simonson, Tversky (2000) “Indifference Curves that Travel with the Choice Set”.\nWe have discussed diagonal shifts along the budget set, meaning that both attributes are varying at once; if only one attribute varied, it’s not clear what a consumer would infer from this. Of course we could formalize a model where the consumer is uncertain about both supply and demand; or we could combine this model with the prior one, where the consumer is uncertain about the price.\n\n\n(3) Magnitude effects.\nFinally it’s easy to come up with a rational magnitude effect, such that when we observe a higher quantity \\(q\\) we infer that the marginal value of each unit is less. Suppose we know the price and we know the consumption-value of a good, when measured in units that are familiar to us, but we do not know the units that are used in the packaging. Then if we observe other people consuming a higher quantity, measured in unfamiliar units, we infer that each unit is worth less: when we observe a 10,000 Kronor bank note we infer each Kronor is not worth a lot; when we observe a 10,000 Watt bulb we infer each Watt is not worth much; when we observe 200mg of Oxytocin we infer the marginal effect of a milligram is not too much."
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#summary",
    "href": "posts/2016-04-30-relative-thinking.html#summary",
    "title": "Relative Thinking",
    "section": "SUMMARY",
    "text": "SUMMARY\nWhen we dig into the intuitions behind comparison effects, we often find that they resemble inferences that we would make every day. The laws that have been proposed to explain comparison effects only work because they coincide with one or other of the inferences in certain subsets of cases – e.g. range-sensitivity coincides with MRS-MRT inference, magnitude-sensitivity coincides with unit-value inference. But these overlaps occur only in a subset of cases, and stepping outside that subset we find that the law fails, while the inference remains.\nDoes this mean that comparison effects are all just rational inferences? What we would like to know is whether comparison effects occur even when inference can be entirely ruled out – e.g. when we run an experiment that explicitly randomizes the choice sets. Some papers do this, but few do it well. I am persuaded that comparisons do affect us on a pre-conscious level, i.e. that our instincts latch onto comparisons without being careful about the significance of the comparison in the particular circumstance, but there’s not a lot of unambiguous evidence on this. I can at least say that most people find the types of example listed above pretty beguiling: they get strong intuitions about relative value, but struggle to explain where the intuitions come from, implying that the inference isn’t entirely conscious.\nSo then why would we make bad inferences that resemble good inferences? I think for the same reason that our perception makes bad inferences that resemble good inferences – because perceptual processes interpret cues according to their ordinary significance, without adjusting for all relevant information. Perception is carried out in a cabinet, whirring through the sense data, and printing out conclusions for the conscious mind to read. The cabinet is locked, we only have access to the output. That is the argument of my paper on ‘implicit knowledge’."
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#miscellaneous-notes",
    "href": "posts/2016-04-30-relative-thinking.html#miscellaneous-notes",
    "title": "Relative Thinking",
    "section": "MISCELLANEOUS NOTES",
    "text": "MISCELLANEOUS NOTES\n\nIt is useful to make a sharp distinction between “goods” and ’bads”. Examples of good-bad tradeoffs are quality vs price; salary vs hours worked. Examples of good-good tradeoffs are bedrooms vs bathrooms; MPG vs horsepower; salary vs holiday-days.\nParducci invented, as well as range-frequency theory, windsurfing.\nBordalo Gennaioli and Shleifer (2013) has a weird feature: the salience of an attribute depends on relative levels (\\(q-\\bar{q}\\) and \\(p-\\bar{p}\\)), but the utility of an attribute depends on absolute levels (\\(q\\) and \\(p\\)). I think this is just a mistake – the underlying intuition is matched much better if \\(U(q,p) = \\hat{\\theta}_q(q-\\bar{q}) + \\hat{\\theta}_p(p-\\bar{q})\\) instead of \\(U(q,p) = \\hat{\\theta}_q q + \\hat{\\theta}_p p\\). This alternation removes a lot of the weird comparative statics of the theory, such as the severe non-monotonicity of the decoy effects. Another note: for two-alternative two-attribute choices the theory (as stated in the paper) has a utility representation, i.e. many of the predictions of that paper are equivalent to a model with menu-independent preferences. For a sufficiently large value of \\(M\\):\n\\[\nU(q,p)=\\begin{cases}\n\\delta q-p & ,\\,q&lt;\\delta p\\\\\nM+\\ln q-\\ln p & ,\\delta p&lt;q&lt;\\delta^{-1}p\\\\\nM+q-\\delta p & ,\\,q&gt;\\delta^{-1}p\n\\end{cases}\n\\]\n\n\n\n\nskeleton"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#references",
    "href": "posts/2016-04-30-relative-thinking.html#references",
    "title": "Relative Thinking",
    "section": "REFERENCES",
    "text": "REFERENCES\nKS: Koszegi and Szeidl (2011) (KS)\nBRS: Bushong Schwarzstein & Rabin (2014)\nMC: Mellers & Cooke ()\nC: Cunningham (2013)\nBGS: Bordalo Gennaioli Shleifer (2012)\nSimonson (2008) “Will I like a Medium Pillow?”\n\n“much of the evidence for preference construction reflects people’s difficulty in evaluating absolute attribute values and tradeoffs and their tendency to gravitate to available relative evaluations … These illustrations suggest that many forms of preference construction reflect a key underlying principle: decision makers tend to avoid absolute value judgments and gravitate to accessible relative evaluations … it is noteworthy that the evidence that has been accumulated to make the case for preference construction might be largely driven by a rather simple common principle. This rather simple, yet important absolute-to-relative principle lends itself to seemingly unrelated demonstrations, which have been treated as distinct phenomena and received unique labels.”\n\nDavid Stove (1991) “What is Wrong With our Thoughts?”"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#another-illusion",
    "href": "posts/2016-04-30-relative-thinking.html#another-illusion",
    "title": "Relative Thinking",
    "section": "Another Illusion",
    "text": "Another Illusion\nAdelson’s “steps” illusion:\n\nIn the first picture the two squares with arrows on them look similar, but in the second picture they seem to have different shades. They are (as you guessed) all the same shade, and in fact the shades are all identical between the first and second image, just arranged a little differently.\nIn particular, the tilt gives an the impression of an angle, and so influences our judgment of where the illumination is coming from. In the first image both squares seem to be on the same plane; in the second image the upper square seems to be on a plane with light squares, and the lower square seems to be on a plane with dark squares. If we use, as proxies of illumination for a square, the shade of squares coplanar with it, then we get the predicted effect."
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#more-examples",
    "href": "posts/2016-04-30-relative-thinking.html#more-examples",
    "title": "Relative Thinking",
    "section": "More Examples",
    "text": "More Examples\n\nIf you have to judge the value of 10 of something – say you’re bidding on 10 bottles of wine in an auction – they’ll seem more valuable if, at the same time, you’re also considering a single bottle of wine; and vice versa if you’re also considering 100 bottles of wine.\nIf you’re choosing between a low-price and medium-price version of a good, seeing that there’s also a high-price version makes the medium-price version seem relatively more attractive.\n(See my ‘comparisons and choice’ paper for more examples)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "heading 1\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n \n\n\n\n\nThinking About Tradeoffs? Draw an Ellipse\n\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n \n\n\n\n\nExperiment Interpretation and Extrapolation\n\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n \n\n\n\n\nAn AI Which Imitates Humans Can Beat Humans\n\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n \n\n\n\n\nSushi-Roll Model of Online Media\n\n\nPreviously: “pizza model”, “salami model”\n\n\n\n\n\n\n\n\n\nSep 8, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n \n\n\n\n\nHow Much has Social Media affected Polarization?\n\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n \n\n\n\n\nThe Paradox of Small Effects\n\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n \n\n\n\n\nRanking by Engagement\n\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n \n\n\n\n\nSocial Media Suspensions of Prominent Accounts\n\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n \n\n\n\n\nOptimal Coronavirus Policy Should be Front-Loaded\n\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2020\n\n\n\n\n\n\n \n\n\n\n\nOn Unconscious Influences (Part 1)\n\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2017\n\n\n\n\n\n\n \n\n\n\n\nThe Work of Art in the Age of Mechanical Production\n\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2017\n\n\n\n\n\n\n \n\n\n\n\nRepulsion from the Prior\n\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2017\n\n\n\n\n\n\n \n\n\n\n\nThe Repeated Failure of Laws of Behaviour\n\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2017\n\n\n\n\n\n\n \n\n\n\n\nSamuelson & Expected Utility\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2017\n\n\n\n\n\n\n \n\n\n\n\nEconomist Explorers\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2017\n\n\n\n\n\n\n \n\n\n\n\nWeber’s Law Doesn’t Imply Concave Representations or Concave Judgments\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2017\n\n\n\n\n\n\n \n\n\n\n\nRelative Thinking\n\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2016\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Tom Cunningham",
    "section": "",
    "text": "Resident fellow at the Integrity Institute."
  },
  {
    "objectID": "about.html#blog",
    "href": "about.html#blog",
    "title": "About Tom Cunningham",
    "section": "Blog",
    "text": "Blog"
  },
  {
    "objectID": "about.html#resume",
    "href": "about.html#resume",
    "title": "About Tom Cunningham",
    "section": "Resume",
    "text": "Resume"
  },
  {
    "objectID": "about.html#working-papers",
    "href": "about.html#working-papers",
    "title": "About Tom Cunningham",
    "section": "Working Papers",
    "text": "Working Papers\n\n2023: Implicit Preferences, Revise and Resubmit, American Economic Review, with Jon de Quidt.\n2019: Interpreting Experiments with Multiple Outcomes (presented at CODE 2019) with Josh Kim.\n2015: Biases and Implicit Knowledge\n2015: Equilibrium Persuasion with Ines Moreno de Barreda.\n2013: Comparisons and Choice\n2013: Relative Thinking and Markups"
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "About Tom Cunningham",
    "section": "Publications",
    "text": "Publications\n\n2019: Improving Treatment Effect Estimators Through Experiment Splitting, WWW, with Dominic Coey.\n2013: The Incumbency Effects of Signalling  Economica, with Ines Moreno de Barreda, Francesco Caselli, and Massimo Morelli.\n2009: Leader Behaviour and the Natural Resource Curse Oxford Economic Papers, with Francesco Caselli."
  },
  {
    "objectID": "about.html#miscellaneous",
    "href": "about.html#miscellaneous",
    "title": "About Tom Cunningham",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\n2010: “LSE CEP 2010 Election Analysis: Macroeconomics and Public Finance” (with Ethan Ilzetski)\n\nComment pieces for the Guardian (2008/2009)\n\nDon’t worry about inflation\nThe great financial stitch-up\nThe rate cut wasn’t big enough\nStuck in the middle\nWe need to outgrow our debt\nYou say inflation, I say deflation\nConfidently predicting darker days"
  },
  {
    "objectID": "posts/2023-04-18-netflix-cider-experimentation-note.html#footnotes",
    "href": "posts/2023-04-18-netflix-cider-experimentation-note.html#footnotes",
    "title": "Four Experimentation Problems",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn fact variation in the success of tech platforms is primarily due to variation in the inflow of new users, not due to variation in retention rates. However growth in new users is driven by the attractiveness of the product and retention is a good proxy for this.↩︎\nThere is an analogy to machine learning: computers have been able to outperform humans at low-dimensional prediction problems, e.g. linear regression, for the last 100 years, but only in the last 10 years have they caught up in high-dimensional problems like recognition of patterns in images, speech, and text.↩︎\nFor simplicitly assume the experiment doesn’t have any effect on variances or covariances of outcomes, the effects are typically small enough that it doesn’t matter.↩︎\nEquivalently, the point-estimate and p-value, or the upper and lower confidence bounds.↩︎\nThis argument holds if the engineer always has to report the most-recent estimate. If they can choose to ignore later datapoints, and report an earlier result, this is essentially a “selection of metrics” case as below, and so the selection rule is relevant for interpretation.↩︎\nBecause \\(t_1\\) and \\(t_2\\) represent independent experiments we’ll have \\(cov(e_1,e_2)=0\\).↩︎\nAndrews et al. (2019) describes some unbiased estimates for treatment effects conditional on them being winners. In general I would say this is an inefficient use of information, because we know much more about the distribution of treatment effects than just whether a specific variant is the winner. However that paper does argue that empirical Bayes estimates struggle when the sample-size is small or when we are estimating the tails of when variants are non-exchangeable, and in those cases the unbiased estimators may be useful.↩︎\nArguably revenue or profit is a more truly final metric, and these are just proxies, but these are probably close enough to final for most purposes.↩︎\nFor derivation see Cunningham and Kim (2019).↩︎\nSee Cunningham and Kim (2019), and see Tripuraneni et al. (2023) for a slightly different setup with weaker assumptions.↩︎"
  },
  {
    "objectID": "posts/2023-04-18-netflix-cider-experimentation-note.html#selection-of-experiments",
    "href": "posts/2023-04-18-netflix-cider-experimentation-note.html#selection-of-experiments",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "2.3 Selection of Experiments",
    "text": "2.3 Selection of Experiments\n\nExample: Selection of experiments. Your team goal is to maximize podcast_time, and you want to know what other teams are hurting that metric. You find the 10 experiments with the biggest negative effect. Should you take their estimated effects at face value?\n\n\nClassical advice is to adjust p-values for the number of experiments you selected from (Bonferroni correction). But from a Bayesian point of view it’s irrelevant whether these 10 experiments are taken from a pool of 10 or 1000 experiments.\nThe set of experiments is informative about appropriate shrinkage. You can use the pool of experiments to estimate the appropriate shrinkage, \\(E[t|\\hat{t}]\\). E.g. if we assume a Normal distribution we can quickly calculate a shrinkage estimate from the average effect and from the fraction of experiments that are statistically significant.\nShrinkage should depend on plausibility of the effect. You can look at how much each of these experiments moves their primary outcomes. Suppose a music-ranking experiment decreases podcast time-spent by 0.4s, and increases music time-spent by 0.2s: the more-than-proportional side-effect seems unlikely, so there is reason to discount (shrink) the likely effect significantly.\nShrink less if the effect is very significant. If the effect-size is 4 standard-errors then, because the distribution of treatment, this is much more likely to be due to treatment than to noise, and so the effect does not require much shrinkage.\n\n\nExample: Selection of Experiments #2. An engineer has an experiment with effect +1% (±0.5%) on your goal metric. They mention that they ran 20 other experiments, and this is the experiment with the biggest effect.\n\nRecommendation: shrink heavily towards the average effect.\n\nFinding out about other experiments with smaller effects means you should shrink more. Finding out about the 20 other experiments is evidence about the size of the typical effect, and you should shrink towards that average. If the engineers are only showing you their best ones, that is reason to shrink your estimates.\nIt matters how selection was done. Suppose the engineer chose the highest-effect one by chance, not intention. You should still shrink by the same amount: the distribution is evidence, not the selection rule. However if they had some independent reason for expecting this experiment would be the most effective, that is relevant evidence.\n\n\nExample: Subgroup Outcomes. You see that the overall time-spent of a feature holdout is -3.5% (±0.5%), but in Korea it’s -9%(±2%). How seriously should you take the Korean effect?\n\nRecommendation: take it seriously, because (a) very significant, and (b) there is high between-country variance.\n\nIs this effect plausible? I.e., do we have reason to expect the effect of this feature to vary a lot by country, and in particular in Korea? We do generally think user behaviour varies a lot by country.\nHow significance is this effect? The effect is 9 standard-errors – i.e., extremely significant – which makes it much less likely to be noise (\\(p\\)=.00001).\nHow much variance in effect is between-country vs within-country? Suppose we see that 1/2 of the countries have effects that are significantly different from the global average effect, this implies that there is a fair amount of variance in effect-sizes, and so reasonable that Korea should be such an outlier.\n\n\nExample: Multiple Outcomes.Your experiment increases music_time, which you expected, and increases podcast_time, which you did not expect.\n\nImplication: The positive effect on podcast_time is bad news about music_time. If outcomes are positively correlated across units but not across treatments then: \\[\\frac{dE[t_1| \\hat{t}_1,\\hat{t}_2]}{d\\hat{t}_2} &lt; 0.\\] In this case good news about one outcome is bad news about the other.\n\nExample: Multiple Outcomes #2. You run an experiment on movie ranking intended to increase watches, and it works. You additionally see an increase in comments-given. Should the increase in comments give you more confidence or less confidence in the increase in likes?\n\nRecommendation: Good news is bad news, if the side-effect is unexpected.\n\nIf the experiment was expected to increase both metrics - e.g. by increasing overall time spent on feed - then this is good news: it is additional evidence for the effect on likes.\nIf the experment was expected to have a null or negative effect on comments – e.g. by boosting like-able posts at the expense of comment-able posts – then this is bad news: the positive effect on comments is likely due to noise, and it should make us expect greater noise in the measure of likes.\n\nGiven two treatment effects \\(t_1\\) and \\(t_2\\), and two outcomes, \\(\\hat{t}_1,\\hat{t}_2\\), and two noise variables, \\(e_1,e_2\\) then we have the following (in the Gaussian case):\n\\[\\frac{dE[t_1|\\hat{t}_1,\\hat{t}_2]}{d\\hat{t}_2} \\propto \\text{covariance}_{t_1,t_2}-\\text{covariance}_{e_1,e_2}.\\]"
  },
  {
    "objectID": "posts/2023-04-18-experiment-interpretation-extrapolation.html",
    "href": "posts/2023-04-18-experiment-interpretation-extrapolation.html",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "",
    "text": "I give a simple Bayesian way of thinking about experiments, and implications for interpretation and extrapolation.\n\n\n Thanks to J. Mark Hou for comments. \nSetup: The canonical tech problem is to choose a policy to maximize long-run user retention. Because the policy space is high-dimensional it’s not feasible to run experiments on every alternative (there are trillions), instead most of the decision-making is done with human intuition based on observational data, and experiments are run to confirm those intuitions.\n\nThe inference problem. The basic problem of experimentation is to estimate the true effect given the observed effect. The problem can become complicated when we have a set of different observed effects, e.g. across experiments, across metrics, across subgroups, or across time. \nTwo common approaches are: (1) adjust confidence intervals (e.g. Bonferroni, always-valid, FDR-adjusted); (2) adjust point estimates based on the distribution (empirical Bayes). Both have significant drawbacks: my suggested approach is to let decision-makers make their own best-estimates of the true effects but provide them with an informative set of benchmark statistics so they can compare the results of any given experiment to the results from a reference group.1\nThe extrapolation problem. Given an effect on metric A what’s our best estimate of the effect on metric B? This problem is common to observational inference, proximal goals, and extrapolation.\nThere are three approaches to solving this: (1) using raw priors; (2) using correlation across units (surrogacy); (3) using correlation across experiments (meta-analysis). I argue that approach #3 is generally the best option but reasonable care needs to be taken in interpreting the results.\n\n1 If the decision-maker is not technical then a data scientist or engineer can summarize for the decision-maker their best-estimate of the true impact on long-run outcomes, taking into account the evidence from the experiment and other sources of evidence, including the distribution of effects from other experiments.I also briefly discuss two additional problems:\n\nThe explore-exploit problem. We would like to choose which experiments to run in an efficient and automated way. I think the technical solution is relatively clear but tech companies have struggled to implement it because good execution requires some discipline. I describe a simple algorithm that is not optimal but very simple and robust.\nThe culture problem. Inside tech companies people keep misusing experiments and misinterpreting the results, especially (1) running under-powered experiments, (2) selectively choosing results, and (3) looking at correlations without thinking about identification.\nA common response is to restrict access to only a subset of experiment resuts. However this often backfires because (1) it is difficult to formally specify the right subset; (2) it reinforces a perception that experimental results can be interpreted as best-estimates of true treatment effects; (3) it reinforces a norm of selecting experimental results as arguments for a desired outcome. I think a better alternative is to explicitly frame the problem as one of predicting the true effect given imperfect evidence, and benchmark peoples’ prior performance in predicting the true effect of an intervention. (This section is unfinished, I hope to add more)."
  },
  {
    "objectID": "posts/2023-04-18-experiment-interpretation-extrapolation.html#strategic-problems",
    "href": "posts/2023-04-18-experiment-interpretation-extrapolation.html#strategic-problems",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "1.1 Strategic Problems",
    "text": "1.1 Strategic Problems\nThere are additionally some strategic problems in experiment interpretation.\n\nStrategic stopping (“peeking”). An engineer will wait until an experiment has a high estimated impact, or low p-value, before presenting it for launch review. A common proposed remedy is that all experiments should be evaluated after the same length of time, or that engineers should pre-specify the length of experiments.\nSelection of treatments (“winners curse”). An engineer will run a dozen variants and only present for launch review the best-performing one. A common proposed remedy is that every variant should be officially presented in launch reviews, even the poorly-performing ones.\nSelection of metrics (“cherry picking”). An engineer will choose to show the experiment results on the metrics that are favorable, not those that are unfavorable. A common proposed remedy is that the set of metrics should be standardized for all launches, or that the set of evaluation metrics should be pre-specified by the engineer (AKA a pre-analysis plan).\n\nI will argue that the commonly proposed remedies are highly imperfect fixes. These are complicated things to think about because the mix together issues of statistical inference and of strategic behaviour. In the discussion that follows I try to separate those out as clearly as possible."
  },
  {
    "objectID": "posts/2023-04-18-experiment-interpretation-extrapolation.html#strategic-stopping",
    "href": "posts/2023-04-18-experiment-interpretation-extrapolation.html#strategic-stopping",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "1.2 Strategic Stopping",
    "text": "1.2 Strategic Stopping\nI will ignore dynamic effects. For simplicity assume that all effects are constant, so the length of an experiment effectively determines just the sample size of that experiment. I.e. I will ignore time-dependent and exposure-dependent effects.\nStopping rules are irrelevant to expected effect sizes. Suppose an experiment has a given estimate. Does it matter to your estimate of the true causal effect if you learn that the experimenter chose the sample size \\(N\\) by a data-dependent rule, e.g. continuing to collect data until the estimate was statistically significant? If you are estimating the true causal effect, \\(E[t|\\hat{t}]\\) then it doesn’t matter, your posterior will be identical either way.6 A simple proof: suppose we observe two noisy signals, \\(x_1\\) and \\(x_2\\): \\[\\begin{aligned}\n         x_1 &= v + e_1 \\\\\n         x_2 &= v + e_2 \\\\\n         v,e_1,e_2 &\\sim  N(0,1)\n      \\end{aligned}\\] Suppose a peeker will report \\(x_1\\) only if \\(x_1&gt;0\\), otherwise they will report \\(x_1+x_2\\). We can compare the expectation of \\(v\\) given the sum, depending on whther the engineer peeked: \\[\\utt{E[v|x_1+x_2]}{estimate}{without peeking} =\n         \\utt{E[v|x_1+x_2|x_1&lt;0]}{estimate}{with peeking}\\]6 This argument holds if the engineer always has to report the most-recent estimate. If they can choose to ignore later datapoints, and report an earlier result, this is essentially a “selection of metrics” case as below, and so the selection rule is relevant for interpretation.\n  This holds because $x_1+x_2$ is a sufficient statistic for the distribution, i.e. $x_1&lt;0$ does not tell us any additional information. Note that peeking is not irrelevant to interpretation of a result if (1) the engineer can choose to report either $x_1$ or $x_2$, (2) the engineer can choose to report $x_1$ alone *after* observing $x_2$.[^deng]\n\n  [^deng]: See @deng2016continuous for a fuller argument that stopping rules are irrelevant, and a review of the prior literature.\nStopping rules would be relevant if we made decisions based on statistical-significance. A stopping rule would be relevant if we conditioned only on statistical-signficance instead of the full estimate. In other words the expected true effect, conditioning only on whether or not the estimated effect is statistically significant, will depend on the stopping rule. For example if people kept running experiments until they were significant then significant experiments would tend to have small effect sizes. However it is clearly bad practice to condition only on this binary piece of information when you have the full estimate, and if you have the full estimate then the stopping rule becomes irrelevant.\nThe optimal stopping rule is data-dependent. The discussion above took a stopping rule as given, we can also ask what’s the efficient stopping rule. It’s clear that a fixed length is inefficient: we should stop an experiment sooner if it does unexpectedly well or unexpectedly badly, in both of those cases the value of collecting more information has decreased because it’s less likely to change our mind about a launch decision. Thus enforcing a static or pre-specific experiment length will lead to inefficient decision-making.\nConsidering engineers’ incentives. Now consider the launch process as a game, with the engineers trying to persuade the director to launch their feature. Suppose the director’s ex post optimal strategy is to launch if \\(E[t|\\hat{t}]&gt;0\\), and suppose the engineers get a bonus whenever their feature is launched. In equilibrium the engineers will keep their experiments running until \\(E[t|\\hat{t}]&gt;0\\), which will cause a skew distribution: the distribution of posteriors will show a cluster just above the threshold. The director’s strategy is ex post optimal but it’s not an efficient use of experimentation resources. In this game the director would likely wish to pre-commit to a different threshold which induces more efficient effort by engineers. However a more direct solution would be to align engineers’ incentives with those of the director by rewarding them for their true impact, i.e. setting their bonuses proportional to \\(\\max\\{E[t|\\hat{t}],0\\}\\), instead of discontinuously rewarding them for whether or not they launched."
  },
  {
    "objectID": "posts/2023-04-18-experiment-interpretation-extrapolation.html#selection-of-treatments",
    "href": "posts/2023-04-18-experiment-interpretation-extrapolation.html#selection-of-treatments",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "1.3 Selection of Treatments",
    "text": "1.3 Selection of Treatments\nIf you learn an experiment is the top-performing variant it should change your asssessment. Suppose we have a result \\(\\hat{t}_1\\), and we are estimating the true treatment effect, \\(t_1\\). If we learn that another variant has a lower treatment effect, \\(\\hat{t_1}&gt;\\hat{t}_2\\), then it is rational to update our assessment of \\(t_1\\):\n\\[\\utt{E[t_1|\\hat{t}_1,\\hat{t}_1&gt;\\hat{t}_2]}{assessment knowing}{it's winner}&lt;\n      \\utt{E[t_1|\\hat{t}_1]}{assessment}{given outcome}\n      \\]\nThis will hold whenever \\(Cov(t_1,t_2)&gt;0\\), i.e. when we have some shared source of uncertainty about the two treatment effects.7 We can write a model for this, however conditioning on this binary information (whether a variant is the winner) is not an efficient way of using the information at your disposal.7 Because \\(t_1\\) and \\(t_2\\) represent independent experiments we’ll have \\(cov(e_1,e_2)=0\\).\nIt’s better to condition on the whole distribution. In almost all cases we know much more than whether \\(\\hat{t}_1\\) is the winner, we also know the value of \\(\\hat{t}_2\\), and then this reduces simply to the empirical Bayes problem, i.e. we simply wish to estimate: \\[E[t_1|\\hat{t}_1,\\ldots,\\hat{t}_n],\\] and we can do that in the usual way.8 E.g. if we have a Normal prior over treatment effects then we can estimate \\(\\sigma_t^2\\) from \\(Var(\\hat{t})\\) and \\(\\sigma_e^2\\). Once we have conditioned on \\(\\sigma_t^2\\) then it becomes irrelevant whether variant 1 is the winner or not, i.e.: \\[E[t_1|\\hat{t}_1,\\sigma_t^2]=E[t_1|\\hat{t}_1,\\sigma_t^2,\\hat{t}_1&gt;\\hat{t}_2].\\]8 Andrews et al. (2019) describes some unbiased estimates for treatment effects conditional on them being winners. In general I would say this is an inefficient use of information, because we know much more about the distribution of treatment effects than just whether a specific variant is the winner. However that paper does argue that empirical Bayes estimates struggle when the sample-size is small or when we are estimating the tails of when variants are non-exchangeable, and in those cases the unbiased estimators may be useful.\nPut another way: the selection rule is irrelevant (just as the stopping rule is irrelevant) once we condition on the distribution of observed outcomes.\nImplication: show the distribution. If we are worried that engineers are selecting variants based on their outcomes then the simplest and cleanest fix is to calculate the distribution of variants and use that to discount any experiment results, either explicitly with an empirical Bayes estimator, or implicitly by showing the decision-maker the distribution."
  },
  {
    "objectID": "posts/2023-04-18-experiment-interpretation-extrapolation.html#selection-of-metrics",
    "href": "posts/2023-04-18-experiment-interpretation-extrapolation.html#selection-of-metrics",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "1.4 Selection of Metrics",
    "text": "1.4 Selection of Metrics\nSuppose engineers are selectively presenting the most favorable metrics. Suppose there are two outcome metrics from a single experiment, and the engineer will present whichever is the most favorable. Knowing this fact should rationally affect your judgment of the treatment effect on the presented metric: \\[\\utt{E[t_1|\\hat{t}_1]}{assessment knowing}{only metric 1} &gt;\n      \\utt{E[t_1|\\hat{t}_1,\\hat{t}_1&gt;\\hat{t}_2]}{assessment knowing}{metric 1 beats metric 2}\\]\nImplication: engineers should present all outcome metrics."
  },
  {
    "objectID": "posts/2023-04-18-experiment-interpretation-extrapolation.html#on-launch-criteria",
    "href": "posts/2023-04-18-experiment-interpretation-extrapolation.html#on-launch-criteria",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "1.5 On Launch Criteria",
    "text": "1.5 On Launch Criteria\nChoosing weights on metrics for a launch decisions involves many considerations: network effects, noise, cross-metric proxy effects, and dynamic effects. In addition launch rules serve a bureaucratic role, and engineers will often want the launch rule to be public and without discretion. To make clear decisions it’s important to peel apart these layers, I recommend these steps:\n\nChoose a set of final metrics. These are the metrics we would care about if we had perfect knowledge of the experimental effect. We can define tradeoffs between them, it’s convenient to express those tradeoffs in terms of percentage changes, e.g. we might be indifferent between 1% DAU, 2% time/DAU, and 5% prevalence of bad content.9\nChoose a set of proximal metrics. These are the metrics on which we are confident we can detect our experiment’s effect, meaning the measured impact will be close to the true impact on these metrics (i.e. has a high signal-noise ratio). To determine whether a metric is moved we can use the fraction of a given class of experiments that have a statistically-significant effect on that metric: if the share is greater than 50% then we can be confident that the estimated effect is close to the true effect.\nIdentify conversion factors between proximal and final metrics. These tell us the best-estimate impact on final metrics given the impact on proximal metrics. Conversion factors can be estimated either from (a) long-running tuning experiments; (b) a meta-analysis of prior experiments with similar designs.\nA final linear launch criteria can then be expressed as a set of conversion-factor weights applied to each of the proximal metrics.10\n\n9 Arguably revenue or profit is a more truly final metric, and these are just proxies, but these are probably close enough to final for most purposes.10 For derivation see Cunningham and Kim (2019)."
  },
  {
    "objectID": "posts/2023-04-18-experiment-interpretation-extrapolation.html#comparing-launch-rules",
    "href": "posts/2023-04-18-experiment-interpretation-extrapolation.html#comparing-launch-rules",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "1.6 Comparing Launch Rules",
    "text": "1.6 Comparing Launch Rules\n\n\n\n\n\n\n\n\n\n\n\nShip if sum is positive\n\n\n\n\n\n\n\n\nI find it useful to visualize different launch rules. For simplicity suppose our utility function is linear: we have two metrics, 1 and 2, and we care about them equally: \\[U(t_1,t_2)=t_1+t_2.\\] But we only observe noisy estimates \\(\\hat{t}_1,\\hat{t}_2\\).\nKohavi et al. (2020) recommend a stat-sig shipping rule. They say (p105):\n\nIf no metrics are positive-significant then do not ship\nIf some are positive-significant and none are negative-significant then ship\nIf some are positive-significant and some are negative-significant then “decide based on the tradeoffs.\n\nI represent this in the first diagram (but I treat condition 3 as a non-ship). The dotted line represents \\(\\hat{t}_1+\\hat{t}_2=0\\).\nThe stat-sig shipping rule has strange consequences. You can see that this rule will recommend shipping things even with negative face-value utility (\\(U(\\hat{t}_1,\\hat{t}_2)&lt;0\\)), when there’s a negative outcome on the relatively noisier metric. This will still hold if we evaluate utility with shrunk estimates, when there’s equal proportional shrinkage on the two metrics, but if there’s greater shrinkage on the noisier metric it will not hold.\nLinear shipping rules are better. In the margin I illustrate (1) a rule to ship wherever the sum is positive; (2) a rule to ship wherever the sum is stat-sig positive. I have drawn the second assuming that \\(cov(\\hat{t}_1,\\hat{t}_2)=0\\). With a positive covariance the threshold would be higher.\n\n\n\n\n\nThe Leontief sandwich. I assumed above that our true utility function is linear. In fact tech companies often explicitly give nonlinear objective functions to teams, e.g.: \\[\\begin{aligned}\n      \\max_k &\\ A(k)\n         && \\text{(goal)} \\\\\n      \\text{s.t.} &\\ B(k)\\leq \\bar{B}\n         && \\text{(guardrail)}\n   \\end{aligned}\\]\nThis is illustrated at right, the indifference curves are L-shaped so I’ll call it a Leontief utility. Having Leontief preferences can cause some unintuitive decision-making, in particular the tradeoff between \\(A\\) and \\(B\\) will varies drastically depending on your location. One important observation is that if your goal is assessed at the end of some time-point (e.g. at the end of the half) then optimal launch decisions will depend on your future expectations, e.g. you’d be willing to launch a feature that boosts A at the cost of B only if you expect a future launch to make up that deficit in B.\nIn practice I think it’s useful to think of this nonlinear objective function as sitting in the middle of the hierarchy of an organization, with approximately linear objective functions above and below it, i.e. a “Leontief sandwich.”\nAt the highest layer the CEO (or shareholders) care about all the metrics in way that is locally linear, i.e. they do not have sharp discontinuities in how they assess the company’s health. At the lowest layer engineers and data scientists are trying to make individual changes that achieve the Org’s overall goals, but because they only account for a small share of the overall org’s impact they can treat their objectives as locally linear (& likewise in a value function we make linear tradeoffs between objectives because we’re in such a small region). Finally even for orgs which have nonlinear objective functions it’s often reasonable to think of the nonlinearities as “soft”, e.g. if an org comes in slightly below a guardrail the punishment is slight, and if they come in above the guardrail then they will be rewarded. This softening makes the effective objective function much closer to linear, and so I think for many practical purposes it’s reasonable to start with a linear objective function."
  },
  {
    "objectID": "posts/2023-04-18-experiment-interpretation-extrapolation.html#with-meta-analysis",
    "href": "posts/2023-04-18-experiment-interpretation-extrapolation.html#with-meta-analysis",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "2.1 With Meta-Analysis",
    "text": "2.1 With Meta-Analysis\nWith \\(n\\) metrics we can write the underlying model as: \\[\\utt{\\pmatrix{\\hat{t}_1\\\\\\vdots\\\\\\hat{t}_n}}{observed}{effects}\n      = \\utt{\\pmatrix{t_1\\\\\\vdots\\\\t_n}}{true}{effects}\n         +\\utt{\\pmatrix{e_1\\\\\\vdots\\\\e_n}}{noise}{(=user variation)}\\]\nHere we are treating \\(\\Delta \\text{DAU}_{SR}\\) and \\(\\Delta \\text{DAU}_{LR}\\) as two different metrics, but for some experiments we only observe the first. We thus want to estimate the effect on long-run retention (DAU\\(_{LR}\\)) given short-run metrics. \\[E[\\Delta\\text{DAU}_{LR} |\n       \\Delta \\widehat{\\text{DAU}}_{SR}, \\ldots, \\Delta\\widehat{\\text{engagement}}_{SR}]\\]\nwhere \\[\\begin{aligned}\n      \\Delta\\text{DAU}_{LR}   &= \\textit{true}\\text{ effect on long-run daily active users (AKA retention)}\\\\\n      \\Delta\\widehat{\\text{DAU}}_{SR} &= \\textit{estimated}\\text{ effect on short-run daily active users} \\\\\n      \\Delta\\widehat{\\text{engagement}}_{SR} &= \\textit{estimated}\\text{ effect on short-run engagement}\n   \\end{aligned}\\]\nRunning a Regression will be Biased. The obvious thing to do is run a regression across experiments: \\[\\Delta\\widehat{\\text{DAU}}_{LR} \\sim\n      \\Delta \\widehat{\\text{DAU}}_{SR} + \\ldots + \\Delta\\widehat{\\text{engagement}}_{SR}\\]\nHowever this will be biased. The simplest way to demonstrate the bias is to show that even with AA tests (where there is zero treatment effect on either metric) we will still get a strong predictive relationship between the observed treatment effects on each of the two metrics (see figure).\n\n\n\n\n\nA simulated scatter-plot showing 20 experiments, with N=1,000,000, \\(\\sigma_{e1}^2=\\sigma_{e2}^2=1\\), with correlation 0.8. The experiments are all AA-tests, i.e. there are no true treatment effects, yet a regression of \\(\\hat{t}_2\\) on \\(\\hat{t}_1\\) will consistently yield statistically-significant coefficients of around 0.8.\n\n\nThe bias is because in the regression our LHS variable is estimated retention (\\(\\Delta\\widehat{\\text{DAU}}_{LR}\\) instead of \\(\\Delta\\text{DAU}_{LR}\\)), and the noise in that estimate will be correlated with the noise in the estimates of short-run metrics. In the linear bivariate case (where we have just one RHS variable) then we can write: \\[\\begin{aligned}\n      \\ut{\\frac{cov(\\hat{t}_2,\\hat{t}_1)}{var(\\hat{t}_1)}}{regression}\n      = \\utt{\\frac{cov(t_2,\\hat{t}_1)}{var(\\hat{t_1})}}{what we}{want to know}\n         + \\ut{\\frac{cov(e_2,e_1)}{var(\\hat{t}_1)}}{bias}\n   \\end{aligned}\\]\nThe bias will be small if the short-run metrics have high signal-noise ratios (SNR), \\(\\frac{var(t_1)}{var(e_1)}\\gg 0\\). A simple test for SNR ratio is the distribution of p-values: if most experiments are significant then the SNR is high. However in the typical case (1) \\(\\Delta \\widehat{\\text{DAU}}_{SR}\\) is the best predictor of \\(\\Delta \\widehat{\\text{DAU}}_{LR}\\); and (2) \\(\\Delta \\widehat{\\text{DAU}}_{SR}\\) has a low signal-noise ratio (i.e. few outcomes are stat-sig). This means the bias is large, and so results are hard to interpret.\n\n2.1.1 Adjusting for the Bias\nHere are some alternatives:\n\nRun a regression just using the high-SNR metrics. We could just drop \\(\\Delta\\widehat{\\text{DAU}}_{SR}\\) as a regressor because of the bias, but we lose predictive power (\\(R^2\\)) so it’s hard to know when this will be a good idea without an explicit model.\nAdjust for bias in linear estimator. If we want a linear estimator then we can estimate and adjust for the bias. \\[\\begin{aligned}\n   \\utt{\\frac{cov(t_2,\\hat{t}_1)}{var(\\hat{t_1})}}{BLUE for}{$t_2$ given $\\hat{t}_1$}\n      &= \\frac{cov(t_2,t_1)}{var(\\hat{t}_1)}\n      = \\ut{\\frac{cov(\\hat{t}_2,\\hat{t}_1)}{var(\\hat{t}_1)}}{regression result}\n         - \\utt{\\frac{cov(e_2,e_1)}{var(\\hat{t}_1)}}{observable}{variables}\n\\end{aligned}\\]\nIf everything is joint normal then the expectation is itself linear, and so this will be optimal. In practice the true distribution of effect-sizes is somewhat fat-tailed, which imply that the conditional expectation will be nonlinear in the observables. Nevertheless I think this is a good start. (One other complication is that the SNR is more complicated to calculate when experiments vary in their sample size).11\nUse experiment splitting. You can randomly assign users in each experiment to one or other sub-experiments. You now effectively have a set of pairs of experiments, each of which has experiments with identical treatment effects (\\(\\Delta \\text{DAU}_{LR}\\)) but independent noise. Thus you can run a regression with LHS from one split, and RHS from other split, and you’ll get an unbiased estimate. Additionally you can easily fit a nonlinear model (Coey and Cunningham (2019) has details of how to do an experiment-splitting).\nRun a regression just using the strongest experiments. If the distribution of experiments is fat-tailed then the strongest experiments will have higher SNR, and so lower bias. A worry about this is that you’re only estimating the relationship from outliers, so nonlinearities are more of a worry. At the same time the assumption of fat-tailed treatment-effects gives reason to believe the expectation will be nonlinear. (This is roughly how I interpret the Peysakhovich and Eckles (2018) experiments-as-instruments paper. They propose using L0 regularization and experiment-splitting cross-validations, which I think effectively selects the strongest experiments.)\n\n11 See Cunningham and Kim (2019), and see Tripuraneni et al. (2023) for a slightly different setup with weaker assumptions.Choosing a Reference Class. It is important to think about the reference-class of experiments which we use to calibrate our estimates. The long-run DAU prediction can be though of as an empirical-bayes estimate, which is our best estimate conditional on the experiment being a random draw from this class of experiments.\nIn many cases a company’s experiments will naturally fall into different classes: e.g. some have a very steep relationship between engagement and DAU, others have a very flat. It’s important to both (1) visualize all the experiments, so that a reference-class can be chosen sensibly; (2) calculate the \\(R^2\\) across experiments, so we can have some sense of confidence in our extrapolation."
  },
  {
    "objectID": "posts/2023-04-18-experiment-interpretation-extrapolation.html#observational-inference",
    "href": "posts/2023-04-18-experiment-interpretation-extrapolation.html#observational-inference",
    "title": "Experiment Interpretation and Extrapolation",
    "section": "2.2 Observational Inference",
    "text": "2.2 Observational Inference\nWhat we want to know: Given the short-run effect of a content experiment on engagement we want to predict the long-run effect on DAU. We can start with a simple regression along these lines: \\[\\utt{\\text{DAU}_{u,t+1}}{long-run}{retention} \\sim \\utt{\\text{engagement}_{u,t}}{short-run}{engagement}\\]\nWe could set up a DAG and discuss the surrogacy conditions. The condition are that (1) all effects of an experiment on DAU are via short-run engagement; and (2) there is no unobserved factor which affects both SR engagement and LR DAU:\n\\[\\xymatrix{\n      &  *+[F-:&lt;6pt&gt;]\\txt{unobserved}\\ar@{.&gt;}[d] \\ar@{.&gt;}[dr] \\\\\n         *+[F]{\\text{experiment}} \\ar[r] \\ar@{.&gt;}@/_1pc/[rr]\n         & *+[F]{\\text{SR engagement}}\\ar[r]\n         & *+[F]{\\text{LR DAU}}\n      }\\]\nIn fact we know that engagement doesn’t literally lie on the causal chain, instead we think engagement is a good proxy for content which might lie on the causal chain.\nIn any case I find the following setup an easier way to think about the assumptions necessary for identification:\nWe can write it out a simple structural model as follows (for compactness I leave out coefficients):\n\\[\\begin{array}{rcccccccc}\n   \\text{engagement}_{u,t}\n      &=& \\utt{\\text{temperament}_{u}}{user-specific}{propensity to engage}  \n      &+& \\utt{\\text{mood}_{u,t}}{time-varying}{mood/holiday/etc.}\n      &+& \\utt{\\text{content}_{u,t}}{content seen}{on platform}\n      &+& \\utt{\\text{distractions}_{u,t}}{other platform effects}{e.g. messages, notifs}\\\\\n   \\text{DAU}_{u,t}\n      &=& \\text{temperament}_{u}\n      &+&\\text{mood}_{u,t}\n      &+&\\utt{\\sum_{s=1}^\\infty\\beta^s\\text{content}_{u,t-s}}{prior experience}{w content}\n      &+&\\text{distractions}_{u,t}\\\\\n\\end{array}\\]\nSome general observations:\n\nWe would get a more credible estimate if we could directly measure content quality. E.g. if we could use the quality of the content available to the user on the RHS, instead of just their engagement on that content. This wouldn’t get perfect identification but it would help.\nThe relative shares of variation in the RHS is important. If most of the variation in engagement is due to variation in content (i.e. high \\(R^2\\) from content), then we don’t need to worry much about confounding from other effects. We can think of introducing control variables as a way of increasing the share of varation in engagement due to content.\nWe should control for distractions. If we have measures of app-related events that don’t affect content-seen but do affect engagement, e.g. notifications, messages, then we should use those as controls. This will increase the relative share of variation in engagement due to content.\nControlling for pre-treatment outcomes changes variation used. If we control for engagement\\(_{t-1}\\) this will change the relative contribution of each factor in the variation of engagement. Specifically it will reduce the share of the terms with higher autocorrelation. Thus by definition temperament will reduce its contribution. However it’s unclear whether mood or content has higher autocorrelation, and so controlling for pre-treatment could either increase or decrease the relative contribution of content. It’s probably worth doing some simple decomposition of variation in engagement into (1) user, (2) content, and (3) mood (the residual), both statically and over time.\nUnivariate linear prediction is usually pretty good. In my experience you can get a fairly good prediction of most user-level metrics with a linear function of the lagged values. If you use a multivariate or nonlinear function you’ll get a better fit but only by a small amount (one exception: when predicting discrete variables like DAU it’s useful to use a continuous lagged variable like time-spent). So I’m skeptical that adding more regressors or adding nonlinearity will significantly change the estimates or the credibility of the estimates.\nEstimand is not \\(\\beta\\) but \\(\\frac{1}{1-\\beta}\\). Suppose we see that 1 unit of engagement causes a certain increase in DAU over the following weeks. We then want to apply that estimate to an experiment which permanently increases engagement by 1 unit. We thus should take the integral over all the subsequent DAU effects. In the simple exponential case the effect of a shock at period \\(t\\) on DAU at period \\(t+s\\) will be \\(\\beta^s\\), and so the cumulative effect on all subsequent periods will be \\(1+\\beta+\\beta^2+\\ldots=\\frac{1}{1-\\beta}\\).\nAutocorrelation in content makes things messier. If there is significant autocorrelation in content then the interpretation of DAU~engagement is more difficult. E.g. if we see that engagement on \\(t\\) is correlated with DAU on \\(t+1\\) this could be because either (1) content on \\(t\\) content caused the DAU on \\(t+1\\), or (2) good content on \\(t\\) is correlated with good content on \\(t+1\\), which in turn causes DAU on \\(t+1\\). I don’t think controlling for pre-treatment levels or trends solves this."
  },
  {
    "objectID": "posts/2023-10-21-pareto-frontiers-experiments-ranking.html",
    "href": "posts/2023-10-21-pareto-frontiers-experiments-ranking.html",
    "title": "Elliptical Pareto Frontiers for Experiments and Products",
    "section": "",
    "text": "#| column: margin\n\\begin{tikzpicture}[scale=1.5]\n   \\draw[&lt;-&gt;] (-1,0) -- (1,0) node[below]{$\\Delta$X};\n   \\draw[&lt;-&gt;] (0,-1) -- (0,1) node[left]{$\\Delta$Y};\n   \\node at (0,-1.1) {experiments};\n   \\clip (0,0) circle[x radius=.4, y radius=.2, rotate=45];\n   \\pgfmathsetseed{24122015}\n   \\foreach \\p in {1,...,200}\n      { \\fill (rand,rand) circle (0.02); }\n\\end{tikzpicture}\n\\begin{tikzpicture}[scale=1.5]\n   \\draw[&lt;-&gt;] (-1,0) -- (1,0) node[below]{$\\Delta$X};\n   \\draw[&lt;-&gt;] (0,-1) -- (0,1) node[left]{$\\Delta$Y};\n   \\draw[color=red, line width=2] (0,0) circle[x radius=.9, y radius=.5, rotate=45];\n   \\node at (0,-1.1) {Pareto frontier};\n\\end{tikzpicture}\nGiven a set of experiments we can draw a Pareto frontier. Suppose we have a set of experiments, each of which can be characterized by its impact on two metrics, \\(\\Delta X\\) and \\(\\Delta Y\\). We visualize such a set of experiments at right.\nIf the set of features is separable, meaning that the impact of each feature is independent of what other features are launched, then a natural question will be the shape of the Pareto frontier formed by all possible combination of experiments.\nWe show below that, if the distribution of experiments is mean zero and joint normal, then the expected Pareto frontier will be an ellipse, and it will have exactly the shape of an isovalue of the density of experiments. Thus knowing the variance and covariance of experiment results allows us to characterize the nature of the Pareto frontier we face.\nGiven a set of classifier predictions we can draw a Pareto frontier. Suppose we are choosing a fixed set of items to show to a user based on predictions of two outcomes, \\(x_1\\) and \\(x_2\\) (e.g. likes and comments, streams and dislikes, etc.). A natural question will be the shape of the Pareto frontier formed by alternative selections of items.\nWe show below that, if the predictions are well calibrated, the outcomes are independent, and the distribution of prediction obeys a joint Normal distribution, then the Pareto frontier will be an ellipse, and it will have exactly the shape of an isovalue of the density of predictions. Thus knowing the variance and covariance of predictions allows us to exactly characterize the nature of the aggregate tradeoffs we face."
  },
  {
    "objectID": "posts/2023-10-21-pareto-frontiers-experiments-ranking.html#introduction",
    "href": "posts/2023-10-21-pareto-frontiers-experiments-ranking.html#introduction",
    "title": "Elliptical Pareto Frontiers for Experiments and Products",
    "section": "",
    "text": "#| column: margin\n\\begin{tikzpicture}[scale=1.5]\n   \\draw[&lt;-&gt;] (-1,0) -- (1,0) node[below]{$\\Delta$X};\n   \\draw[&lt;-&gt;] (0,-1) -- (0,1) node[left]{$\\Delta$Y};\n   \\node at (0,-1.1) {experiments};\n   \\clip (0,0) circle[x radius=.4, y radius=.2, rotate=45];\n   \\pgfmathsetseed{24122015}\n   \\foreach \\p in {1,...,200}\n      { \\fill (rand,rand) circle (0.02); }\n\\end{tikzpicture}\n\\begin{tikzpicture}[scale=1.5]\n   \\draw[&lt;-&gt;] (-1,0) -- (1,0) node[below]{$\\Delta$X};\n   \\draw[&lt;-&gt;] (0,-1) -- (0,1) node[left]{$\\Delta$Y};\n   \\draw[color=red, line width=2] (0,0) circle[x radius=.9, y radius=.5, rotate=45];\n   \\node at (0,-1.1) {Pareto frontier};\n\\end{tikzpicture}\nGiven a set of experiments we can draw a Pareto frontier. Suppose we have a set of experiments, each of which can be characterized by its impact on two metrics, \\(\\Delta X\\) and \\(\\Delta Y\\). We visualize such a set of experiments at right.\nIf the set of features is separable, meaning that the impact of each feature is independent of what other features are launched, then a natural question will be the shape of the Pareto frontier formed by all possible combination of experiments.\nWe show below that, if the distribution of experiments is mean zero and joint normal, then the expected Pareto frontier will be an ellipse, and it will have exactly the shape of an isovalue of the density of experiments. Thus knowing the variance and covariance of experiment results allows us to characterize the nature of the Pareto frontier we face.\nGiven a set of classifier predictions we can draw a Pareto frontier. Suppose we are choosing a fixed set of items to show to a user based on predictions of two outcomes, \\(x_1\\) and \\(x_2\\) (e.g. likes and comments, streams and dislikes, etc.). A natural question will be the shape of the Pareto frontier formed by alternative selections of items.\nWe show below that, if the predictions are well calibrated, the outcomes are independent, and the distribution of prediction obeys a joint Normal distribution, then the Pareto frontier will be an ellipse, and it will have exactly the shape of an isovalue of the density of predictions. Thus knowing the variance and covariance of predictions allows us to exactly characterize the nature of the aggregate tradeoffs we face."
  },
  {
    "objectID": "posts/2023-10-21-pareto-frontiers-experiments-ranking.html#using-pareto-frontiers-for-launch-decisions",
    "href": "posts/2023-10-21-pareto-frontiers-experiments-ranking.html#using-pareto-frontiers-for-launch-decisions",
    "title": "Elliptical Pareto Frontiers for Experiments and Products",
    "section": "Using Pareto Frontiers for Launch Decisions",
    "text": "Using Pareto Frontiers for Launch Decisions"
  },
  {
    "objectID": "posts/2023-10-21-pareto-frontiers-experiments-ranking.html#pareto-frontiers-and-network-effects",
    "href": "posts/2023-10-21-pareto-frontiers-experiments-ranking.html#pareto-frontiers-and-network-effects",
    "title": "Elliptical Pareto Frontiers for Experiments and Products",
    "section": "Pareto Frontiers and Network Effects",
    "text": "Pareto Frontiers and Network Effects"
  },
  {
    "objectID": "posts/2023-10-21-pareto-frontiers-experiments-ranking.html#footnotes",
    "href": "posts/2023-10-21-pareto-frontiers-experiments-ranking.html#footnotes",
    "title": "Elliptical Pareto Frontiers for Experiments and Products",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nTwitter, tcunningham@twitter.com.↩︎\nTwitter, tcunningham@twitter.com.↩︎\nTwitter, tcunningham@twitter.com.↩︎\nhttps://www.wolframcloud.com/env/tomcunningham/Ellipses.nb↩︎\nFrom Section 4.7 of the 1st edition (2002) of the book Introduction to Probability, by D. P. Bertsekas and J. N. Tsitsiklis, http://athenasc.com/Bivariate-Normal.pdf↩︎"
  },
  {
    "objectID": "posts/2023-10-23-pareto-frontiers-experiments-ranking.html",
    "href": "posts/2023-10-23-pareto-frontiers-experiments-ranking.html",
    "title": "Thinking About Tradeoffs? Draw an Ellipse",
    "section": "",
    "text": "This material was first presented at MIT CODE 2021. Thanks to Sean Taylor among others for comments.\nThinking about tradeoffs? draw an ellipse. When making a tradeoff between two outcomes, \\(X\\) and \\(Y\\), it’s useful to sketch out what the tradeoff looks like, and an ellipse is a good first-order approximation. The ellipse helps visualize the most interesting parameter: the tightness, i.e. how much the tradeoff varies as you vary the weights.\nChoosing launch criteria? draw an ellipse. You can think of the set of features (or experiments) as each having some metric impact, \\(\\Delta X\\) and \\(\\Delta Y\\). It is useful to sketch the combined impact of \\(\\Delta X\\) and \\(\\Delta Y\\) that are achievable (the Pareto frontier), it will often look like an ellipse. Additionally you can prove that the Pareto frontier will be exactly an ellipse if the set of experiment-effects have a joint Normal distribution.\nChoosing ranking weights? draw an ellipse. When ranking items in a recommender there’s always some tradeoff between different outcomes. We can prove that the aggregate tradeoff is exactly an ellipse if the outcome is additive and the individual item-level features are Normally distributed.\nAllocating headcount? draw an ellipse. When you shuffle headcount around a company it’s hard to precisely measure the impact on different goals, but it’s still useful to sketch ellipses to characterize the tradeoffs you face. Most usefully this helps characterize the effect of changing goals on within-team and between-team reallocation of effort.\n\nTight and Loose Tradeoffs\n\n\n\n\n\n\n\n\n\n\nSuppose we are considering how much weight to put on metric \\(X\\) vs metric \\(Y\\) when making a product choice. We can characterize the effect of variation in the weight by drawing a Pareto frontier: the efficient outcome will be the point on the Pareto frontier which has a slope equal to the ratio of weights on Y and X (i.e. where it is tangent with an indifference curve). We can distinguish between “tight” and “loose” Pareto frontiers. The first figure shows a tight tradeoff: in this case it doesn’t matter whether we maximize X or Y or a weighted average, we’ll end up in roughly the same place anyway. The second figure shows a loose tradeoff: in this case the outcome does depend substantially on the relative weight we put on \\(X\\) and \\(Y\\). Thus if, among experiments, we observe a high positive correlation between \\(\\Delta X\\) and \\(\\Delta Y\\) then the choice of shipping criteria is relatively unimportant: we should ship the same experiments anyway. Similarly if, among items in a recommender, we observe a high positive correlation between prediction of the two outcomes, then the choice of weights is relatively unimportant: we would show the same items anyway.\n\n\n\nEllipses for Experiments\n\n\n\n\n\nSuppose we have a set of experiments, each of which can be characterized by its impact on two metrics, \\(\\Delta X\\) and \\(\\Delta Y\\). We visualize such a set of experiments at right.\nIf the set of features is separable, meaning that the impact of each feature is independent of what other features are launched, then a natural question will be the shape of the Pareto frontier formed by all possible combination of experiments.\nWe show below that, if the distribution of experiments is mean zero and joint normal, then the expected Pareto frontier will be an ellipse, and it will have exactly the shape of an isovalue of the density of experiments. Thus knowing the variance and covariance of experiment results allows us to characterize the nature of the Pareto frontier we face.\n\n\nEllipses for Ranking\n\n\n\n\n\nSuppose we are choosing a fixed set of items to show to a user based on predictions of two outcomes, \\(x_1\\) and \\(x_2\\) (e.g. likes and comments, streams and dislikes, etc.). A natural question will be the shape of the Pareto frontier formed by alternative selections of items.\nWe show below that, if the predictions are well calibrated, the outcomes are independent, and the distribution of prediction obeys a joint Normal distribution, then the Pareto frontier will be an ellipse, and it will have exactly the shape of an isovalue of the density of predictions. Thus knowing the variance and covariance of predictions allows us to exactly characterize the nature of the aggregate tradeoffs we face.\n\n\nEllipses for Company Strategy\nTradeoffs are looser higher in the decision hierarchy. We can distinguish between different levels of decision in the hierarchy of the firm: \\[\\substack{\\text{company objective}\\\\\\text{(choose headcount)}}\n      &gt; \\substack{\\text{team objective}\\\\\\text{(choose projects)}}\n      &gt; \\substack{\\text{shipping objective}\\\\\\text{(choose experiments)}}\n      &gt; \\substack{\\text{algorithm objective}\\\\\\text{(choose items)}}\n      \\] We can think of each successive level as holding more variables fixed, and so we expect the Pareto frontiers to become successively tighter (Le Chatelier principle). We thus expect the tradeoff to be loosest at the level of overall company objectives, where we reallocate headcount. For this reason we should expect that, if the company as a whole pivots form metric X to metric Y, the principal effect will be a reallocation of effort between products rather than reallocation within products.\n\n\n\n\n\nDifferent product areas have different Pareto frontiers. Typically two different product areas will have substantially different ability to affect different metrics, and we may observe a situation like that shown on the right: team A primarily moves metric \\(X\\), team B primarily moves metric \\(Y\\).\n\n\n\n\n\n\nWe can also draw a combined Pareto frontier. Here we add up the effects on each. In this case neither invidual Pareto frontier shows a substantial effect from changing weights (if we restrict weights to be positive), and so accordingly the combined Pareto frontier shows little response to a change in weights. Note that I have not derived an expression for the combined Pareto frontier and here I am using a purely geometric argument. It would be lovely for formalize this part of the argument but I have not yet been able to.\n\n\n\n\n\n\nGreater investment will shift Pareto frontiers out. Here we visualize reallocating employees from team B (the frontier shifts in) to team A (the frontier shifts out).\n\n\n\n\n\n\nA combined company Pareto frontier will be loose. Here the green curve represents all the possibile outcomes as you shift resources between team A and B: we have now turned a tight tradeoff into a loose tradeoff. In this case this represents that a change in company objectives will be reflected mainly in reallocation of effort between teams rather than within teams.\n\n\n\nAppendix: Simulations\nIn this section I compare Pareto frontiers generated from different distribution of \\((X,Y)\\) from different joint distribution, and then draw the Pareto frontiers.\nThe left-hand plot shows the raw distribution, the right-hand plot shows the Pareto frontier.\nI don’t have anything conclusive to say except that broadly speaking the Pareto frontiers seem to me typically egg-shaped, i.e. not-too-far from ellipses.\nJoint normal with positive correlation:\n\n\n\n\n\nIndependent laplace:\n\n\n\n\n\nCommon Laplace factor with independent Gaussian noise:\n\n\n\n\n\nIndependent uniform:\n\n\n\n\n\nCommon uniform factor plus independent uniform noise:\n\n\n\n\n\n\n\nModel\nSuppose we have a set of items with, \\(x_1\\) and \\(x_2\\), distributed Normally: \\[\\binom{x_1}{x_2}\\sim N\\left(\\binom{0}{0},\n      \\begin{pmatrix}\\sigma_1^2 & \\rho\\sigma_1\\sigma_2 \\\\ \\rho\\sigma_1\\sigma_2 & \\sigma_2^2\\end{pmatrix}\\right).\\]\nWe additionally let each item have a score, \\(v\\), which is simply a weighted sum of the two characteristics (normalizing the weight on the first characteristic to be 1): \\[v=x_1 + wx_2.\\]\nWe can write the covariance between the characteristics and the score as follows:\n\\[Cov\\begin{bmatrix}x_1\\\\x_2\\\\v\\end{bmatrix}=\n\\begin{bmatrix}\n    \\sigma^2_1           & \\sigma_1\\sigma_2\\rho & \\sigma_1^2+ w\\rho\\sigma_1\\sigma_2 \\\\\n    \\sigma_1\\sigma_2\\rho &  \\sigma_2^2          & \\rho\\sigma_1\\sigma_2+w\\sigma_2^2 \\\\\n    \\sigma_1^2+w\\rho\\sigma_1\\sigma_2          &\n          \\rho\\sigma_1\\sigma_2+w\\sigma_2^2    &\n          \\sigma_1^2+w^2\\sigma_2^2 + 2\\rho w\\sigma_1\\sigma_2 \\\\\n\\end{bmatrix}\\]\nWe wish to know the total number of actions of each type, \\(X_1\\) and \\(X_2\\), for a given score threshold \\(\\bar{v}\\):\n\\[\\begin{aligned}\n   X_1   &=P(v\\geq \\bar{v})E[x_1|v\\geq \\bar{v}] \\\\\n   X_2   &=P(v\\geq \\bar{v})E[x_2|v\\geq \\bar{v}].\n\\end{aligned}\\]\nWe first calculate the conditional expectations:\n\\[\\begin{aligned}\nE[x_1|v\\geq \\bar{v}]\n   =& \\sigma_1 \\frac{Cov(x_1,v)}{\\sqrt{Var(x_1)Var(v)}}\n      \\frac{\\phi(\\frac{\\bar{v}}{\\sqrt{Var(v)}})}{\\Phi(\\frac{\\bar{v}}{\\sqrt{Var(v)}})} \\\\\n   =& \\sigma_1\n      \\frac{\\sigma_1^2+w\\rho\\sigma_1\\sigma_2}\n         {\\sqrt{\\sigma_1^2(\\sigma_1^2+w^2\\sigma_2^2 + 2\\rho w\\sigma_1\\sigma_2)}}\n      \\frac{\\phi(\\frac{\\bar{v}}{\\sqrt{Var(v)}})}{\\Phi(\\frac{\\bar{v}}{\\sqrt{Var(v)}})}\\\\\n  =&  \\frac{\\sigma_1^2+w\\rho\\sigma_1\\sigma_2}\n           {\\sqrt{\\sigma_1^2+w^2\\sigma_2^2 + 2\\rho w\\sigma_1\\sigma_2}}\n   \\frac{\\phi(\\frac{\\bar{v}}{\\sqrt{Var(v)}})}{\\Phi(\\frac{\\bar{v}}{\\sqrt{Var(v)}})}\n\\end{aligned}\\]\nNext we will assume that the expected quantity of items is fixed. This implies that both both \\(P(v\\geq \\bar{v})\\) and \\(\\frac{\\bar{v}}{\\sqrt{Var(v)}}\\) will be constant, and we will define: \\[\\begin{aligned}\n      \\gamma\\equiv\n         &\\frac{\\phi\\left(\\frac{\\bar{v}}{\\sqrt{Var(v)}}\\right)}\n            {\\Phi\\left(\\frac{\\bar{v}}{\\sqrt{Var(v)}}\\right)}P(w\\geq w^*) \\\\\n      X_1 =& \\frac{\\sigma_1^2+w\\rho\\sigma_1\\sigma_2}\n                  {\\sqrt{\\sigma_1^2+w^2\\sigma_2^2 + 2\\rho w\\sigma_1\\sigma_2}}\\gamma \\\\\n      X_2 =& \\frac{w\\sigma_2^2+\\rho\\sigma_1\\sigma_2}\n                  {\\sqrt{\\sigma_1^2+w^2\\sigma_2^2 + 2\\rho w\\sigma_1\\sigma_2}}\\gamma\n   \\end{aligned}\\]\nWe thus have expressions for \\(X_1\\) and \\(X_2\\) as a function of the relative weight \\(w\\). We wish to rearrange these to express \\(X_1\\) directly in terms of \\(X_2\\). To help we turn to Mathematica, with the following input:11 see notebook\nF1[w_,p_,s1_,s2_,g_]:=g(s1^2+w p s1 s2 )/Sqrt[s1^2+w^2 s2^2+2w p s1 s2]\nF2[w_,p_,s1_,s2_,g_]:=g(w s2^2 +p s1 s2)/Sqrt[s1^2+w^2 s2^2+2w p s1 s2]\nSolve[{X1==F1[w,p,s1,s2,g],X2==F2[w,p,s1,s2,g]}, {X1,w}]\nSimplify[First[%1]]\nThis returns a large expression:\n\\[\\begin{aligned}\n   X_1(X_2) &=\n      \\frac{\n         g^2 \\rho s_1 s_2^3 X_2\n         - p s_1 s_2 X_2^3\n         + g^3 s_2^4\n            \\sqrt{\\frac{-g^2 (-1 + p^2) s_1^2 s_2^2}{g^2 s_2^2 - X_2^2}}\n         - g s_2^2 X_2^2\n            \\sqrt{-\\frac{g^2 (-1 + p^2) s_1^2 s_2^2}{ g^2 s_2^2 - X_2^2}}\n         + X_2 \\sqrt{(-1 + p^2) s_1^2 s_2^2 X_2^2 (-g^2 s_2^2 + X_2^2)}\n      }{g^2 s_2^4 - s_2^2 X_2^2}\\end{aligned}\\]\nWe can however substantially simplify this: \\[\\begin{aligned}\n   X_1 &= \\frac{\n         s_1s_2X_2 (g^2 \\rho s_2^2  - p X_2^2)\n         + g^2s_2^2(g^2 s_2^2-  X_2^2)\n            \\sqrt{-\\frac{(-1 + p^2) s_1^2 s_2^2}{g^2 s_2^2 - X_2^2}}\n         - X_2^2s_1s_2 \\sqrt{(p^2-1) (g^2 s_2^2-X_2^2)}\n      }{g^2 s_2^4 - s_2^2 X_2^2} \\\\\n   &= \\frac{\n         s_1s_2X_2 p(g^2 s_2^2  - X_2^2)\n         + gs_2^3s_1 g\n            \\sqrt{(p^2-1)(g^2 s_2^2 - X_2^2)}\n         - X_2^2s_1s_2 \\sqrt{(p^2-1) (g^2 s_2^2-X_2^2)}\n      }{s_2^2(g^2 s_2^2 - X_2^2)} \\\\\n   &= \\frac{s_1}{s_2}X_2p + \\frac{\n         s_1s_2(g^2s_2^2 - X_2^2 )\n            \\sqrt{(p^2-1) (X_2^2-g^2 s_2^2)}\n      }{s_2^2(g^2 s_2^2 - X_2^2)} \\\\\n   &= X_2 \\rho \\frac{s_1}{s_2}\n      +\\frac{s_1}{s_2}\\sqrt{(p^2-1) (X_2^2-g^2 s_2^2)}.\n\\end{aligned}\\]\n\n\n\n\n\nWe now wish to show that this curve is equal to an isovalue of the joint distribution of \\(x_1\\) and \\(x_2\\) (illustrated at right). We can write the isovalue of the joint Normal distribution of \\((x_1,x_2)\\) as follows:22 From Section 4.7 of the 1st edition (2002) of the book Introduction to Probability, by D. P. Bertsekas and J. N. Tsitsiklis, http://athenasc.com/Bivariate-Normal.pdf\n\\[k = \\frac{x_1^2}{\\sigma_1^2}+\\frac{x_2^2}{\\sigma_2^2}-2\\rho\\frac{x_1x_2}{\\sigma_1\\sigma_2}.\\]\nSolving this quadratic we can write: \\[\\begin{aligned}\n      x_1 &= x_2 \\rho \\frac{\\sigma_1}{\\sigma_2}\n               \\pm \\frac{\\sigma_1}{\\sigma_2}\\sqrt{-x_2^2+x_2^2\\rho^2+k\\sigma_2^2} \\\\\n         &= x_2 \\rho \\frac{\\sigma_1}{\\sigma_2}\n               \\pm \\frac{\\sigma_1}{\\sigma_2}\\sqrt{k\\sigma_2^2-(1-\\rho^2)x_2^2}.\n   \\end{aligned}\\]\nWe can see that the two curves will be equal where \\(k=\\frac{\\sigma_2^2}{\\sigma_1^2}(\\rho^2-1)g^2\\)."
  },
  {
    "objectID": "posts/2023-10-25-test-quarto-folding.html",
    "href": "posts/2023-10-25-test-quarto-folding.html",
    "title": "heading 1",
    "section": "",
    "text": "PRIORITY"
  },
  {
    "objectID": "posts/2023-10-25-test-quarto-folding.html#bender-et-al.-stochastic-parrots",
    "href": "posts/2023-10-25-test-quarto-folding.html#bender-et-al.-stochastic-parrots",
    "title": "heading 1",
    "section": "Bender et al., Stochastic Parrots",
    "text": "Bender et al., Stochastic Parrots\nBender, Gebru, McMillan-Major, Mitchell (2021, FAccT) “On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?”\n\n“LMs are not performing natural language understanding (NLU), and only have success in tasks that can be approached by manipulating linguistic form [14].”\n\n\n“the tendency of human interlocutors to impute meaning where there is none can mislead both NLP researchers and the general public into taking synthetic text as meaningful.”\n\n\n“Combined with the ability of LMs to pick up on both subtle biases and overtly abusive language patterns in training data, this leads to risks of harms, including encountering derogatory language and experiencing discrimination at the hands of others who reproduce racist, sexist, ableist, extremist or other harmful ideologies reinforced through interactions with synthetic language.”\n\nCoherence in the Eye of the Beholder. It produces “apparently” coherent text but not really coherent.\nRisks and Harms. Generally: absorbing hegemonic worldview. E.g.: 1. Assuming doctor will be male, nurse female; 2. Outputting abusive language; 3. Generate meaningless text & used, e.g., to recruit terrorists.\n  Especially this point: *apparent* fluency will mislead people into thinking that there's some genuine content.\n  \n  &gt; *\"the human tendency to attribute meaning to text, in combination with large LMs’ ability to learn patterns of forms that humans associate with various biases and other harmful attitudes, leads to risks of real-world harm.\"*\nAlso discuss environmental cost.\nProposal. Value-sensitive design.\n2023-02: Goldstein et al. “Generative Language Models and Automated Influence Operations: Emerging Threats and Potential Mitigations” - \n\nArms race in google search SEO\n\nLow trust in the 19th century:\n\nDavid Henkin book: in 19th century NY 40pct of banknotes circulating were fake.\nAdulterated foods\nPartisan news\n\nBenson (2023, Wired) “Humans Aren’t Mentally Ready for an AI-Saturated ‘Post-Truth World’” — quotes from psychologists saying things like “Anxiety is just going to ratchet up as we’re faced with this unknown thing in our world.”\nAlex Rosenblatt (2023) “Human Review is no Longer the Gold Standard”\nPrediction that LLMs will be used for Spear Phishing: https://twitter.com/emollick/status/1681374663505575936\nGlukhov et al. (2023) LLM CENSORSHIP: A MACHINE LEARNING CHALLENGE OR A COMPUTER SECURITY PROBLEM?"
  },
  {
    "objectID": "posts/2023-10-25-test-quarto-folding.html#will-a-deepfake-cause-damage-make-the-front-page-of-a-major-news-source-in-2023-10",
    "href": "posts/2023-10-25-test-quarto-folding.html#will-a-deepfake-cause-damage-make-the-front-page-of-a-major-news-source-in-2023-10",
    "title": "heading 1",
    "section": "Will a deepfake cause damage & make the front page of a major news source in 2023? 10%",
    "text": "Will a deepfake cause damage & make the front page of a major news source in 2023? 10%\nThe technology to create “deepfakes” started to emerge around 2018. (Nancy Pelosi video)\nThe Metaculus prediction shot up from 40% to 80% in May after a fake image of a Pentagon bombing began to circulate on Twitter.\nIn 2023 will a successful deepfake attempt causing real damage make the front page of a major news source?"
  },
  {
    "objectID": "posts/2023-10-25-test-quarto-folding.html#will-a-deepfake-be-blamed-by-g20-politician-for-an-election-loss-2025",
    "href": "posts/2023-10-25-test-quarto-folding.html#will-a-deepfake-be-blamed-by-g20-politician-for-an-election-loss-2025",
    "title": "heading 1",
    "section": "Will a deepfake be blamed by G20 politician for an election loss? 2025",
    "text": "Will a deepfake be blamed by G20 politician for an election loss? 2025\nWill a politician claim they lost a major election due to a “deepfake” image, video, or audio recording in a G20 country before 2025?"
  },
  {
    "objectID": "posts/2023-10-25-test-quarto-folding.html#will-ai-be-used-in-an-attack-on-infrastructure-costing-1b-2025",
    "href": "posts/2023-10-25-test-quarto-folding.html#will-ai-be-used-in-an-attack-on-infrastructure-costing-1b-2025",
    "title": "heading 1",
    "section": "Will AI be used in an attack on infrastructure costing >$1B? 2025",
    "text": "Will AI be used in an attack on infrastructure costing &gt;$1B? 2025\nWill a infrastructure disaster costing &gt;$1B in a G20 country be widely attributed to an AI cyberattack before 2025?"
  },
  {
    "objectID": "posts/2023-10-25-test-quarto-folding.html#will-ai-be-used-in-a-theft-of-intellectual-property-cost-10m-2025",
    "href": "posts/2023-10-25-test-quarto-folding.html#will-ai-be-used-in-a-theft-of-intellectual-property-cost-10m-2025",
    "title": "heading 1",
    "section": "Will AI be used in a theft of intellectual property cost >$10M? 2025",
    "text": "Will AI be used in a theft of intellectual property cost &gt;$10M? 2025\nWill a theft of &gt;$10M of intellectual property be widely attributed to an AI cyberattack before 2025?"
  },
  {
    "objectID": "posts/2023-10-25-test-quarto-folding.html#will-ai-cause-a-stock-exchange-to-halt-trading-for-24-hours-2025",
    "href": "posts/2023-10-25-test-quarto-folding.html#will-ai-cause-a-stock-exchange-to-halt-trading-for-24-hours-2025",
    "title": "heading 1",
    "section": "Will AI cause a stock exchange to halt trading for >24 hours? 2025",
    "text": "Will AI cause a stock exchange to halt trading for &gt;24 hours? 2025\nWill a stock exchange halt trading for &gt;24 hours with a cause widely attributed to AI before 2025?"
  },
  {
    "objectID": "posts/2023-10-25-test-quarto-folding.html#will-ai-be-used-in-a-major-attack-on-voting-systems-in-g20-2025",
    "href": "posts/2023-10-25-test-quarto-folding.html#will-ai-be-used-in-a-major-attack-on-voting-systems-in-g20-2025",
    "title": "heading 1",
    "section": "Will AI be used in a major attack on voting systems in G20? 2025",
    "text": "Will AI be used in a major attack on voting systems in G20? 2025\nWill a major attack on voting systems in a G20 country be widely attributed to an AI before 2025?"
  },
  {
    "objectID": "posts/2023-10-25-test-quarto-folding.html#resolved-predictions",
    "href": "posts/2023-10-25-test-quarto-folding.html#resolved-predictions",
    "title": "heading 1",
    "section": "Resolved Predictions",
    "text": "Resolved Predictions\nWill a “Deepfake” video about a national U.S. political candidate running for office in 2018 get 2M+ views?\nResolved No, was at 20% for a couple of months around August.\nBy late 2017, will there be a wide-scale hoax be created using video-alteration technology to put words in a famous figure’s mouth?\nResvolved No, was 50% for a long time."
  },
  {
    "objectID": "posts/2023-10-25-test-quarto-folding.html#jsdfklsjadfk",
    "href": "posts/2023-10-25-test-quarto-folding.html#jsdfklsjadfk",
    "title": "heading 1",
    "section": "jsdfklsjadfk",
    "text": "jsdfklsjadfk\nasdfjdfksl"
  },
  {
    "objectID": "posts/2023-10-25-test-quarto-folding.html#sdjfakfjdlks",
    "href": "posts/2023-10-25-test-quarto-folding.html#sdjfakfjdlks",
    "title": "heading 1",
    "section": "sdjfakfjdlks",
    "text": "sdjfakfjdlks"
  },
  {
    "objectID": "posts/2023-10-25-test-quarto-folding.html#observations-on-internal-properties",
    "href": "posts/2023-10-25-test-quarto-folding.html#observations-on-internal-properties",
    "title": "heading 1",
    "section": "Observations on Internal Properties",
    "text": "Observations on Internal Properties\n\nHistorically it was feasible to do filtering with human judgment. The technology of mass media means that a small amount of content was shown to a large audience (books, magazines, radio, television, movies), which made it feasible to have a human censor who manually reviewed the majority of content.\nFiltering with crude classifier. Motivated senders will be able to cheaply obfuscate their content to get around the classifier.\nFiltering with good classifier. Receiver will be able to filter perfectly, content will be eliminated.\nSelecting with crude classifier. Suppose you want to select posts that are pretty, funny, clickable, etc.. In these cases the incentives are more likely to be aligned: I can’t think of many cases where a sender wishes to be classified as having some good property, but also prefers to not actually have that property.\nSelecting with good classifier. This should eliminate the Goodhart problem."
  },
  {
    "objectID": "posts/2023-10-25-test-quarto-folding.html#observations-on-external-properties",
    "href": "posts/2023-10-25-test-quarto-folding.html#observations-on-external-properties",
    "title": "heading 1",
    "section": "Observations on External Properties",
    "text": "Observations on External Properties\n\nControlling on external properties typically uses metadata. The primary method for controlling spam is not using content-based classifiers but instead maintaining blacklists of servers: Reiley and Rao (2012) say “[t]he single most effective weapon in the spam-blocking arsenal turns out to be blacklisting an email server.” Similarly for misinformation: the most effective measures have not been proactive classifiers (typically low precision, e.g. 10%) or third-party-fact checking (typically high latency). Instead it has been discouragements to posting misinformation, and changes in the overall ranking system.\nCryptographically signing messages. It still means you must trust someone. Digital signing always had limited uptake for email and for SSL on websites: the attention tradeoff implies that it’s better if an intermediary does the validation."
  },
  {
    "objectID": "posts/2023-10-25-test-quarto-folding.html#a-model-of-both",
    "href": "posts/2023-10-25-test-quarto-folding.html#a-model-of-both",
    "title": "heading 1",
    "section": "A model of both",
    "text": "A model of both\n\nThere’s a set of features 1,…,n, a model can be defined as the number of features that you condition on. We can then characterize three parties with an integer representing the number of features: (1) human, (2) sender, (3) receiver.\nground truth is all features - as classifier gets better u get squeezed ;\nsender has to hide a “1” from the classifier , gradually get squeezed ;\nother adversarial : putting police around town to prevent crime (don’t post it publicly), auditing tax returns , applying antibiotics ; (there’s some dynamics : try to wipe out population).\nkey is whether sender has a constraint in state space ; but conditional independence won’t hold\nhuman affairs : jigsaw puzzle it’s all there ; vs human things it’s all empirical vs theory ;"
  },
  {
    "objectID": "posts/2023-10-25-test-quarto-folding.html#will-a-deepfake-cause-damage-make-the-front-page-of-a-major-news-source-in-2023-10-1",
    "href": "posts/2023-10-25-test-quarto-folding.html#will-a-deepfake-cause-damage-make-the-front-page-of-a-major-news-source-in-2023-10-1",
    "title": "heading 1",
    "section": "Will a deepfake cause damage & make the front page of a major news source in 2023? 10%",
    "text": "Will a deepfake cause damage & make the front page of a major news source in 2023? 10%\nThe technology to create “deepfakes” started to emerge around 2018. (Nancy Pelosi video)\nThe Metaculus prediction shot up from 40% to 80% in May after a fake image of a Pentagon bombing began to circulate on Twitter.\nIn 2023 will a successful deepfake attempt causing real damage make the front page of a major news source?"
  },
  {
    "objectID": "posts/2023-10-25-test-quarto-folding.html#will-a-deepfake-be-blamed-by-g20-politician-for-an-election-loss-2025-1",
    "href": "posts/2023-10-25-test-quarto-folding.html#will-a-deepfake-be-blamed-by-g20-politician-for-an-election-loss-2025-1",
    "title": "heading 1",
    "section": "Will a deepfake be blamed by G20 politician for an election loss? 2025",
    "text": "Will a deepfake be blamed by G20 politician for an election loss? 2025\nWill a politician claim they lost a major election due to a “deepfake” image, video, or audio recording in a G20 country before 2025?"
  },
  {
    "objectID": "posts/2023-10-25-test-quarto-folding.html#will-ai-be-used-in-an-attack-on-infrastructure-costing-1b-2025-1",
    "href": "posts/2023-10-25-test-quarto-folding.html#will-ai-be-used-in-an-attack-on-infrastructure-costing-1b-2025-1",
    "title": "heading 1",
    "section": "Will AI be used in an attack on infrastructure costing >$1B? 2025",
    "text": "Will AI be used in an attack on infrastructure costing &gt;$1B? 2025\nWill a infrastructure disaster costing &gt;$1B in a G20 country be widely attributed to an AI cyberattack before 2025?"
  },
  {
    "objectID": "posts/2023-10-25-test-quarto-folding.html#will-ai-be-used-in-a-theft-of-intellectual-property-cost-10m-2025-1",
    "href": "posts/2023-10-25-test-quarto-folding.html#will-ai-be-used-in-a-theft-of-intellectual-property-cost-10m-2025-1",
    "title": "heading 1",
    "section": "Will AI be used in a theft of intellectual property cost >$10M? 2025",
    "text": "Will AI be used in a theft of intellectual property cost &gt;$10M? 2025\nWill a theft of &gt;$10M of intellectual property be widely attributed to an AI cyberattack before 2025?"
  },
  {
    "objectID": "posts/2023-10-25-test-quarto-folding.html#will-ai-cause-a-stock-exchange-to-halt-trading-for-24-hours-2025-1",
    "href": "posts/2023-10-25-test-quarto-folding.html#will-ai-cause-a-stock-exchange-to-halt-trading-for-24-hours-2025-1",
    "title": "heading 1",
    "section": "Will AI cause a stock exchange to halt trading for >24 hours? 2025",
    "text": "Will AI cause a stock exchange to halt trading for &gt;24 hours? 2025\nWill a stock exchange halt trading for &gt;24 hours with a cause widely attributed to AI before 2025?"
  },
  {
    "objectID": "posts/2023-10-25-test-quarto-folding.html#will-ai-be-used-in-a-major-attack-on-voting-systems-in-g20-2025-1",
    "href": "posts/2023-10-25-test-quarto-folding.html#will-ai-be-used-in-a-major-attack-on-voting-systems-in-g20-2025-1",
    "title": "heading 1",
    "section": "Will AI be used in a major attack on voting systems in G20? 2025",
    "text": "Will AI be used in a major attack on voting systems in G20? 2025\nWill a major attack on voting systems in a G20 country be widely attributed to an AI before 2025?"
  },
  {
    "objectID": "posts/2023-10-25-test-quarto-folding.html#resolved-predictions-1",
    "href": "posts/2023-10-25-test-quarto-folding.html#resolved-predictions-1",
    "title": "heading 1",
    "section": "Resolved Predictions",
    "text": "Resolved Predictions\nWill a “Deepfake” video about a national U.S. political candidate running for office in 2018 get 2M+ views?\nResolved No, was at 20% for a couple of months around August.\nBy late 2017, will there be a wide-scale hoax be created using video-alteration technology to put words in a famous figure’s mouth?\nResvolved No, was 50% for a long time."
  }
]