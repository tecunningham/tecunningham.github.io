[
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html",
    "href": "posts/2023-01-31-social-media-suspensions-data.html",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "",
    "text": "Tom Cunningham. (@testingham) First version Jan 31 2023, last updated April 12 2023.11 Thanks to comments from Sahar Massachi, Katie Harbath, Nichole Sessego, and many others.\nThis note describes the suspension practices of the major social media platforms. I have collected a dataset of around 200 suspensions of prominent people across 12 platforms, stored in a google spreadsheet. The chart below summarizes the full dataset:\nThe data helps illuminate what platforms are doing. It is very difficult for an outside observer to see how a platform moderates their content. The advantages of studying the suspension of prominent users are that (1) the data is public and (2) the outcomes are comparable across platforms.\nKey findings.\nI am working on a separate essay about why platforms suspend users. It is difficult to give clear reasons why platforms suspend users. In a separate essay I try to break down how much their action can be attributed to influence from owners, from employees, from users, from advertisers, or from governments. Having this dataset of suspensions is very useful to be able to make generalizations about platform behavior."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#politicians",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#politicians",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Politicians",
    "text": "Politicians\nAmong US Federal politicians only Republicans have been suspended. In the US 8 Republicans have had one or more suspension, but no Democrats. Among the Republicans the suspensions were for a variety of reasons: related to the Jan 6 riots (Trump, Barry Moore, MTG), related to COVID (Ron Johnson, Rand Paul, MTG), for misgendering (Jim Banks), for tweeting a threat (Briscoe Cain), for animal blood on a profile photo (Steve Daines), one by a rogue employee (Trump).\nIt seems to me that the asymmetry in suspensions is primarily due to Republicans being more likely to violate the policies, rather than asymmetric enforcement of existing policies. I am not aware of any cases where a Democratic politician violated one of these policies but was not suspended.\n\n\n\n\n\nSuspension of national politicians outside the US has been relatively rare. My dataset contains 13 national politicians who were suspended in the world outside the US, compared to 8 in the US. This is a big asymmetry, and something of a puzzle. I have discussed this with a number of people who worked in enforcement and they attribute to a mixture of (1) less policy-violating behaviour from non-US politicians; (2) looser enforcement against non-US politicians; (3) lower overall social media usage outside the US; and (4) lower coverage of non-US politicians in my dataset."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#us-prominent-figures",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#us-prominent-figures",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "US Prominent Figures",
    "text": "US Prominent Figures\nThis shows all suspensions of US “notable people”:\n\n\n\n\n\nBetween 2015 and 2017 there were a series of alt-right personalities suspended from Twitter. The suspensions were often not for their views but their behaviour:\n\n2015: Charles Johnson from Twitter for a threat.\n2016: Milo Yiannopoulos from Twitter for harassment, Richard Spencer from Twitter for manipulation.\n2017: Roger Stone from Twitter for abuse.\n2018: Alex Jones from Twitter for incitement and abuse.\n\nBeginning in late 2017 more alt-right accounts were suspended. Either for hate speech, for offline behaviour, or without any public reason given:\n\nLate 2017: Baked Alaska from Twitter for hate speech.\n2018: Owen Benjamin from Twit with no reason given, Alex Jones from FB and YouTube for hate speech.\n2019: Nick Fuentes from Meta with no reason given.\n\nBetween November 2020 and January 2021 a large set of prominent figures were suspended for election-related reasons. The most suspensions were on Twitter but there were also from other platforms.\n\nSince November 2022 Twitter has unsuspended a large fraction of the suspended users that I track, probably around 1/2.\nSome people have been suspended simultaneously across multiple platforms (e.g. Alex Jones)"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#meta",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#meta",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Meta",
    "text": "Meta"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#twitter",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#twitter",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Twitter",
    "text": "Twitter\n\n\n\n\n\nThe following chart shows just accounts that were un-suspended under Musk, i.e. people with Twitter suspension that started before Oct 27 2022 and ended after that date. See below for a more fine-grained dataset of accounts unsuspended under Musk.\nYou can see that the primary original reasons for suspension were hate speech COVID misinformation. Kanye West and Nick Fuentes were re-suspended under Musk."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#youtube",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#youtube",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "YouTube",
    "text": "YouTube"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#tik-tok",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#tik-tok",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Tik Tok",
    "text": "Tik Tok"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Other Platforms",
    "text": "Other Platforms"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#hate-speech",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#hate-speech",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Hate Speech",
    "text": "Hate Speech"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#twitter-1",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#twitter-1",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Twitter",
    "text": "Twitter\nWikipedia page on Twitter Suspensions. Wikipedia has a list of around 400 Twitter suspensions. I chose not to create my own database (partly drawing from Wikipedia) for a few reasons: (1) I would want to add a lot of annotations to the Wikipedia data, e.g. about reasons for suspension or types of suspension. (2) Parsing the data is nontrivial: date ranges are given in various formats and would require some work on a regex to parse consistently. (3) There is some missing and inconsistent data, e.g. it has Trump’s suspension start-date but not end-date, and the names of people are not consistent (e.g. sometimes “Donald Trump”, sometimes “Donald J Trump”).\nThe Wikipedia dataset shows a similar basic pattern to what I document above: a dramatic increase in the rate of suspensions around mid-2017\n\n\n\n\n\nWikipedia-reported Twitter suspension by year\n\n\n\n\n\n\n\n\n\nAll Wikipedia-reported Twitter suspension, highlighting accounts with more than 1M followers (not all suspensions list the number of followers).\n\n\n\n\nTravis Brown: Twitter Watch This project appears to have data on almost all suspensions on Twitter since Feb 2022, and also tracks whether the suspension have been reversed. It does not include any suspensions which started prior to Feb 2022. There is a giant CSV file with 600K rows, suspensions.csv. Some visualizations:\n\n\n\n\n\nObservations by date of suspension\n\n\n\n\n\n\n\n\n\nObservation by date of unsuspension\n\n\n\n\n\n\n\n\n\nObservation by date of account creation\n\n\n\n\n\n\n\n\n\nSuspensions for accounts with &gt;1M followers\n\n\n\n\nTravis Brown: Twitter Unsuspensions. This is a collection of users who Twitter has un-suspended since Oct 27 2022 (when Musk took over). For some accounts there is a date of suspension but some have missing dates, I think suspension-date is only observed if after Feb 2022. (The content of this dataset is neither a subset nor a superset of the previous daatset). Unfortunately the dataset doesn’t have follower-count or twitter handle, so it’s not easy to join with other datasets or find the most prominent accounts.\n\n\n\n\n\nObservations by date of suspension\n\n\n\n\n\n\n\n\n\nObservations by date of unsuspension\n\n\n\n\nTravis Brown: Deleted Tweets / Suspended Accounts. This project scrapes profiles from the Wayback Machine, and seems to have a large set of accounts that were suspended with fairly long retention, I have not yet investigated further.\nTwitter Transparency Reports. This has data on the aggregate number of suspensions per half between July 2018 and Dec 2021. Note that the website is down but the CSV files can still be downloaded. \n\n\n\n\n\nTotal Accounts Suspended on Twitter by Reason, 2018H2-2021H2\n\n\n\n\nCounterHate list of unsuspensions. The organization CounterHate has a list of 10 large accounts reinstated by Twitter since Musk’s takeover. Note I believe they incorrectly listed Rizza Islam as an account re-activated by Twitter: I can find no evidence that the acccount @RizzaIslam was ever suspended, it seems to have been continuously tweeting from November 2022 through Feb 2023. I have added all 10 accounts to my database, and checked activity across all platforms."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#youtube-1",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#youtube-1",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "YouTube",
    "text": "YouTube\nWikipedia page on YouTube suspensions. See above for reasons why I chose not to use this dataset as the primary source.\nWikitubia: Terminated YouTubers. A list of around 2300 YouTubers that have been permanently banned, including date of ban, subscribers, reason for ban, and citation. They don’t have a date when unbanned."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#meta-facebook-instagram",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#meta-facebook-instagram",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Meta / Facebook / Instagram",
    "text": "Meta / Facebook / Instagram\nThere is no Wikipedia page of suspensions on Facebook, Instagram or WhatsApp.\nMeta’s “Community Standards Enforcement Report” is shown below. Meta’s data does not include any data on account suspensions, however there are a few other patterns of interest.\n\nContent actioned is relatively stable. There are fairly few notable upward or downward trends across the different types of content actioned: terrorism content actioned has increased significantly on both platforms, hate speech actions increased up to the end of 2020, then declined.\nThe proactive detection rate is close to 100% for most categories. there were dramatic improvements for bullying and for hate speech over 2017-2021. Note that the proactive detection rate is the share of actioned content that is automatically detected, the share of true positives that are automatically detected is surely much lower.\nThe prevalence of volations has fallen significantly. The log axis diminishes the magnitude of the decline: prevalence has fallen by a factor of 2-5 for nudity, bullying, hate speech, and graphic content. (I only show the prevalence upper bound, but the lower bound generally tracks the same course).\n\n\n\n\n\n\nFacebook’s dangerous organizations list. This list was leaked in 2021 by the Intercept. Unfortunately it does not include the dates of when each organization was added. The list is organized into the following categories:\n\nTerror Organizations (e.g. Islamic State)\nCrime Organizations (e.g. Bloods, Crips)\nHate Organizations (e.g. Aryan Nation, includes bands and websites)\nMilitarized Social Movements (e.g. United States Patrio Defense Force)\nViolent Non-State Actors (e.g. Free Syrian Army)\nHate (e.g. David Duke)\nIndividuals: Crime (e.g. Denton Suggs, Gangster Disciples)\nIndividuals: Terror (e.g. Osama bin Laden)"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#tiktok",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#tiktok",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "TikTok",
    "text": "TikTok\nCommunity Standards Report. Shows an increase in suspensions from around 1M accounts/quarter per 2020 to 6M accounts/quarter in 2023."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#twitch",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#twitch",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Twitch",
    "text": "Twitch\nStreamerBans. They seem to have a pretty comprehensive database of bans on Twitch."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms-1",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms-1",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Other Platforms",
    "text": "Other Platforms\n\nSpotify. The only unambiguous suspension from Spotify I found was Alex Jones’ podcast. Spotify removed some episodes of Joe Rogan’s podcast, and removed R Kelly and XXXtentacion’s music from playlists. They remove some white-supremacist artists and music. They removed all music from the band LostProphets after their lead singer was convicted of child sexual abuse.\nSubstack. I’m not aware of anybody who’s been kicked off Substack, they present themselves as very pro-free-speech.\nReddit. I’m not aware of any data on reddit account suspensions.\nRumble. The Rumble video-hosting platform has become quite large (they claim 70M MAU, and have a market cap of ). Their terms of service restrict content that is “abusive, inciting violence, harassing, harmful, hateful, anti-semitic, racist or threatening.” However I have not yet found a single example of a prominent user who has been suspended from Rumble."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#other-data-sources",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#other-data-sources",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Other Data Sources",
    "text": "Other Data Sources\nCan find more suspensions by searching Wikipedia for “suspended from XXX”. E.g. site:wikipedia.org \"suspended from facebook\". Possibly worth doing the same search for Google News.\nSocialBlade has data on number of followers by month since 2018, across Twitter, FB, YouTube. I’m not sure how easy it would be to scrape this data. They have a paid API, they say “up to 3 years of Historical statistics on creators.” However the website seems to have data back to at least April 2018.\nBallotpedia list of elected officials suspended from social media. It is an excellent resource, appears comprehensive and cites original reporting. I have added all of their data to the database as of January 2023.\nGlobal Internet Forum to Counter Terrorism (GIFCT). They mainly work on sharing hashes of terrorist content between platforms. They have some dicussion papers about “terror designation lists” but I don’t think they maintain any lists themselves.\nSpecially Designated National / Global Terrorist (SDN/SDGT). This is a public list maintained by the US government, and consumed by a number of tech companies. The full history is available, but it would be extremely difficult to parse.\nLumen. This has an international database of government takedown requests. They also seem to include whether the request was honored.\nCCDH Disinformation Dozen. This is a list from March 2021 of prominent accounts who were spreading anti-vax information on social media: original report, followup report from April 2021). They also have a “toxic ten” report. It’s probably worth adding both lists to the database."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#third-party-sources-on-platform-policies",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#third-party-sources-on-platform-policies",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Third-Party Sources on Platform Policies",
    "text": "Third-Party Sources on Platform Policies\nThere are a variety of third-party resources comparing policies across platforms, however none seem to have data comparable to the list above, i.e. a summary of specific content policy changes over time.\nComparisons at a single point in time.\n\nDNC (2020) Comparison of Misinformation Policies, 2020\nConsumer Reports (Aug 13 2020) Comparison of Misinformation Policies, 2020. Data as of 2020, with three levels: “allowed”, “sometimes”, and “prohibited”.\nUNC CITAP (May 22 2020) Comparison of Misinformation Policies, 2020. Has tables comparing misinfo policies as of 2020, three levels: “prohibited”, “flagged”, “allowed.”\nElection Integrity Partnership (Oct 28 2020) Comparison of Election Policies, 2020.\nCarnegie Endowment (April 1 2021) Existence of Policies, 2021. Table just marks whether a platform has a policy on some type of content, not nature of policy. They also they have a database of platform policies but it seems to only have data from February 2021.\nVirality Project Comparison of COVID Vaccine Policies in 2021.\n\nPolicies tracked over time.\n\nMchangama, Fanlo and Alkiviadou (2023) Scope Creep: An Assessment of 8 Social Media Platforms’ Hate Speech Policies. They document the hate speech policies over time for 8 platforms using a consistent rubric. They document that hate speech policies have become more broad-reaching over time. The data is available in Excel sheets here.\nKatie Harbath and Collier Fernenkes (August 2022) Election Policy Announcements, 2003-2022. Google spreadsheet with links to around 600 policy announcements, organized by platform, author, date, product-type, and country. Focussed on election-related policies, and they don’t include summaries of the policy announcement. They also wrote up analyses: (1) “A Brief History of Tech and Elections”; (2) 2022 election announcements.\nRanking Digital Rights Index, Comparison of Privacy and Transparency Policies, 2017-2022. They collect perhaps 100 different indicators across around 15 tech companies, mostly related to privacy and transparency, earliest data from 2017. All the data is available.\nGLAAD Comparison of LGBTQ user safety, 2021-2022\nCELE, Letra Chica. Tracks all public policy changes on Meta, YouTube, and Twitter. Most data from May 2020, but they go back to 2019 for Facebook by using FB’s Transparency Center. Each policy update includes a short summary of what’s changed. Tracks both Spanish and English versions. Data stored on coda.io, I think it’s queryable.\nLinterna Verdes, Circuito. Has about 15 in-depth case studies of platform moderation decisions.\nHumboldt Institute, Platform Governance Archive. Comprehensive archive of ToS, Privacy Policy, and Community Guidlines, from 2004 until late 2021, for FB, IG, Twitter, and YouTube. The data will not be updated.\nOpen Terms Archive. Started by the French Ambassador for Digital Affairs, but now a collaboration. Tracks terms for many different online services in a github repo. The Platform Governance Archive has moved to be part of this project, here.\nEFF, TOSback. Database of historical ToS documents from different services, with cross-platform comparisons. The most recent updates seem to be from May 2021, possibly was succeeded by Open Terms Archive.\nEuropean Commission, Copyright Content Moderation and Removal. This PDF report includes a lot of work which maps the copyright policies of major platforms.\n\nNarrative histories:\n\nCatherine Buni and Soraya Chemaly (2016, the Verge) History of Moderation.\nSarah Jeong (2016, Vice) The History of Twitter’s Rules\nBergen (2022) Like, Comment, Subscribe. A book on the history of YouTube, it has a lot of detail on policy changes."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-film-and-television",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-film-and-television",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Exclusions in Film and Television",
    "text": "Exclusions in Film and Television\n\n\n\n\n\n\n\n\n\n\n\nalleged crime\nresult\n\n\n\n\n1920s\nFatty Arbuckle\nrumors of immorality\nfilm industry blacklisted\n\n\n\n\n\n\n\n\n1940s\nOrson Welles\ncommunist associations\nblacklisted, moved to Switzerland\n\n\n\nDalton Trumbo\ncommunist associations\nblacklisted\n\n\n\n(around 100 people)\ncommunist associations\nblacklisted for a decade\n\n\n\n\n\n\n\n\n1950s\nCharlie Chaplin\ncommunist associations\nbanned from US\n\n\n\nElia Kazan\ntestifying before HUAC\nlost some relationships in Hollywood\n\n\n\n\n\n\n\n\n1960s\nJane Fonda\nopposition to Vietnam war\nblacklisted\n\n\n\n\n\n\n\n\n1970s\nRoman Polanski\nrape of 13yo girl\nmild disapproval from Hollywood\n\n\n\n\n\n\n\n\n1990s\nO J Simpson\nmurdered his wife\nblacklisted\n\n\n\nWoody Allen\nmolested 7yo daughter\n\n\n\n\n\n\n\n\n\n2000s\nMel Gibson\nracism & anti-semitism\n“blacklisted in Hollywood for almost a decade”\n\n\n\nMira Sorvino\nrejecting Harvey Weinstein\nblacklisted\n\n\n\nRose McGowan\nrejecting Harvey Weinstein\nblacklisted\n\n\n\nIsaiah Washington\nhomophobic remarks\nblacklisted\n\n\n\nMichael Richards\nracist remarks\nblacklisted\n\n\n\nKathy Griffin\n“told Jesus to suck it”\nbanned from talk shows and TV appearances\n\n\n\nSean Penn\nopposition to Iraq war\ndropped from movie\n\n\n\n\n\n\n\n\n2010s\nBill Cosby\nsexual assault\nblacklisted (& later imprisoned)\n\n\n\nHarvey Weinstein\nsexual assault\nblacklisted (& later imprisoned)\n\n\n\nStacy Dash\nconservative advocacy\nblacklisted\n\n\n\nKirk Camerson\ncriticism of homosexuality\nblacklisted\n\n\n\nJames Woods\nanti-Obama tweets\nblacklisted\n\n\n\nCeeLo Green\nsexual assault\nblacklisted\n\n\n\nLouis CK\nsexual harassment\nblacklisted\n\n\n\nKathy Griffin\nphoto with head of Trump\nfired by CNN, lost endorsement, cancelled tour\n\n\n\nT J Miller\nsubstance abuse, sexual assault\nblacklisted\n\n\n\nGina Carano\npolitical social media posts\nfired from TV show\n\n\n\nKevin Spacey\nsexual harassment\nlost roles in films\n\n\n\nJussie Smollett\nlied about an attack\nlost roles in TV shows\n\n\n\nNeil deGrasse Tyson\nrape, sexual harassment\ntemporarily lost roles in TV shows\n\n\n\nRoseanne Barr\nracist tweet\nlost TV show\n\n\n\n\n\n\n\n\n2020s\nWill Smith\nslapping someone at Oscars\nfilm projects put on hold\n\n\n\nJohnny Depp\ndomestic violence\nlost roles in films\n\n\n\nAmber Heard\ninvolvement in trial w Johnny Depp\nlost roles in films\n\n\n\nJustin Roiland\nsexual harassment & abuse\nlost roles in shows"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-music",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-music",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Exclusions in Music",
    "text": "Exclusions in Music\n\n\n\n\n\n\n\n\n\n\n\nalleged crime\nresult\n\n\n\n\n1940s\nPaul Robeson\ncommunist associations\nblacklist and passport revoked\n\n\n\n\n\n\n\n\n1950s\nLeonard Bernstein\ncommunist associations\nbrief blacklist\n\n\n\nLena Horne\ncommunist associations\nblacklist\n\n\n\nPete Seeger\ncommunist associations\nblacklist\n\n\n\n\n\n\n\n\n1960s\nBeatles\nsaying they’re bigger than Jesus\nconsumer boycott\n\n\n\nLovin Spoonful\ncooperating with FBI\nmusic industry boycott\n\n\n\nNina Simone\n“Mississippi Goddam”\nboycott in the South\n\n\n\nJohn Lennon\ncriticism of US and Vietnam war\nrefused entry into US\n\n\n\nEartha Kitt\ncriticism of Vietnam war\nblacklist through LBJ and CIA\n\n\n\n\n\n\n\n\n1970s\nSex Pistols\ncriticizing the Queen, swearing on TV\nbanned by the BBC, dropped by EMI\n\n\n\n\n\n\n\n\n1980s\nNWA\n“Fuck the Police” & similar songs\nradio station boycott, police boycott\n\n\n\n\n\n\n\n\n1990s\nBruce Springseen\nsong against police brutality\nbrief police boycott\n\n\n\nMarilyn Manson\ntransgressive lyrics\nbanned from performing in some states\n\n\n\nBody Count\nsong “cop killer”\nalbum withdrawn and reissued\n\n\n\n\n\n\n\n\n2000s\nDixie Chicks\nfor opposition to Iraq war\nblacklisting and consumer boycott\n\n\n\nJanet Jackson\nshowing nipple\nVH1, MTV, & Viacom radio stopped playing her music\n\n\n\nR Kelly\nsexual abuse\nbroad blacklist\n\n\n\nChris Brown\ndomestic violence\nweak boycott and blacklist\n\n\n\n\n\n\n\n\n2010s\nLostprophets\nsexual abuse\nbroad blacklist\n\n\n\nMichael Jackson\nchild molestation\nsome radio stations stop playing music\n\n\n\n\n\n\n\n\n2020s\nBeyonce\nsong against police brutality\nbrief police boycott\n\n\n\nMorgan Wallen\nusing n-word\ntemporarily dropped from radio/streaming playlists\n\n\n\nKanye West\npraise of Hitler\nlost sponsors\n\n\n\nOthers.\n\nIn radio: Father Coughlin, Rush Limbaugh, Don Imus fired from CBS for calling womens’ basketball team “nappy-headed hos”, Howard Stern fired from various radio shows for comments.\nIn sport. Colin Kapaernick blacklisted from NFL for kneeling for the anthem. Pete Rose banned from MLB for gambling.\nNazi sympathisers/collaborators. Charles Lindbergh, Henry Ford, Charles Coughlin, PG Wodehouse, Ezra Pound.\nWriters: DH Lawrence, Henry Miller, Salman Rushdie (Nicole Bonoff).\nJournalists. Jeffrey Toobin (New Yorker writer masturbated on zoom call),\nNote on R Kelly disappearing from radio"
  },
  {
    "objectID": "posts/2017-09-27-work-of-art-age-mechnical-production.html",
    "href": "posts/2017-09-27-work-of-art-age-mechnical-production.html",
    "title": "The Work of Art in the Age of Mechanical Production",
    "section": "",
    "text": "When I heard about the neural nets that copy the styles of famous painters I thought it would be the same old junk.\nAcademics have been saying forever that they were on the verge of discovering the principles of aesthetics, and that they would soon be able to automate the production of beauty – melody, harmony, proportion, plot.\nWhen I was a kid I was excited to read about this sort of thing. But they always turn out to be fatuous, catastrophically oversimplified and overconfident, written - I’m guessing - by people who are intimidated & resentful of the culture around them. Our technical understanding of what makes something look good is still weak, and I don’t think it’s improving very fast. I learned to, when I come across an article about art written by a scientist, turn the page.\nBut now I think that maybe the automatic production of beauty will arrive soon. The machine learning algorithms work by extrapolating from existing examples, which means that they can produce new examples that fit some pattern (such as the pattern of beauty) without anyone involved having any explicit understanding of what the pattern is or how it can be defined.\nThis extrapolation without understanding is what happened in the study of visual perception – i.e. making inferences from images. Our understanding of perception is slowly moving forward, as it has been for centuries, but our ability to automate perception has shot ahead. In the 15th century Leonard da Vinci studied how the light reflected by an object is related to its distance – more distant objects tend to be bluer – these are relationships that we all know unconsciously, but which take a lot of work to dig out, such that we consciously understand them. Psychologists and computer scientists are still discovering things about the physics of light which we all know unconsciously. But computer models which incorporate our explicit knowledge of the physics of light are being thrashed by pure machine-learning models, which are fed a huge databases of pictures and simply extrapolate from what they’ve already seen.[2]\nI think the same basic point is true of aesthetic things. We really struggle trying to explain why we like a picture or dislike a melody, because most of the work is done at an unconscious level. The progress in understanding those principles will probably continue to be slow.\nBut now it seems likely to me that, before long, machines will be able to do all these things on demand – play some brand new Mozart, make elegant little drawings of animals, write a pretty good pop song. And the programmer who implements them could be – probably will be – some bozo who has no clue why it works.\n\n[1] 2017-05: SIGGRAPH video with style transfer - https://www.youtube.com/watch?v=HYhzZ-Abku8\n[2] 2017-05: Michael Elad “Deep, Deep Trouble: Deep Learning’s Impact on Image Processing, Mathematics, and Humanity” https://sinews.siam.org/Details-Page/deep-deep-trouble-4"
  },
  {
    "objectID": "posts/2017-09-27-work-of-art-age-mechnical-production.html#aka-machine-learning-aesthetics-the-unconscious",
    "href": "posts/2017-09-27-work-of-art-age-mechnical-production.html#aka-machine-learning-aesthetics-the-unconscious",
    "title": "The Work of Art in the Age of Mechanical Production",
    "section": "",
    "text": "When I heard about the neural nets that copy the styles of famous painters I thought it would be the same old junk.\nAcademics have been saying forever that they were on the verge of discovering the principles of aesthetics, and that they would soon be able to automate the production of beauty – melody, harmony, proportion, plot.\nWhen I was a kid I was excited to read about this sort of thing. But they always turn out to be fatuous, catastrophically oversimplified and overconfident, written - I’m guessing - by people who are intimidated & resentful of the culture around them. Our technical understanding of what makes something look good is still weak, and I don’t think it’s improving very fast. I learned to, when I come across an article about art written by a scientist, turn the page.\nBut now I think that maybe the automatic production of beauty will arrive soon. The machine learning algorithms work by extrapolating from existing examples, which means that they can produce new examples that fit some pattern (such as the pattern of beauty) without anyone involved having any explicit understanding of what the pattern is or how it can be defined.\nThis extrapolation without understanding is what happened in the study of visual perception – i.e. making inferences from images. Our understanding of perception is slowly moving forward, as it has been for centuries, but our ability to automate perception has shot ahead. In the 15th century Leonard da Vinci studied how the light reflected by an object is related to its distance – more distant objects tend to be bluer – these are relationships that we all know unconsciously, but which take a lot of work to dig out, such that we consciously understand them. Psychologists and computer scientists are still discovering things about the physics of light which we all know unconsciously. But computer models which incorporate our explicit knowledge of the physics of light are being thrashed by pure machine-learning models, which are fed a huge databases of pictures and simply extrapolate from what they’ve already seen.[2]\nI think the same basic point is true of aesthetic things. We really struggle trying to explain why we like a picture or dislike a melody, because most of the work is done at an unconscious level. The progress in understanding those principles will probably continue to be slow.\nBut now it seems likely to me that, before long, machines will be able to do all these things on demand – play some brand new Mozart, make elegant little drawings of animals, write a pretty good pop song. And the programmer who implements them could be – probably will be – some bozo who has no clue why it works.\n\n[1] 2017-05: SIGGRAPH video with style transfer - https://www.youtube.com/watch?v=HYhzZ-Abku8\n[2] 2017-05: Michael Elad “Deep, Deep Trouble: Deep Learning’s Impact on Image Processing, Mathematics, and Humanity” https://sinews.siam.org/Details-Page/deep-deep-trouble-4"
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html",
    "title": "Sushi-Roll Model of Online Media",
    "section": "",
    "text": "\\[\n\\def\\RR{{\\bf R}}\n\\def\\bold#1{{\\bf #1}}\n\\]\nA model of internet media: the platform chooses the composition, the user chooses the quantity. I think this is a nice crisp way of modeling how media platforms (FB, YouTube, TikTok) make their decisions about content: they chooses the mix of content, i.e. the shares of each type, and then their users choose the quantity. The platform is choosing the fillings for the sushi roll and the consumer is choosing how much to eat. Their decisions jointly determine the total amount of each ingredient consumed.\nThis gives a unified model of feed ranking inclusive of ad-load, revenue-sharing, producer-side effects, and advertiser demand elasticity.\nWe can break down four different ways in which increasing the share of a given content-type \\(i\\) will affect total revenue:\nAn efficient mix of content will choose the shares such that each type of content has the same marginal value, i.e. for each the type four components of value all sum to the same number.\nRelated literature. There are some nice models of ad-media tradeoff in Anderson and Jullien (2015), but I believe they don’t consider the effects on production by producers, nor the tradeoff between different types of content (though it’s a long time since I read it).\nExpressed formally: Suppose the platform chooses \\(x_1,\\ldots,x_n\\) which represent the impression-shares of each type of content such that \\(\\sum_{i=1}^nx_i=1\\). User demand depends on the average quality of each type of content (\\(q_i\\)), and they have diminishing returns in each type of content. The platform receives \\(p_i\\) for showing an impression of type \\(i\\), but that price depends on the number of impressions-seen. We can write the maximization problem as:\n\\[\\begin{aligned}\n      \\max_{x_1,\\ldots,x_n} \\utt{\\left(\\sum_{i=1}^n q_i(x_i)x_i^\\gamma\\right)}{total}{impressions}\n                     \\utt{\\left(\\sum_{i=1}^n x_ip_i(x_i)\\right)}{avg revenue}{per impression}\n      ,\\text{ s.t. }\\sum_{i=1}^n x_i=1\n\\end{aligned}\n\\]\nThe first order condition shows us the four components of value:\n\\[\\frac{\\partial L}{\\partial x_i} =\n   \\ut{\n      (\\utt{q_i \\gamma x_i^{-(1-\\gamma)}}{incrementality}{}\n      + \\utt{q_i'(x_i)x_i^\\gamma}{effect through}{quality})\n      \\utt{\\left(\\sum_{j=1}^n p_j x_j\\right)}{avg revenue}{per impression}\n   }{effect on revenue through total impressions}\n   +\n   \\utt{\n      (\n         \\utt{p_i(x_i)}{revenue from}{additional impressions}+\n         \\utt{p'_i(x_i)x_i}{revenue from}{change in price}\n      )\n      \\utt{\\left(\\sum_{j=1}^n q_j x_j^{\\gamma}\\right)}{total}{impressions}}\n      {effect on revenue}{through impressions on $i$}\n    + \\utt{\\lambda}{avg marginal}{effect}=0\n\\]\nThe final term, \\(\\lambda\\), is the Lagrangian, representing the average marginal value of the outside option, i.e. the other types of content that are being replaced. In some cases we can simplify this model and we get a closed-form solution for the optimal content composition.\nWhat this model doesn’t include:"
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html#more-details",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html#more-details",
    "title": "Sushi-Roll Model of Online Media",
    "section": "More Details",
    "text": "More Details\nWe can walk through a series of models from simple to complicated, to build up to the full sushi-roll model:\n\nPlatform chooses share of ads. The platform chooses the share of impressions that are ads, and consumers choose how many total impressions to consume. If we assume the price of ads (CPMs) is fixed then the platform will set the ad-load to maximize the total number of ad-impressions. If the platform can influence the price of ads by their choice of quantity (i.e. they act as a monopolist) then the platform may choose to reduce ad-load to drive up CPMs.1\nIn this model we’re letting the platform set the quantity of ads, but we would get the same result if the platform instead set the price of ads, e.g. they posted a specific CPM and advertisers can buy as much as they want.\nPlatforms chooses shares of organic content. Suppose ad-load is fixed but platforms can vary the shares of different types of organic content. Users’ consumption depends on the quality of the content but they also have a taste for variety (i.e. diminishing returns in each type of content). We then get a nice closed-form solution where the share of each type of content is increasing in its relative quality. On the margin the incrementality of each type of content will be zero: i.e. increasing the share of that type of content will have no effect on total impressions.\nPlatform choses shares of ads and organic content. Now lets treat both advertisers and organic producers as the same: each producer has a quality \\(q_i\\) but they also will pay a certain price \\(p_i\\) for impressions on their content. The platform takes those prices as given. We can then distinguish between three types of producer:\n\nAdvertisers: \\(p_i&lt;0\\): the producer will pay the platform per impression.\nProfessional producers: \\(p_i&gt;0\\): the producer asks to be paid per impression.\nAmateurs: \\(p_i=0\\): there is no monetary exchange, the content is in the public domain or generated by an ordinary user.\n\nIn equilibrium the share of impressions allocated to a given producer (\\(x_i\\)) will depend both on its quality \\(q_i\\) (AKA incrementality) and the price \\(p_i\\) that the producer sets. I don’t have a closed-form solution but we can derive a first-order condition that has a straight-forward interpretation.\nThis model is easy to state but I think is primarily applicable to small platforms where they take prices as given. E.g. suppose you run an app where you (1) license certain content, or use user-generated content; (2) run ads from various different ad networks, the ads vary in CPMs but they also vary in incrementality (i.e. how obnoxious they are to your userbase).\nBecause prices are taken as given, this model doesn’t help us calculate the optimal revenue share. It will tell us the optimal ad-load, but not taking into account the elasticity of supply from the advertiser.\nNote that most platforms do not explicitly discriminate between advertisers based on their incrementality however they can implicitly discriminate by having a “quality score” or “organic bid”. This score is quite clearly designed to measure the incrementality of the advertisements, and so I think can be used to implement an efficient pricing scheme.\nPlatform choses shares of ads and organic content, prices endogenous. We can easily extend the model above to allow the price of each type of content to depend on the quantity shown (\\(p_i(x_iM)\\)). For example the price of ads will depend on the number of ad impression shown (due to advertisers’ diminishing marginal returns from ads shown). This gives platforms a reason to restrict the quantity of ad-impressions.\nPlatform chooses both shares and prices. Up to this point the platform chose only the share of each type of content.\nNow suppose the platform can set a price to pay producers (e.g. “revenue share”), and it’s a homogenous price. The price should roughly depend on (1) the producer’s elasticity of quality to price (i.e. their cost function), and (2) the incrementality of quality on the consumer side. We could set this up with a single price for all producers, or set a producer-specific price."
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-1-platform-chooses-ad-load",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-1-platform-chooses-ad-load",
    "title": "Sushi-Roll Model of Online Media",
    "section": "Model 1: Platform Chooses Ad-Load",
    "text": "Model 1: Platform Chooses Ad-Load\n(see previous paper)"
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-2-platform-chooses-organic-composition",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-2-platform-chooses-organic-composition",
    "title": "Sushi-Roll Model of Online Media",
    "section": "Model 2: Platform Chooses Organic Composition",
    "text": "Model 2: Platform Chooses Organic Composition\nWe have a model where there are \\(n\\) producers, the platform assigns to each producer a share of total content \\(x_i\\), with \\(\\sum_ix_i=1\\), and the consumer will choose how many total impressions to consume (\\(M\\)) based on the average quality, but with diminishing returns in each .\n\\[\\begin{aligned}\n   q_i &\\in \\mathbb{R}^+\n      && \\text{quality of producer $i$}  \\\\\n   x_i &\\in [0,1]\n      && \\text{share of impressions on producer $i$}\\\\\n   \\sum_i x_i &= 1\n      && \\text{shares must sum to 1}\\\\\n   M  &= \\sum_{i=1}^nq_ix_i^\\gamma\n      && \\text{total impressions, diminishing returns in each producer, $0&lt;\\gamma&lt;1$} \\\\\n\\end{aligned}\\]\nThe platform wishes to maximize total impressions, \\(M\\). We want to solve for the resultant impression-share of each producer, i.e. \\(x_i\\) as a function of the qualities \\(q_1,..,q_n\\) and parameter \\(\\gamma\\). We get the following impression-maximizing shares:\n\\[x_i=\\frac{q_i^\\frac{1}{1-\\gamma}}{\\gamma\\sum_{j=1}^nq_j^\\frac{1}{1-\\gamma}}.\\]\nImplication: impression-share will be proportional to quality. Interesting the elasticity will be increasing in quality: a 1% increase in quality will get a more than 1% increase in share of impressions, because \\(\\frac{1}{1-\\gamma}&gt;1\\).\nAdding money. Suppose now that the platform gets paid for showing certain impressions. We can make various different assumption about the price paid:\n\nUniform homogenous price: the platform takes the price as given. This only makes sense if there are a subset of producers who are advertisers.\nEach producer sets a payment rate per impression.\nThe platform chooses a single price for all producers to get extra impressions."
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-3-platform-chooses-composition-prices-fixed",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-3-platform-chooses-composition-prices-fixed",
    "title": "Sushi-Roll Model of Online Media",
    "section": "Model 3: Platform Chooses Composition, Prices Fixed",
    "text": "Model 3: Platform Chooses Composition, Prices Fixed\nWe have a model with a consumer, a platform, and a set of \\(n\\) producers. The platform chooses the share of content from each producer, \\(x_i\\in[0,1]\\) with \\(\\sum_i x_i=1\\). The consumer chooses the total amount of impressions they consume, \\(M\\), based on the mixture of content and the quality of each type of content \\(q_i\\). Finally producers can set a price \\(p_i\\) for each impression that they receive from the consumer. A positive price \\(p_i&gt;0\\) means\n\\[\\begin{aligned}\n   q_i &\\in \\mathbb{R}\n      && \\text{quality of producer $i$}  \\\\\n   p_i &\\in \\mathbb{R}\n      && \\text{price offered by producer $i$}  \\\\\n   x_i &\\in [0,1]\n      && \\text{share of impressions on producer $i$}\\\\\n   \\sum_i x_i &= 1\n      && \\text{shares must sum to 1}\\\\\n   M  &= \\sum_{i=1}^nq_ix_i^\\gamma\n      && \\text{total impressions, diminishing returns in each producer, $0&lt;\\gamma&lt;1$} \\\\\n\\end{aligned}\n\\]\nIf the platform simply wanted to maximize total impressions, \\(M\\), then they can derive the optimal impression-shares as follows:\n\\[x_i^*=\\frac{q_i^\\frac{1}{1-\\gamma}}{\\gamma\\sum_{j=1}^nq_j^\\frac{1}{1-\\gamma}}.\\]\nHowever we want the platform to maximize profit, which we can write as follows:\n\\[\\begin{aligned}\n   \\text{profit} &= \\utt{\\left(\\sum_{i=1}^n q_ix_i^\\gamma\\right)}{total}{impressions}\n                  \\utt{\\left(\\sum_{i=1}^n x_ip_i\\right)}{avg revenue}{per impression}\n\\end{aligned}\n\\]\nI’m not sure if we can get a closed-form solution but we can at least get a first-order condition for each \\(x_i\\) that tells us useful stuff about comparative statics:\n\\[\\frac{\\partial L}{\\partial x_i} =\n   \\ut{\\utt{q_i \\gamma x_i^{-(1-\\gamma)}}{effect on}{total impressions}\n      \\utt{\\left(\\sum_{j=1}^n p_j x_j\\right)}{avg revenue}{per impression}\n   }{effect on revenue through total impressions}\n   +\n   \\utt{p_i\\utt{\\left(\\sum_{j=1}^n q_j x_j^{\\gamma}\\right)}{total}{impressions}}\n      {effect on revenue}{through impressions on $i$}\n    + \\utt{\\lambda}{avg marginal}{effect}=0\n\\]\nObservations:\n\nIf producers offer more, increasing \\(p_i\\), then \\(x_i\\) will go down until the marginal effect on total impression declines to balance the additional revenue.\nIf \\(p_i&lt;0\\), meaning a producer charges for impressions, then they can still have a positive number of impressions if their effect on total impressions is higher than the average of other types of content. (We could have added an additional constraint that \\(x_i\\geq 0\\).)"
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-4-platform-chooses-composition-monopolist",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-4-platform-chooses-composition-monopolist",
    "title": "Sushi-Roll Model of Online Media",
    "section": "Model 4: Platform Chooses Composition, Monopolist",
    "text": "Model 4: Platform Chooses Composition, Monopolist\nNow we allow the price of each type of content to depend on the quantity used, e.g. the price of ads will be higher when the quantity of ad-impressions is smaller (monopolist in the ad market). Strictly we should write \\(p_i(Mx_i)\\), but it’s somewhat easier to write \\(p_i(x_i)\\) and the answer should be similar for any type of content that is a small share.\n\\[\\begin{aligned}\n      \\text{profit} &= \\utt{\\left(\\sum_{i=1}^n q_ix_i^\\gamma\\right)}{total}{impressions}\n                     \\utt{\\left(\\sum_{i=1}^n x_ip_i(x_i)\\right)}{avg revenue}{per impression}\n\\end{aligned}\n\\]\nThere’s now one additional term in the first order condition:\n\\[\\frac{\\partial L}{\\partial x_i} =\n   \\ut{\\utt{q_i \\gamma x_i^{-(1-\\gamma)}}{effect on}{total impressions}\n      \\utt{\\left(\\sum_{j=1}^n p_j x_j\\right)}{avg revenue}{per impression}\n   }{effect on revenue through total impressions}\n   +\n   \\utt{\n      (\n         \\utt{p_i(x_i)}{revenue from}{additional impressions}+\n         \\utt{p'_i(x_i)x_i}{revenue from}{change in price}\n      )\n      \\utt{\\left(\\sum_{j=1}^n q_j x_j^{\\gamma}\\right)}{total}{impressions}}\n      {effect on revenue}{through impressions on $i$}\n    + \\utt{\\lambda}{avg marginal}{effect}=0\n\\]\nThe additional term represents the platform’s monopoly power with respect to the price paid. This has a natural interpretation for advertisers: showing fewer ads will drive up the price. For paid content-providers it could perhaps represent bulk discounts, I’m not sure whether this is a significant consideration."
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-2-derivation",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html#model-2-derivation",
    "title": "Sushi-Roll Model of Online Media",
    "section": "Model 2 Derivation",
    "text": "Model 2 Derivation\nThis is derivation of model #2. (I had chatGPT help with this derivation, was very useful)\n\nSetting up the Lagrangian. The objective is to maximize the total impressions, \\(M\\), subject to the constraint that the allocated shares of impressions sum to one. We start by writing the Lagrangian: \\[\\mathcal{L} = \\sum_{i=1}^n q_i x_i^\\gamma - \\lambda \\left(\\sum_{i=1}^n x_i - 1\\right)\\]\nwhere \\(\\lambda\\) is the Lagrange multiplier associated with the constraint.\nSolving for the multiplier. To solve for the value of \\(\\lambda\\), we take the derivative of the Lagrangian with respect to \\(x_i\\) and set it equal to zero: \\[\\frac{\\partial \\mathcal{L}}{\\partial x_i} = \\gamma q_i x_i^{\\gamma - 1} - \\lambda = 0\\]\nRearranging this equation yields: \\[x_i = \\left(\\frac{\\lambda}{\\gamma q_i}\\right)^{\\frac{1}{\\gamma-1}}\\]\nTaking the sum of this expression over all producers and using the constraint that the shares of impressions must sum to one, we obtain: \\[1 = \\sum_{i=1}^n x_i = \\sum_{i=1}^n \\left(\\frac{\\lambda}{\\gamma q_i}\\right)^{\\frac{1}{\\gamma-1}}\\]\nSimplifying this equation gives:\n\\[\\lambda^{\\frac{1}{\\gamma-1}} = \\gamma \\sum_{i=1}^n q_i^{-\\frac{1}{\\gamma-1}}\\]\nSubstituting this expression for \\(\\lambda\\) back into the equation for \\(x_i\\) results in:\n\\[x_i = \\frac{q_i^{-\\frac{1}{\\gamma-1}}}{\\gamma \\sum_{j=1}^n q_j^{-\\frac{1}{\\gamma-1}}}\\]\nThis is our final expression for the share of impressions on each producer as a function of the exogenous qualities and \\(\\gamma\\).\n\nSummary:\nThe Lagrangian: \\(\\mathcal{L} = \\sum_{i=1}^n q_i x_i^\\gamma - \\lambda \\left(\\sum_{i=1}^n x_i - 1\\right)\\).\nExpression for the multiplier: \\[\\lambda^{\\frac{1}{\\gamma-1}} = \\gamma \\sum_{i=1}^n q_i^{-\\frac{1}{\\gamma-1}}\\]\nThe resultant expression for \\(x_i\\): \\[x_i = \\frac{q_i^{-\\frac{1}{\\gamma-1}}}{\\gamma \\sum_{j=1}^n q_j^{-\\frac{1}{\\gamma-1}}}.\\]\nSlightly rearranged (by me):\n\\[x_i=\\frac{q_i^\\frac{1}{1-\\gamma}}{\\gamma\\sum_{j=1}^nq_j^\\frac{1}{1-\\gamma}}.\\]"
  },
  {
    "objectID": "posts/2023-03-06-social-media-business-models-sushi-roll.html#footnotes",
    "href": "posts/2023-03-06-social-media-business-models-sushi-roll.html#footnotes",
    "title": "Sushi-Roll Model of Online Media",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIn the profit-maximizing solution either the consumer-side or advertiser-side first-order-condition will be binding. It depends on the relative elasticity of the two sides of the platform.↩︎"
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html",
    "title": "Repulsion from the Prior",
    "section": "",
    "text": "the shortest version: contrary to recent reports, I do not think it’s possible for you to be a Bayesian and consistently exaggerate things.\n{: .center-image }"
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#short-version",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#short-version",
    "title": "Repulsion from the Prior",
    "section": "Short Version",
    "text": "Short Version\n\nIf we think of perception as inference, it has implications about the types of biases we would have.\nYet many biases and illusions seems to go in the exact opposite direction – sometimes called “anti-Bayesian” biases – in particular there are ubiquitous contrast effects, while Bayesian inference seems to imply assimilation effects.\nWei and Stocker (2015) say they can rationalize these contrast effects, under the assumption that our sensory mechanisms are tuned to the environment, such that they are relatively more sensitive to more likely signals. They say that this will imply contrast effects (that the bias is inversely proportional to the slope of the prior).\nYet their results contradict some simple laws of Bayesian inference – the law of iterated expectations, and law of total variance – so there is something odd going on.\n(If this explanation doesn’t work, then why do we get repulsion? I think that Ted Adelson explained the basic reason in the 70s. Will write another post on this.)"
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#shortish-version",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#shortish-version",
    "title": "Repulsion from the Prior",
    "section": "Shortish Version",
    "text": "Shortish Version\n\nHere’s a nice crisp problem: in what cases does inference attract towards the prior, and in what cases does it repulse away from it?\nGiven an unknown variable \\(x\\) and a signal \\(s\\), let’s say that there’s “attraction” at a given value of \\(x\\) if the average inferred value of \\(x\\) is closer to the prior than \\(x\\) itself is –\n\\[|E[E[x|s]|x]-\\mu|&lt;|x-\\mu|\\]\nAttraction effects are typically treated as the norm. For example if \\(x\\) is drawn from a normal distribution and if \\(s\\) is equal to \\(x\\) plus normal noise, then you’ll always get attraction to the prior. I.e., if \\(x\\) is above the mean, then it’ll be, on average, estimated to be closer to the mean than it actually is.\nHowever this has sometimes been treated as a puzzle in studies of perception: perception seems like inference, but we also find what look like repulsion effects. For example “contrast” effects, in which an object seems less dark when you put it next to another, darker, object. If we assume that the colour of the neighboring objects affects your prior about the target object, then this would imply an attraction effect. Yet repulsion effects seems to be the norm across all sorts of judgments (lightness, colour, volume, orientation, size), and similar contrast effects occur in time as well as in space (i.e., something seems less dark if it is preceded by something darker) – though of course there are exceptions. These types of illusion are sometimes called “anti-Bayesian.”\nA common explanation of these contrast effects is that we ‘code for differences’ – i.e. that something about our neural wiring causes us to encode differences, rather than levels, and this causes us to exaggerate differences, i.e. get contrast effects.\nBut this assumes that we encode the difference and then forget to decode (AKA coding catastrophe, AKA the el Greco fallacy). If you write down a Bayesian model, which makes its best effort to infer the level from the difference, you typically do not find the desired contrast effects (Schwartz, Hsu & Dayan (2007)).\nWei and Stocker (2015) announce that they have made a breakthrough – a fully Bayesian model which generates contrast/repulsion effects generically. They say that the key assumption is that we are more sensitive to differences in areas where signals are more likely to fall – i.e., sensitivity is proportional to the density of the prior.\nFormally, let \\(x\\sim f\\), and \\[s=F(x)+\\varepsilon.\\] This means that sensitivity is proportional to the density of the prior – and it implies that \\(s\\) will be roughly uniformly distributed – so in some sense it’s an efficient use of signal capacity. Given this setup, and some simplifications, they find that the bias is proportional to the slope of the prior – so if the prior is symmetric & single-peaked then for values above the mean, the bias will be positive, and vice versa – i.e. repulsion away from the prior everywhere.\nIn the note below I give a proof that implies that it is impossible to have repulsion effects everywhere – which seems to contradict the results of Wei & Stocker.\nI’m not sure what the source of the contradiction is – it could be either (a) Wei & Stocker’s results are true locally, but do not apply at the tails of the distribution, and so things balance out that way; (b) there is a difference in the implicit assumption used when taking conditional expectations (AKA the Borel-Kolmogorov paradox); or (c) I made a mistake.\nI also mention below a related result, that there cannot be a consistent upward or downward bias (i.e., it cannot be that \\(E[\\hat{x}\\|x]&gt;x\\) for all \\(x\\)). This is relevant for Wei & Stocker’s result applied to asymmetric priors – e.g. if the prior is everywhere decreasing – where the result seems to imply a consistent upward bias. \n\n{: .center-image }"
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#summary-of-proof",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#summary-of-proof",
    "title": "Repulsion from the Prior",
    "section": "Summary of proof",
    "text": "Summary of proof\n\nSuppose that there is repulsion from the prior everywhere, i.e. for all \\(x\\), \\(\\|E[\\hat{x}\\|x]-\\mu\\|&gt;\\|x-\\mu\\|\\).\nThis implies that \\(Var[\\hat{x}]&gt;Var[x]\\).\nBut this contradicts the law of total variance, which says that \\(Var[E[A\\|B]]\\leq Var[A]\\)."
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#detail",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#detail",
    "title": "Repulsion from the Prior",
    "section": "Detail:",
    "text": "Detail:\nSuppose there are two random variables \\(x\\) and \\(s\\), and let \\(\\hat{x}=E[x\\|s]\\). Let \\(x\\) be mean-zero, and let’s assume repulsion from the prior everywhere, i.e. for all \\(x\\):\n\\[\nE[\\hat{x}|x]|&gt;|x|\n\\]\nFrom this repulsion assumption I think it’s clear that there’s more variance in \\(E[\\hat{x}\\|x]\\) than in \\(x\\):\n\\[Var[E[\\hat{x}|x]]&gt;Var[x]\\]\nNow let’s apply the law of total variance:\n\\[\n\\begin{aligned}\nVar[A]=& E[Var[A|B]]+Var[E[A|B]] \\\\\\\\\nVar[\\hat{x}]=& E[Var[\\hat{x}|x]]+Var[E[\\hat{x}|x]]\n\\end{aligned}\n\\]\nThus implying that:\n\\[Var[\\hat{x}]\\equiv Var[E[x|s]]&gt;Var[x]\\]\nApplying the law of total variance again we get:\n\\[\\begin{aligned}\nVar[x]=& E[Var[x|s]]+Var[E[x|s]] \\\\\\\\\n      &gt;& Var[x]\n\\end{aligned}\\]\nA contradiction."
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#no-consistent-upwarddownward-bias",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#no-consistent-upwarddownward-bias",
    "title": "Repulsion from the Prior",
    "section": "No consistent upward/downward bias",
    "text": "No consistent upward/downward bias\nThe law of iterated expectations states that, for any \\(A\\) and \\(B\\):\n\\[E[E[A|B]]=E[A]\\]\nThis implies that there cannot be a consistent upward or downward bias, i.e. it cannot be true that:\n\\[E[\\hat{x}|x]&gt;x, \\forall x\\]"
  },
  {
    "objectID": "posts/2017-05-26-repulsion-from-the-prior.html#references",
    "href": "posts/2017-05-26-repulsion-from-the-prior.html#references",
    "title": "Repulsion from the Prior",
    "section": "References",
    "text": "References\n\nSchwartz, Hsu & Dayan (2007, Nature Review Neuro) “Space and Time in Visual Context”\nWei & Stocker (2015, Nature Neuroscience) “A Bayesian observer model constrained by efficient coding can explain ‘anti-Bayesian’ percepts”"
  },
  {
    "objectID": "posts/2023-09-05-model-of-ai-imitation.html",
    "href": "posts/2023-09-05-model-of-ai-imitation.html",
    "title": "An AI Which Imitates Humans Can Beat Humans",
    "section": "",
    "text": "Thanks to comments from many, especially Grady Ward.\nThe ground truth (black line), human observations (red dots), and human performance (red line).\nIn a formal model, imitative AI can beat humans for three reasons:\nAdditionally AI can learn and use tacit human knowledge. The majority of human knowledge is tacit, meaning it is used in forming judgments but we do not have conscious access to that knowledge. If AI models can accurately predict human judgments then the weights in those models effectively contain that tacit knowledge, and so the model can be re-engineered to use that knowledge in ways that humans cannot. I will discuss tacit knowledge more in a followup post.\nThis blog post contains: (1) a simple visual illustration of the argument; (2) a formal model and derivation of the claims above; (3) discussion of evidence that LLMs are able to perform super-human tasks; (4) discussion of applications, related literature, and complications. This is work in progress and I’m planning to keep adding material to this post."
  },
  {
    "objectID": "posts/2023-09-05-model-of-ai-imitation.html#model-implications",
    "href": "posts/2023-09-05-model-of-ai-imitation.html#model-implications",
    "title": "An AI Which Imitates Humans Can Beat Humans",
    "section": "Model Implications",
    "text": "Model Implications\n\nIf one human records all their observations then the computer will perfectly imitate them.\n\nSuppose that there is one human and they write down all of their observations, \\(\\hat{Q}=Q\\). Then the computer’s beliefs will be the same as the human’s beliefs (\\(\\hat{\\bm{w}}=\\bar{\\bm{w}}\\)), and so the computer will answer every question exactly as the human does, though neither knows the truth (\\(\\bar{\\bm{w}}\\neq\\bm{w}\\)).\n\nIf humans do not record all their observation then the computer will perform worse.\n\nSuppose humans only write down some of their observations, i.e. \\(\\hat{Q}\\) is a row-wise subset of \\(Q\\). Then computers and humans will give the same answers for any question in the training set, but outside of that set computers will generally do worse than humans. And so for every question \\(\\bm{q}\\) the computer will do worse in expectation: \\[E[\\ut{(\\bm{q}\\bm{a}-\\bm{q}\\bar{\\bm{a}})^2}{computer error}]\\geq\n  E[\\ut{\\bm{q}(\\bm{a}-\\hat{\\bm{a}})}{human error}].\\]\n\n\n\nIf there are multiple humans then the computer will outperform them both.\n\nSuppose there are two humans who each observe answers to different question, \\(Q_A\\) and \\(Q_B\\), and they both write them all down, so \\(\\bar{Q}=(\\smallmatrix{Q_A\\\\Q_B})\\) and \\(\\bar{\\bm{a}}=(\\smallmatrix{Q_A\\bm{w}\\\\Q_B\\bm{w}})\\). Now the computer clearly has superior information to either human, and so for both \\(i\\in\\{1,2\\}\\) and every question \\(\\bm{q}\\) we can write: \\[E[\\ut{(\\bm{q}\\bm{a}-\\bm{q}\\bar{\\bm{a}})^2}{computer error}]\\leq\n  E[\\ut{\\bm{q}(\\bm{a}-\\hat{\\bm{a}}_i)}{human error}].\\]\n\nIf there are multiple humans then the computer can answer question no human can answer.\n\nSuppose two humans observe the answers to the following questions: \\[\\begin{aligned}\n  Q_A &= \\bmatrix{1 & 1 & 1 \\\\ 1 & -1 & 1} \\\\\n  Q_B &= \\bmatrix{1 & 1 & 1 \\\\ 1 & 1 & -1}\n   \\end{aligned}\\] The first human will learn the exact value of \\(w_2\\), and the second human will learn the exact value of \\(w_3\\), but neither will learn both, and so neither could perfectly predict the answer to this question: \\[\\begin{aligned}\n     \\tilde{q} &= \\bmatrix{1 & -1 & -1} \\\\\n  \\end{aligned}\\]\nHowever if they both recorded their observations, so the computer observes \\(\\bar{\\bm{a}}=(\\smallmatrix{Q_1\\bm{w}\\\\Q_2\\bm{w}})\\), the computer will be able to infer both \\(w_2\\) and \\(w_3\\), and thus will be able to perfectly answer \\(\\tilde{q}\\).\nWe can see this behaviour in LLMs: they sometimes combine a pair of facts or a pair of abilities which no single human has access to, e.g. when an LLM translates between two languages, for which there exists no human speaker of both.\n\nIf humans write outside their expertise then the computer will do worse.\n\nIn the cases above we assumed that the two humans recorded only what they directly observed. This means the computer essentially had a window directly to the world. However the humans could instead have written down their estimated answers to other questions. Suppose they both wrote down answers to every possible question, then the computer would learn the average of the two human’s estimated weights:12 \\[\\bar{\\bm{w}}=\\frac{1}{2}\\hat{\\bm{w}}_A+\\frac{1}{2}\\hat{\\bm{w}}_B.\\]12 We would have to augment the computer’s learning rule to allow for noise in answers - I need to confirm that the weighting will be exactly 1/2.\nHere the computer will do worse than the two humans on the original questions, \\(Q_A\\) and \\(Q_B\\).\nThe implication is that LLMs work so well only because people tend to write about what they know. Put another way, when an LLM answers a question, it will not predict the answer given by the average person, but will predict the answer given by people who are likely to answer that question in the real world, and luckily those people tend to be people who are subject-matter experts.\n\n\n\nIf humans have tacit knowledge, the computer model can outperform humans in creation of new artefacts.\n\nSuppose humans have tacit knowledge of the world, we can model this with two separate sets of beliefs: \\[\\begin{aligned}\n  \\hat{\\bm{w}}^T   &= \\text{tacit knowledge}\\\\\n  \\hat{\\bm{w}}^E &= \\text{explicit knowledge}\\\\\n   \\end{aligned}\\]\nWhen the human encounters a new question \\(\\tilde{\\bm{q}}\\) they will use their tacit knowledge to form an estimate of the answer, \\(\\hat{a}=\\tilde{\\bm{q}}'\\hat{\\bm{w}}^T\\). For simplicity assume tacit knowledge is perfectly accurate \\(\\hat{\\bm{w}}^T=\\bm{w}\\), and explicit knowledge is imperfect.\nThe distinction becomes important when we want to create a new question. Here it’s useful to interpret \\(\\bm{q}\\) not as a question but as an artefact, e.g. text or image, and \\(\\bm{a}=\\bm{q}'\\bm{w}\\) represents some abstract property, e.g. how beautiful or how rhythmic. Suppose we want to chosose \\(\\bm{q}\\in\\{-1,1\\}^n\\) to maximize \\(\\bm{w}\\bm{q}\\). If we had perfect access to our beliefs \\(\\bm{w}^T\\) this would be simple, however if we have access only to imperfect explicit knowledge \\(\\hat{\\bm{w}}^E\\), the artefact which maximizes that function will not generally be the one which maximizes \\(a\\).\nHere the computer model is less constrained. Suppose the computer has observed sufficiently many questions until they have perfectly learned the tacit knowledge, \\(\\bar{\\bm{w}}=\\hat{\\bm{w}}^T\\). Then if computation is free the computer could be used to query every single \\(\\bm{q}\\in\\{-1,1\\}^n\\) to find the best possible artefact."
  },
  {
    "objectID": "posts/2023-09-05-model-of-ai-imitation.html#derivation",
    "href": "posts/2023-09-05-model-of-ai-imitation.html#derivation",
    "title": "An AI Which Imitates Humans Can Beat Humans",
    "section": "Derivation",
    "text": "Derivation\n\\[\\begin{aligned}\n   Q &= \\bmatrix{q_1^1 & \\ldots & q^1_n \\\\ & \\ddots \\\\ q^m_1 & \\ldots & q^m_n}\n      && \\text{(set of questions)} \\\\\n   w'  &= \\bmatrix{w_1 \\ldots w_n} \\\\\n   a    &= \\bmatrix{a^1 \\\\ \\vdots \\\\ a^m}\n      = \\bmatrix{q_1^1 w_1 + \\ldots q_n^1w_n \\\\ \\vdots \\\\ q_1^m w_1 + \\ldots q_n^mw_n} \\\\\n\\end{aligned}\\]\n\\[\\begin{aligned}\n      \\bm{w} &\\sim N(0,\\Sigma)\n         && (n\\times 1\\text{ vector of parameters of the world)}\\\\\n      Q      &\\in \\{-1,1\\}^{n\\times m}\n         && \\text{($m$ questions, each has $n$ binary parameters)}\\\\\n      \\ut{\\bm{a}}{$m\\times1$}   &= \\ut{Q}{$m\\times n$}\\ut{\\bm{w}}{$n\\times1$}\n         && \\text{(answers provided by the world)}\\\\\n      \\hat{\\bm{w}} &= E[\\bm{w}|Q,\\bm{a}]\n            && \\text{(human beliefs about the world)}\\\\\n         &= \\ut{\\Sigma Q'}{$Cov(\\bm{w},\\bm{a})$}\n            (\\ut{Q\\Sigma Q'}{$Var(\\bm{a})$})^{-1}\n            \\bm{a}\n   \\end{aligned}\\]\nWith one observation and two weights. Suppose \\(m=1, n=1\\), then we have: \\[\\begin{aligned}\n      Q  &= \\bmatrix{q_1 & q_2} \\\\\n      \\bm{a}'  &= \\bmatrix{a} \\\\\n      \\bm{w}'  &= \\bmatrix{w_1 & w_2 } \\\\\n      \\Sigma &= \\bmatrix{\\sigma_1^2 & \\rho \\\\ \\rho & \\sigma_2^2}\\\\\n      \\Sigma Q' &= \\bmatrix{ \\sigma_1^2q_1 + \\rho q_2 \\\\ \\rho q_1 + \\sigma_2^2 q_2 } \\\\\n      Q\\Sigma Q' &= \\bmatrix{ \\sigma_1^2q_1^2 + 2\\rho q_1q_2 + \\sigma_2^2 q_2^2} \\\\\n      \\hat{\\bm{w}}=\\Sigma Q'(Q\\Sigma Q')^{-1}\\bm{a}\n         &= \\bmatrix{ \\frac{\\sigma_1^2q_1 + \\rho q_2}{\\sigma_1^2q_1^2 + 2\\rho q_1q_2 + \\sigma_2^2 q_2^2} \\\\\n                  \\frac{\\rho q_1 + \\sigma_2^2 q_2}{\\sigma_1^2q_1^2 + 2\\rho q_1q_2 + \\sigma_2^2 q_2^2}} a\n   \\end{aligned}\n   \\]\nWe can normalize \\(q_1=q_2=1\\), then we have \\[\\hat{w}_1 = \\frac{\\sigma_1^2+\\rho}{\\sigma_1^2+2\\rho+\\sigma_2^2}a,\\]\nThis means we are dividing up responsibility for the answer (\\(a\\)) into the contributions of each component, nice and simple.\nWith two observations and one weight. Here we’re over-identified. \\[\\begin{aligned}\n      Q  &= \\bmatrix{q^1 \\\\ q^2} \\\\\n      \\bm{a}  &= \\bmatrix{a^1 \\\\ a^2} \\\\\n      \\bm{w}  &= \\bmatrix{w } \\\\\n      \\Sigma &= \\bmatrix{\\sigma^2 }\\\\\n      \\Sigma Q' &= \\bmatrix{ \\sigma^2 q^1 & \\sigma^2 q^2 } \\\\\n      Q\\Sigma Q' &= \\bmatrix{ \\sigma^2 q^1q^1 & \\sigma^2q^1q^2 \\\\ \\sigma^2q^1q^2 & \\sigma^2q^2q^2}\n         && \\text{(this matrix doesn't have an inverse)}\n   \\end{aligned}\n   \\]\nWith noise. Suppose we only observe the answers with random noise, then we get this: \\[\\begin{aligned}\n   \\ut{\\bm{a}}{$m\\times1$}   &= \\ut{Q}{$m\\times n$}\\ut{\\bm{w}}{$n\\times1$}\n      + \\ut{\\bm{e}}{$n\\times 1$} \\\\\n   \\bm{e} &\\sim N(\\bm{0},s^2I_m) && \\text{(i.i.d. noise w variance $s^2$)}\\\\\n   Cov(\\bm{w},\\bm{a})   &= \\Sigma Q' \\\\\n   Var(\\bm{a}) &= Q\\Sigma Q' + s^2I_m \\\\\n   E[\\bm{w}|Q,\\bm{q}]   &= \\Sigma Q'(Q\\Sigma Q' + s^2I_m)^{-1}\\bm{a}\n\\end{aligned}\\]\nCompare to Bayesian linear regression. We can compare this result to Bayesian linear regression (e.g. Wikipedia): \\[\\begin{aligned}\n      \\bar{\\beta}  &= \\Sigma Q'(Q\\Sigma Q' + s^2I_m)^{-1}\\bm{a}\n         && \\text{(our result)} \\\\\n      \\tilde{\\beta} &= (Q'Q+s^{2}\\Sigma^{-1})^{-1}Q'\\bm{a}\n         && \\text{(Bayesian linear regression)}\\\\\n   \\end{aligned}\\]\nI believe that these can be shown to be equivalent by the matrix inversion lemma, though I haven’t confirmed this. There’s a note online that appears to show equivalence."
  },
  {
    "objectID": "posts/2020-04-05-front-loading-restrictions.html",
    "href": "posts/2020-04-05-front-loading-restrictions.html",
    "title": "Optimal Coronavirus Policy Should be Front-Loaded",
    "section": "",
    "text": "Q: How should you sequence policies over time? E.g. suppose you want to manage the epidemic until a vaccine arrives and you have policies (lockdowns, distancing, masks) each of which is associated with a certain effect on the growth-rate of cases, but each also has some fixed social cost per day. How should you apply the policies over time?\nA: The severity of the policies should be gradually decreasing, i.e. they should gradually become less severe, as you approach the availability of a vaccine. There should not be zig-zagging between policies in this setup.\nAny justification for zig-zagging must come from some additional consideration like (a) non-separabilities in the costs, e.g. psychological/economic need for occasional respite, (b) uncertainty about the end-date, (c) uncertainty about the effect of the policies, such that there is informational-value from varying policies, or (d) desire to maintain a steady flow of cases, in order to reach herd immunity (the “mitigation” strategy)."
  },
  {
    "objectID": "posts/2020-04-05-front-loading-restrictions.html#corollary-you-should-never-expect-policy-to-get-stricter",
    "href": "posts/2020-04-05-front-loading-restrictions.html#corollary-you-should-never-expect-policy-to-get-stricter",
    "title": "Optimal Coronavirus Policy Should be Front-Loaded",
    "section": "Corollary: you should never expect policy to get stricter",
    "text": "Corollary: you should never expect policy to get stricter\nYou should never find yourself in the situation where you expect policy to get stricter in the future. If you anticipate that a stricter policy will be appropriate next week then that strict policy is appropriate this week!\nCountries in early stages of the epidemic should be doing as much or more as countries in later stages."
  },
  {
    "objectID": "posts/2020-04-05-front-loading-restrictions.html#intuition",
    "href": "posts/2020-04-05-front-loading-restrictions.html#intuition",
    "title": "Optimal Coronavirus Policy Should be Front-Loaded",
    "section": "Intuition",
    "text": "Intuition\nSuppose that there’s some tradeoff across policiers between the growth-rate and the social cost.\nThen given any fixed time-path of policies: e.g., (A,A,B,C), if it is not monotonically decreasing in severity from high-cost to low-cost, then you can do strictly better by rearranging the path of policies to be monotonically decreasing. The social cost will be identical, because the set of policies will be the same, but the number of cases will be lower at every point in time, since at any given point the cumulative growth rates, up to that point, will be lower. Thus the final cumulative number of cases will be lower."
  },
  {
    "objectID": "posts/2020-04-05-front-loading-restrictions.html#additional-reason-to-front-load-extinction",
    "href": "posts/2020-04-05-front-loading-restrictions.html#additional-reason-to-front-load-extinction",
    "title": "Optimal Coronavirus Policy Should be Front-Loaded",
    "section": "Additional Reason to Front-Load: Extinction",
    "text": "Additional Reason to Front-Load: Extinction\nAll of this is treating the number of cases as a continuous variable which means you can never completely extinguish the disease. However if that’s a possibility that’s within sight (e.g. as in NZ), then that’s a significantly stronger case for starting with very severe policies, to try to kill the disease entirely, and then you can go back to the garden of Eden."
  },
  {
    "objectID": "posts/2020-04-05-front-loading-restrictions.html#prior-discussion",
    "href": "posts/2020-04-05-front-loading-restrictions.html#prior-discussion",
    "title": "Optimal Coronavirus Policy Should be Front-Loaded",
    "section": "Prior Discussion",
    "text": "Prior Discussion\nThere’s been some discussion of zig-zagging by the Imperial group (paper) and by Timothy Gowers (twitter & post)\nGowers says the optimal policy is very short zig-zags (changing policy every other day), however I think this is misleading. It comes from fixing the lower-threshold and optimizing the upper-threshold. If instead you fixed the upper-threshold and optimized the lower-threshold, then the optimal cycle-length will be long.\nIf you choose both the upper and lower threshold (both T and S) then he notes that they’ll both be arbitarily low. However this ignores the cost of getting to zero given current cases.\nInstead a well-defined problem is to choose an optimal time-path of policy given some start-point and end-point. In that case it’ll be a path of gradually decreasing strictness (without zig-zags).\nYou can see the intuition in the diagram below: the total infections is approximately the area under the zig-zag (not quite: because the y-axis is ln(cases), but this won’t matter for the argument). Thus you can reduce the area under the line by lowering the upper threshold. However if you instead take the upper threshold as fixed, then it’s optimal to choose a lower threshold that is as low as possible, i.e. you want long cycles, not short cycles.\n\n\n\nabc"
  },
  {
    "objectID": "posts/2017-02-25-samuelson-expected-utility.html",
    "href": "posts/2017-02-25-samuelson-expected-utility.html",
    "title": "Samuelson & Expected Utility",
    "section": "",
    "text": "clown\n\n\n\n“If in every event which can possibly occur the consequence of action I is not preferred to that of action II, and if in some possible event the consequence of II is preferred to that of I, then any sane preferer would prefer II to I.”\n\nThis sentence, in a letter from Savage in 1950, finally persuaded Samuelson that rational choices must obey expected utility. He had been skeptical – thinking that expected utility was just a simple approximation, like exponential discounting or like separability. Marschack and Savage and Friedman all wrote letters trying to persuade him, and though they were right, they kept using bad arguments, and Samuelson disposed of them.\nSavage and Friedman wrote a paper saying that, because people buy both insurance and lottery tickets, expected utility implies that the utility-of-money must be concave then convex. Samuelson saw that this was ridiculous, & said “there’s as much to be learned about gambling from Dostoyevsky as from Pascal.”\nBut the sentence from the letter above finally persuaded Samuelson.\nThis is all from Ivan Moscati’s “How Economists Came to Accept Expected Utility Theory”."
  },
  {
    "objectID": "posts/2017-02-25-economist-explorers.html",
    "href": "posts/2017-02-25-economist-explorers.html",
    "title": "Economist Explorers",
    "section": "",
    "text": "explorers\n\n\nImagine a group of adventurers set out to explore a new continent, each one choosing a different valley to map. And suppose that, instead of sending back reports of what they found (“a forest, a swamp, mosquitoes”), each one wrote reports calculated to appear as exciting as possible (“a forest, abundant water, rich fauna, couldn’t disconfirm rumors of a city of gold”).\nThis is how I feel when I’m refereeing economics papers: everyone’s trying to tell an exciting story, and it takes a great deal of work to figure out what actual novel facts they have discovered. It’s frustrating because it’s such an inefficient way to explore the territory: so many people have spent so much time on this, and we have so little to show. What have we discovered about decision-making in the last 50 years after proposing thousands of different models, running tens of thousands of experiments, and regressing millions of variables? Couldn’t we have accumulated more knowledge if we’d organized things differently?\n(Although I have to admit that many of my own papers do include some speculation about cities of gold. But I think the most useful thing a referee can do is to suggest changes to the title and abstract, to make it more transparent exactly what the paper has found.)\nMy original post on Facebook\n–\nFollowup: what have we discovered about decision-making in the last 50 years?\nOnce, when I taught a graduate class in behavioural economics, a couple of students came from civil engineering & dentistry, hoping to learn something useful that they could use in modelling decision-making - e.g. in modelling how people make decisions about commuting. I was embarrassed in how little I could help them. I was able to think of a lot of useful stuff about decision making that’s ~50 years or older: utility & expected utility, exponential (& hyperbolic) discounting, Engel curves, responses to permanent & temporary income, tractable demand estimation, the offsetting income & substitution effects of wages. All of this is immediately useful in quantifying decision-making. But I can think of few recent examples of quantitatively useful findings. Special mention to kahneman & tversky. If you ask, say, about attitudes to uncertainty, to time, to temptation, to intertemporal complementarities, I would begin my answer with “there are various schools of thought…”."
  },
  {
    "objectID": "posts/2017-04-15-the-mechanical-and-the-rational.html",
    "href": "posts/2017-04-15-the-mechanical-and-the-rational.html",
    "title": "The Repeated Failure of Laws of Behaviour",
    "section": "",
    "text": "krazy kat"
  },
  {
    "objectID": "posts/2017-04-15-the-mechanical-and-the-rational.html#footnotes",
    "href": "posts/2017-04-15-the-mechanical-and-the-rational.html#footnotes",
    "title": "The Repeated Failure of Laws of Behaviour",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nAlso Seligman (1970) On the Generality of Laws of Learning, “That all events are equally associable and obey common laws is a central assumption of general process learning theory … A review of data from the traditional learning paradigms shows that the assumption of equivalent associability is false … it is speculated that the laws of learning themselves may vary with the preparedness of the organism for the associa- tion and that different physiological and cognitive mechanisms may covary with the dimension.”↩︎"
  },
  {
    "objectID": "posts/2017-02-25-weber-fechner-law.html",
    "href": "posts/2017-02-25-weber-fechner-law.html",
    "title": "Weber’s Law Doesn’t Imply Concave Representations or Concave Judgments",
    "section": "",
    "text": "runningman\nNutshell."
  },
  {
    "objectID": "posts/2017-02-25-weber-fechner-law.html#linear-representation-multiplicative-noise",
    "href": "posts/2017-02-25-weber-fechner-law.html#linear-representation-multiplicative-noise",
    "title": "Weber’s Law Doesn’t Imply Concave Representations or Concave Judgments",
    "section": "Linear Representation & Multiplicative Noise",
    "text": "Linear Representation & Multiplicative Noise\nAssume people get signals about underlying value with multiplicative noise, \\(s=v\\cdot e\\), with \\(e\\) lognormal. For conciseness let \\(\\delta=JND(v_{1},p)\\), then \\(\\delta\\) can be implicitly defined as:\n\\[\n\\begin{aligned}\np   =&  P(E[v_{1}+\\delta|s_{2}]&gt;E[v_{1}|s_{1}]) \\\\\n    =&  P((v_{1}+\\delta)e_{2}&gt;v_{1}e_{1}) \\\\\n    =&  P(\\ln(v_{1}+\\delta)+\\ln e_{2}&gt;\\ln v_{1}+\\ln e_{1}) \\\\\n    =&  \\Phi\\left(\\frac{\\ln(v_{1}+\\delta)-\\ln v_{1}}{\\sigma_{e}^{2}+\\sigma_{e}^{2}}\\right)\n\\end{aligned}\n\\]\nwhere \\(\\Phi\\) is the CDF of a standard normal distribution. Then,\n\\[\n\\begin{aligned}\n\\ln(v_{1}+\\delta)-\\ln v_{1} =&   2\\sigma_{e}^{2}\\Phi^{-1}(p) \\\\\n\\frac{v_{1}+\\delta}{v_{1}}  =&   \\exp(2\\sigma_{e}^{2}\\Phi^{-1}(p))\\\\\nJND(v_{1},p)=\\delta         =&   v_{1}\\left[\\exp(2\\sigma_{e}^{2}\\Phi^{-1}(p))-1\\right]\n\\end{aligned}\n\\]\nIn other words, the just noticeable difference is proportional to the value, \\(v_{1}\\), as found by Weber."
  },
  {
    "objectID": "posts/2017-02-25-weber-fechner-law.html#a-concave-representation-additive-noise",
    "href": "posts/2017-02-25-weber-fechner-law.html#a-concave-representation-additive-noise",
    "title": "Weber’s Law Doesn’t Imply Concave Representations or Concave Judgments",
    "section": "A Concave Representation & Additive Noise",
    "text": "A Concave Representation & Additive Noise\nSuppose that the decision-maker receives a concave signal of value with additive noise, i.e. \\(s=\\ln v+e\\), with Gaussian \\(e\\). Then the derivation is very similar:\n\\[\n\\begin{aligned}\n  p =& P(E[v_{1}|s_{2}]&gt;E[v_{1}|s_{1}]) \\\\\n    =& P(\\ln(v_{1}+\\delta)+e_{2}&gt;\\ln v_{1}+e_{1}).\n\\end{aligned}\n\\]\nThe rest of the derivation is the same: i.e., the JND in the neighborhood of \\(v_{1}\\) will be proportional to \\(v_{1}\\)."
  },
  {
    "objectID": "posts/2017-02-25-weber-fechner-law.html#multiplicative-noise-posteriors-are-concave-in-v",
    "href": "posts/2017-02-25-weber-fechner-law.html#multiplicative-noise-posteriors-are-concave-in-v",
    "title": "Weber’s Law Doesn’t Imply Concave Representations or Concave Judgments",
    "section": "Multiplicative Noise => Posteriors are Concave in \\(v\\)",
    "text": "Multiplicative Noise =&gt; Posteriors are Concave in \\(v\\)\nSuppose we have lognormal priors for both \\(v\\) and \\(e\\):\n\\[\n\\begin{eqnarray*}\n\\ln v & \\sim & N(\\mu_{v},\\sigma_{v}^{2})\\\\\n\\ln e & \\sim & N(\\mu_{e},\\sigma_{e}^{2}),\n\\end{eqnarray*}\n\\]\nand \\(s=v\\cdot e\\), then we will have posteriors like:\n\\[\n\\begin{eqnarray*}\nf(\\ln v|s) & \\sim & N(\\frac{\\sigma_{v}^{2}}{\\sigma_{e}^{2}+\\sigma_{v}^{2}}\\ln s,\\left(\\sigma_{v}^{-2}+\\sigma_{e}^{-2}\\right)^{-1})\\\\\nE[v|s] & = & \\exp\\left(\\frac{\\sigma_{v}^{2}}{\\sigma_{e}^{2}+\\sigma_{v}^{2}}\\ln s+\\frac{1}{2}\\left(\\sigma_{v}^{-2}+\\sigma_{e}^{-2}\\right)^{-1}\\right)\\\\\n& = & s^{\\frac{\\sigma_{v}^{2}}{\\sigma_{e}^{2}+\\sigma_{v}^{2}}}e^{\\frac{1}{2}(\\sigma_{v}^{-2}+\\sigma_{e}^{-2})^{-1}}\n\\end{eqnarray*}\n\\]\nThis means that the expected \\(v\\) is concave in the signal \\(s\\) (because the exponent is less than one). Intuitively: a doubling of the value, which causes a doubling of the stimulus, will cause a less than doubling of the expected value conditional on that stimulus, because it will cause us to revise upwards our beliefs about both \\(v\\) and \\(e\\).\nFinally, we are also interested in the average posterior for a given \\(v\\). This will also be concave (abbreviating \\(\\alpha=\\frac{\\sigma_{v}^{2}}{\\sigma_{e}^{2}+\\sigma_{v}^{2}}\\), and dropping the constant coefficient in \\(E[v|s]\\)):\n\\[\n\\begin{eqnarray*}\nE[E[v|s]|v] & = & \\int(v\\cdot e)^{\\alpha}f(e)de\\\\\n& = & v^{\\alpha}\\int e^{\\alpha}f(e)de.\n\\end{eqnarray*}\n\\]"
  },
  {
    "objectID": "posts/2017-02-25-weber-fechner-law.html#additive-noise-posteriors-are-linear-in-v",
    "href": "posts/2017-02-25-weber-fechner-law.html#additive-noise-posteriors-are-linear-in-v",
    "title": "Weber’s Law Doesn’t Imply Concave Representations or Concave Judgments",
    "section": "Additive Noise => Posteriors are Linear in \\(v\\)",
    "text": "Additive Noise =&gt; Posteriors are Linear in \\(v\\)\nSuppose again that the decision-maker receives a logarithmic signal with additive noise: \\(s=\\ln v+u\\), and let \\(u\\) be Gaussian. (I changed notation from \\(e\\) to \\(u\\) because I use a lot of exponential functions in the derivation.) Now assume that, in addition, \\(v\\) is drawn from an improper uniform \\((0,\\infty)\\). Consider the expected value of \\(v\\) given the signal \\(s\\) (I drop the constant term from the Gaussian distribution for conciseness):\n\\[\n\\begin{eqnarray*}\nE[v|s] & = & \\frac{\\int_{0}^{\\infty}ve^{-\\left(s-\\ln v\\right)^{2}}dv}{\\int_{0}^{\\infty}e^{-\\left(s-\\ln v\\right)^{2}}dv}.\n\\end{eqnarray*}\n\\]\nNow exchange variables, so that \\(v=e^{z}\\):\n\\[\n\\begin{eqnarray*}\nE[v|s] & = & \\frac{\\int_{-\\infty}^{\\infty}e^{z}e^{-(s-z)^{2}}e^{z}dz}{\\int_{-\\infty}^{\\infty}e^{-(s-z)^{2}}e^{z}dz}\\\\\n& = & \\frac{\\int_{-\\infty}^{\\infty}e^{-s^{2}+2(1+s)z-z^{2}}dz}{\\int_{-\\infty}^{\\infty}e^{z-(s-z)^{2}}dz}\\\\\n& = & \\frac{\\int_{-\\infty}^{\\infty}e^{-(z-1-s)^{2}}e^{1+2s}dz}{\\int_{-\\infty}^{\\infty}e^{s+\\frac{1}{4}}e^{-((s+\\frac{1}{2})-z)^{2}}dz}\\\\\n& = & e^{s}e^{3/4}\\frac{\\int_{-\\infty}^{\\infty}e^{-(z-1-s)^{2}}dz}{\\int_{-\\infty}^{\\infty}e^{-((s+\\frac{1}{2})-z)^{2}}dz}\n\\end{eqnarray*}\n\\]\nNote that both of the integrals are independent of \\(s\\) (because the integration is between \\(-\\infty\\) and \\(\\infty\\)), so there exists some \\(\\kappa\\) such that:\n\\[\nE[x|s]=\\text{e}^{s}\\kappa.\n\\]\nFinally we are interested in the average posterior for a given \\(v\\) (here I’m again ignoring all constant terms):\n\\[\n\\begin{eqnarray*}\nE[E[v|s]|v] & = & \\int_{-\\infty}^{\\infty}E[v|s=\\ln v+u]\\text{e}^{-u^{2}}du\\\\\n& = & \\int_{-\\infty}^{\\infty}\\text{e}^{(\\ln v+u)}\\kappa\\text{e}^{-u^{2}}du\\\\\n& = & v\\int_{-\\infty}^{\\infty}\\kappa\\text{e}^{u-u^{2}}du.\n\\end{eqnarray*}\n\\]\nI.e., despite the logarithmic internal representation, the average posterior is linear in the value."
  },
  {
    "objectID": "posts/2023-07-27-meta-2020-elections-experiments.html",
    "href": "posts/2023-07-27-meta-2020-elections-experiments.html",
    "title": "How Much has Social Media affected Polarization?",
    "section": "",
    "text": "TL;DR: The experiments run by Meta during the 2020 elections were not big enough to test the theory that social media has made a substantial contribution to polarization in the US. Nevertheless there are other reasons to doubt it."
  },
  {
    "objectID": "posts/2023-07-27-meta-2020-elections-experiments.html#discussion",
    "href": "posts/2023-07-27-meta-2020-elections-experiments.html#discussion",
    "title": "How Much has Social Media affected Polarization?",
    "section": "Discussion",
    "text": "Discussion\nTrends in affective polarization. Boxell et al. (2022) document affective polarization across a dozen countries, 1978-2020:\n\n\nIn the US affective polarization index increased from around 25 to 50, “an increase of 1.08 standard deviations as measured in the 1978 distribution.” (I’m not sure if the SD increased).\nAcross the world there’s no clear trend: some countries increased, other countries decreased. This weakens the simple argument that polarization has increased at the same time as social media use.\nIn the US the trend seems to be almost entirely due to increasing negative feelings about the opposing party:\n\n\nThe US timeseries can be seen online from the ANES.\nObservational data finds that much of the growth in polarization in the US was among people who were not online. Boxell et al. (2017) say\n\n“the growth in polarization in recent years [1996-2012] is largest for the demographic groups least likely to use the internet and social media”\n\nContent on Meta platforms. Guess et al. (2023b) has data from the control group in their 2020 experiments:\n\n\n\nShare of Impressions\nFacebook\nInstagram\n\n\n\n\nPolitical content\n14%\n5%\n\n\nPolitical news content\n6%\n-\n\n\nContent from untrustworthy sources\n3%\n1%\n\n\nUncivil content\n3%\n2%\n\n\n\nPew 2022 has data on where people get their news from:\n\n\n\n\npct adults regularly get news from\n\n\n\n\ntelevision\n65%\n\n\nnews websites\n63%\n\n\nsearch\n60%\n\n\nsocial media\n50%\n\n\nradio\n47%\n\n\nprint\n33%\n\n\npodcasts\n23%\n\n\n\nRadio show popularity. Around half of the top 20 most-listened radio shows in the US are conservative talk, with around 90M weekly listeners (this is double-counting overlapping users). Data from 2021.\nTelevision. Fox News is Cable TV’s most-watched network with around 5M regular viewers. (source from 2016).\nTime spent on social media. Statista: Average time-spent 150 minutes/day/person on social networks\nThe academic literature has identified other possible causes of polarization. Some potential causes: southern realignment, 1968 changes to the primary system, the Obama presidency, the tea party movement (though each of these could be in part proximal causes). Martin & Yurcoglu (2017) argue that a large part of recent growth is due to cable news: &gt; “the cable news channels can explain an increase in political polarization of similar size to that observed in the US population over [2000-2008]. … In absolute terms, however, this increase is fairly small.”\nSee also Haidt and Bail’s long document Social Media and Political Dysfunction: A Collaborative Review\nDoes Allcott et al. (2020) find that Facebook use increases polarization? This paper reports on an experiment paying people to stop using Facebook for a month. They find an effect of -0.16 SDs (\\(\\pm\\) 0.08) on a measure they describe as “political polarization,” however there are some subtleties:\n\n\nUnlike the questions used in typical population surveys the questions were explicitly about their feelings during the period of the experiment, e.g. “Thinking back over the last 4 weeks, how warm or cold did you feel towards the parties and the president on the feeling thermometer?”\nPolarization is measured by a composite of different measures. By far the largest effect was on the “congenial news exposure” question: “over the last 4 weeks how often did you see news that made you better understand the point of view of the Democrat (Republican) party?” The score was the difference between the answer for their own party vs the other-side party. It seems to me that it’s not surprising that deactivating Facebook would affect one’s exposure to such news, but that this wouldn’t normally be called a measure of “polarization” in the literature. The paper mentions in a footnote that “the effect on the political polarization index is robust to excluding each of the seven individual component variables,” but it turns out that removing “congenial news exposure” halves the effect-size and shifts the p-value from 0.00 to 0.09 (i.e. from very significant to non-significant). I’m not sure I would describe this as a finding that is “robust”.\nThe paper finds no significant effect on their two “affective polarization” measures (-0.08 \\(\\pm\\) 0.08 SD, and 0 \\(\\pm\\) 0.04 SD), however the 2020 papers which cite Allcott et al. (2020) seem to treat it as finding that Facebook has a positive effect on “polarization” without noting that it has a null effect on affective polarization."
  },
  {
    "objectID": "posts/2023-08-02-small-effects.html",
    "href": "posts/2023-08-02-small-effects.html",
    "title": "The Paradox of Small Effects",
    "section": "",
    "text": "In summary:\n\nAttitudes are hard to change. Many fields in social science have adopted a doctrine of “small effects”: high quality studies tend to show that peoples’ attitudes are not very sensitive to exposure to media, or to their peers’ attitudes.\nYet attitudes do change. We see very wide society-level variation in attitudes, which are hard to explain without peer or media effects.\nResolution of the paradox: each effect is small, but there are a lot of them.\n\n(see an earlier Facebook post)\n\n(1) Attitudes are Hard to Change\nMany fields in social science tend to say that attitudes show little influence from either peer effects or from media exposure:\n\nAngrist (2014) says studies of peer effects “have mostly uncovered little in the way of socially significant causal effects.”\nPolitical scientists talk about “the paradox of minimal effects”, Ansolabehere (2006) says that election campaigns “seem to be inessential to understanding who wins and who loses.”\nDavid Stromberg says “the lesson from the last 50 years of media research is that it is very hard to manipulate voters … evidence of [supply side bias] effects is weak or non-existent”\n\nThere are many studies which find large effects but they tend to be treated with extreme skepticism by the methodologists: they are overwhelmingly from lab experiments or observational data and so can be very biased.\n\n\n(2) Attitudes do Change\nAttitudes vary a huge amount across time and space:\n\nVariation in political and religious attitudes.\nVariation in attitudes towards other races, sexes, sexualities, religions.\nVariation in preferences over food, e.g. for rice vs wheat vs corn.\nVariation in preferences over how many children to have.\n\nIt is hard to explain this variation with individual economic circumstances: when someone migrates to another country they face different economic circumstances (different prices and income) but they typically maintain their attitudes for decades.\nIt is hard to explain this variation with genetic variation, because attitudes vary so much over time, while genes move very slowly.\nSo it seems like peer and media effects must be substantial proximal determinants of attitudes.\n\n\n(3) Resolution: Each Effect is Small, but There are Many\nHow can we resolve small treatment effects with big variation in outcomes? It makes sense if we’ve only been testing very small treatments. Each individual effect is small but there are millions of them, so collectively the effects are large.\nPeer effect studies tend to find small effects when looking at random assignment of peers, e.g. random assignment of roommates, but this may be because time with your roommate constitutes only a very small share of your overall exposure to other people and ideas.1 Collectively that exposure must be hugely important in your attitudes.1 Kremer and Levy (2008) say “Most studies do not find effects of these predetermined characteristics on the whole sample of students … conventional peer effects on academic achievement … are not estimated to be particularly important.”\nMedia studies tend to find small effects from exposure to social media or to television, but in most cases the media exposure is only a single-digit percentage-point share of their lifetime exposure to media. So the aggregate effect can be far larger than that measured in any credible experiment or natural experiment (in addition, much of the effect likely propagates through peer effects).\nAn individual campaign advertisement might have very small effects on voting intention, but an individual campaign advertisement is only a tiny tiny share of your lifetime exposure to political communication. Small individual effects are consistent with peoples’ political attitudes being overwhelmingly determined by exposure and persuasion.\nMore technically: we can reconcile small effects with big variations if:\n\nEffects have a long half-life, e.g. exposure in childhood can affect your attitudes as an adult.\nPeer effects are propagated through many weak links, instead of a few strong links: i.e. there are substantial influences from all of society in addition to your family.\nPersuasion works even with indirect channels, e.g. your political views aren’t just affected by campaign ads, but also by the implicit attitudes to politics reflected in all the media you’re exposed to\nAttitudes are sensitive to the average rather than the total amount of persuasive material you’re exposed to, thus marginal effects can be small while total effects are large.\n\n(As a footnote: from my time in social media companies I learned that individual peer effects are tiny, yet we also know that social media demand is entirely peer effects, i.e. people only use Facebook because other people use Facebook.)\n\n\nOther Notes\nThe paradox of large effects. Tosh et al. (2021) discuss an opposite problem: in some fields there are many claims of large effects, but it is not possible to reconcile the aggregate variance in the data with so many large effects. E.g. they discuss a paper claiming to show that exposure to age-related words tends to lower a subject’s subsequent walking speed by 13%. If people are exposed to many such primes, and they are uncorrelated, then we should expect huge and implausible variation in peoples’ day-to-day walking speed.\nTheir problem is somewhat the opposite: they are talking about a literature which has many non-credible effects from lab experiments or observational data. Instead I’m talking about literature which has credible but small effects.\n\n\n\n\n\n\n\n\n\nReferences\n\nAngrist, Joshua D. 2014. “The Perils of Peer Effects.” Labour Economics 30: 98–108. https://doi.org/https://doi.org/10.1016/j.labeco.2014.05.008.\n\n\nAnsolabehere, Stephen. 2006. “The Paradox of Minimal Effects.” Capturing Campaign Effects, 29–44.\n\n\nTosh, Christopher, Philip Greengard, Ben Goodrich, Andrew Gelman, Aki Vehtari, and Daniel Hsu. 2021. “The Piranha Problem: Large Effects Swimming in a Small Pond.” arXiv Preprint arXiv:2105.13445."
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html",
    "href": "posts/2017-12-10-unconscious-influences.html",
    "title": "On Unconscious Influences (Part 1)",
    "section": "",
    "text": "Over a couple of years I spent a lot of time in offices looking out the window, thinking about decision-making & the unconscious, scribbling little bits & pieces in a notebook.\n\n\n\nNBER\n\n\nI ended up writing two papers - “Hierarchical Aggregation of Information and Decision-Making” by myself and “Implicit Preferences Inferred from Choice” with Jon de Quidt. The papers are fairly technical, and this post is going to be a layperson’s guide to the background, what’s known about unconscious knowledge, and a tiny bit about the ideas in those papers.\nHere is the argument in a nutshell:\n\nThere are plenty of reasons to think that unconscious influences are strong – in other words, that people have limited insight into what factors influence their decisions.\nThe idea of unconscious influences has been in and out of the mainstream of psychology for the last 200 years, but always hounded by arguments over what it means, i.e. over what evidence would be sufficient to show that a decision was influenced by an unconscious factor. The battle has had many reversals: a new types of evidence has been proposed which is thought to reveal unconscious influences, and then later the technique or interpretation is shown to have substantial flaws and the line of inquiry fizzles out. A couple of decades pass and a different approach becomes popular.\nTwo broad classes of evidence are the following: (A) people reveal their unconscious preoccupations in their involuntary responses – in how their pupils dilate, how quickly they respond to a stimulus, in their word associations, dreams, slips of the tongue; (B) people reveal unconscious influences in discrepancies between how they act and how they explain their behaviour. Both sources of evidence have got tangled in debates about interpretation, and there are substantial camps on either side with not much agreement on what constitutes sufficient evidence for unconscious influences.\nA third type of evidence is less common but, I think, more powerful: evidence from inconsistencies in decision-making. The idea being that unconscious factors are by their nature isolated from conscious factors, i.e. they don’t interact with conscious beliefs and desires, and this isolation will cause certain characteristic inconsistencies among decisions.\nThis can be made precise with an analogy: the relationship between the conscious and unconscious brain is like the relationship between a blind man and his guide dog. The blind man makes decisions based, in part, on which direction the guide dog is pulling towards, so the guide dog’s beliefs and desires influence the man’s decisions, but without the man knowing exactly what those beliefs and desires are, and so he couldn’t tell you how much any particular factor contributed to his decision. Testing for unconscious influences in behaviour is just testing the degree to which your brain is being led by a guide dog.\nThe internal-consistency definition of unconscious influences implies two ways of looking for them: (1) testing whether people can accurately answer hypothetical questions about decisions they would make if factors changed - i.e. navigating without your guide dog; and (2) testing whether people make consistent judgments when judging two outcomes at a time.\nFirst, hypothetical questions. We can ask people, how would your judgment change if this factor changed? Would you still like this painting if the name of the artist was different? Would this drawing look more like your cousin if the nostrils were bigger? Unconscious influences imply that people will not be able to give accurate answers to these hypothetical questions because if the description of the situation is abstract then their unconscious brain won’t be able to evaluate it (AKA, they don’t know which direction they would go in without knowing what their guide dog will say).\nThe second way of testing for unconscious influences is what my paper with Jon is about: unconscious influences particularly leave their mark in comparisons, where you evaluate two outcomes simultaneously or consecutively, or when you choose between two outcomes. When confronted with two outcomes you surface two unconscious judgments and that gives you some insight into what is affecting those judgments, which in turn will affect your conscious decision.\nSuppose you had an unconscious preference for men over women, but a conscious preference to be indifferent, this will manifest in the following: (A) when you see two CVs which are identical, except that one is a man and one is a woman, then you’re indifferent between them; (B) when you see two CVs which differ in some other respect (e.g. one has a PhD, the other has an MBA), then you consistently have a preference for the CV belonging to the man. Your guide dog has a bias towards men, which you’re not aware of: the bias will only sway your decision in the second case because, in the second case, when your guide dog pulls you towards the man with a PhD, you cannot figure out how much of that pull is due to his being a man, and how much is due to his PhD.\nIn the end I think that our brains are full of guide dogs all pulling in different directions. If we had the stomach for it we could plot out our decisions all on a map – measure how each factor influences our judgment – and we would be able to see both the surface influences and the deeper latent influences.\n\n\n\n\nMotivating examples\nSome definitions & theory\nWays of measuring implicit preferences\nThe proposal\n\n\n\n\nLittauer"
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html#contents",
    "href": "posts/2017-12-10-unconscious-influences.html#contents",
    "title": "On Unconscious Influences (Part 1)",
    "section": "",
    "text": "Motivating examples\nSome definitions & theory\nWays of measuring implicit preferences\nThe proposal\n\n\n\n\nLittauer"
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html#involuntary-responses",
    "href": "posts/2017-12-10-unconscious-influences.html#involuntary-responses",
    "title": "On Unconscious Influences (Part 1)",
    "section": "Involuntary responses",
    "text": "Involuntary responses\nFreud is the most famous theorist of extracting unconscious factors from involuntary responses – he wrote three books on different methods: one on dreams, one on jokes, one on mistakes (mis-reading, mis-hearing, mis-speaking). An example from the last book: “A woman who is very anxious to get children always reads ‘storks’ instead of ‘stocks’.” Most of Freud’s examples of unconscious influences are much more complex than this one, and more often the hidden factor influencing behaviour is something unpleasant or shameful.\nAnother way of measuring unconscious cognition is through measuring arousal. Most famous is the “Iowa card task” from Bechara et al. (1996). They had their subjects choose among playing cards, and receive rewards if they chose certain cards. They found that people gradually learned which types of cards were rewarded, but they also found that the subjects’ automatic responses (measured by skin conductance, i.e. sweating) would show an awareness of the pattern more quickly than the subjects’ choices would: after a while, when the subject’s hand hovered over one of the cards which was rewarded, the subject would sweat a little more, even though the subject wasn’t any more likely to choose that card. They said that this showed that unconscious learning was outpacing conscious learning. Antonio Damasio, one of the authors of this study, went on to write Descartes’ Error which accused Descartes’ of starting the great misapprehension that emotions and reason are in competition – Damasio said that his experiments show how emotions inform reason and improve decision-making. A lot of subsequent papers tried to show that snap decisions, which avoid conscious processing, can produce better outcomes than slow considered decisions.\nEven more famous is the “Implicit Association Test” (IAT) (Greenwald, McGhee and Schwartz (1998)). Subjects are told to press a button whenever they see something from either of two different categories of stimuli, e.g. press the button if you see either a black face or a word with a positive association. Their finding, much-replicated, was that people are relatively quicker at tasks (meaning they have shorter response times) when they are asked to identify a set such as “black face or negative word” or “white face or positive word” than to identify a set like “black face or positive word” or “white face or negative word.” They find that this occurs even among people who report no conscious negative feelings towards black people, and they interpret this as revealing an unconscious association between black people and negative feelings, and they argue that this association could affect your decision-making without you being aware of it.\nMany other measures of automatic responses have been popular at different times: hypnosis and word association (Freud used both of these before moving to talking therapy); Rorsach blots (AKA inkblot tests); thematic apperception test (interpret an ambiguous drawing, still widely used); lie detectors AKA polygraphs (they measure autonomic responses - blood pressure, pulse, respiration, and skin conductivity - as you are asked different questions).\nUnfortunately a great deal of this research turns out to be both hard to replicate, and reliant on strong assumptions in order to interpret as surfacing unconscious associations. Newell and Shanks (2014) give strong arguments for both of these points, covering many of the methods I mentioned here.\nIt is worth mentioning that, although Freud’s more elaborate theories died off, his idea that psychosomatic illness is an indirect expression of a psychological stress, especially about something shameful, I believe remains one of the standard theories of modern neurology (O’Sullivan, 2015).\nHowever even if we had solid evidence for unconscious influences on involuntary responses, this still stops short of unconscious influences on decision-making. It’s possible that our associations show up in sweating, response time, and dreams, but have little effect on decision-making, and if that’s so then unconscious associations are not terribly important for social science. Most of the authors in this literature have assumed that the unconscious factors they identify affect real decisions but have left that extrapolation untested. Blanton et al. (2009) say that there’s no persuasive evidence that implicit racial bias, as measured by the IAT, predicts peoples’ decision-making, once you control for measures of explicit racial bias, i.e. when you just ask people how they feel about black people. (Singal (2016) has a long discussion on this point)."
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html#ability-to-describe-the-influence",
    "href": "posts/2017-12-10-unconscious-influences.html#ability-to-describe-the-influence",
    "title": "On Unconscious Influences (Part 1)",
    "section": "Ability to Describe the Influence",
    "text": "Ability to Describe the Influence\nA second type of evidence is to compare self-reported influences on behaviour with actual influences on behaviour. Here are some examples:\n\nIn the mid 20th-century behaviourists found that they could shape their subjects choices through conditioning with rewards and punishments, and the subjects seemed to remain ignorant of this shaping. For example if you say ‘mm-hmm’ whenever someone uses a plural noun, then after a while that person ends up using plural nouns more often, apparently unaware of the influence (Thorndike and Rock (1934); Greenspoon (1955)).\nSince the 1970s social psychologists have published all sorts of experiments in which they vary an apparently irrelevant factor and find that this can affect peoples’ decision-making. Nisbett and Wilson (1977) summarize a lot of experiments and say “subjects are sometimes (a) unaware of the existence of a stimulus that importantly influenced a response, (b) unaware of the existence of the response, and (c) unaware that the stimulus has affected the response.”\nAnother paradigm from the 1970s asks people to make a judgement - e.g. which stock to pick - and also to rate the importance of factors which contributed to their decision. Slovic et al. (1972) find a low correlation (0.34) between the ratings that a stockbrokers put on factors, and the actual influence of these factors on their decisions. There is a small literature with similar findings across a variety of tasks.\nFinally, since the 1970s a smaller group of psychologists have been running experiments in which people learn a complicated pattern, and then are asked about their insight into it. E.g. in Arthur Reber’s “artificial grammar” experiments subjects learn, through trial and error, to discriminate between two categories of words. After some time they become very good at the task, but when asked to explain how they are making decisions they often say they don’t know, or they come up with rules that do not match their actual performance.\n\nAs in the previous category, a lot of this evidence is very fragile: either hard to replicate, or based on delicate interpretations of what is happening in the experiment. Newell and Shanks (2014) again give a good summary.\nAn additional problem is that these findings could reflect knowledge being difficult to articulate, without it being unconscious. And this literature is full of reversals which bear this out: when experiments are repeated it has often turned out that the subjects do report awareness of the pattern that they have learned if they are asked the question in a different way. Mitchell et al. (2009) say “[i]t is very difficult to provide a satisfactory demonstration of unaware conditioning simply by showing conditioning in the absence of awareness. This is because it is very difficult to be sure that the awareness measure and the conditioning measure are equally sensitive.”\n\n\n\nIIES"
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html#isolation-of-unconscious-influences",
    "href": "posts/2017-12-10-unconscious-influences.html#isolation-of-unconscious-influences",
    "title": "On Unconscious Influences (Part 1)",
    "section": "Isolation of Unconscious Influences",
    "text": "Isolation of Unconscious Influences\nFinally there’s a third type of evidence which is more strictly behavioural: an unconscious factor is one which is isolated from your other conscious beliefs and desires – i.e. it does not interact with conscious factors – and that isolation will be reflected in your behaviour. This isolation criterion has been given various names, but I don’t think it’s ever been explained as clearly as it could be.\nTo be precise think of the blind man (the conscious part of the brain) and the guide dog (the unconscious). The guide dog can know something – e.g. she knows when the crossing light is flashing – which the man does not know, and her knowledge will influence the man’s decisions through her recommendation of when to cross. However the guide dog’s knowledge is isolated from the man’s knowledge: it only influences his decisions through the narrow channel of pulling on the leash. Suppose you tell the man that the crossing lights are not working properly, and so whatever color they show is entirely at random and uninformative. The man and dog, considered as a system, has two pieces of information: (1) the light is green (i.e. indicating ready to cross); and (2) the color is uninformative. However the two pieces of information are known by different actors, implying that they will not be integrated, because neither the man or dog knows both. This will be reflected in the man’s behaviour: he will be influenced by the guide-dog’s recommendation, because the dog sees other things in addition to the crossing-light, such as oncoming traffic. And so the man’s behaviour will still be influenced by the color of the light, even though he knows that the color is irrelevant.\nIf information is separated in the brain, we ought to see characteristic patterns of that in behavior. I know of just a few cases where the isolation of knowledge has come up clearly in trying to define or measure unconscious influences.\nStich (1978) said that certain mental states are “inferentially unintegrated”: \n\n“[unconscious beliefs are] largely inferentially isolated from the large body of inferentially integrated beliefs to which a subject has access”\n\nHe gives an example: suppose Noam Chomsky has a theory of grammar, and that there exists some grammatical rule which is a counterexample to that theory. If a linguist knows that rule consciously, then the linguist will immediately infer that Chomsky’s theory is false. But if the linguist only knows the rule unconsciously, then they won’t be able to make that inference, because the knowledge is “inferentially unintegrated” – i.e. the knowledge is isolated from the knowledge regarding Chomsky’s theory. 1\nA separate place where this separation has come up is in the work of Zenon Pylyshyn and Jerry Fodor since the 1980s regarding perception being “cognitively impenetrable,” or “informationally encapsulated.” They mean that perceptual processes often make inferences without taking into account all the information that is available, i.e. by drawing only on a subset of information. Their principal argument was from perceptual illusions: they argue that illusions can typically be understood as rational inferences from a subset of the information available. Helmholtz had a nice example: if you close one eye and press with your finger on the edge of your eyelid then you’ll perceive a point of light, but the light will be coming from the opposite side of your field of vision from where your finger is. This is because the left side of your retina receives light from the right side of your visual field and vice versa. So when your retina receives some stimulation on the left-hand side your brain makes infers that light is coming from the right-hand side. This is a sensible inference given only the information that your eye has, i.e. just the information from the retina. In this case there is additional information - the fact your finger is pressing on your eyelid - which should give a different interpretation to the stimulation, but your visual cortex is not wired up to incorporate that information, and so it misinterpret the signals it receives.\nThe Helmholtz-Fodor-Pylyshyn model of encapsulated inference isn’t quite the same as the case of the blind man and the guide dog. In their examples the pre-conscious process have a strict subset of the information available to the conscious brain. In other words the man isn’t blind, it’s just a case where the dog leads in a different direction than the man would. Fodor (1983) does have a brief discussion on whether early perceptual processes have access to information not available to the conscious brain, which would imply unconscious influences, in my sense.\nFinally the isolation argument has appeared in the literature on human “associative learning,” in testing whether or not the associations that we learn through conditioning are conscious. A typical experiment involves ringing a bell and then giving subjects a small electric shock. After a while people learn to flinch when they hear the bell. For a long time psychologists tried to map out the logic of how such associations would form, trying to figure out the rule which governed learning of associations. However in the last few decades an argument has been made that these learned associations are not in fact mechanical - there is no simple rule - instead they are more-or-less optimal responses to the environment based on the entirety of the information available, i.e. they are not isolated from other knowledge, though the argument isn’t usually put in terms of conscious vs unconscious knowledge. For example Colgan (1970) told subjects, after they learned an association, that the association is no longer valid (“from now on the bell will not signal an electric shock”) and he found that, although this didn’t entirely extinguish the flinching, it did cause it to markedly decrease. This implies the flinching is not isolated from your conscious knowledge: the association, at least to some degree, interacts with more abstract knowledge. There are many other circumstances where rule-based theories of association-learning have foundered because it turns out that peoples’ responses respond to outside considerations. De Houwer, Vandorpe and Beckers (2005) summarize the evidence against associative models (which can be interpreted as models with unconscious knowledge):\n\nThe two types of models can be differentiated … by manipulating variables that influence the likelihood that people will reason in a certain manner but that should have no impact on the operation of the associative model. We have seen that such variables (e.g., instructions, secondary tasks, ceiling effects, nature of the cues and outcomes) do indeed have a huge effect. Given these results, it is justified to entertain the belief that participants are using controlled processes such as reasoning and to look for new ways to model and understand these processes.\n\nMitchell says:\n\n“The results consistently show evidence for skin conductance [effects] only in participants who are aware of the [relationship] … [a]lthough there are many papers arguing for unaware conditioning, close inspection reveals, in almost all cases, that the measure of conditioning was most likely more sensitive than that of awareness.”\n\nIn retrospect a lot of behavior that was studied in the lab, which was thought to be telling us about the wiring of the animals, actually was telling us about the world outside the animal, because it has turned out that the animals’ response is the optimal response to the typical circumstances it faces in the world. (See my other post The Repeated Failure of Laws of Behaviour , and also Mitchell et al. (2009) section 4.3)\nIf this line of thought were entirely correct – if all information was integrated and fed into every decision – then there would be no unconscious influences in my sense. However I do think that there’s plenty of evidence that remains for a lack of integratation between cognitive processes.\nIn Part 2 of this essay I will give a more formal statement of how decisions can reveal unconscious knowledge (and unconscious motivations), and a survey what I think is the strength of the evidence.\n\n\n\nCaltech"
  },
  {
    "objectID": "posts/2017-12-10-unconscious-influences.html#footnotes",
    "href": "posts/2017-12-10-unconscious-influences.html#footnotes",
    "title": "On Unconscious Influences (Part 1)",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nQuine says you shouldn’t call this type of thing unconscious knowledge – your linguistic practice may obey some rule, but you can’t say that you unconsciously know that rule, because there are infinitely many different rules that would imply that pattern of behavior. But this skeptical objection is too tough: Quine would deny that a cow can have a belief about where a water trough is, & instead admit only that the cow’s behavior is consistent with a particular belief among infinitely many others.↩︎"
  },
  {
    "objectID": "posts/2023-04-28-ranking-by-engagement.html",
    "href": "posts/2023-04-28-ranking-by-engagement.html",
    "title": "Ranking by Engagement",
    "section": "",
    "text": "Thanks to comments from Jeff Allen, Jacquelyn Zehner, David Evan Harris, Jonathan Stray, and others. If you find this note useful for your work send me an email and tell me :).\nSix observations on ranking by engagement on social media platforms:\nIn an appendix I formalize the argument. I show that all these observations can be expressed as covariances between different properties of content, e.g. between the retentiveness, predicted engagement rates, and other measures of content quality. From those covariances we can derive Pareto frontiers and visualize how platforms are trading-off between different outcomes."
  },
  {
    "objectID": "posts/2023-04-28-ranking-by-engagement.html#formal-observations",
    "href": "posts/2023-04-28-ranking-by-engagement.html#formal-observations",
    "title": "Ranking by Engagement",
    "section": "Formal Observations",
    "text": "Formal Observations\nHere I describe a few formal properties of a model of ranking based on a joint-normal distribution of attributes. I have a longer writeup with proofs of these results which I hope to publish soon, I am happy to share a draft on request.\n\nThe covariance between item attributes will determine a Pareto frontier among outcomes. Suppose we know the joint distribution of attributes and we can choose a subset with share \\(p\\) of the distribution (e.g. a fixed number of impressions given a pool of possible stories to show), and we want to calculate the average value of each attribute in the subset of content shown to the user. Then we can describe the Pareto frontier over subsets, i.e. the set of realized average outcomes, and it will be a function of the covariances among attributes over pieces of content. With 2 attributes the Pareto frontier will be an ellipse with shape exactly equal to an isoprobability curve from the joint density.\nThe shape of the ellipse has a simple interpretation. If two attributes are positively correlated then the Pareto frontier will be tight meaning there is little tradeoff, i.e. we will have similar aggregate outcomes independent of the relative weights put on each outcome in ranking. If instead two attributes are negatively correlated then the Pareto frontier will be loose meaning outcomes will vary a lot with the relative weights used in ranking.\nOur assumption that the share \\(p\\) is fixed is equivalent to assuming that any ranking rule will get the same number of impressions. This assumption obviously has some tension with retentiveness being an outcome variable: if some ranking rule has low retentiveness, then we would expect lower impressions. Accounting for this would make the Pareto frontier significantly more complicated to model, for simplicity we can interpret every attribute except retentiveness as a short-run outcome. Alternatively we could interpret them as relative instead of absolute outcomes, e.g. as engagement/impression or engagement/DAU.\nImproving a classifiers will stretch the Pareto frontier. As a classifier gets better the average prediction will stay the same but the variance will increase, meaning the Pareto frontier will stretch out, and given a linear indifference curve we can derive the effect on outcomes.\nThe joint distribution plus utility weights will determine ranking weights. If we observe only some outcomes then we can calculate the conditional expectation for other outcomes. Typically we want to know retentiveness, and we can write the conditional expectation as follows: \\[E[\\text{retentiveness}|\n   \\text{engagement},\\ldots,\\text{user preference}].\\] This expectation has a closed-form solution when the covariance matrix is joint normal. When we have just two signals, for example engagement and quality, we can write:\n\\[\\begin{aligned}\n   E[r|e,q] &= \\frac{1}{1-\\gamma^2}(\\rho_e-\\gamma\\rho_q)e +\n               \\frac{1}{1-\\gamma^2}(\\rho_q-\\gamma\\rho_e)q\\\\\n   r     &= \\text{retentiveness}\\\\\n   e     &= \\text{engagement (predicted)}\\\\\n   q     &= \\text{quality (predicted)}\\\\\n   \\rho_{e}     &= \\text{covariance of engagement and retentiveness}\\\\\n   \\rho_{q}     &= \\text{covariance of quality and retentiveness}\\\\\n   \\gamma     &= \\text{covariance of engagement and quality}\n\\end{aligned}\\]\nNote that the slope of the iso-retentiveness line in \\((e,q)\\)-space will be \\(-\\frac{\\rho_e-\\gamma\\rho_q}{\\rho_q-\\gamma\\rho_e}\\).\nExperiments which vary ranking weights tell us about covariances. We can write findings from experiments as follows. First, suppose we find that retention is higher when ranked by engagement than when unranked, this can be written:\n\\[\\begin{aligned}\n      \\utt{E[r|e&gt;e^*]}{ranked by}{engagement} &&gt; \\ut{E[r]}{unranked}\n   \\end{aligned}\\]\nHere \\(e^*\\) is chosen such that \\(P(e&gt;e^*)=p\\) for some \\(p\\), representing the share of potential inventory that the user consumes. This implies that engagement must positively correlate with retentiveness, \\(\\rho_e&gt;0\\).\nNext we can express that retention is higher when we put some weight \\(\\beta\\) on quality:\n\\[\\begin{aligned}\n   \\utt{E[r|e+\\beta q&gt;\\kappa^*]}{ranked by}{engagement and quality} &&gt; \\utt{E[r|e&gt;e^*]}{ranked by}{engagement}\n\\end{aligned}\\]\nHere \\(\\kappa^*\\) is chosen such that \\(P(e+\\beta q &gt; \\kappa^*)=P(e&gt;e^*)=p\\). If \\(\\beta\\) is fairly small then we can infer that the iso-retentiveness line is downward-sloping, implying: \\[\\frac{\\rho_e-\\gamma\\rho_q}{\\rho_q-\\gamma\\rho_e}&gt;0.\\]\nThis implies that both engagement and quality have the same sign. I don’t think they both can be negative, so they both must be positive:\n\\[\\begin{aligned}\n      \\rho_e - \\gamma \\rho_q &&gt; 0 \\\\\n      \\rho_q - \\gamma \\rho_e &&gt; 0.\n   \\end{aligned}\\]\nI think it’s reasonable to treat preferences as locally linear. To have a well-defined maximization problem (with an interior solution) we need either nonlinear preferences or a nonlinear Pareto frontier. It’s always easier to treat things as linear when you can, so a relevant question is which of these two is closer to linear? Internally companies often treat their preferences as nonlinear, e.g. setting specific goals and guardrails, but those are always flexible and often have justifications as incentive devices. Typical metric changes are small, only single-digit percentage points, over that range the Pareto frontier does show significant diminishing returns while (it seems to me) value to the company does not."
  },
  {
    "objectID": "posts/2023-04-28-ranking-by-engagement.html#overviews-of-recommender-systems-chronological",
    "href": "posts/2023-04-28-ranking-by-engagement.html#overviews-of-recommender-systems-chronological",
    "title": "Ranking by Engagement",
    "section": "Overviews of Recommender Systems (chronological)",
    "text": "Overviews of Recommender Systems (chronological)\n\nAdomavicius and Tuzhilin (2005) “Toward the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible Extensions”\n\nAn influential overview of recommender systems (14,000 citations!). The canonical example is recommending movies to get the highest predicted rating. They use “rating” as similar to “engagement”. A more recent survey is Roy and Dutta (2022).\n\nDavidson et al. (2010) “The YouTube Video Recommendation System”\n\n\n“[videos] are scored and ranked using … signals [which] can be broadly categorized into three groups corresponding to three different stages of ranking: 1) video quality, 2) user specificity and 3) diversification.””\n\n\n\n\n“The primary metrics we consider include click through rate (CTR), long CTR (only counting clicks that led to watches of a substantial fraction of the video), session length, time until first long watch, and recommendation coverage (the fraction of logged in users with recommendations).”\n\n\n\nThey say recommendations are good because they have high click-through rate. - A blog post from 2012 discusses a switch from views to watch time: “Our video discovery features were previously designed to drive views. This rewarded videos that were successful at attracting clicks, rather than the videos that actually kept viewers engaged. (Cleavage thumbnails, anyone?)”\n\nGomez-Uribe and Hunt (2015) “The Netflix Recommender System: Algorithms, Business Value, and Innovation”\n\nClearly states that they evaluate AB tests using engagement, but it is regarded as an imperfect proxy for retention:\n\n\n\n“we have observed that improving engagement—the time that our members spend viewing Netflix content—is strongly correlated with improving retention. Accordingly, we design randomized, controlled experiments … to compare the medium-term engagement with Netflix along with member cancellation rates across algorithm variants. Algorithms that improve these A/B test metrics are considered better.”\n\n\n\nCovington et al. (2016) “Deep Neural Networks for YouTube Recommendations”\nThis paper proposed a very influential architecture for content recommendation (the paper has 3000 citations). They say:\n\n“Our final ranking objective is constantly being tuned based on live A/B testing results but is generally a simple function of expected watch time per impression. Ranking by click-through rate often promotes deceptive videos that the user does not complete (“clickbait”) whereas watch time better captures engagement”\n\nLada, Wang, & Yan (2021, FB Blog) How does news feed predict what you want to see?\n\nThorburn, Bengani, & Stray (2022, Understanding Recommenders) “How Platform Recommenders Work”\n\nThis is an excellent short article with description and illustration of the stages in building a slate of content: moderation, candidate generation, ranking, and reranking. Includes links to many posts from platforms describing their systems.\n\n\n\n\n\n\n\nArvin Narayanan (2023) “Understanding Social Media Recommendation Algorithms”\n\nA good overview of recommendation algorithms, with an in-depth discussion of Facebook’s MSI.\n\n\nCriticisms of social media recommendation: (1) harm users because “implicit-feedback-based feeds cater to our basest impulses,” (2) harm creators because “engagement optimization … is a fickle overlord,” (3) harms society because “social media platforms are weakening institutions by undermining their quality standards and making them less trustworthy. While this has been widely observed in the case of news … my claim is that every other institution is being affected, even if not to the same degree.”\n\n\nThe technical part of the essay is excellent but I found some of the arguments about harm and social effects hard to follow."
  },
  {
    "objectID": "posts/2023-04-28-ranking-by-engagement.html#proposals-for-change-chronological",
    "href": "posts/2023-04-28-ranking-by-engagement.html#proposals-for-change-chronological",
    "title": "Ranking by Engagement",
    "section": "Proposals for Change (chronological)",
    "text": "Proposals for Change (chronological)\n\nAndrew Mauboussin (2022, SurgeAI) “Moving Beyond Engagement: Optimizing Facebook’s Algorithms for Human Values”\n\nSays that the problem is “the most engaging content is often the most toxic.” They propose using human raters, e.g. ask people “did this post make you feel closer to your friends and family on a 1-5 scale?” They label a small set of FB posts as a proof of concept.\n\nBengani, Stray, & Thorburn (2022,Medium) “What’s Right and What’s Wrong with Optimizing for Engagement”\n\nThey define engagement as “a set of user behaviors, generated in the normal course of interaction with the platform, which are thought to correlate with value to the user, the platform, or other stakeholders.” Reviews evidence for good and bad effects of ranking by engagement.\n\n\nOvadya & Thorburn (2023). Bridging Systems: Open Problems for Countering Destructive Divisiveness across Ranking, Recommenders, and Governance\n\nStray, Iyer, Larrauri (2023) “The Algorithmic Management of Polarization and Violence on Social Media”\n\n\nOur overall goal should be to minimize “destructive conflict”.\n\n\n\n\nThe major lever used has been content moderation: changing the visibility of content based on semantic criteria (e.g. downranking toxic, disallowing hate speech).\n\n\n\n\nHowever we should put relatively more work on system design, e.g. adding friction or changing the mechanics of sharing or engagement-based ranking. In part because there’s a robust correlation between content that causes destructive conflict and content that is engaging.\n\n\nMilli, Pierson and Garg (2023) Choosing the Right Weights: Balancing Value, Strategy, and Noise in Recommender Systems\n\nI find the model a little hard to follow. \n\n\n\n\nLubin & Gilbert (2023) “Accountability Infrastructure: How to implement limits on platform optimization to protect population health”\n\nA very wide-ranging and loose discussion of issues related to ranking content. Makes an analogy with 19th century measures to control public health. I think the main proposal is that firms come up with metrics to measure their effect on social problems such as mental health, and regularly report on how they’re doing. They suggest requirements for platforms of different sizes:\n\n\n\n\n\n\n\n\n\n1M+\nSubmitted plan for metrics and methods for evaluation of potential structural harms\n\n\n10M+\nConsistent data collection on potential structural harms\n\n\n50M+\nQuarterly, enforceable assessments on product aggregate effects on structural harms, with breakouts for key subgroups\n\n\n100M+\nMonthly, enforceable assessments on product aggregate effects as well as targeted assessments of specific product rollouts for any subproduct used by at least 50 million users, with breakouts for key subgroups"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html",
    "href": "posts/2016-04-30-relative-thinking.html",
    "title": "Relative Thinking",
    "section": "",
    "text": "peaches\nThere are a lot of cute thought experiments where the apparent value of something depends on what it’s compared to:\nMany people have felt that there’s a common principle at work, in particular: that the sensitivity to an attribute (price, probability, square feet) depends on the set of quantities that you’re considering. But different people have proposed different principles:\nAll of these models can be thought about as indifference curves that change slope as you change the elements in the choice set, e.g. below adding option C makes the indifference curves rotate clockwise, and so makes you prefer B to A:\n\\[\n   \\xymatrix{\\, &  &  &  &  & . & \\,\\, &  & \\,\\\\\n   \\, & A &  &  &  &  & A\\\\\n   \\, &  &  & B & \\ar@{-}[uullll] &  &  &  & B & \\ar@{-}[uul]\\\\\n   \\, &  &  &  &  &  &  &  &  &  C \\\\\n   \\ar[uuuu]\\ar[rrrr] &  &  &  & \\ar@{-}[uullll] & \\ar[uuuu]\\ar[rrrr] &  &  & \\ar@{-}[uuuull] & \\,\n   }\n\\]\nBut each theory has different assumption about how the slope of the indifference curves depends on the placement of the options.\nI’m going to try to make the following points:\nA few other points that I’ll leave for later:"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#digression-perception",
    "href": "posts/2016-04-30-relative-thinking.html#digression-perception",
    "title": "Relative Thinking",
    "section": "DIGRESSION: PERCEPTION",
    "text": "DIGRESSION: PERCEPTION\nComparison effects have been studied in perception for a long time, and the same points I make here also apply there. At first people proposed that perceptual comparison effects were hardwired things, mechanical effects, but on further study it turned out that they were context-dependent, in a way that makes them look like sensible inferences.\nHere’s a classic contrast effect:\n\nthe same shade of grey looks darker when surrounded by white, than when surrounded by black.\nFor a long time psychology textbooks gave this as an example of a hardwired contrast effect – i.e. this is caused by the basic wiring of neurons in our eyes. (Some still do).\nBut take a look at this (White’s illusion):\n\nhere the same shade of grey looks lighter on the left than on the right, despite the surroundings being relatively lighter on the left than on the right. This is exactly the opposite of what’s predicted by a hardwired contrast effect in perception.\n(An even cleaner example would be ceteris paribus, where making some part of the background lighter, all else held equal, makes the foreground appear lighter. The figure above does imply that there must exist at least one such case: imagine a third set of grey rectangles which are surrounded only by white. That third case must serve as a ceteris paribus case for one or other of the two cases above, probably both.)\nThe general point is this: There is not a stable relationship between the perceived-lightness of an object and the lightness of surrounding objects. There isn’t even an all-else-equal relationship. The relationship can run in either direction depending on the context.\nBut that’s not the last word. The set of contexts where it goes one way or the other way aren’t arbitrary (“contrast effects” and “assimilation effects”). Adelson (2001) shows that you can usually predict when you’ll observe one effect or the other: roughly, whether or not the surrounding lightness is a positive or negative ecological cue for illumination. I.e., in typical circumstances, is the surrounding lightness positively or negatively associated with illumination? (See an example at the bottom of this post.)"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#digression-2",
    "href": "posts/2016-04-30-relative-thinking.html#digression-2",
    "title": "Relative Thinking",
    "section": "DIGRESSION 2",
    "text": "DIGRESSION 2\nI would write out lists of comparison-effect examples over and over while working on my PhD. My train of thought would get detached and I’d end up asking myself questions like: What are you doing sitting in this office, a continent away from your friends and an ocean and a continent away from your family? Why do you spend your weekends in this sad building, where people stare at the carpet when they pass each other? What rock did you hit in adolescence that knocked you out of orbit, and sent you here? Are you trying to make your mother proud? Avenge your father? Do these professors you work with look like the kind of man you want to be? Did you stumble into one of those academic fields that people snigger about? Why, when you talk about your work, do the people you admire glaze over, and the people who bore you perk up? Do you think that giving your life to intellectual things makes you better than other people? Do you look down on people who don’t think so clearly? What are you doing on a Friday night at the NBER eating a tuna subway sandwich and reading reddit? If it takes you 5 years to get straight one point about relative thinking – one corner of one shelf in one cupboard – then how long is it going to take to tidy up the whole house? When an undergraduate corners you, asking earnest & tedious questions, doesn’t it remind you of yourself?\n\n\n\ndesk"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#alternative-heuristics",
    "href": "posts/2016-04-30-relative-thinking.html#alternative-heuristics",
    "title": "Relative Thinking",
    "section": "ALTERNATIVE HEURISTICS",
    "text": "ALTERNATIVE HEURISTICS\nOK. Well, here’s the body of the argument: I’m going to discuss a few perfectly reasonable reasons why we might infer the value of different attributes from the choice set, and each reason will imply one of the above laws in a subset of cases. However I also show that the same reason can imply the exact opposite of that law, for cases outside that subset.\n\n(1) MRS shifting towards MRT.\nA choice set which varies in different attributes has an implicit rate of tradeoff between the attributes – i.e. the marginal rate of transformation (MRT) – and it is easy to think of cases where our preferences would naturally adapt to the implicit tradeoff, i.e., where our relative value, AKA our marginal rate of substitution (MRS), would rotate towards the tradeoff that is implicit in the choice set (i.e. the MRT).\nThe MRS shifting towards the MRT could be rationalized in two separate ways. First, suppose you believe the social MRS to be informative about your own MRS, and you believe that the choice set reflects the market price, and finally that the price reflects the social MRS (as it would in a competitive equilibrium). Second, suppose you believe the person who constructed the choice set to be cooperative (in the sense of Grice) - i.e., they only include things in the choice set which they think you might want: this implies that the MRT in the choice set reflects their beliefs about your MRS, which is itself informative. If I’m staying in your spare room and and you ask me “would you prefer a poached egg or gruel for breakfast?” then I will figure that your gruel must be pretty good.\nIf there are just two attributes (i,j) and two alternatives (a,b) then the implicit tradeoff is \\(MRT_{i,j}=\\frac{\\|a_{i}-b_{i}\\|}{\\|a_{j}-b_{j}\\|}\\). If the MRS rotates to meet this MRT then the sensitivity to attribute \\(i\\) will be decreasing in the range observed along that dimension, exactly as implied by the range-sensitivity theory (i.e., V, M&C, BR&S).\nHowever the MRS-MRT effect implies range sensitivity only for a 2-attribute, 2-alternative case. Outside of that case the intuitions depart.\nA marginal rate of transformation can only be directly identifed from a menu if the number of alternatives is equal to the number of attributes. I.e., to define a plane in \\(n\\) dimensions from a set of points, you’ll need exactly \\(n\\) points. If you have fewer then it becomes the statistical problem of fitting a line to a set of points. Here is a simple example where the MRT theory and other theories (e.g. range-sensitivity) give qualitatively different answers, and in which the MRS-MRT theory seems more faithful to the intuition. Suppose we have the following three options:\n\\[\n\\xymatrix@C=1em@R=1em{\n& \\binom{\\mbox{100K salary}}{\\mbox{199 days off}}\\\\\n\\\\\n&  & \\binom{\\mbox{105K salary}}{\\mbox{189 days off}} & \\binom{\\mbox{110K salary}}{\\mbox{189 days off}}\\\\\n\\\\\n\\ar[rrrr]\\ar[uuuu] & & &  &  & \\, \\\\\n}\n\\]\nThe intermediate option is dominated by the by the option on the right, and intuitively - to me - the existence of the intermediate option makes the rightmost option more desirable - because the choice set makes 10-days-off seem to be worth between \\$5K and \\$10K, meaning the higher salary seems to come at a low cost in terms of days-off. This intuition is not captured by range sensitivity, because the intermediate option does not change the range in either dimension. However the intermediate option does change the implicit MRS, in the sense of the best-fitting line (e.g. orthogonal regression), and the change will be in favor of the rightmost option – fitting my own intuition.\nEven in the 3-attribute 3-alternative case, it is no longer true that \\(MRT_{i,j}\\) is equal to the ratio of ranges on dimension i and j, it’s now a more complicated function.\nWhen one attribute is a good and the other is a bad (e.g. price and quality; salary and hours) it is sometimes reasonable to think that choosing neither alternative is an additional implicit element of the choice set, i.e. the point (0,0). In these cases the ratio of the ranges reduces to the ratio of the maximum values (\\(\\frac{\\max_{c\\in C}c_{i}}{\\max_{c\\in C}c_{j}}\\)), which has similar comparative statics to the theory in (C) - which depends on the ratio of average values - than the theory in (V,M&C,BR&S) - which depends on the ratio of the ranges.\nAn interesting fact: the effects of this MRS-MRT theory will not be detectable when the choice set is binary: suppose your MRS shifts towards the MRT implicit in the choice set, then although your final MRS will be closer to the MRT, it will not cross the MRT, i.e. the shift in MRS will not alter which of the two elements you prefer. This implies that the MRS-MRT theory cannot rationalize the existence of cycles in binary choice, and so cannot explain evidence for ‘subadditivity’ of different dimensions, such as probability, money, or delay (see Read (2001) for citations). For example, if your have a prior belief that \\(a\\) is better than \\(b\\), and then you observe a choice set containing \\(a\\) and \\(b\\), then you may revise upward your valuation of \\(b\\) relative to \\(a\\), but this observation wouldn’t cause you to switch preference, i.e. to think that \\(b\\) is now better than \\(a\\). (This could be violated under some unusual priors, e.g. if you had bimodal beliefs about the value of \\(b\\)).\n\n\n(2) MRS shifting towards demand.\nThere is a second strong intuition for choice sets influencing preferences: combinations offered often reflect combinations desired, so a relative increase in attribute 1 could be interpreted as a positive signal about the value of attribute 1. Suppose we manipulate the choice set, while keeping the relative price fixed, for example consider these two choice sets, trading off the price and quantity of some good:\n\\[\n\\xymatrix@C=.5em@R=.5em{\\ar[rrrrr]\\ar[ddddd] & & &  &  & \\text{apples}\\\\\n& \\binom{\\text{1 apple}}{\\$1}\\\\\n&  & \\binom{\\text{2 apples}}{\\$2}\\\\\n&  & & \\binom{\\text{3 apples}}{\\$3}\\\\\n\\\\\n\\\\\n\\$ }\n\\]\n\\[\n\\xymatrix@C=.5em@R=.5em{\\ar[rrrrr]\\ar[ddddd] & & & &  & \\text{apples}\\\\\n& \\,\\,\\,\\,\\,\\, & \\\\\n& & \\binom{\\text{2 apples}}{\\$2}\\\\\n& & & \\binom{\\text{3 apples}}{\\$3}\\\\\n& & & & \\binom{\\text{4 apples}}{\\$4}\\\\\n\\\\\n\\$ }\n\\]\nA natural intuition is that people will tend to switch from \\(\\binom{2}{\\$2}\\) to \\(\\binom{3}{\\$3}\\), when going from the first to the second choice set. None of the theories discussed above gives an unambiguous prediction about the change in MRS between these two choice sets - because both the range and magnitude change by the same amount on each dimension - yet the intuition remains quite clear (I think).\nThis idea could be easily formalized - suppose people know the supply curve but are uncertain about the demand curve - then when they observe an increase in quantity they attribute this to a higher demand, and so they infer an increase in value of the good. I think this is similar to the intuition given in Kamenica (2008) - when you observe a higher price/quantity combination, you infer that demand is higher, and so you infer that the value of each marginal good must be higher. I think that a similar foundation is used in Drolet, Simonson, Tversky (2000) “Indifference Curves that Travel with the Choice Set”.\nWe have discussed diagonal shifts along the budget set, meaning that both attributes are varying at once; if only one attribute varied, it’s not clear what a consumer would infer from this. Of course we could formalize a model where the consumer is uncertain about both supply and demand; or we could combine this model with the prior one, where the consumer is uncertain about the price.\n\n\n(3) Magnitude effects.\nFinally it’s easy to come up with a rational magnitude effect, such that when we observe a higher quantity \\(q\\) we infer that the marginal value of each unit is less. Suppose we know the price and we know the consumption-value of a good, when measured in units that are familiar to us, but we do not know the units that are used in the packaging. Then if we observe other people consuming a higher quantity, measured in unfamiliar units, we infer that each unit is worth less: when we observe a 10,000 Kronor bank note we infer each Kronor is not worth a lot; when we observe a 10,000 Watt bulb we infer each Watt is not worth much; when we observe 200mg of Oxytocin we infer the marginal effect of a milligram is not too much."
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#summary",
    "href": "posts/2016-04-30-relative-thinking.html#summary",
    "title": "Relative Thinking",
    "section": "SUMMARY",
    "text": "SUMMARY\nWhen we dig into the intuitions behind comparison effects, we often find that they resemble inferences that we would make every day. The laws that have been proposed to explain comparison effects only work because they coincide with one or other of the inferences in certain subsets of cases – e.g. range-sensitivity coincides with MRS-MRT inference, magnitude-sensitivity coincides with unit-value inference. But these overlaps occur only in a subset of cases, and stepping outside that subset we find that the law fails, while the inference remains.\nDoes this mean that comparison effects are all just rational inferences? What we would like to know is whether comparison effects occur even when inference can be entirely ruled out – e.g. when we run an experiment that explicitly randomizes the choice sets. Some papers do this, but few do it well. I am persuaded that comparisons do affect us on a pre-conscious level, i.e. that our instincts latch onto comparisons without being careful about the significance of the comparison in the particular circumstance, but there’s not a lot of unambiguous evidence on this. I can at least say that most people find the types of example listed above pretty beguiling: they get strong intuitions about relative value, but struggle to explain where the intuitions come from, implying that the inference isn’t entirely conscious.\nSo then why would we make bad inferences that resemble good inferences? I think for the same reason that our perception makes bad inferences that resemble good inferences – because perceptual processes interpret cues according to their ordinary significance, without adjusting for all relevant information. Perception is carried out in a cabinet, whirring through the sense data, and printing out conclusions for the conscious mind to read. The cabinet is locked, we only have access to the output. That is the argument of my paper on ‘implicit knowledge’."
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#miscellaneous-notes",
    "href": "posts/2016-04-30-relative-thinking.html#miscellaneous-notes",
    "title": "Relative Thinking",
    "section": "MISCELLANEOUS NOTES",
    "text": "MISCELLANEOUS NOTES\n\nIt is useful to make a sharp distinction between “goods” and ’bads”. Examples of good-bad tradeoffs are quality vs price; salary vs hours worked. Examples of good-good tradeoffs are bedrooms vs bathrooms; MPG vs horsepower; salary vs holiday-days.\nParducci invented, as well as range-frequency theory, windsurfing.\nBordalo Gennaioli and Shleifer (2013) has a weird feature: the salience of an attribute depends on relative levels (\\(q-\\bar{q}\\) and \\(p-\\bar{p}\\)), but the utility of an attribute depends on absolute levels (\\(q\\) and \\(p\\)). I think this is just a mistake – the underlying intuition is matched much better if \\(U(q,p) = \\hat{\\theta}_q(q-\\bar{q}) + \\hat{\\theta}_p(p-\\bar{q})\\) instead of \\(U(q,p) = \\hat{\\theta}_q q + \\hat{\\theta}_p p\\). This alternation removes a lot of the weird comparative statics of the theory, such as the severe non-monotonicity of the decoy effects. Another note: for two-alternative two-attribute choices the theory (as stated in the paper) has a utility representation, i.e. many of the predictions of that paper are equivalent to a model with menu-independent preferences. For a sufficiently large value of \\(M\\):\n\\[\nU(q,p)=\\begin{cases}\n\\delta q-p & ,\\,q&lt;\\delta p\\\\\nM+\\ln q-\\ln p & ,\\delta p&lt;q&lt;\\delta^{-1}p\\\\\nM+q-\\delta p & ,\\,q&gt;\\delta^{-1}p\n\\end{cases}\n\\]\n\n\n\n\nskeleton"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#references",
    "href": "posts/2016-04-30-relative-thinking.html#references",
    "title": "Relative Thinking",
    "section": "REFERENCES",
    "text": "REFERENCES\nKS: Koszegi and Szeidl (2011) (KS)\nBRS: Bushong Schwarzstein & Rabin (2014)\nMC: Mellers & Cooke ()\nC: Cunningham (2013)\nBGS: Bordalo Gennaioli Shleifer (2012)\nSimonson (2008) “Will I like a Medium Pillow?”\n\n“much of the evidence for preference construction reflects people’s difficulty in evaluating absolute attribute values and tradeoffs and their tendency to gravitate to available relative evaluations … These illustrations suggest that many forms of preference construction reflect a key underlying principle: decision makers tend to avoid absolute value judgments and gravitate to accessible relative evaluations … it is noteworthy that the evidence that has been accumulated to make the case for preference construction might be largely driven by a rather simple common principle. This rather simple, yet important absolute-to-relative principle lends itself to seemingly unrelated demonstrations, which have been treated as distinct phenomena and received unique labels.”\n\nDavid Stove (1991) “What is Wrong With our Thoughts?”"
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#another-illusion",
    "href": "posts/2016-04-30-relative-thinking.html#another-illusion",
    "title": "Relative Thinking",
    "section": "Another Illusion",
    "text": "Another Illusion\nAdelson’s “steps” illusion:\n\nIn the first picture the two squares with arrows on them look similar, but in the second picture they seem to have different shades. They are (as you guessed) all the same shade, and in fact the shades are all identical between the first and second image, just arranged a little differently.\nIn particular, the tilt gives an the impression of an angle, and so influences our judgment of where the illumination is coming from. In the first image both squares seem to be on the same plane; in the second image the upper square seems to be on a plane with light squares, and the lower square seems to be on a plane with dark squares. If we use, as proxies of illumination for a square, the shade of squares coplanar with it, then we get the predicted effect."
  },
  {
    "objectID": "posts/2016-04-30-relative-thinking.html#more-examples",
    "href": "posts/2016-04-30-relative-thinking.html#more-examples",
    "title": "Relative Thinking",
    "section": "More Examples",
    "text": "More Examples\n\nIf you have to judge the value of 10 of something – say you’re bidding on 10 bottles of wine in an auction – they’ll seem more valuable if, at the same time, you’re also considering a single bottle of wine; and vice versa if you’re also considering 100 bottles of wine.\nIf you’re choosing between a low-price and medium-price version of a good, seeing that there’s also a high-price version makes the medium-price version seem relatively more attractive.\n(See my ‘comparisons and choice’ paper for more examples)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "An AI Which Imitates Humans Can Beat Humans\n\n\n\n\n\n\n\n\n\n\n\n\nSep 19, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n \n\n\n\n\nSushi-Roll Model of Online Media\n\n\nPreviously: “pizza model”, “salami model”\n\n\n\n\n\n\n\n\n\nSep 8, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n \n\n\n\n\nHow Much has Social Media affected Polarization?\n\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n \n\n\n\n\nThe Paradox of Small Effects\n\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n \n\n\n\n\nRanking by Engagement\n\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n \n\n\n\n\nSocial Media Suspensions of Prominent Accounts\n\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n \n\n\n\n\nOptimal Coronavirus Policy Should be Front-Loaded\n\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2020\n\n\n\n\n\n\n \n\n\n\n\nOn Unconscious Influences (Part 1)\n\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2017\n\n\n\n\n\n\n \n\n\n\n\nThe Work of Art in the Age of Mechanical Production\n\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2017\n\n\n\n\n\n\n \n\n\n\n\nRepulsion from the Prior\n\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2017\n\n\n\n\n\n\n \n\n\n\n\nThe Repeated Failure of Laws of Behaviour\n\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2017\n\n\n\n\n\n\n \n\n\n\n\nSamuelson & Expected Utility\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2017\n\n\n\n\n\n\n \n\n\n\n\nEconomist Explorers\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2017\n\n\n\n\n\n\n \n\n\n\n\nWeber’s Law Doesn’t Imply Concave Representations or Concave Judgments\n\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2017\n\n\n\n\n\n\n \n\n\n\n\nRelative Thinking\n\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2016\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Tom Cunningham",
    "section": "",
    "text": "Resident fellow at the Integrity Institute."
  },
  {
    "objectID": "about.html#blog",
    "href": "about.html#blog",
    "title": "About Tom Cunningham",
    "section": "Blog",
    "text": "Blog"
  },
  {
    "objectID": "about.html#resume",
    "href": "about.html#resume",
    "title": "About Tom Cunningham",
    "section": "Resume",
    "text": "Resume"
  },
  {
    "objectID": "about.html#working-papers",
    "href": "about.html#working-papers",
    "title": "About Tom Cunningham",
    "section": "Working Papers",
    "text": "Working Papers\n\n2023: Implicit Preferences, Revise and Resubmit, American Economic Review, with Jon de Quidt.\n2020: Interpreting Experiments with Multiple Outcomes (presented at CODE) with Josh Kim.\n2015: Biases and Implicit Knowledge\n2015: Equilibrium Persuasion with Ines Moreno de Barreda.\n2013: Comparisons and Choice\n2013: Relative Thinking and Markups"
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "About Tom Cunningham",
    "section": "Publications",
    "text": "Publications\n\n2019: Improving Treatment Effect Estimators Through Experiment Splitting, WWW, with Dominic Coey.\n2013: The Incumbency Effects of Signalling  Economica, with Ines Moreno de Barreda, Francesco Caselli, and Massimo Morelli.\n2009: Leader Behaviour and the Natural Resource Curse Oxford Economic Papers, with Francesco Caselli."
  },
  {
    "objectID": "about.html#miscellaneous",
    "href": "about.html#miscellaneous",
    "title": "About Tom Cunningham",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\n2010: “LSE CEP 2010 Election Analysis: Macroeconomics and Public Finance” (with Ethan Ilzetski)\n\nComment pieces for the Guardian (2008/2009)\n\nDon’t worry about inflation\nThe great financial stitch-up\nThe rate cut wasn’t big enough\nStuck in the middle\nWe need to outgrow our debt\nYou say inflation, I say deflation\nConfidently predicting darker days"
  },
  {
    "objectID": "posts/2023-09-05-model-of-ai-imitation.html#performance-by-task",
    "href": "posts/2023-09-05-model-of-ai-imitation.html#performance-by-task",
    "title": "An AI Which Imitates Humans Can Beat Humans",
    "section": "Performance by Task",
    "text": "Performance by Task\nArithmetic: computers passed humans 300 years ago. Machines have been used to do calculations since the 17th century, e.g. Pascal’s calculator from 1642.\n\n\n\n\n\nBackgammon\n1979\n\n\nChess\n1997\n\n\nJeopardy\n2005\n\n\nAtari games\n2013\n\n\nGo\n2016\n\n\nStarcraft\n2019\n\n\n\n(source)\nPlaying games: computers passed humans over the last 45 years. See the table in the margin for games. I am not aware of any well-known games in which computers cannot reliably beat the best humans.\n\n\n (source)\nImage recognition: computers surpassed humans in the 2010s. With the qualifications above about the limitations of benchmark tasks.\nQuestion answering: computers surpassed humans in the 2010s. With the qualifications above about the limitations of benchmark tasks.\nCoding: computers still below expert. See the benchmarks on PapersWithCode, also a graph on OurWorldInData, specifically APPS and MBPP. The best-performing computers are still imperfect at solving these coding challengers (which presumably can be solved by an expert programmer), but progress is rapid.\nWriting persuasive text: computer comparable to average human. A number of recent papers compare the persuasive power of LLM-generated text to human-generated text (Bai et al. (2023), Goldstein et al. (2023), Hackenburg and Margetts (2023), Matz et al. (2023), Palmer and Spirling (2023), Qin et al. (2023)). They all find that LLMs do relatively well, but none show clear signs of computer superiority.\nWriting creative blurbs: computer comparable to average human. Koivisto and Grassini (2023) compared GPT4 to online recruited humans (£2 for a 13 minute task) in giving “creative” uses for everyday items. The prompt was to “come up with original and creative uses for an object”, objects were “rope”, “box”, “pencil” and “candle.” The responses were rated by humans for their “creativity” or “originality.” GPT-4 responses were perhaps 1SD above the average human score, but the difference was smaller when choosing just the best response for each user.\nSummarizing text: computer beats average human. Lee et al. (2023) reports that LLM-generated summaries of articles were preferred by humans to human-generated summaries 80% of the time. However the paper doesn’t give much detail about the conditions of the humans writing the summaries.33 “RLAIF summaries are preferred over the reference [human-written] summaries 79% of the time, and RLHF are preferred over the reference summaries 80% of the time.”\nDoing math problems: computer comparable to expert. The latest score on the MATH benchmark is 84%, compared to 90% by a three-time IMO gold medalist. The scores have been rising very rapidly so it seems likely that computers will soon surpass humans.44 Hendrycks et al. (2021) says “We found that a computer science PhD student who does not especially like mathematics attained approximately 40% on MATH, while a three-time IMO gold medalist attained 90%”"
  }
]