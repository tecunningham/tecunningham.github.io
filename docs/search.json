[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Knowledge-Creating LLMs\n\n\n\n\n\n\n\n\n\n\n\nFeb 6, 2026\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nLLM verification\n\n\n\n\n\n\n\n\n\n\n\nDec 30, 2025\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nForecasts of AI & Economic Growth\n\n\n\n\n\n\n\n\n\n\n\nNov 6, 2025\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nEconomics and Transformative AI\n\n\n\n\n\n\n\n\n\n\n\nOct 2, 2025\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nOn Deriving Things\n\n\n\n\n\n\n\n\n\n\n\nJan 30, 2025\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nToo Much Good News is Bad News\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2024\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nPremature Optimization and the Valley of Confusion\n\n\n\n\n\n\n\n\n\n\n\nMay 10, 2024\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nPeer Effects, Culture, and Taxes\n\n\n\n\n\n\n\n\n\n\n\nApr 28, 2024\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nBloodhounds and Bulldogs\n\n\nOn Perception, Judgment, & Decision-Making\n\n\n\n\n\n\n\n\nApr 27, 2024\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nThe Influence of AI on Content Moderation and Communication\n\n\n\n\n\n\n\n\n\n\n\nDec 11, 2023\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nThe History of Automated Text Moderation\n\n\n\n\n\n\n\n\n\n\n\nNov 18, 2023\n\n\nIntegrity Institute collaborators: Alex Rosenblatt, Jeff Allen, Ejona Varangu, Dave Sullivan, Tom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nThinking About Tradeoffs? Draw an Ellipse\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\nTom Cunningham, OpenAI.\n\n\n\n\n\n\n\n\n\n\n\nExperiment Interpretation and Extrapolation\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nAn AI Which Imitates Humans Can Beat Humans\n\n\n\n\n\n\n\n\n\n\n\nOct 6, 2023\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nSushi-Roll Model of Online Media\n\n\nPreviously: “pizza model”, “salami model”\n\n\n\n\n\n\n\n\nSep 8, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n\n\n\n\n\nHow Much has Social Media affected Polarization?\n\n\n\n\n\n\n\n\n\n\n\nAug 7, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n\n\n\n\n\nThe Paradox of Small Effects\n\n\n\n\n\n\n\n\n\n\n\nAug 2, 2023\n\n\nTom Cunningham, Integrity Institute\n\n\n\n\n\n\n\n\n\n\n\nRanking by Engagement\n\n\n\n\n\n\n\n\n\n\n\nMay 8, 2023\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nSocial Media Suspensions of Prominent Accounts\n\n\n\n\n\n\n\n\n\n\n\nJan 31, 2023\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nOptimal Coronavirus Policy Should be Front-Loaded\n\n\n\n\n\n\n\n\n\n\n\nApr 5, 2020\n\n\n\n\n\n\n\n\n\n\n\nOn Unconscious Influences (Part 1)\n\n\n\n\n\n\n\n\n\n\n\nDec 8, 2017\n\n\n\n\n\n\n\n\n\n\n\nThe Work of Art in the Age of Mechanical Production\n\n\n\n\n\n\n\n\n\n\n\nSep 27, 2017\n\n\n\n\n\n\n\n\n\n\n\nRepulsion from the Prior\n\n\n\n\n\n\n\n\n\n\n\nMay 26, 2017\n\n\n\n\n\n\n\n\n\n\n\nThe Repeated Failure of Laws of Behaviour\n\n\n\n\n\n\n\n\n\n\n\nApr 15, 2017\n\n\nTom Cunningham\n\n\n\n\n\n\n\n\n\n\n\nEconomist Explorers\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2017\n\n\n\n\n\n\n\n\n\n\n\nSamuelson & Expected Utility\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2017\n\n\n\n\n\n\n\n\n\n\n\nWeber’s Law Doesn’t Imply Concave Representations or Concave Judgments\n\n\n\n\n\n\n\n\n\n\n\nFeb 25, 2017\n\n\n\n\n\n\n\n\n\n\n\nRelative Thinking\n\n\n\n\n\n\n\n\n\n\n\nApr 30, 2016\n\n\nTom Cunningham\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html",
    "href": "posts/2023-01-31-social-media-suspensions-data.html",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "",
    "text": "Tom Cunningham. (@testingham) First version Jan 31 2023, data last updated April 2023, text updated July 2024.1\nThis note describes the suspension practices of the major social media platforms. I have collected a dataset of around 200 suspensions of prominent people across 12 platforms between 2011 and early 2023, stored in a google spreadsheet. The chart below summarizes the full dataset:\nThe data helps illuminate what platforms are doing. It is very difficult for an outside observer to see how a platform moderates their content. The advantages of studying the suspension of prominent users are that (1) the data is public and (2) the outcomes are comparable across platforms.\nKey findings.\nI am working on a separate essay about why platforms suspend users. It is difficult to give clear reasons why platforms suspend users. In a separate essay I try to break down how much their action can be attributed to influence from owners, from employees, from users, from advertisers, or from governments. Having this dataset of suspensions is very useful to be able to make generalizations about platform behavior."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#politicians",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#politicians",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Politicians",
    "text": "Politicians\nAmong US Federal politicians only Republicans have been suspended. In the US 8 Republicans have had one or more suspension, but no Democrats. Among the Republicans the suspensions were for a variety of reasons: related to the Jan 6 riots (Trump, Barry Moore, MTG), related to COVID (Ron Johnson, Rand Paul, MTG), for misgendering (Jim Banks), for tweeting a threat (Briscoe Cain), for animal blood on a profile photo (Steve Daines), one by a rogue employee (Trump).\nIt seems to me that the asymmetry in suspensions is primarily due to Republicans being more likely to violate the policies, rather than asymmetric enforcement of existing policies. I am not aware of any cases where a Democratic politician violated one of these policies but was not suspended.\n\n\n\n\n\n\n\n\n\nSuspension of national politicians outside the US has been relatively rare. My dataset contains 13 national politicians who were suspended in the world outside the US, compared to 8 in the US. This is a big asymmetry, and something of a puzzle. I have discussed this with a number of people who worked in enforcement and they attribute to a mixture of (1) less policy-violating behaviour from non-US politicians; (2) looser enforcement against non-US politicians; (3) lower overall social media usage outside the US; and (4) lower coverage of non-US politicians in my dataset."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#us-prominent-figures",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#us-prominent-figures",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "US Prominent Figures",
    "text": "US Prominent Figures\nThis shows all suspensions of US “notable people”:\n\n\n\n\n\n\n\n\n\nBetween 2015 and 2017 there were a series of alt-right personalities suspended from Twitter. The suspensions were often not for their views but their behaviour:\n\n2015: Charles Johnson from Twitter for a threat.\n2016: Milo Yiannopoulos from Twitter for harassment, Richard Spencer from Twitter for manipulation.\n2017: Roger Stone from Twitter for abuse.\n2018: Alex Jones from Twitter for incitement and abuse.\n\nBeginning in late 2017 more alt-right accounts were suspended. Either for hate speech, for offline behaviour, or without any public reason given:\n\nLate 2017: Baked Alaska from Twitter for hate speech.\n2018: Owen Benjamin from Twit with no reason given, Alex Jones from FB and YouTube for hate speech.\n2019: Nick Fuentes from Meta with no reason given.\n\nBetween November 2020 and January 2021 a large set of prominent figures were suspended for election-related reasons. The most suspensions were on Twitter but there were also from other platforms.\n\nSince November 2022 Twitter has unsuspended a large fraction of the suspended users that I track, probably around 1/2.\nSome people have been suspended simultaneously across multiple platforms (e.g. Alex Jones)"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#meta",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#meta",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Meta",
    "text": "Meta"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#twitter",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#twitter",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Twitter",
    "text": "Twitter\n\n\n\n\n\n\n\n\n\nThe following chart shows just accounts that were un-suspended under Musk, i.e. people with Twitter suspension that started before Oct 27 2022 and ended after that date. See below for a more fine-grained dataset of accounts unsuspended under Musk.\nYou can see that the primary original reasons for suspension were hate speech COVID misinformation. Kanye West and Nick Fuentes were re-suspended under Musk."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#youtube",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#youtube",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "YouTube",
    "text": "YouTube"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#tik-tok",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#tik-tok",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Tik Tok",
    "text": "Tik Tok"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Other Platforms",
    "text": "Other Platforms"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#hate-speech",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#hate-speech",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Hate Speech",
    "text": "Hate Speech"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#twitter-1",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#twitter-1",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Twitter",
    "text": "Twitter\nWikipedia page on Twitter Suspensions. Wikipedia has a list of around 400 Twitter suspensions. I chose not to create my own database (partly drawing from Wikipedia) for a few reasons: (1) I would want to add a lot of annotations to the Wikipedia data, e.g. about reasons for suspension or types of suspension. (2) Parsing the data is nontrivial: date ranges are given in various formats and would require some work on a regex to parse consistently. (3) There is some missing and inconsistent data, e.g. it has Trump’s suspension start-date but not end-date, and the names of people are not consistent (e.g. sometimes “Donald Trump”, sometimes “Donald J Trump”).\nThe Wikipedia dataset shows a similar basic pattern to what I document above: a dramatic increase in the rate of suspensions around mid-2017\n\n\n\n\n\nWikipedia-reported Twitter suspension by year\n\n\n\n\n\n\n\n\n\nAll Wikipedia-reported Twitter suspension, highlighting accounts with more than 1M followers (not all suspensions list the number of followers).\n\n\n\n\nTravis Brown: Twitter Watch This project appears to have data on almost all suspensions on Twitter since Feb 2022, and also tracks whether the suspension have been reversed. It does not include any suspensions which started prior to Feb 2022. There is a giant CSV file with 600K rows, suspensions.csv. Some visualizations:\n\n\n\n\n\nObservations by date of suspension\n\n\n\n\n\n\n\n\n\nObservation by date of unsuspension\n\n\n\n\n\n\n\n\n\nObservation by date of account creation\n\n\n\n\n\n\n\n\n\nSuspensions for accounts with &gt;1M followers\n\n\n\n\nTravis Brown: Twitter Unsuspensions. This is a collection of users who Twitter has un-suspended since Oct 27 2022 (when Musk took over). For some accounts there is a date of suspension but some have missing dates, I think suspension-date is only observed if after Feb 2022. (The content of this dataset is neither a subset nor a superset of the previous daatset). Unfortunately the dataset doesn’t have follower-count or twitter handle, so it’s not easy to join with other datasets or find the most prominent accounts.\n\n\n\n\n\nObservations by date of suspension\n\n\n\n\n\n\n\n\n\nObservations by date of unsuspension\n\n\n\n\nTravis Brown: Deleted Tweets / Suspended Accounts. This project scrapes profiles from the Wayback Machine, and seems to have a large set of accounts that were suspended with fairly long retention, I have not yet investigated further.\nTwitter Transparency Reports. This has data on the aggregate number of suspensions per half between July 2018 and Dec 2021. Note that the website is down but the CSV files can still be downloaded. \n\n\n\n\n\nTotal Accounts Suspended on Twitter by Reason, 2018H2-2021H2\n\n\n\n\nCounterHate list of unsuspensions. The organization CounterHate has a list of 10 large accounts reinstated by Twitter since Musk’s takeover. Note I believe they incorrectly listed Rizza Islam as an account re-activated by Twitter: I can find no evidence that the acccount @RizzaIslam was ever suspended, it seems to have been continuously tweeting from November 2022 through Feb 2023. I have added all 10 accounts to my database, and checked activity across all platforms."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#youtube-1",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#youtube-1",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "YouTube",
    "text": "YouTube\nWikipedia page on YouTube suspensions. See above for reasons why I chose not to use this dataset as the primary source.\nWikitubia: Terminated YouTubers. A list of around 2300 YouTubers that have been permanently banned, including date of ban, subscribers, reason for ban, and citation. They don’t have a date when unbanned."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#meta-facebook-instagram",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#meta-facebook-instagram",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Meta / Facebook / Instagram",
    "text": "Meta / Facebook / Instagram\nThere is no Wikipedia page of suspensions on Facebook, Instagram or WhatsApp.\nMeta’s “Community Standards Enforcement Report” is shown below. Meta’s data does not include any data on account suspensions, however there are a few other patterns of interest.\n\nContent actioned is relatively stable. There are fairly few notable upward or downward trends across the different types of content actioned: terrorism content actioned has increased significantly on both platforms, hate speech actions increased up to the end of 2020, then declined.\nThe proactive detection rate is close to 100% for most categories. there were dramatic improvements for bullying and for hate speech over 2017-2021. Note that the proactive detection rate is the share of actioned content that is automatically detected, the share of true positives that are automatically detected is surely much lower.\nThe prevalence of volations has fallen significantly. The log axis diminishes the magnitude of the decline: prevalence has fallen by a factor of 2-5 for nudity, bullying, hate speech, and graphic content. (I only show the prevalence upper bound, but the lower bound generally tracks the same course).\n\n\n\n\n\n\n\n\n\n\nFacebook’s dangerous organizations list. This list was leaked in 2021 by the Intercept. Unfortunately it does not include the dates of when each organization was added. The list is organized into the following categories:\n\nTerror Organizations (e.g. Islamic State)\nCrime Organizations (e.g. Bloods, Crips)\nHate Organizations (e.g. Aryan Nation, includes bands and websites)\nMilitarized Social Movements (e.g. United States Patrio Defense Force)\nViolent Non-State Actors (e.g. Free Syrian Army)\nHate (e.g. David Duke)\nIndividuals: Crime (e.g. Denton Suggs, Gangster Disciples)\nIndividuals: Terror (e.g. Osama bin Laden)"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#tiktok",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#tiktok",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "TikTok",
    "text": "TikTok\nCommunity Standards Report. Shows an increase in suspensions from around 1M accounts/quarter per 2020 to 6M accounts/quarter in 2023."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#twitch",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#twitch",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Twitch",
    "text": "Twitch\nStreamerBans. They seem to have a pretty comprehensive database of bans on Twitch."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms-1",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#other-platforms-1",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Other Platforms",
    "text": "Other Platforms\n\nSpotify. The only unambiguous suspension from Spotify I found was Alex Jones’ podcast. Spotify removed some episodes of Joe Rogan’s podcast, and removed R Kelly and XXXtentacion’s music from playlists. They remove some white-supremacist artists and music. They removed all music from the band LostProphets after their lead singer was convicted of child sexual abuse.\nSubstack. I’m not aware of anybody who’s been kicked off Substack, they present themselves as very pro-free-speech.\nReddit. I’m not aware of any data on reddit account suspensions.\nRumble. The Rumble video-hosting platform has become quite large (they claim 70M MAU, and have a market cap of ). Their terms of service restrict content that is “abusive, inciting violence, harassing, harmful, hateful, anti-semitic, racist or threatening.” However I have not yet found a single example of a prominent user who has been suspended from Rumble."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#other-data-sources",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#other-data-sources",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Other Data Sources",
    "text": "Other Data Sources\nCan find more suspensions by searching Wikipedia for “suspended from XXX”. E.g. site:wikipedia.org \"suspended from facebook\". Possibly worth doing the same search for Google News.\nSocialBlade has data on number of followers by month since 2018, across Twitter, FB, YouTube. I’m not sure how easy it would be to scrape this data. They have a paid API, they say “up to 3 years of Historical statistics on creators.” However the website seems to have data back to at least April 2018.\nBallotpedia list of elected officials suspended from social media. It is an excellent resource, appears comprehensive and cites original reporting. I have added all of their data to the database as of January 2023.\nGlobal Internet Forum to Counter Terrorism (GIFCT). They mainly work on sharing hashes of terrorist content between platforms. They have some dicussion papers about “terror designation lists” but I don’t think they maintain any lists themselves.\nSpecially Designated National / Global Terrorist (SDN/SDGT). This is a public list maintained by the US government, and consumed by a number of tech companies. The full history is available, but it would be extremely difficult to parse.\nLumen. This has an international database of government takedown requests. They also seem to include whether the request was honored.\nCCDH Disinformation Dozen. This is a list from March 2021 of prominent accounts who were spreading anti-vax information on social media: original report, followup report from April 2021). They also have a “toxic ten” report. It’s probably worth adding both lists to the database."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#third-party-sources-on-platform-policies",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#third-party-sources-on-platform-policies",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Third-Party Sources on Platform Policies",
    "text": "Third-Party Sources on Platform Policies\nThere are a variety of third-party resources comparing policies across platforms, however none seem to have data comparable to the list above, i.e. a summary of specific content policy changes over time.\nComparisons at a single point in time.\n\nDNC (2020) Comparison of Misinformation Policies, 2020\nConsumer Reports (Aug 13 2020) Comparison of Misinformation Policies, 2020. Data as of 2020, with three levels: “allowed”, “sometimes”, and “prohibited”.\nUNC CITAP (May 22 2020) Comparison of Misinformation Policies, 2020. Has tables comparing misinfo policies as of 2020, three levels: “prohibited”, “flagged”, “allowed.”\nElection Integrity Partnership (Oct 28 2020) Comparison of Election Policies, 2020.\nCarnegie Endowment (April 1 2021) Existence of Policies, 2021. Table just marks whether a platform has a policy on some type of content, not nature of policy. They also they have a database of platform policies but it seems to only have data from February 2021.\nVirality Project Comparison of COVID Vaccine Policies in 2021.\n\nPolicies tracked over time.\n\nMchangama, Fanlo and Alkiviadou (2023) Scope Creep: An Assessment of 8 Social Media Platforms’ Hate Speech Policies. They document the hate speech policies over time for 8 platforms using a consistent rubric. They document that hate speech policies have become more broad-reaching over time. The data is available in Excel sheets here.\nKatie Harbath and Collier Fernenkes (August 2022) Election Policy Announcements, 2003-2022. Google spreadsheet with links to around 600 policy announcements, organized by platform, author, date, product-type, and country. Focussed on election-related policies, and they don’t include summaries of the policy announcement. They also wrote up analyses: (1) “A Brief History of Tech and Elections”; (2) 2022 election announcements.\nRanking Digital Rights Index, Comparison of Privacy and Transparency Policies, 2017-2022. They collect perhaps 100 different indicators across around 15 tech companies, mostly related to privacy and transparency, earliest data from 2017. All the data is available.\nGLAAD Comparison of LGBTQ user safety, 2021-2022\nCELE, Letra Chica. Tracks all public policy changes on Meta, YouTube, and Twitter. Most data from May 2020, but they go back to 2019 for Facebook by using FB’s Transparency Center. Each policy update includes a short summary of what’s changed. Tracks both Spanish and English versions. Data stored on coda.io, I think it’s queryable.\nLinterna Verdes, Circuito. Has about 15 in-depth case studies of platform moderation decisions.\nHumboldt Institute, Platform Governance Archive. Comprehensive archive of ToS, Privacy Policy, and Community Guidlines, from 2004 until late 2021, for FB, IG, Twitter, and YouTube. The data will not be updated.\nOpen Terms Archive. Started by the French Ambassador for Digital Affairs, but now a collaboration. Tracks terms for many different online services in a github repo. The Platform Governance Archive has moved to be part of this project, here.\nEFF, TOSback. Database of historical ToS documents from different services, with cross-platform comparisons. The most recent updates seem to be from May 2021, possibly was succeeded by Open Terms Archive.\nEuropean Commission, Copyright Content Moderation and Removal. This PDF report includes a lot of work which maps the copyright policies of major platforms.\n\nNarrative histories:\n\nCatherine Buni and Soraya Chemaly (2016, the Verge) History of Moderation.\nSarah Jeong (2016, Vice) The History of Twitter’s Rules\nBergen (2022) Like, Comment, Subscribe. A book on the history of YouTube, it has a lot of detail on policy changes."
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-film-and-television",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-film-and-television",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Exclusions in Film and Television",
    "text": "Exclusions in Film and Television\n\n\n\n\n\n\n\n\n\n\n\nalleged crime\nresult\n\n\n\n\n1920s\nFatty Arbuckle\nrumors of immorality\nfilm industry blacklisted\n\n\n\n\n\n\n\n\n1940s\nOrson Welles\ncommunist associations\nblacklisted, moved to Switzerland\n\n\n\nDalton Trumbo\ncommunist associations\nblacklisted\n\n\n\n(around 100 people)\ncommunist associations\nblacklisted for a decade\n\n\n\n\n\n\n\n\n1950s\nCharlie Chaplin\ncommunist associations\nbanned from US\n\n\n\nElia Kazan\ntestifying before HUAC\nlost some relationships in Hollywood\n\n\n\n\n\n\n\n\n1960s\nJane Fonda\nopposition to Vietnam war\nblacklisted\n\n\n\n\n\n\n\n\n1970s\nRoman Polanski\nrape of 13yo girl\nmild disapproval from Hollywood\n\n\n\n\n\n\n\n\n1990s\nO J Simpson\nmurdered his wife\nblacklisted\n\n\n\nWoody Allen\nmolested 7yo daughter\n\n\n\n\n\n\n\n\n\n2000s\nMel Gibson\nracism & anti-semitism\n“blacklisted in Hollywood for almost a decade”\n\n\n\nMira Sorvino\nrejecting Harvey Weinstein\nblacklisted\n\n\n\nRose McGowan\nrejecting Harvey Weinstein\nblacklisted\n\n\n\nIsaiah Washington\nhomophobic remarks\nblacklisted\n\n\n\nMichael Richards\nracist remarks\nblacklisted\n\n\n\nKathy Griffin\n“told Jesus to suck it”\nbanned from talk shows and TV appearances\n\n\n\nSean Penn\nopposition to Iraq war\ndropped from movie\n\n\n\n\n\n\n\n\n2010s\nBill Cosby\nsexual assault\nblacklisted (& later imprisoned)\n\n\n\nHarvey Weinstein\nsexual assault\nblacklisted (& later imprisoned)\n\n\n\nStacy Dash\nconservative advocacy\nblacklisted\n\n\n\nKirk Camerson\ncriticism of homosexuality\nblacklisted\n\n\n\nJames Woods\nanti-Obama tweets\nblacklisted\n\n\n\nCeeLo Green\nsexual assault\nblacklisted\n\n\n\nLouis CK\nsexual harassment\nblacklisted\n\n\n\nKathy Griffin\nphoto with head of Trump\nfired by CNN, lost endorsement, cancelled tour\n\n\n\nT J Miller\nsubstance abuse, sexual assault\nblacklisted\n\n\n\nGina Carano\npolitical social media posts\nfired from TV show\n\n\n\nKevin Spacey\nsexual harassment\nlost roles in films\n\n\n\nJussie Smollett\nlied about an attack\nlost roles in TV shows\n\n\n\nNeil deGrasse Tyson\nrape, sexual harassment\ntemporarily lost roles in TV shows\n\n\n\nRoseanne Barr\nracist tweet\nlost TV show\n\n\n\n\n\n\n\n\n2020s\nWill Smith\nslapping someone at Oscars\nfilm projects put on hold\n\n\n\nJohnny Depp\ndomestic violence\nlost roles in films\n\n\n\nAmber Heard\ninvolvement in trial w Johnny Depp\nlost roles in films\n\n\n\nJustin Roiland\nsexual harassment & abuse\nlost roles in shows"
  },
  {
    "objectID": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-music",
    "href": "posts/2023-01-31-social-media-suspensions-data.html#exclusions-in-music",
    "title": "Social Media Suspensions of Prominent Accounts",
    "section": "Exclusions in Music",
    "text": "Exclusions in Music\n\n\n\n\n\n\n\n\n\n\n\nalleged crime\nresult\n\n\n\n\n1940s\nPaul Robeson\ncommunist associations\nblacklist and passport revoked\n\n\n\n\n\n\n\n\n1950s\nLeonard Bernstein\ncommunist associations\nbrief blacklist\n\n\n\nLena Horne\ncommunist associations\nblacklist\n\n\n\nPete Seeger\ncommunist associations\nblacklist\n\n\n\n\n\n\n\n\n1960s\nBeatles\nsaying they’re bigger than Jesus\nconsumer boycott\n\n\n\nLovin Spoonful\ncooperating with FBI\nmusic industry boycott\n\n\n\nNina Simone\n“Mississippi Goddam”\nboycott in the South\n\n\n\nJohn Lennon\ncriticism of US and Vietnam war\nrefused entry into US\n\n\n\nEartha Kitt\ncriticism of Vietnam war\nblacklist through LBJ and CIA\n\n\n\n\n\n\n\n\n1970s\nSex Pistols\ncriticizing the Queen, swearing on TV\nbanned by the BBC, dropped by EMI\n\n\n\n\n\n\n\n\n1980s\nNWA\n“Fuck the Police” & similar songs\nradio station boycott, police boycott\n\n\n\n\n\n\n\n\n1990s\nBruce Springseen\nsong against police brutality\nbrief police boycott\n\n\n\nMarilyn Manson\ntransgressive lyrics\nbanned from performing in some states\n\n\n\nBody Count\nsong “cop killer”\nalbum withdrawn and reissued\n\n\n\n\n\n\n\n\n2000s\nDixie Chicks\nfor opposition to Iraq war\nblacklisting and consumer boycott\n\n\n\nJanet Jackson\nshowing nipple\nVH1, MTV, & Viacom radio stopped playing her music\n\n\n\nR Kelly\nsexual abuse\nbroad blacklist\n\n\n\nChris Brown\ndomestic violence\nweak boycott and blacklist\n\n\n\n\n\n\n\n\n2010s\nLostprophets\nsexual abuse\nbroad blacklist\n\n\n\nMichael Jackson\nchild molestation\nsome radio stations stop playing music\n\n\n\n\n\n\n\n\n2020s\nBeyonce\nsong against police brutality\nbrief police boycott\n\n\n\nMorgan Wallen\nusing n-word\ntemporarily dropped from radio/streaming playlists\n\n\n\nKanye West\npraise of Hitler\nlost sponsors\n\n\n\nOthers.\n\nIn radio: Father Coughlin, Rush Limbaugh, Don Imus fired from CBS for calling womens’ basketball team “nappy-headed hos”, Howard Stern fired from various radio shows for comments.\nIn sport. Colin Kapaernick blacklisted from NFL for kneeling for the anthem. Pete Rose banned from MLB for gambling.\nNazi sympathisers/collaborators. Charles Lindbergh, Henry Ford, Charles Coughlin, PG Wodehouse, Ezra Pound.\nWriters: DH Lawrence, Henry Miller, Salman Rushdie (Nicole Bonoff).\nJournalists. Jeffrey Toobin (New Yorker writer masturbated on zoom call),\nNote on R Kelly disappearing from radio"
  },
  {
    "objectID": "posts/2025-10-19-forecasts-of-AI-growth.html",
    "href": "posts/2025-10-19-forecasts-of-AI-growth.html",
    "title": "Forecasts of AI & Economic Growth",
    "section": "",
    "text": "Validation Checks\n\nOverall: ⚠️ Warning\n\n✅ [36/36] Cited sources exist in posts/ai.bib (programmatic)\n✅ [26/26] Table rows have required fields (programmatic)\n✅ [26/26] QMD quotes match posts/ai.bib (programmatic)\n✅ [26/26] QMD growth values match posts/ai.bib (programmatic)\n⚠️ [34/36] Abstracts present for all cited sources (programmatic)\n❌ [15/18] Bib quotes present in local fulltext version (programmatic)\n\nLast checked: 2026-02-22"
  },
  {
    "objectID": "posts/2025-10-19-forecasts-of-AI-growth.html#footnotes",
    "href": "posts/2025-10-19-forecasts-of-AI-growth.html#footnotes",
    "title": "Forecasts of AI & Economic Growth",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI classified Epoch’s GATE model (Erdil et al. (2025)) as by “AI people”, though the authors are a mixture of academic economists and people who work in AI.↩︎\nIt seems to me quite plausible that these papers over-estimate the productivity impact of existing LLMs: (1) the AB tests showing productivity improvements are on unrepresentatively self-contained tasks and are likely distorted by publication selection; (2) the Eloundou et al. (2023) estimates of very large time-savings from GPT-4 are based just on intuitions.↩︎\nComin and Mestieri (2014) say “the average adoption lag across all technologies (and countries) is 44 years,” but since the 1950s it has been 7-18 years.↩︎\n“Between 1 and 5% of all work hours are currently assisted by generative AI, and respondents report time savings equivalent to 1.4% of total work hours. … implies a potential productivity gain of 1.1%.”↩︎\nSuppose the total valuation of AI-related companies is $10T, which is perhaps around 10% of all capital stock. Using P/E of 15, a $10T valuation implies a stream of $600B in earnings/year, which is 2% of GDP.↩︎"
  },
  {
    "objectID": "posts/2026-01-29-knowledge-creating-llms.html",
    "href": "posts/2026-01-29-knowledge-creating-llms.html",
    "title": "Knowledge-Creating LLMs",
    "section": "",
    "text": "Thanks to Zoë Hitzig & Parker Whitfill, among others, for helpful comments."
  },
  {
    "objectID": "posts/2026-01-29-knowledge-creating-llms.html#footnotes",
    "href": "posts/2026-01-29-knowledge-creating-llms.html#footnotes",
    "title": "Knowledge-Creating LLMs",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMany other technologies share knowledge – speaking, writing, printing, the internet – LLMs just continue this progression but further lower the costs of sharing.↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Tom Cunningham",
    "section": "",
    "text": "Economics/AI Research, METR (ex-OpenAI)."
  },
  {
    "objectID": "about.html#recent-talks",
    "href": "about.html#recent-talks",
    "title": "About Tom Cunningham",
    "section": "Recent Talks",
    "text": "Recent Talks\n\n\n\nDec 11 2025\nBay Area Tech-Economics seminar\n\n\nDec 5-6 2025\nZurich Economics of AI Keynote\n\n\nNov 25 2025\nBank of England Methods Seminar\n\n\nOct 24 2025\nChicago Center for Applied AI seminar\n\n\nOct 23 2025\nYale Research in Motion seminar\n\n\nSep 15 2025\nStanford lab talk, Erik Brynjolfsson lab"
  },
  {
    "objectID": "about.html#blog",
    "href": "about.html#blog",
    "title": "About Tom Cunningham",
    "section": "Blog",
    "text": "Blog"
  },
  {
    "objectID": "about.html#resume",
    "href": "about.html#resume",
    "title": "About Tom Cunningham",
    "section": "Resume",
    "text": "Resume"
  },
  {
    "objectID": "about.html#working-papers",
    "href": "about.html#working-papers",
    "title": "About Tom Cunningham",
    "section": "Working Papers",
    "text": "Working Papers\n\n2025: How People use ChatGPT, with Aaron Chatterji, David J. Deming, Zoe Hitzig, Christopher Ong, Carl Yan Shan & Kevin Wadman.\n2023: Implicit Preferences, with Jon de Quidt.\n2019: Interpreting Experiments with Multiple Outcomes (presented at CODE 2019) with Josh Kim.\n2015: Biases and Implicit Knowledge\n2015: Equilibrium Persuasion with Ines Moreno de Barreda.\n2013: Comparisons and Choice\n2013: Relative Thinking and Markups"
  },
  {
    "objectID": "about.html#publications",
    "href": "about.html#publications",
    "title": "About Tom Cunningham",
    "section": "Publications",
    "text": "Publications\n\n2025: Ranking by Engagement and Non-Engagement Signals: Learnings from Industry, with Sana Pandey, Leif Sigerson, Jonathan Stray, Jeff Allen, Bonnie Barrilleaux, Ravi Iyer, Mohit Kothari, Behnam Rezaei, Sanjay Kairam, Smitha Milli.\n2019: Improving Treatment Effect Estimators Through Experiment Splitting, WWW, with Dominic Coey.\n2013: The Incumbency Effects of Signalling  Economica, with Ines Moreno de Barreda, Francesco Caselli, and Massimo Morelli.\n2009: Leader Behaviour and the Natural Resource Curse Oxford Economic Papers, with Francesco Caselli."
  },
  {
    "objectID": "about.html#miscellaneous",
    "href": "about.html#miscellaneous",
    "title": "About Tom Cunningham",
    "section": "Miscellaneous",
    "text": "Miscellaneous\n\n2010: “LSE CEP 2010 Election Analysis: Macroeconomics and Public Finance” (with Ethan Ilzetski)\n\nComment pieces for the Guardian (2008/2009)\n\nDon’t worry about inflation\nThe great financial stitch-up\nThe rate cut wasn’t big enough\nStuck in the middle\nWe need to outgrow our debt\nYou say inflation, I say deflation\nConfidently predicting darker days"
  },
  {
    "objectID": "posts/2026-02-26-hallucinations-and-alignment.html",
    "href": "posts/2026-02-26-hallucinations-and-alignment.html",
    "title": "Knowledge of Success",
    "section": "",
    "text": "LLMs are much more useful if they tell you their confidence.\n\nWhen asking an LLM to do a task it’s reassuring to know that it’s successful in some fraction of cases, but it’s far more useful to know which cases it’s successful in:\n\nIf you’re answering a question, I want to know your confidence in the answer.\nIf you’re supplying a proof for a theorem, I want to know whether you think the proof is valid.\nIf you’re fixing a bug, I want to know if you think the fix is going to work.\nIf you’re writing a poem for me, I want to know if the poem is good.\n\nUnfortunately until recently LLMs were trained just to maximize their success rates, and for that reason they often wouldn’t report useful signals of confidence, which made them much less useful. This I think is a good explanation of why LLMs hallucinate (argued in Kalai et al. (2025)), but the same logic illuminates some other cases.\nThe discussion below mostly follows Kalai et al. (2025), but adds some arguments and visualization I did when working with those authors in 2024 at OpenAI. I think these points are fairly well-known within the industry but ought to be better known outside it.\n\nThis follows from a very simple model.\n\nSuppose I have to make a choice among \\(N\\) options, and I have no priors about which is most likely to be right. Then it’s fine if the LLM just tells me which is the most-likely option, without telling me its probability.\nHowever if we add a touch of realism, then it suddenly becomes much more useful if the LLM tells me its probability (or it admits when it doesn’t know). This will happen if any of the following are true: (1) I have some private information about the different options; (2) I can choose to spend some time verifying the proposed option, or searching for solutions; (3) I have the option of abstaining and not making a choice.\n\nSome implications.\n\n\nModels seem overconfident because they are trained only on accuracy, not on calibration. When models aren’t allowed to fold they learn to bluff.\nThe value of a model will be a convex in its accuracy. Going from 90% to 100% accuracy is more than twice as valuable as going from 80% to 90%, because it lowers the cost of verification, and lowers the likelihood of abstention. This is only true when models don’t report their confidence.\nBenchmarks should report both accuracy and reliability. If you’re choosing betwen two models it’s useful to know not just the share of correct responses, but also whether the model will report when it fails (i.e. accuracy).\nModels are good self-critics. Somewhat surprisingly, a model can often identify its own mistakes. This makes sense for models that are trained only on accuracy, not on calibration, because they systematically exaggerate their success.\n\n\nIn this post.\n\nI state the model very briefly and give a nice visual aid, to show the optimal threshold for abstaining.\nI also show how we can use a simplex diagram to illustrate tradeoffs between accuracy and confidence, showing both the frontier (plotting results from benchmarks) and ."
  },
  {
    "objectID": "posts/2026-02-26-hallucinations-and-alignment.html#chronological-sketch",
    "href": "posts/2026-02-26-hallucinations-and-alignment.html#chronological-sketch",
    "title": "Knowledge of Success",
    "section": "Chronological sketch",
    "text": "Chronological sketch\n\nChow (1970): introduces the Bayes-optimal reject option (“indecision”) and derives the posterior-threshold rule.\nHerbei and Wegkamp (2006): formalizes “classification with a reject option” in modern statistical learning terms; there are many followups on learning and using reject policies (e.g. (Bartlett and Wegkamp 2008; El-Yaniv and Wiener 2010; Geifman and El-Yaniv 2019)).\nKadavath et al. (2022): finds that language models can often estimate whether their own answers are correct, which is exactly the signal needed to implement a threshold rule in practice.\nRecent LLM work tries to implement the missing abstain/verify channel via refusal-aware tuning and explicit “IDK” tokens (Zhang et al. 2024; Cohen et al. 2024), verification loops (Dhuliawala et al. 2023; Altinisik et al. 2026), and black-box uncertainty proxies like sampling-based checks or semantic uncertainty (Manakul, Liusie, and Gales 2023; Farquhar et al. 2024).\nKalai et al. (2025): argues LLM hallucinations are a predictable consequence of binary grading that penalizes abstention; proposes a scoring rule that makes abstaining optimal below a stated confidence threshold.\n\n\nChow (1970): Optimal reject rules\nChow (1970) introduces the reject option (which he calls the “Indecision class” \\(I\\)) into pattern recognition and derives the optimal error-reject tradeoff. Chow’s terminology maps directly onto ours:\n\n\n\nChow’s term\nOur term\n\n\n\n\ncorrect recognition\nsucceed\n\n\nerror (misclassification)\nfail\n\n\nrejection / indecision\nabstain\n\n\n\nChow’s setup adds a third action (reject) to ordinary classification. In one common normalization, the costs are \\(0\\) for a correct classification, \\(1\\) for an error, and \\(t\\) for a rejection.\nThe key result (“Chow’s rule”) is a posterior-threshold rule: accept and classify when confident enough, otherwise reject: \\[\n\\max_i P(G_i \\mid x) \\ge 1-t\n\\quad\\Rightarrow\\quad \\text{accept;}\n\\qquad\n\\max_i P(G_i \\mid x) &lt; 1-t\n\\quad\\Rightarrow\\quad \\text{reject.}\n\\]\nIt is optimal in the sense that, for a given rejection threshold (equivalently, a given reject rate), no other rule achieves a lower error rate.\nThe threshold \\(t\\) is related to the costs of the three outcomes as \\(t = (C_r - C_c)/(C_e - C_c)\\), where \\(C_e, C_r, C_c\\) are the costs of error, rejection, and correct recognition. In our payoff notation \\((\\pi_s,\\pi_f,\\pi_a)\\), Chow’s rule becomes: predict iff\n\\[\n\\max_y P(y \\mid x) \\ge \\frac{\\pi_a - \\pi_f}{\\pi_s - \\pi_f}.\n\\]\nIn the Marschak-Machina triangle, this threshold corresponds to one of the indifference lines: the boundary between the region where prediction is preferred and the region where abstention is preferred."
  },
  {
    "objectID": "posts/2026-02-26-hallucinations-and-alignment.html#herbei-and-wegkamp-2006-and-followups-learning-a-reject-option",
    "href": "posts/2026-02-26-hallucinations-and-alignment.html#herbei-and-wegkamp-2006-and-followups-learning-a-reject-option",
    "title": "Knowledge of Success",
    "section": "Herbei and Wegkamp (2006) and followups: Learning a reject option",
    "text": "Herbei and Wegkamp (2006) and followups: Learning a reject option\nHerbei and Wegkamp (2006) (and a large followup literature) reframes the reject option as a learning problem: you want a predictor that is accurate on the examples it attempts, while explicitly controlling how often it refuses.\nFor this post, the key takeaway is less about any one algorithm and more about the framing: “abstention is a first-class action” is standard in the statistical decision-theory literature, and the same expected-utility thresholds show up once you make the third outcome explicit."
  },
  {
    "objectID": "posts/2026-02-26-hallucinations-and-alignment.html#kadavath-et-al.-2022-models-can-sometimes-score-their-own-answers",
    "href": "posts/2026-02-26-hallucinations-and-alignment.html#kadavath-et-al.-2022-models-can-sometimes-score-their-own-answers",
    "title": "Knowledge of Success",
    "section": "Kadavath et al. (2022): Models can sometimes score their own answers",
    "text": "Kadavath et al. (2022): Models can sometimes score their own answers\nIf you want the simple \\(p^*\\) rule to be usable, you need some per-question measure of correctness probability (like \\(p_{\\max}(x)\\) or a direct \\(P(\\text{correct}\\mid x)\\) proxy).\nKadavath et al. (2022) study this in language models, finding that they can often estimate whether their own answers are correct. That makes “give the user a probability” a practical interface choice, not just a theoretical one."
  },
  {
    "objectID": "posts/2026-02-26-hallucinations-and-alignment.html#kalai-nachum-vempala-and-zhang-2025-why-language-models-hallucinate",
    "href": "posts/2026-02-26-hallucinations-and-alignment.html#kalai-nachum-vempala-and-zhang-2025-why-language-models-hallucinate",
    "title": "Knowledge of Success",
    "section": "Kalai, Nachum, Vempala, and Zhang (2025): Why language models hallucinate",
    "text": "Kalai, Nachum, Vempala, and Zhang (2025): Why language models hallucinate\nKalai et al. (2025) argue that hallucinations are not a mysterious glitch but a predictable consequence of how models are trained and evaluated. Their central thesis is that the three-outcome structure (succeed, fail, abstain) is systematically distorted by binary evaluation:\nThe paper makes two distinct arguments:\n1. Pretraining origin. Even with error-free training data, the statistical objective of pretraining produces hallucinations. The authors reduce the problem to binary classification (“Is-It-Valid”), showing that\n\\[\n\\text{generative error rate} \\gtrsim 2 \\cdot \\text{IIV misclassification rate}.\n\\]\nFor arbitrary facts (like someone’s birthday) where there is no learnable pattern, the hallucination rate after pretraining is at least the fraction of facts appearing exactly once in the training data.\n2. Post-training persistence. Even after RLHF and other interventions, hallucinations persist because nearly all evaluation benchmarks use binary grading that penalizes abstention:\nThe fix they propose is exactly the payoff structure from our Marschak-Machina framework: penalize errors more than abstentions, with an explicit confidence threshold \\(t\\) stated in the prompt. Their proposed scoring rule awards \\(+1\\) for a correct answer, \\(-t/(1-t)\\) for an incorrect answer, and \\(0\\) for abstaining—so that answering is optimal iff confidence exceeds \\(t\\). This is Chow’s reject-option rule rediscovered in the LLM evaluation context."
  },
  {
    "objectID": "posts/2026-02-26-hallucinations-and-alignment.html#recent-mechanisms-refusal-uncertainty-signals-and-verification",
    "href": "posts/2026-02-26-hallucinations-and-alignment.html#recent-mechanisms-refusal-uncertainty-signals-and-verification",
    "title": "Knowledge of Success",
    "section": "Recent mechanisms: refusal, uncertainty signals, and verification",
    "text": "Recent mechanisms: refusal, uncertainty signals, and verification\nOur model is deliberately abstract: it assumes the user has access to (i) a usable confidence signal \\(p\\) and (ii) an abstain / verify channel. A lot of recent work can be read as ways of engineering those two ingredients:\n\nMake abstention an explicit output. Refusal-aware fine-tuning can teach a model to say “I don’t know” on out-of-knowledge questions (Zhang et al. 2024). Similarly, adding an explicit uncertainty token and training it to soak up probability mass on incorrect predictions effectively adds an abstain action to the model’s output space (Cohen et al. 2024).\nPay a cost to verify. Inference-time verification loops like Chain-of-Verification (CoVe) can be viewed as “spend extra tokens/compute to reduce \\(p_f\\)” (Dhuliawala et al. 2023). Very recent work trains this kind of behavior directly, with structured self-verification traces and an explicit final decision to answer vs abstain (Altinisik et al. 2026).\nConstruct a confidence signal without logits. When output probabilities are unavailable (or untrustworthy), disagreement across samples can act as a proxy confidence signal. SelfCheckGPT does this with sampling-based consistency checks (Manakul, Liusie, and Gales 2023); semantic-uncertainty methods like semantic entropy similarly use semantic variability across generations to predict and filter confabulations (Farquhar et al. 2024), and followup work proposes cheaper “semantic entropy probes” in the same spirit (Kossen et al. 2024).\n\nA cautionary note: neither uncertainty proxies nor “self-verification” are automatically reliable. Some hallucinations happen with high confidence (so uncertainty-based filters can miss them) (Simhi et al. 2025), and in logical reasoning settings models can struggle to identify their own errors (so internal self-checks can fail without external grounding) (Hong et al. 2024).\nBeyond factual QA, Mohamadi, Wang, and Li (2025) show on GSM8K/MedQA/GPQA that replacing binary RLVR rewards with a ternary scheme \\((+1,0,-\\lambda)\\) produces controllable answer-vs-abstain tradeoffs and useful abstention-aware cascades. Jha et al. (2026) report on MedMCQA and Hendrycks Math that moderate abstention rewards reduce wrong answers without collapsing coverage, especially when paired with supervised abstention training. In code generation, Dai et al. (2025) frame the task as “find a correct program or abstain” and use semantic triangulation to improve abstention decisions on LiveCodeBench/CodeElo. Complementarily, Oehri et al. (2025) fuse multiple uncertainty signals into calibrated correctness probabilities and enforce user-specified risk budgets via refusal, including experiments on code generation with execution tests."
  },
  {
    "objectID": "posts/2026-02-26-hallucinations-and-alignment.html#appendix-cross-benchmark-outcome-table",
    "href": "posts/2026-02-26-hallucinations-and-alignment.html#appendix-cross-benchmark-outcome-table",
    "title": "Knowledge of Success",
    "section": "Appendix: Cross-Benchmark Outcome Table",
    "text": "Appendix: Cross-Benchmark Outcome Table\nTo make cross-model plotting easier, the table below standardizes outputs from multiple benchmarks into a common schema.\n\n\n\nbenchmark\nmodel\np_s_pct\np_f_pct\np_a_pct\n\n\n\n\nSimpleQA\nClaude-3-haiku (2024-03-07)\n5.1\n19.6\n75.3\n\n\nSimpleQA\nClaude-3-sonnet (2024-02-29)\n5.7\n19.3\n75.0\n\n\nSimpleQA\nClaude-3-opus (2024-02-29)\n23.5\n36.9\n39.6\n\n\nSimpleQA\nClaude-3.5-sonnet (2024-06-20)\n28.9\n36.1\n35.0\n\n\nSimpleQA\nGPT-4o-mini\n8.6\n90.5\n0.9\n\n\nSimpleQA\nGPT-4o\n38.2\n60.8\n1.0\n\n\nSimpleQA\nOpenAI o1-mini\n8.1\n63.4\n28.5\n\n\nSimpleQA\nOpenAI o1-preview\n42.7\n48.1\n9.2\n\n\nAbstain-QA\nGPT-4 Turbo\n66.1\n19.7\n14.2\n\n\nAbstain-QA\nGPT-4 32K\n72.0\n19.1\n8.9\n\n\nAbstain-QA\nGPT-3.5 Turbo\n61.1\n37.4\n1.5\n\n\nAbstain-QA\nMixtral 8x7b\n54.1\n37.0\n8.9\n\n\nAbstain-QA\nMixtral 8x22b\n59.0\n29.1\n11.9\n\n\n\nNotes: - Table columns are constructed to look like probabilities in the \\((p_s,p_f,p_a)\\) simplex: \\(p_s=\\text{p\\_s\\_pct}/100\\), \\(p_f=\\text{p\\_f\\_pct}/100\\), \\(p_a=\\text{p\\_a\\_pct}/100\\). - For SimpleQA, these correspond directly to {Correct, Incorrect, Not attempted} shares. - For Abstain-QA, these are constructed from the paper’s summary metrics; interpret them as a mapping into a common coordinate system, not as identical underlying evaluation protocols.\n\nData extraction details by source\nSimpleQA (Wei et al., 2024) (Wei et al. 2024). The table uses all model rows shown in the main SimpleQA model-comparison table (8 models). Here, p_s_pct is Correct, p_a_pct is Not attempted, and p_f_pct is computed as \\(100-\\text{Correct}-\\text{Not attempted}\\). I chose this slice because it is the paper’s canonical cross-model summary and directly exposes explicit non-attempt behavior.\nAbstain-QA (Madhusudhan et al., 2024) (Madhusudhan et al. 2024). This is a deliberate subset, not all values in the paper: I take the MMLU / Standard clause / Base rows (5 models) from the main result table. The paper reports AAC (answerable accuracy) and AR (abstention rate). I set \\(p_{a,\\%}=\\text{AR}\\) and construct \\(p_{s,\\%}\\) and \\(p_{f,\\%}\\) by treating AAC as attempt-conditional accuracy: \\[\np_s = \\text{AAC}\\cdot(1-p_a),\\qquad\np_f = (1-\\text{AAC})\\cdot(1-p_a),\n\\] all expressed in percent.\nImportant comparability caveats. Even after mapping all results into \\((p_s,p_f,p_a)\\) coordinates, the underlying tasks and abstention protocols differ: SimpleQA is short-form factual QA with optional non-attempts, while Abstain-QA is multiple-choice QA with an explicit IDK/NOTA option. So the combined table is useful for geometric intuition and directional comparisons, but not for strict leaderboard ranking across benchmarks.\nSources: Wei et al. (2024); Madhusudhan et al. (2024)."
  }
]