Workflows and Automation
Philip Trammell1
December 5, 2025

Labor is typically bundled into jobs, comprising multiple tasks, instead of transacted by the task. This may be because performing a task increases oneâ€™s productivity not only at the task itself but at related tasks. I show that these learning
spillovers can introduce a convexity to the relationship between automation and
output. Automating the first few tasks in a cluster with high spillovers (a â€œworkflowâ€) has little or no effect on output, since workers still perform some automatable tasks to increase their productivity at the non-automatable tasks. Automating
the remainder of the tasks in such a cluster increases output more than automating the first few, as it becomes efficient to adopt the automation fully. Automation
at a larger scale increases output by even more, if machines are intelligent enough
also to â€œlearn by doingâ€ and can do a wider range of tasks than any single human.

1 Digital Economy Lab, Stanford University.

Contact: trammell@stanford.edu. I thank Pamela
Mishkin for the conversation that inspired this paper and Chris Tonetti, Peyman Shahidi, Basil Halperin,
Chad Jones, and Enrique Ide for helpful feedback.

1

1

Introduction

The simple task framework.

Call a simple task model one in which output ex-

hibits constant returns to scale in the performance of a set of necessary tasks (see in
particular Zeira (1998) and Aghion et al. (2019)).1 In these models, each task can be performed by labor and/or capital. As long as capital is plentiful enough, labor performs
only those tasks which capital cannot. Making more tasks automatable increases output by allowing labor to shift to the remaining non-automatable tasks, which are in
shorter supply.
Simple task models have been widely used to forecast the impact on output of
advances in AI. Aghion et al. (2019) use their task-based model to argue that AI may
simply continue a long-running process in which constant growth is sustained by
a constant process of task-by-task automation. Acemoglu (2025) combines a similar
model with Eloundou et al..â€™s (2024) estimates of each O*NET taskâ€™s LLM exposure
to forecast the GDP impact of LLMs. Aghion and Bunel (2024), responding to a draft
of Acemogluâ€™s paper, contest practically all of Acemogluâ€™s assumptions except the
central one: that output is a function of task quantities, where each taskâ€™s quantity is
proportional to the capital or labor assigned to it.
The economic impacts of AI to date seem to have been much smaller than those
predicted by a simple task model. This may be due to what is arguably the central
limitation of such a model, namely that it does not explain the structure of work. In
particular, it does not capture the gains from specialization (why it is efficient for each
worker to perform a small fraction of all tasks), nor the limits to specialization (why
almost every worker performs a set of rather disparate tasks, instead of a single task
or set of almost identical tasks). This paper presents a minimally modified task model
that can accommodate these two facts about how work is structured, and considers
how we should revise our analysis of the impact of advances in task-level automation
in light of it.2
1 Unlike e.g. Acemoglu and Restrepo (2018), we will suppose the set of tasks is fixed.

2 Autor and Thompson (2025) and Freund and Mann (2025) study the labor market impacts of task

automation, especially from AI, under the assumption that each job consists of exogenously fixed bundle
of tasks, or the subset of these that is not yet automated. I offer a stylized account of why jobs are
composed as they are, given the tasks that are and are not automatable. The model predicts various
phenomena which cannot occur in a simple task model with exogenous bundling, e.g. that it may be

2
Specialization due to learning by doing. The standard account of specialization,
dating at least to Smith (1776) and formalized by Yang and Borland (1991), Becker and
Murphy (1992), and much subsequent work is that when a worker performs a task
many times, he is more productive at it. This productivity may come directly from
learning by doing or from the fact that more doing motivates more education. As
Becker and Murphy (1992) show, both can be modeled for our purposes as learning by
doing, as I will do for simplicity.
Literal learning by doing, in turn, may consist of learning general lessons about
how to perform a task efficiently, or of the knowledge of the end of a project that comes
from having performed the beginning. Writing may be a single task, but an author will
write the second chapter of a book much better if she wrote the first chapter, even if
writing the first chapter hardly improved her as a writer in general.3
Task clustering due to learning spillovers. Despite the benefits of specialization,
we are far from working in a fully â€œTaylorizedâ€ economy in which everyone continually performs a single task with minor modifications. Lindbeck and Snower (2000)
propose that this is due to learning spillovers: each worker is more productive at a
task when she also performs related tasks, so excessive specialization can decrease
productivity.
Consider the O*NET occupation of Economist. One of its 14 associated tasks
(#7538) is to â€œanalyze... data to explain economic phenomenaâ€. Another (#21106) is
to â€œ[e]xplain [the] economic impact of policies to the publicâ€. Though data analysis
and public communication are in some sense very distinct tasks, they cannot easily
be assigned to different people: analyzing the data makes an economist much better
efficient for labor to perform some automatable tasks for their learning spillovers.
3 The latter kind of learning by doing can straightforwardly account for the fact that in many occupations, compensation increases superlinearly in hours worked. Ameriks et al. (2020) find that older
Americans would generally prefer to reduce their working hours gradually if it were possible to do so
without reducing their hourly wage, instead of moving discretely from full-time work to retirement,
implying that the relative dearth of high-paying part-time work is a feature of the production function
and not of worker preferences. Recent work by Jarosch et al. (2025) analogously finds that workers in
Germany and the UK (though interestingly not the US) would prefer fewer hours without sacrificing
hourly pay. Intuitively, the very long hours often worked in law, consulting, and medical residencies
seem attributable in part to the difficulty of partitioning large, time-sensitive projects across multiple
employees rather than entirely to employee preferences for long hours: see e.g. Prescott et al.â€™s (2009)
model in which workers choose workweek length given a potentially superlinear function from hours
worked to labor services provided.

3
at credibly discussing its implications. Or consider the Computer Systems Engineer:
â€œ[v]erify[ing the] stability, interoperability, portability, security, or scalability of [a]
system architectureâ€ (#14669) is much easier when one is also the architect (#14676,
#14689).
In short, performing task A may increase our productivity at task B. On some
margins, it may do so by even more than performing more of task B. It may thus be
efficient for each worker to perform multiple tasks.
The two examples above illustrate learning spillovers within the context of a particular work project, but they may also appear across projects, in a sense more reminiscent of conventional learning by doing. For example, oneâ€™s productivity in teaching
may increase in both the number of hours one has spent teaching and the number of
hours one has spent doing research, including research on subjects other than the one
being taught.
Task automation given learning.

Given learning, within and across tasks, au-

tomating a small number of tasks in isolation may have macroeconomic impacts
greater or less than in a simple task model, depending on the structure of learning
spillovers. In particular, technology that can automate just a few tasks in a task-cluster
with high learning spillovers may have little or no effect on output, because adopting
the technology would deprive workers of valuable learning. Suppose a coder splits
her day evenly between coding and debugging, and an AI tool can only automate the
coding. If the coder is less than half as productive at debugging code she did not write,
it is not worth adopting the tool at all.
In the limit of full automation, however, the impact of automation is unambiguously greater with learning by doing than without, given intelligent machines that
can do more (or learn from more instances of â€œdoingâ€) than any single human worker.
Because machines in this case are performing and learning from every task, this holds
regardless of the structure of learning spillovers.
Thus, when we observe task-level automation having little impact on output today,
this may not indicate that the predictions of a simple task model will always be too
aggressive. Instead, it may indicate that, due to learning, output is highly convex in
the fraction of tasks automated.

4
Maximizing learning spillovers vs. minimizing handoff costs. Another explanation for task clustering is that splitting a project across many workers incurs coordination costs, especially communication costs. As a result, it may be efficient to assign
a worker some task, even if in isolation it could be done more cheaply by another
worker, because there are costs to temporarily transferring the project in which the
task is embedded. A large literature studies the implications of these costs for optimal
firm and team structure.4 Demirer et al. (2025) study the impact of task automation
given these costs, which they call handoff costs. This framing lets them study the impact of task automation on microeconomic questions including skill premia and firm
structure, applying insights from the literature cited above.
The â€œlearning spilloverâ€ and â€œhandoff costâ€ models of task clustering are very similar in spirit. Indeed, foregone handoff costs can be interpreted as a kind of learning
spillover. We might say that a coder is more productive at debugging code she has
written than code she has not, for example, because she does not have to pay the
handoff cost of reading her own documentation.
The learning spillover framing, however, is arguably more useful when considering the macroeconomic impact of task automation. This is because learning offers an
immediate explanation for the â€œfirst factâ€ about the structure of work, namely that
there is specialization to begin with. A learning model thus allows task automation
not only to attenuate but also magnify the useful learning that workers gain on the
job. When applied to the case of intelligent machines, furthermore, a learning model
sheds light on how automation may ultimately increase economic returns to scale, by
allowing machines to learn from ever wider corpora of experience, as humans cannot (Farboodi et al., 2025). By contrast, in a model in which the structure of work is
explained entirely by coordination costs, because the costs can only fall to zero, the
returns to scale are constant in the limit when all work is done by machines for which
these costs are eliminated.5
Outline.

I introduce a simple model of learning spillovers. The space of tasks is

4 See e.g. Yang and Borland (1991), Becker and Murphy (1992), Bolton and Dewatripont (1994), Gar-

icano (2000), and Dessein and Santos (2006).
5 A model can separately feature both specialization due to task-level learning and limits on specialization due to coordination costs (Becker and Murphy, 1992). Such a model would produce conclusions
similar to those presented here, but arguably be less parsimonious.

5
partitioned into â€œworkflowsâ€. Within a workflow, learning spillovers are strong, specialization is counterproductive, the economic impact of partial automation is smaller
than in a simple task model. Across workflows, learning spillovers are weak, specialization is valuable, and the economic impact of automation is greater than in a simple
task model.
In the simplest setting, introduced in Section 2, there are no cross-workflow
spillovers. Furthermore, the learning drawn from tasks within a workflow is Leontief: i.e. it exhibits the strongest possible diminishing returns. It is thus efficient for
each worker to perform (one or more) complete workflows rather than an arbitrary
assortment of tasks.
I then consider the implications of this model for the GDP impact of advances in
task automation. I find that its predictions differ from those of a simple task model in
two ways.
1. Making the first few tasks in a workflow automatable increases output less than
in a simple task model. This is because it remains efficient for workers to perform some automatable tasksâ€”i.e. to forego some automationâ€”to increase their
productivity at the non-automatable tasks.
2. Making later tasks in a workflow automatable increases output more than earlier
tasks, as it becomes efficient to automate all automatable tasks, as in a simple
task model.
In short, I argue that the relationship between automation and growth, within each
cluster with strong learning spillovers, should exhibit a convexity not found in simple
task models.
In Section 3, I extend the model by weakening the distinction between tasks that
are and are not in the same workflow. In particular, I allow the performance of tasks
in workflow A to improve a workerâ€™s productivity in workflow B to an intermediate
degree, which decreases continuously with the â€œdistanceâ€ between A and B. This extension accommodates the intuition that, even though a worker may perform multiple
workflowsâ€”so that there is room for further gains from specializationâ€”it is efficient
for each job to consist of a cluster of adjacent workflows instead of an arbitrary assortment. This extended model preserves both the results above for the GDP impact
of automation. Furthermore, arguably a key distinction between AI-based and earlier

6
waves of automation is that intelligent machines can themselves learn as they work,
either continuously or because widespread deployment of one generation of AI and
robotics systems facilitates the data collection used to train the next generation. By
accounting for this, the extended model also yields a third result:
3. Automation speeds growth by even more than in a simple model in the limit,
given that the machines can do more (or learn from more instances of â€œdoingâ€)
than any single human worker.
Machines can increase their productivity at a workflow not only as a result of performing its tasks at a large scale but also as a result of performing tasks in other workflows.
Section 5 concludes with a discussion of how models without learning may be
leading us astray about the impact of automation in practice, and how a deeper understanding of how tasks and workflows compose into useful work might be valuable.

2

Basic model

Model. The global set of tasks is partitioned into workflows. Each workflow consists
of a unit continuum of tasks. We will begin with the case in which there is only one
workflow, as this is most similar to the setup of a simple task model.
Tasks ğ‘– range from 0 to 1. For some threshold ğ‘‡ < 1, tasks ğ‘– â‰¤ ğ‘‡ are automatable,
in that they can be performed by capital or labor. Tasks ğ‘– > ğ‘‡ can only be performed
by labor. ğ¾ğ‘– and ğ“ğ‘– are the (integrable) densities of capital and labor assigned to task ğ‘–
respectively. For simplicity, we will say that output ğ‘Œ (equivalently, the output of the
workflow) is Leontief in the quantities of the tasks ğ‘Œğ‘– , and that there is only worker.
The above is simply the Leontief case of a simple task model. To introduce learning
spillovers within the workflow, we will say that an individualâ€™s productivity at task ğ‘–,
ğ´ğ‘– , increases in her performance of tasks ğ‘— â‰¤ ğ‘–. In particular, we will say that
ğ›¼

ğ´ğ‘– = ( inf ğ“ğ‘— ) ,
ğ‘—â‰¤ğ‘–

ğ›¼ > 0.

(1)

Note that the ğ›¼ = 0 case corresponds to the simple case without learning.
Given capital stock ğ¾ , an allocation of capital and labor across tasks is efficient if

7
it maximizes
ğ‘Œ â‰¡ inf ğ‘Œğ‘–

(2)

â§
âª
âªğ¾ğ‘– + ğ´ğ‘– ğ“ğ‘– , ğ‘– â‰¤ ğ‘‡ ;
ğ‘Œğ‘– = â¨
âª
ğ‘– > ğ‘‡;
âª
â©ğ´ğ‘– ğ“ğ‘– ,

(3)

ğ‘–

subject to

1

âˆ«

0

1

ğ¾ğ‘– ğ‘‘ğ‘– â‰¤ ğ¾ ,

âˆ«

0

ğ“ğ‘– ğ‘‘ğ‘– â‰¤ ğ“,

(4)

where ğ“ is the (exogenous) quantity of labor the worker supplies.
Finally, to simplify even further, we will assume that as long as ğ‘‡ < 1, lack of capital is not a constraint on the margin. Nevertheless, given learning, it may be efficient
for our worker to spend fraction ğ¹ > 0 of her total labor on automatable tasks. The
direct value of performing these tasks is zero, as she simply displaces the capital that
could otherwise have performed them; the value is entirely the productivity gained
on the remaining tasks.
Proposition 1 (The optimal allocation).
Given tasks up to ğ‘‡ automatable, it is uniquely optimal6 to set
â§
ğ¹ (ğ‘‡ )
âª
ğ‘– â‰¤ ğ‘‡;
âª ğ‘‡ ğ“,
ğ“ğ‘– = â¨
1âˆ’ğ¹ (ğ‘‡ )
âª
âª
â© 1âˆ’ğ‘‡ ğ“, ğ‘– > ğ‘‡ ,

(5)

yielding
ğ‘Œ =(

ğ¹ (ğ‘‡ ) ğ›¼ 1 âˆ’ ğ¹ (ğ‘‡ ) 1+ğ›¼
ğ“ ,
ğ‘‡ ) 1âˆ’ğ‘‡

where
ğ¹ (ğ‘‡ ) = min (ğ‘‡ ,

ğ›¼
.
1 + ğ›¼)

(6)

(7)

Proof. Given that the worker spends fraction ğ¹ âˆˆ [0, 1] of her time on automatable

6 Up to null sets.

8
tasks, it is uniquely optimal (up to null sets) to set
â§
âª
ğ‘– â‰¤ ğ‘‡;
âª ğ¹ ğ“,
ğ“ğ‘– = â¨ ğ‘‡
1âˆ’ğ¹
âª
âª
â© 1âˆ’ğ‘‡ ğ“, ğ‘– > ğ‘‡ .

(8)

An unequal allocation of ğ¹ ğ“ to automatable tasks features ğ“ğ‘– < ğ¹ ğ“/ğ‘‡ for some ğ‘– â‰¤ ğ‘‡ ,
and thus lower ğ´ğ‘– (1) for ğ‘– > ğ‘‡ than under allocation (8). An alternative allocation of
(1 âˆ’ ğ¹ )ğ“ to non-automatable tasks must feature a positive-measure task set îˆ¿ âŠ‚ (ğ‘‡ , 1]
1âˆ’ğ¹ ğ“ for all ğ‘– âˆˆ îˆ¿ and some ğ‘– â‰¤ inf îˆ¿: so for all ğ‘– âˆˆ îˆ¿, ğ´ is weakly lower
such that ğ“ğ‘– < 1âˆ’ğ‘‡
ğ‘–

and ğ“ğ‘– strictly lower than in allocation (8).
Likewise, ğ¹ > ğ‘‡ would require a positive-measure task set îˆ¿ âŠ‚ (ğ‘‡ , 1] such that
1âˆ’ğ¹ ğ“ < ğ¹ ğ“ for all ğ‘– âˆˆ îˆ¿ and some ğ‘– â‰¤ inf îˆ¿, leaving both ğ´ and ğ“ strictly lower
ğ“ğ‘– < 1âˆ’ğ‘‡
ğ‘–
ğ‘–
( ğ‘‡ )

than in allocation (8) with ğ¹ â‰¤ ğ‘‡ .
Since allocation (8) with ğ¹ â‰¤ ğ‘‡ yields output
ğ¹ ğ›¼ 1 âˆ’ ğ¹ 1+ğ›¼
(ğ‘‡ ) 1 âˆ’ ğ‘‡ ğ“ ,
it only remains to find the ğ¹ âˆˆ [0, ğ‘‡ ] that maximizes the above given ğ‘‡ .
How learning changes automationâ€™s impact on output. In a simple task model,
the elasticity of output to increases in automatabilityâ€”more precisely,
âˆ’

ğ‘‘ ln(ğ‘Œ )
ğ‘‘ ln(1 âˆ’ ğ‘‡ )

â€”equals 1 (in the long run, or immediately given unlimited capital).
ğ›¼ , it is not efficient to use capital
Here, from equation (7), we see that when ğ‘‡ < 1+ğ›¼

at all, because performing the automatable tasks gives the worker much more context
on the remainder of the workflow and performing them costs the worker only a small
fraction of her time. The elasticity of output to increases in automatability is 0. When
ğ›¼ , on the other hand, substituting (7) into (6) yields
ğ‘‡ > 1+ğ›¼

ğ‘Œ =

ğ›¼ğ›¼
ğ‘‡ âˆ’ğ›¼ 1+ğ›¼
ğ“ ,
(1 + ğ›¼)1+ğ›¼ 1 âˆ’ ğ‘‡

(9)

9
whose local elasticity to 1 âˆ’ ğ‘‡ exceeds 1 due to the ğ‘‡ âˆ’ğ›¼ now in the numerator (but
falls to 1 as ğ‘‡ â†’ 1).
To compare the total impact of automation on output here with that in a simple
task model, recall that in the latter, automation up to ğ‘‡ < 1 multiplies output by
1/(1 âˆ’ ğ‘‡ ). The proportion of this effect retained hereâ€”
ğ‘Œlearning (ğ‘‡ )/ğ‘Œlearning (0)
ğ‘Œsimple (ğ‘‡ )/ğ‘Œsimple (0)
â€”equals 1 âˆ’ ğ‘‡ for ğ‘‡ â‰¤ ğ›¼/(1 + ğ›¼). It declines further in ğ‘‡ for ğ‘‡ > ğ›¼/(1 + ğ›¼), as can be
found by dividing (9) by ğ“1+ğ›¼ /(1 âˆ’ ğ‘‡ ) to get
ğ›¼ğ›¼
ğ‘‡ âˆ’ğ›¼ ,
(1 + ğ›¼)1+ğ›¼
to an infimum of ğ›¼ ğ›¼ /(1 + ğ›¼)1+ğ›¼ as ğ‘‡ â†’ 1. Doing provides learning, and more automation takes more learning away.
Specialization and multiple workflows. The model of this section offers a simple
explanation for why it may be efficient for each worker to perform a suite of tasks.
It also offers an explanation for why wages might increase in hours worked. If
ğ‘‡ = 0, for example, a worker working ğ“ hours per week (or with ğ“ hours of cumulative experience, on the modelâ€™s â€œexperienceâ€-based interpretation) spreads these hours
equally across all tasks within the workflow, enjoying productivity ğ“ğ›¼ at each task and
thus producing ğ“1+ğ›¼ units of output.
With only one workflow, the model predicts that each worker will perform all
tasks, as long as ğ‘‡ is low. This is of course unrealistic, and fails to explain the gains
from specialization. Suppose instead therefore that there are ğ‘Š symmetric workflows
and ğ‘ individuals, with ğ‘Š â‰« ğ‘ . Let ğ‘Œğ‘¤ denote the output of workflow ğ‘¤, and again
for simplicity, let output ğ‘Œ (ğ‘Œ1 , ..., ğ‘Œğ‘Š ) be symmetric and Leontief in each ğ‘Œğ‘¤ . Then,
fixing each individualâ€™s labor supply at 1, it is uniquely efficient for each individual to
perform ğ‘Š /ğ‘ workflows, dedicating ğ‘ /ğ‘Š units of labor to each.
Absent automation, the output of each task, and aggregate output, then equals
(ğ‘ /ğ‘Š )1+ğ›¼ so aggregate output per person equals ğ‘ ğ›¼ /ğ‘Š 1+ğ›¼ . This increases in ğ‘
because a larger population allows each worker to specialize in fewer workflows.

10
If, for all ğ‘¤, tasks ğ‘¤ğ‘– with ğ‘– â‰¤ ğ‘‡ are automatable, the optimal allocation of individuals to workflows does not change. The optimal allocation of each individualâ€™s work
within her workflows is still characterized by (5) and (7), and output still equals (6),
with ğ“ = ğ‘ /ğ‘Š .

3

A richer task space

Motivation. The model of the previous section offers a simple explanation of specialization and its limits, but it predicts that it is efficient for each worker to perform
an arbitrary set of workflows, rather than a related set. It also predicts, implausibly,
that the economic returns to scale from proportionally increasing the population will
be as large as those from proportionally increasing each workerâ€™s work experience (ğ›¼).
These results are artifacts of the simplifying assumption that tasks are either entirely
necessary for learning (if they are downstream in the same workflow) or entirely irrelevant (if they are in different workflows). This section weakens that assumption.
Qualitatively, the central points are that:
â€¢ Insofar as we learn more by doing a given task (or related tasks) more than
by doing more distant tasks, it is efficient to specialize and reap the resulting
increasing returns.
â€¢ Insofar as there are diminishing returns to learning from a given task in isolation,
specialization can be counterproductive. This is true regardless of the state of
automation, but it means that feasible automation can be inefficient by requiring
us to specialize in inefficient ways.
These points can be separated simply in a two-dimensional task space.
Allowing the model to feature specialization despite some degree of learning
spillovers across the economy is also valuable for illustrating a potential implication
of automation. As shown in the next section, an automated economy will exhibit more
strongly increasing returns to scale than observed presently, insofar as machines can
learn from work done by machines throughout the economy and humans only learn
from their own narrow experience.
Model.

There is an interval of workflows ğ‘¤ âˆˆ (0, 1], each of which contains a unit

11
interval of tasks ğ‘¤ğ‘–. As in Section 2, output is Leontief in tasks (or equivalently, in
the output ğ‘Œğ‘¤ of each workflow, which is Leontief in the output of each of its tasks).
There are ğ‘ workers, indexed by ğ‘›. ğ“ğ‘›ğ‘¤ğ‘– denotes the density of ğ‘›â€™s performance of task
ğ‘¤ğ‘– and ğ¿ğ‘›ğ‘¤ denotes the density of ğ‘›â€™s performance of tasks in workflow ğ‘¤. Worker ğ‘›
exogenously supplies one unit of labor:
1

1

âˆ«

0

âˆ«

0

ğ“ğ‘›ğ‘¤ğ‘– ğ‘‘ğ‘– ğ‘‘ğ‘¤ = âˆ«

0

1

ğ¿ğ‘›ğ‘¤ ğ‘‘ğ‘¤ = 1.

(10)

The allocation of labor across workflows is left-continuous.
Now, however, let the productivity of worker ğ‘› at task ğ‘¤ğ‘– equal
ğ›¼

ğ´ğ‘›ğ‘¤ğ‘– = (ğ´ğ‘›ğ‘¤ â‹… inf ğ“ğ‘›ğ‘¤ğ‘— ) ,
ğ´ğ‘›ğ‘¤ = ( âˆ«

0

ğ‘—â‰¤ğ‘–
ğ‘¤

(11)

ğ›¼ > 0;
ğ›¾

(ğ‘¤ âˆ’ ğ‘£)âˆ’ğ›½ ğ¿ğ‘›ğ‘£ ğ‘‘ğ‘£) ,

ğ›¾ âˆˆ (0, 1), ğ›½ â‰¡

ğ›¾
.
1 + 2ğ›¾

(12)

Because (â‹…)âˆ’ğ›½ is decreasing, more distant workflows ğ‘£ â‰ª ğ‘¤ contribute less to productivity at ğ‘¤.7
In this setup, a workerâ€™s productivity at workflow ğ‘¤ increases only in her performance of workflows ğ‘£ < ğ‘¤. Our model of cross-workflow learning is closely analogous
in this way to our model of learning spillovers within a workflow. This is mainly motivated by tractability. If cross-workflow learning occurs in both directions, with contributions to learning decreasing in |ğ‘¤âˆ’ğ‘£|, the obvious qualitative result is maintainedâ€”it
is efficient for each person to perform an interval of workflows (see below)â€”but it is
more difficult to produce a model in which an optimal allocation can be found analytically.
Specialization into intervals of workflows.

Here, the learning contributions of

workflows ğ‘£1 , ğ‘£2 < ğ‘¤ to productivity at ğ‘¤ combine additively, and so are perfect substitutes: the opposite extreme of the Leontief learning maintained within a workflow. As
a result, since (â‹…)âˆ’ğ›½ is decreasing, efficiency requires partitioning the set of workflows
into ğ‘ equal intervals, each performed exclusively by a single worker.
7 We could alternatively assume that sufficiently distant workflows offer each other no learning

spillovers at all. Given a population of ğ‘ , no one will perform a pair of tasks more than 1/ğ‘ apart in
equilibrium, so this would make no difference.

12
Proposition 2 (The optimal allocation across workflows).
Absent automation, there is a unique efficient allocation,8 in which
â€¢ the set of workflows is partitioned into ğ‘ equal intervals, each performed exclusively by a single worker;
â€¢ given that worker ğ‘› performs workflows (ğ‘¤, ğ‘¤],
ğ¿ğ‘›ğ‘¤ = (1 âˆ’ ğ›½) (ğ‘¤ âˆ’ ğ‘¤)ğ›½âˆ’1 (ğ‘¤ âˆ’ ğ‘¤)âˆ’ğ›½ ,
2

so ğ´ğ‘›ğ‘¤ = ğ´âˆ— (ğ‘¤ âˆ’ ğ‘¤)ğ›¾ (ğ›½âˆ’1) (ğ‘¤ âˆ’ ğ‘¤)ğ›½

ğ›¾
(Î“(1 âˆ’ ğ›½))
for ğ´ â‰¡ (1 âˆ’ ğ›½)
,
(
Î“(2 âˆ’ 2ğ›½) )
âˆ—

so ğ´ğ‘›ğ‘¤ ğ¿ğ‘›ğ‘¤ = ğ´âˆ— (1 âˆ’ ğ›½) (ğ‘¤ âˆ’ ğ‘¤)(ğ›¾ +1)(ğ›½âˆ’1) âˆ€ğ‘¤ âˆˆ (ğ‘¤, ğ‘¤];
and thus
â€¢ ğ‘Œ = ğ´âˆ— (1 âˆ’ ğ›½) ğ‘ 1+ğ›¼(1+ğ›½ğ›¾ ) .
Proof. See Appendix A.1.
Within a workflow, absent automation, labor is of course still allocated equally
across tasks.
Automation. Suppose that, across all workflows, tasks ğ‘¤ğ‘– with ğ‘– â‰¤ ğ‘‡ are automatable. Then the results of this section and the previous section are unchanged. It is still
optimal to assign each worker an interval of workflows and to allocate ğ¿ğ‘› (= 1) so that
ğ´ğ‘›ğ‘¤ ğ¿ğ‘›ğ‘¤ is equalized across the interval. The allocation of ğ´ğ‘›ğ‘¤ ğ¿ğ‘›ğ‘¤ across workflow ğ‘¤ that
equalizes ğ´ğ‘›ğ‘¤ğ‘– ğ“ğ‘›ğ‘¤ğ‘– across ğ‘–, and the impact on output of increasing ğ‘‡ , are precisely as
described in Section 2.
If instead tasks ğ‘¤ğ‘– with ğ‘¤ â‰¤ ğ‘‡ğ‘Š are automatable, for some ğ‘‡ğ‘Š âˆˆ (0, 1), these
are fully automated, since it is always more efficient to allocate a population across
a narrower range of workflows. A proportional decrease to 1 âˆ’ ğ‘‡ğ‘Š functions like a
proportional increase to ğ‘ .

8 Up to relabeling of the workers.

13

4

Machine learning by doing and the returns to scale

We will now briefly explore the third and last implication of learning for automation:
that once automation is sufficiently advanced, if machines can learn by doing as humans can, growth will rise by more than in a simple task model.
We have been assuming for simplicity that output is Leontief across tasks and that,
while ğ‘‡ < 1, output is bottlenecked by labor and not by the tasks capital can perform.
In our setting, before full automation, capitalâ€™s productivity at tasks ğ‘¤ğ‘– with ğ‘– < ğ¼ is
irrelevant. In particular, it is irrelevant whether we model capital in the traditional,
learning-free way or as learning from the tasks it performs. â€œMachine learning by
doingâ€9 thus has interesting implications here only after full automation, so we will
consider its implications only in this case. It will hopefully be clear that similar implications can emerge more continuously in a model in which capital partially substitutes
for labor on the margin while work remains only partially automated.
In a simple task model, output after full automation is ğ´ğ¾ . Once the labor bottleneck is fully relieved, output grows exponentially in the absence of further technological development (given positive net saving), and new learning can speed growth only
by increasing the replication rate of capital. Here, suppose that capitalâ€™s productivity
at workflow ğ‘¤ (ğ´ğ‘¤ ) and at task ğ‘¤ğ‘– (ğ´ğ‘¤ğ‘– ) are given by (11)â€“(12), with ğ¾ and ğ‘˜ğ‘¤ğ‘— in place
of ğ¿ğ‘› and ğ“ğ‘›ğ‘¤ğ‘— . Instead of the unit labor supply constraint faced by each individual in
(10), capital faces the constraint
1

âˆ«

0

1

âˆ«

0

1

ğ‘˜ğ‘¤ğ‘– ğ‘‘ğ‘– ğ‘‘ğ‘¤ = âˆ«

0

ğ¾ğ‘¤ ğ‘‘ğ‘¤ = ğ¾ .

The entire capital stock thus learns collectively, without being affected by fragmentation across workflows.
Then output exhibits increasing returns in capital. In particular, recalling that in

9 I.e. some process by which the data gathered while doing workâ€”including human evaluationsâ€”

improves subsequent performance. This may include both continuous learning and simply the use of
data gathered on the job in training future AI models.

14
this setting (ğ‘¤, ğ‘¤] = (0, 1], we have
ğ¾ğ‘¤ = (1 âˆ’ ğ›½)ğ‘¤âˆ’ğ›½
âŸ¹ ğ´ğ‘¤ âˆ ğ¾ ğ›¾ ğ‘¤ğ›½ ,

ğ´ğ‘¤ğ‘– âˆ ğ¾ ğ›¼(1+ğ›¾ ) ğ‘¤ğ›½ .

The elasticity of productivity (i.e. at the task level) to capital is ğ›¼(1 + ğ›¾ ), even greater
than the ğ›¼(1+ğ›½ğ›¾ ) elasticity of productivity to population that follows from Proposition
2. The gap is driven by the greater elasticity of workflow productivity to capital than
to population, which equals ğ›¾ rather than ğ›½ğ›¾ .
This higher elasticity arises because, unlike in the human case, every unit of capital
can learn from the experience of every other unit. When capital performs all workflows, the optimal allocation of a doubled capital stock doubles the density of capital
on each workflow. By contrast, a doubled population must divide itself across workflows more finely, so that the mind performing each task benefits from a narrower
range of learning.

5

Extrapolating task productivity gains from AI

Eloundou et al. (2024) evaluate the extent to which GPT-4, perhaps accompanied by
necessary â€œwrapperâ€ software, can automate each task in the O*NET database. At
face value, the analysis implies that a lightly augmented GPT-4 could automate 38% of
the wage bill.10 Since GPT-4 does not seem on track to increase output by anything
close to 38%, recent attempts to quantify the GDP impact of LLMs (Acemoglu, 2025;
Aghion and Bunel, 2024) have produced more intuitively reasonable figures largely by
assuming, somewhat ad hoc,
â€¢ that tasks judged less than â€œ75% automatableâ€ (Eloundou et al. categories 0â€“2)
generate no labor savings at all and
â€¢ that for tasks judged at least â€œ75% automatableâ€ (Eloundou et al. categories 3 and
4), their labor savings in practice will permanently equal their judged automatability multiplied by recent empirical estimates of time savings on a handful of
tasks in real-world settings.
10 Following Acemoglu (2025), I am weighting each task by its occupationâ€™s share of the 2019â€“2022

wage bill.

15
100%

Automatability

75%

(Eloundou et al.)
and labor savings
(Aghion and Bunel,
Acemoglu)

50%
25%
0%

0.21

0.52

0.73

0
1
2
3
Share-weighted tasks by category

0.99

4

Figure 1: Distribution of â€œautomatabilityâ€ (Eloundou et al.) and â€œlabor cost savingsâ€
(Acemoglu, Aghion and Bunel) across share-weighted tasks
After these and other adjustments, the impact on GDP, even of systems that can truly
automate 38% of wage-adjusted tasks in isolation, is estimated to be a few percent at
most.
These adjustments perhaps produce reasonable estimates of the impacts of LLMs
on GDP today and in the very near future. However, because they permanently multiply task-level estimates of automatability by a factor of 0.27â€“0.4 even for fully automatable tasks, they have the strange implication that full automation would only
yield 27â€“40% labor savings.
By contrast suppose that, as in the model of this paper, the lack of full labor savings on fully automatable tasks is due to the fact that some labor on task A remains
necessary for the worker to understand how to integrate its output with the not-yetautomated task B (and indeed that for tasks with sufficiently low automatability, perhaps it is not worth automating at all, as the authors above assume). Then labor savings per automatable task will rise to 1 as more tasks become automatable. As AI
advances, the GDP impact of AI will rise, perhaps quickly, to that suggested by a plain
reading of Eloundou et al. or beyond.

16

References
Acemoglu, Daron, â€œThe Simple Macroeconomics of AI,â€ Economic Policy, 2025, 40 (121), 13â€“
58.
and Pascual Restrepo, â€œThe Race Between Man and Machine: Implications of Technology for Growth, Factor Shares, and Employment,â€ American Economic Review, 2018, 108 (6),
1488â€“1542.
Aghion, Philippe and Simon Bunel, â€œAI and Growth: Where Do We Stand?,â€ 2024. Working
paper.
, Benjamin F. Jones, and Charles I. Jones, â€œArtificial Intelligence and Economic Growth,â€
in Ajay Agrawal, Joshua Gans, and Avi Goldfarb, eds., The Economics of Artificial Intelligence:
An Agenda, National Bureau of Economic Research, 2019, pp. 237â€“282.
Ameriks, John, Joseph Briggs, Andrew Caplin, Minjoon Lee, Matthew D. Shapiro, and
Christopher Tonetti, â€œOlder Americans Would Work Longer if Jobs Were Flexible,â€ American Economic Journal: Macroeconomics, 2020, 12 (1), 174â€“209.
Autor, David and Neil Thompson, â€œExpertise,â€ 2025.
Becker, Gary S. and Kevin M. Murphy, â€œThe Division of Labor, Coordination Costs, and
Knowledge,â€ Quarterly Journal of Economics, 1992, 107 (4), 1137â€“60.
Bolton, Patrick and Mathias Dewatripont, â€œThe Firm as a Communication Network,â€ The
Quarterly Journal of Economics, 1994, 109 (4), 809â€“39.
Demirer, Mert, John J. Horton, Nicole Immorlica, Brendan Lucier, and Peyman
Shahidi, â€œThe Economic Impacts of Generative AI on the Structure of Work,â€ 2025. Working
paper.
Dessein, Wouter and Tano Santos, â€œAdaptive Organizations,â€ Journal of Political Economy,
2006, 114 (5), 956â€“95.
Eloundou, Tyna, Sam Manning, Pamela Mishkin, and Daniel Rock, â€œGPTs are GPTs:
Labor market impact potential of LLMs,â€ Science, 2024, 384 (6702), 1306â€“8.
Farboodi, Maryam, Andrew Koh, and Anchi Xia, â€œData-driven Automation,â€ 2025.
Freund, Lukas and Lukas Mann, â€œJob Transformation, Specialization, and the Labor Market
Effects of AI,â€ 2025.
Garicano, Luis, â€œHierarchies and the Organization of Knowledge in Production,â€ Journal of
Political Economy, 2000, 108 (5), 874â€“904.

17
Jarosch, Gregor, Laura Pilossoph, and Anthony Swaminathan, â€œShould Friday be the
New Saturday? Hours Worked and Hours Wanted,â€ 2025. NBER working paper 33577.
Lindbeck, Assar and Dennis J. Snower, â€œMultitask Learning and the Reorganization of
Work: From Tayloristic to Holistic Organization,â€ Journal of Labor Economics, 2000, 18 (3),
353â€“76.
Prescott, Edward C., Richard Rogerson, and Johanna Wallenius, â€œLifetime Aggregate
Labor Supply with Endogenous Workweek Length,â€ Review of Economic Dynamics, 2009, 12
(1), 23â€“36.
Smith, Adam, An Inquiry into the Nature and Causes of the Wealth of Nations 1776.
Yang, Xiaokai and Jeff Borland, â€œA Microeconomic Mechanism for Economic Growth,â€ Journal of Political Economy, 1991, 99 (3), 460â€“82.
Zeira, Joseph, â€œWorkers, Machines, and Economic Growth,â€ Quarterly Journal of Economics,
1998, 113 (4), 1091â€“1117.

A

Proofs

A.1

Proof of Proposition 2
ğ›½

Let ğ›¾ â‰¡ 1âˆ’2ğ›½ .
Call an allocation {ğ¿ğ‘›ğ‘¤ } uniform-partitioning if it partitions the set of workflows into
ğ‘ equal intervals, each performed exclusively by a single worker almost everywhere.
Observe that the implied skill distributions {ğ´ğ‘›ğ‘¤ } are continuous. For each ğ‘›, let
ğ»ğ‘¤ğ‘› â‰¡ âˆ«
0

ğ‘¤

(ğ‘¤ âˆ’ ğ‘£)âˆ’ğ›½ ğ¿ğ‘›ğ‘£ ğ‘‘ğ‘£,

so ğ´ğ‘›ğ‘¤ = (ğ»ğ‘¤ğ‘› )ğ›¾ . (We will sometimes write these schedules as functions of the allocağ‘¤
tion {ğ¿ğ‘›ğ‘¤ }.) Let ğ‹ğ‘›ğ‘¤ denote âˆ«0 ğ¿ğ‘›ğ‘£ ğ‘‘ğ‘£, and observe that it is continuous. Let ğ‹ â‰¡ âˆ‘ğ‘› ğ‹ğ‘› .
(ğ‘›)
When subscripted with a set ğ‘ƒ rather than a number, ğ‹(ğ‘›)
ğ‘ƒ â‰¡ âˆ«ğ‘ƒ ğ¿ğ‘¤ ğ‘‘ğ‘¤. Likewise let
ğ˜ğ‘›ğ‘ƒ â‰¡ âˆ«ğ‘ƒ ğ´ğ‘›ğ‘¤ ğ¿ğ‘›ğ‘¤ ğ‘‘ğ‘¤ and ğ˜ğ‘ƒ â‰¡ âˆ‘ğ‘› ğ‘Œğ‘ƒğ‘› .

Lemma 1: Existence of optimal allocation of a worker over an interval. Specify a worker and a workflow interval (ğ‘¤, ğ‘¤]. Since we are here considering the case in

18
which only one worker performs the workflows in an interval, we will suppress the
â€œğ‘›â€ superscripts.
Let

{
}
ğ‘¤
ğ‘† â‰¡ {ğ¿ğ‘¤ }ğ‘¤âˆˆ(ğ‘¤,ğ‘¤] âˆ¶ ğ¿ğ‘¤ > 0 âˆ€ğ‘¤, âˆ« ğ¿ğ‘¤ ğ‘‘ğ‘¤ = 1 .
ğ‘¤

Observe that ğ‘† is the set of probability measures on [ğ‘¤, ğ‘¤], and thus that by Alaogluâ€™s
theorem, ğ‘† is compact under the weak-âˆ— topology: for any sequence of allocations in
ğ‘†, there is a subsequence {ğ¿ğ‘¤ }(ğ‘˜) âŠ‚ ğ‘† and a limit allocation {ğ¿ğ‘¤ }âˆ— such that {ğ¿ğ‘¤ }(ğ‘˜) â†’
{ğ¿ğ‘¤ }âˆ— in the weak-âˆ— sense.
By the ArzelÃ -Ascoli theorem, because (â‹…)âˆ’ğ›½ is integrable and ğ»ğ‘¤ (â‹…) = 0, ğ»ğ‘¤ (â‹…) is
a compact operator mapping to ğ¶([ğ‘¤, ğ‘¤]), the space of continuous functions on the
interval with the sup norm. And because compact operators map weakly convergent
ğ›½

sequences to strongly convergent sequences, and because (â‹…) 1âˆ’2ğ›½ is continuous, the
function ğ´(ğ‘˜) â‰¡ ğ´({ğ¿ğ‘¤ }(ğ‘˜) ) converges stronglyâ€”i.e., under the sup norm, uniformlyâ€”
to ğ´âˆ— â‰¡ ğ´({ğ¿ğ‘¤ }âˆ— ).
We will now show that
}
{
ğ‘Œ ({ğ¿ğ‘¤ }) â‰¡ infğ‘£âˆˆ(ğ‘¤,ğ‘¤] ğ´ğ‘£ ({ğ¿ğ‘¤ }) ğ¿ğ‘£
is upper semi-continuous (in the weak-* topology): i.e. that ğ‘Œ âˆ— â‰¥ lim supğ‘˜â†’âˆ ğ‘Œ (ğ‘˜) ,
where ğ‘Œ âˆ— â‰¡ ğ‘Œ ({ğ¿ğ‘¤ }âˆ— ) and ğ‘Œ (ğ‘˜) â‰¡ ğ‘Œ ({ğ¿ğ‘¤ }(ğ‘˜) ).
Let ğœ™ be any continuous function on [ğ‘¤, ğ‘¤]. First, observe that
| ğ‘¤
|
ğ‘¤
|
|
(ğ‘˜)
âˆ— âˆ—
lim | âˆ« ğœ™ğ‘¤ ğ´(ğ‘˜)
ğ¿
ğ‘‘ğ‘¤
âˆ’
ğœ™
ğ´
ğ¿
ğ‘‘ğ‘¤
|
ğ‘¤ ğ‘¤ ğ‘¤
ğ‘¤
ğ‘¤
âˆ«
|
ğ‘˜â†’âˆ | ğ‘¤
ğ‘¤
|
|
| ğ‘¤
|
|
|
(ğ‘˜)
âˆ— (ğ‘˜)
âˆ— (ğ‘˜)
âˆ— âˆ—
ğ¿
âˆ’
ğ´
ğ¿
+
ğ´
ğ¿
âˆ’
ğ´
ğ¿
ğ‘‘ğ‘¤
= lim | âˆ« ğœ™ğ‘¤ (ğ´(ğ‘˜)
|
ğ‘¤
ğ‘¤
ğ‘¤
ğ‘¤
ğ‘¤
ğ‘¤
ğ‘¤
ğ‘¤
)
|
ğ‘˜â†’âˆ | ğ‘¤
|
|
| ğ‘¤
| | ğ‘¤
|
|
| |
|
âˆ—
(ğ‘˜)
âˆ—
(ğ‘˜)
âˆ—
â‰¤ lim | âˆ« ğœ™ğ‘¤ (ğ´(ğ‘˜)
âˆ’
ğ´
ğ¿
ğ‘‘ğ‘¤
+
ğœ™
ğ´
ğ¿
âˆ’
ğ¿
ğ‘‘ğ‘¤
|
|
|
ğ‘¤ ğ‘¤( ğ‘¤
ğ‘¤
ğ‘¤) ğ‘¤
ğ‘¤)
âˆ«
|
|
|
|]
ğ‘˜â†’âˆ [ ğ‘¤
ğ‘¤
|
| |
|
by the triangle inequality. The first term vanishes by the uniform convergence of ğ´(ğ‘˜)
ğ‘¤
to ğ´âˆ— and the fact that âˆ«ğ‘¤ ğ¿(ğ‘˜)
ğ‘¤ ğ‘‘ğ‘¤ = 1 âˆ€ğ‘˜. The second vanishes by the continuity of

ğœ™ğ´âˆ— and the weak-* convergence of ğ¿(ğ‘˜) to ğ¿âˆ— .

19
Also,
ğ‘¤

âˆ«

ğ‘¤

(ğ‘˜)
(ğ‘˜)
ğœ™ğ‘¤ ğ´(ğ‘˜)
ğ‘¤ ğ¿ğ‘¤ ğ‘‘ğ‘¤ â‰¥ ğ‘Œ
âˆ«

ğ‘¤

ğ‘¤

ğœ™ğ‘¤ ğ‘‘ğ‘¤.

Taking the lim sup of both sides and rearranging,
ğ‘¤

âˆ«

ğ‘¤

ğœ™ğ‘¤ (ğ´âˆ—ğ‘¤ ğ¿âˆ—ğ‘¤ âˆ’ lim sup ğ‘Œ (ğ‘˜) )ğ‘‘ğ‘¤ â‰¥ 0.
ğ‘˜ â†’âˆ

Because this holds for any continuous ğœ™, the function in parentheses must be nonnegative a.e. Thus ğ‘Œ âˆ— â‰¥ lim supğ‘˜ â†’âˆ ğ‘Œ (ğ‘˜) . This completes the proof upper semi-continuity.
Finally, ğ‘Œ (â‹…) attains a maximum on ğ‘† by the extreme value theorem for topological
spaces.
Lemma 2: Constancy of effective labor.

Suppose there is a left-continuous al-

location {ğ¿ğ‘¤ } over an interval, open below and closed above, such that ğ´ğ‘¤ ğ¿ğ‘¤ is not
constant throughout the interval. Assume for simplicity that the interval is (0, 1]; the
proof will be analogous for any interval open below and closed above. Then there is
a ğ›¿ > 0 and an interval ğ‘‰Ì„ such that
ğ´ğ‘¤ ğ¿ğ‘¤ â‰¥ ğ‘Œ (1 + ğ›¿) âˆ€ğ‘¤ âˆˆ ğ‘‰Ì„ .
Because, for any ğ‘¤,
ğ‘¤

ğ›¾

âˆ’ğ›½
( âˆ«0 (ğ‘¤ âˆ’ ğ‘£) ğ¿ğ‘£ ğ‘‘ğ‘£)
ğ‘§ğ‘¤;ğ‘¤ â‰¡ 1 âˆ’
ğ´ğ‘¤

(13)

is continuous in ğ‘¤ and equals 0 at ğ‘¤ = ğ‘¤, for any ğ›¿2 > 0 there is a ğ‘¤ and an ğœ€ > 0
such that ğ‘‰ â‰¡ [ğ‘¤, ğ‘¤ + ğœ€) âŠ† ğ‘‰Ì„ and ğ‘§ğ‘¤;ğ‘¤ â‰¤ ğ›¿2 âˆ€ğ‘¤ âˆˆ ğ‘‰ . Choose such a ğ‘‰ for
ğ›¿1 =

1
1
1
âˆ’
,
2(
1 + ğ›¿)

and let ğ‘‰âˆ’ â‰¡ [ğ‘¤, ğ‘¤ + ğœ€/2), ğ‘‰+ â‰¡ [ğ‘¤ + ğœ€/2, ğ‘¤ + ğœ€).

20
Let

1
1
ğ›¿2 â‰¡ (1 âˆ’
2
(1 âˆ’ ğ›¿1 )(1 + ğ›¿) )

and

â§
âª
ğ¿ğ‘¤ ,
ğ‘¤ âˆ‰ ğ‘‰;
âª
âª
âª
3)
ğ¿(ğ›¿
ğ¿ğ‘¤ (1 âˆ’ ğ›¿2 ),
ğ‘¤ âˆˆ ğ‘‰âˆ’ ;
ğ‘¤ =â¨
âª
âª
âª
âª
â©ğ¿ğ‘¤ + (1 âˆ’ ğ›¿3 )ğ›¿2 /ğœ€ â‹… ğ‹ğ‘‰âˆ’ , ğ‘¤ âˆˆ ğ‘‰+ .

(14)

(ğœ‚)

ğ¿ğ‘¤ removes fraction ğ›¿2 of the labor at each ğ‘¤ âˆˆ ğ‘‰âˆ’ , discards fraction ğ›¿3 of it, and
allocates the rest uniformly across ğ‘‰+ . The resulting effective labor at ğ‘¤ âˆˆ ğ‘‰ can be
bounded below by
(1 âˆ’ ğ›¿1 )(1 âˆ’ ğ›¿2 )ğ´ğ‘¤ ğ¿ğ‘¤ â‰¥ (1 âˆ’ ğ›¿1 )(1 âˆ’ ğ›¿2 )(1 + ğ›¿)ğ‘Œ > ğ‘Œ .
Furthermore, letting

3)
ğ´ğ‘¤ ({ğ¿(ğ›¿
ğ‘¤ })
,
ğ‘¤â‰¥ğ‘¤+ğœ€
ğ´ğ‘¤

Ì‚ 3 ) â‰¡ inf
ğ´(ğ›¿

Ì‚
Ì‚ decreases continuously in ğ›¿3 , so there is a ğ›¿3 > 0 with ğ´(ğ›¿
Ì‚ 3 ) > 1.
ğ´(0)
> 1 and ğ´(â‹…)
Choose such a ğ›¿3 , and consider the allocation that reallocates the saved labor uniformly across [0, ğ‘¤):
â§
âª
âªğ¿ğ‘¤ + ğ›¿3 /ğ‘¤ â‹… ğ‹ğ‘‰âˆ’ ,
3) =
Ì‚ğ¿(ğ›¿
â¨
ğ‘¤
(ğ›¿3 )
âª
âª
â©ğ¿ğ‘¤ ,

ğ‘¤ < ğ‘¤;
ğ‘¤ â‰¥ ğ‘¤.

(15)

(ğ›¿3 )
ğ‘Œğ‘¤ (ğ¿Ì‚ ğ‘¤
) > ğ‘Œğ‘¤ âˆ€ğ‘¤.

The optimal allocation of a worker over an interval. An allocation {ğ¿ğ‘¤ } across
(ğ‘¤, ğ‘¤] equalizes ğ‘Œğ‘¤ across the range iff, for some ğ‘˜,
ğ‘¤

( âˆ«ğ‘¤

ğ›¾

(ğ‘¤ âˆ’ ğ‘£)âˆ’ğ›½ ğ¿ğ‘›ğ‘£ ğ‘‘ğ‘£) ğ¿ğ‘¤ = ğ‘˜ âˆ€ğ‘¤.

(We will quantify over (ğ‘¤, ğ‘¤] unless otherwise specified.) Because the corresponding
productivity schedule {ğ´ğ‘¤ } is continuous, {ğ¿ğ‘¤ } must also be continuous.

21
Let ğ‘šğ‘¤ â‰¡ ğ¿ğ‘¤ (ğ‘¤ âˆ’ ğ‘¤)ğ›½ . After the change of variables ğ‘£ â‰¡ ğ‘¤ + ğ‘ (ğ‘¤ âˆ’ ğ‘¤) (from which
ğ‘‘ğ‘£ = (ğ‘¤ âˆ’ ğ‘¤)ğ‘‘ğ‘ ),
ğ‘¤

ğ»ğ‘¤ = âˆ«

ğ‘¤
1

(ğ‘¤ âˆ’ ğ‘£)âˆ’ğ›½ ğ‘šğ‘£ (ğ‘£ âˆ’ ğ‘¤)âˆ’ğ›½ ğ‘‘ğ‘£
âˆ’ğ›½

((1 âˆ’ ğ‘ )(ğ‘¤ âˆ’ ğ‘¤))

=âˆ«

0

= (ğ‘¤ âˆ’ ğ‘¤)1âˆ’2ğ›½ âˆ«

1

âˆ’ğ›½

(ğ‘ (ğ‘¤ âˆ’ ğ‘¤))
âˆ’ğ›½

((1 âˆ’ ğ‘ )ğ‘  )

0

ğ‘šğ‘¤+ğ‘ (ğ‘¤âˆ’ğ‘¤) (ğ‘¤ âˆ’ ğ‘¤)ğ‘‘ğ‘ 

ğ‘šğ‘¤+ğ‘ (ğ‘¤âˆ’ğ‘¤) ğ‘‘ğ‘ .

Letting ğ½ğ‘¤ denote the integral of the last line above, so
ğ½ğ‘¤ = (ğ‘¤ âˆ’ ğ‘¤)2ğ›½âˆ’1 ğ»ğ‘¤ ,
constant effective labor requires
1âˆ’2ğ›½

((ğ‘¤ âˆ’ ğ‘¤)

ğ›¾

ğ›¾

ğ½ğ‘¤ ) (ğ‘¤ âˆ’ ğ‘¤)âˆ’ğ›½ ğ‘šğ‘¤ = ğ½ğ‘¤ ğ‘šğ‘¤ = ğ‘˜.

(16)

Let
1

ğ‘†â‰¡âˆ«

0

âˆ’ğ›½

((1 âˆ’ ğ‘ )ğ‘  )

ğ‘‘ğ‘ ,

ğ‘šğ‘¤ â‰¡ sup ğ‘šğ‘£ ,
ğ‘£âˆˆ(ğ‘¤,ğ‘¤]

ğ‘šğ‘¤ â‰¡

inf ğ‘šğ‘£ .

ğ‘£âˆˆ(ğ‘¤,ğ‘¤]

Observe that ğ½ğ‘¤ âˆˆ [ğ‘†ğ‘šğ‘¤ , ğ‘†ğ‘šğ‘¤ ]. We will now show that there are ğ‘š > 0, ğ‘š < âˆ with
ğ‘šğ‘¤ âˆˆ [ğ‘š, ğ‘š] âˆ€ğ‘¤.
Suppose not. Then, by continuity of ğ‘š(â‹…) , there is either a sequence {ğ‘¤ğœ… } â†’ 0 with
ğ‘šğ‘¤ğ‘˜ â†’ 0 or one with ğ‘šğ‘¤ğ‘˜ â†’ âˆ. In the latter case, if there is not also a sequence with
ğ‘šğ‘¤ğ‘˜ â†’ 0, then ğ‘šğ‘¤ and thus ğ½ğ‘¤ is lower-bounded above 0, so (16) fails. Likewise, in
the former case, if there is not also a sequence with ğ‘šğ‘¤ğ‘˜ â†’ âˆ, then ğ‘šğ‘¤ and thus ğ½ğ‘¤
is upper-bounded, so (16) fails.
Suppose both sequences exist. Then for any ğœ€ > 0, there is a ğ‘¤1 with ğ‘šğ‘¤1 = ğœ€ (and
ğ½ğ‘¤1 = (ğ‘˜/ğœ€)âˆ’ğ›¾ ) and, by continuity, a ğ‘¤0 < ğ‘¤ such that ğ‘šğ‘¤0 = ğ‘˜ (and ğ½ğ‘¤0 = 1) and

22
ğ‘šğ‘¤ < ğ‘˜ âˆ€ğ‘¤ âˆˆ (ğ‘¤0 , ğ‘¤1 ]. Observe that, by definition of ğ» ,
ğ‘¤1

ğ»ğ‘¤1 < ğ»ğ‘¤0 + âˆ«

ğ‘¤0

(ğ‘¤1 âˆ’ ğ‘£)âˆ’ğ›½ ğ¿ğ‘£ ğ‘‘ğ‘£

âŸ¹ (ğ‘˜/ğœ€)1/ğ›¾ (ğ‘¤1 âˆ’ ğ‘¤)1âˆ’2ğ›½ < (ğ‘¤0 âˆ’ ğ‘¤)1âˆ’2ğ›½ + âˆ«

ğ‘¤1

ğ‘¤0

âŸ¹ (ğ‘˜/ğœ€)1/ğ›¾ < 1 + (ğ‘¤1 âˆ’ ğ‘¤)2ğ›½âˆ’1 âˆ«

ğ‘¤1

ğ‘¤

= 1 + (ğ‘¤1 âˆ’ ğ‘¤)

2ğ›½âˆ’1

(ğ‘¤1 âˆ’ ğ‘£)âˆ’ğ›½ ğ‘˜(ğ‘£ âˆ’ ğ‘¤)âˆ’ğ›½ ğ‘‘ğ‘£
(ğ‘¤1 âˆ’ ğ‘£)âˆ’ğ›½ ğ‘˜(ğ‘£ âˆ’ ğ‘¤)âˆ’ğ›½ ğ‘‘ğ‘£

(Î“(1 âˆ’ ğ›½))2
ğ‘˜
(ğ‘¤1 âˆ’ ğ‘¤)1âˆ’2ğ›½
Î“(2 âˆ’ 2ğ›½)

= 1 + ğ‘˜ğ‘†.
But this fails for small ğœ€. This establishes that âˆƒ ğ‘š > 0, ğ‘š < âˆ âˆ¶ ğ‘šğ‘¤ âˆˆ [ğ‘š, ğ‘š] âˆ€ğ‘¤.
For all ğ‘¤,
ğ›¾

ğ‘˜ â‰¤ ğ‘šğ‘¤ (ğ‘šğ‘¤ ğ‘† ) â‰¤ ğ‘šğ‘¤ (ğ‘šğ‘†)ğ›¾

ğ›¾

and

ğ›¾

ğ‘˜ â‰¥ ğ‘šğ‘¤ (ğ‘šğ‘¤ ğ‘† ) â‰¥ ğ‘šğ‘¤ (ğ‘šğ‘† ) .

So, since for any ğ‘šğ‘¤ > ğ‘š there is a corresponding ğ‘¤ for which the first expression
holds and for any ğ‘šğ‘¤ < ğ‘š a corresponding ğ‘¤ for which the second expression holds,
we have
ğ‘š â‰¥ ğ‘˜(ğ‘šğ‘†)âˆ’ğ›¾ ,

ğ‘š â‰¤ ğ‘˜(ğ‘šğ‘†)âˆ’ğ›¾

âŸ¹ ğ‘š ğ‘šğ›¾ â‰¤ ğ‘˜ğ‘† âˆ’ğ›¾ â‰¤ ğ‘š ğ‘šğ›¾
âŸ¹ ğ‘šâ‰¥ğ‘š
because ğ›¾ < 1. So ğ‘šğ‘¤ = ğ‘š is constant.
To find ğ‘š, subject ğ¿ğ‘¤ = ğ‘š(ğ‘¤ âˆ’ ğ‘¤)âˆ’ğ›½ to the unit labor constraint:
ğ‘¤

âˆ«

ğ‘¤

ğ¿ğ‘¤ ğ‘‘ğ‘¤ = 1

âŸ¹

ğ‘š = (1 âˆ’ ğ›½)(ğ‘¤ âˆ’ ğ‘¤)ğ›½âˆ’1 .

(17)

23
Substituting into (12) yields
ğ‘¤

ğ´ğ‘¤ = (ğ‘š âˆ«

ğ‘¤

âˆ’ğ›½

((ğ‘¤ âˆ’ ğ‘£)(ğ‘£ âˆ’ ğ‘¤))
2

ğ‘‘ğ‘£)

ğ›¾

ğ›½

1âˆ’2ğ›½
(Î“(1 âˆ’ ğ›½))
âˆ (ğ‘¤ âˆ’ ğ‘¤)ğ›½ ,
= ğ‘š
(ğ‘¤ âˆ’ ğ‘¤)1âˆ’2ğ›½
)
( Î“(2 âˆ’ 2ğ›½)

(18)

confirming that ğ´ğ‘¤ ğ¿ğ‘¤ is constant.
The optimal allocation, disjoint case. Suppose {ğ¿ğ‘›ğ‘¤ } is disjoint but not uniformpartitioning. Let ğœ ğ‘› denote the support of ğ¿ğ‘› . Let ğ¿âˆ—ğ‘¤ be the optimal allocation of a
unit of labor across the interval [0, 1/ğ‘ ]. Let {ğ¿âˆ—ğ‘›
ğ‘¤ } denote the uniform-partitioning
allocation with ğ¿ğ‘›ğ‘¤ = ğ¿âˆ—ğ‘¤+(ğ‘›âˆ’1)/ğ‘ . Let ğœ‡(â‹…) denote the Lebesgue measure.
Suppose that for all ğ‘›,

ğ¿ğ‘›ğ‘¤ = ğ¿âˆ—ğœ‡([0,ğ‘¤]âˆ©ğœğ‘› ) âˆ€âˆ€ğ‘¤ âˆˆ ğœ ğ‘› .

(19)

Note the implication that ğœ‡(ğœ ğ‘› ) = 1/ğ‘ for all ğ‘›. By the assumption that {ğ¿ğ‘›ğ‘¤ } is not
uniform-partitioning, we have that for some ğ‘› and ğ‘¤, ğ‘† â‰¡ [ğ‘¤, 1] âˆ© ğœ ğ‘› is positive
measure and ğ´ğ‘›ğ‘¤ is smaller throughout ğ‘†â€”and thus ğ´ğ‘›ğ‘¤ ğ¿ğ‘›ğ‘¤ is smaller a.e. in ğ‘†â€”than if
ğ‘›
ğœ ğ‘› were an interval but (19) were maintained. Thus ğ‘Œ ({ğ¿âˆ—ğ‘›
ğ‘¤ }) > ğ‘Œ ({ğ¿ğ‘¤ }).

Suppose that, for some ğ‘š, (19) fails with respect to all optimal {ğ¿âˆ—ğ‘¤ }. Then let
{ğ¿Ì„ ğ‘›ğ‘¤ } denote a compression of {ğ¿ğ‘›ğ‘¤ }: an allocation with supports {ğœÌ„ ğ‘› } that partition the
interval, maintaining for all ğ‘›
ğ¿Ì„ ğ‘›ğœ‡([0,ğ‘¤]âˆ©ğœÌ„ ğ‘› ) = ğ¿ğ‘›ğœ‡([0,ğ‘¤]âˆ©ğœğ‘› ) âˆ€ğ‘¤ âˆˆ ğœÌ„ ğ‘› .

(20)

Note the implication that ğœ‡(ğœÌ„ ğ‘› ) = ğœ‡(ğœ ğ‘› ) for all ğ‘›, and that because every workflow in
ğœÌ„ ğ‘› can be mapped to a workflow in ğœÌ„ ğ‘› with equal labor but weakly less effective labor,
ğ‘Œ ({ğ¿Ì„ ğ‘›ğ‘¤ }) â‰¥ ğ‘Œ ({ğ¿ğ‘›ğ‘¤ }). Now observe that either ğœ‡(ğœÌ„ ğ‘š ) > 1/ğ‘ for some ğ‘š or ğœ‡(ğœÌ„ ğ‘› ) = 1/ğ‘
for all ğ‘›. In the first case, since the highest infimum achievable over a wider interval is
Ì„ğ‘›
less than that over a narrower interval, it follows immediately that ğ‘Œ ({ğ¿âˆ—ğ‘›
ğ‘¤ }) > ğ‘Œ ({ğ¿ğ‘¤ }).
In the second case, {ğ¿Ì„ ğ‘›ğ‘¤ } is uniform-partitioning; but by the assumption that (19) fails
Ì„ğ‘›
for some ğ‘›, ğ‘Œ ({ğ¿âˆ—ğ‘›
ğ‘¤ }) > ğ‘Œ ({ğ¿ğ‘¤ }).

24
The optimal allocation, non-disjoint case. Suppose {ğ¿ğ‘›ğ‘¤ } is not disjoint. We will
first construct, for any ğ›¿ > 0, a disjoint allocation {ğ¿ğ‘›ğ‘¤ }(ğ›¿) with ğ‘Œ (ğ›¿) â‰¥ (1 âˆ’ ğ›¿)ğ‘Œ . We
will then find that there is another disjoint allocation {ğ¿Ì‚ ğ‘›ğ‘¤ }(ğ›¿) and a ğœ‰ > 1 such that
ğ‘ŒÌ‚ğ‘¤(ğ›¿) â‰¥ ğœ‰(1 âˆ’ ğ›¿)ğ‘Œ or ğ‘ŒÌ‚ğ‘¤(ğ›¿) â‰¥ ğœ‰ğ‘Œ (ğ›¿) . This will imply that for ğ›¿ < 1 âˆ’ 1/ğœ‰, ğ‘ŒÌ‚ (ğ›¿) > ğ‘Œ , and
thus that any optimal allocation is disjoint.
Given a set ğ‘ƒ, let îˆºğ‘ƒ â‰¡ {ğ‘› âˆ¶ ğ‹ğ‘›ğ‘ƒ > 0}. Choose a ğ‘‰ = (ğ‘£, ğ‘£] such that |îˆºğ‘‰ | > 1 and,
for all ğ‘› âˆˆ îˆºğ‘‰ , ğ‹ğ‘›ğ‘‰âˆ’ > 0 and ğ‹ğ‘›ğ‘‰+ > 0, where ğ‘‰âˆ’ â‰¡ (0, ğ‘£] and ğ‘‰+ â‰¡ (ğ‘£, 1]. Let îˆ¼ 0 be the

partition of (0, 1] consisting of ğ‘‰âˆ’ , ğ‘‰ , and ğ‘‰+ .
Let

ğ‘¤ğ‘› â‰¡ max{ğ‘¤ âˆ¶ ğ‹ğ‘›ğ‘¤ = 0}.
Given ğ›¿ > 0, further (infinitely) partition the interval as follows.
ğ‘› as in (13) (with the ğ‘› superscripts restored), let
Defining ğ‘§(â‹…)
ğ‘›
.
ğ‘ ğ‘› (ğ‘¤; ğ‘¤) â‰¡ max ğ‘§ğ‘¤;ğ‘¤
ğ‘¤âˆˆ[ğ‘¤,ğ‘¤]

(21)

ğ‘› and compactness of [ğ‘¤, ğ‘¤],
Observe that ğ‘ ğ‘› (â‹…; ğ‘¤) is defined, by the continuity of ğ‘§(â‹…;â‹…)

and is strictly decreasing, with ğ‘ ğ‘› (ğ‘¤ğ‘› , ğ‘¤) = 1 and ğ‘ ğ‘› (ğ‘¤, ğ‘¤) = 0 for all ğ‘¤ > ğ‘¤ğ‘› . It
ğ‘›
ğ‘›
= 0. To
and the fact that ğ‘§ğ‘¤;ğ‘¤
is trivially right-continuous, by the continuity of ğ‘§(â‹…);ğ‘¤

show that it is left-continuous, take an increasing sequence ğ‘£ğ‘˜ â†’ ğ‘£0 , and for each ğ‘˜,
choose ğ‘¤ğ‘˜ âˆˆ [ğ‘£ğ‘˜ , ğ‘¤] such that ğ‘§ğ‘£ğ‘›ğ‘˜ ;ğ‘¤ğ‘˜ = ğ‘ ğ‘› (ğ‘£ğ‘˜ ; ğ‘¤). The sequence {ğ‘¤ğ‘˜ } must have a

convergent subsequence, so relabel the sequence so that ğ‘¤ğ‘˜ â†’ ğ‘¤âˆ— . By continuity of
ğ‘› ,
ğ‘§(â‹…;â‹…)

ğ‘ ğ‘› (ğ‘£ğ‘˜ ; ğ‘¤) = ğ‘§ğ‘£ğ‘›ğ‘˜ ;ğ‘¤ğ‘˜ â†’ ğ‘§ğ‘£ğ‘›0 ;ğ‘¤âˆ— â‰¤ ğ‘ ğ‘› (ğ‘£0 ; ğ‘¤).

Since ğ‘ ğ‘› (â‹…; ğ‘¤) is decreasing, the above must hold with equality.
Let
ğ›¿1 â‰¡ 1 âˆ’ (1 âˆ’ ğ›¿)1/3 .
For each ğ‘›, let ğœˆ0ğ‘› = 1, and then for ğ‘˜ = 0, 1, ... let
ğ‘› = [ğ‘¤ âˆ¶ ğ‘ ğ‘› (ğ‘¤; ğœˆ ğ‘› ) = ğ›¿ ] (defined by the continuity of ğ‘ ğ‘› (â‹…, ğœˆ ğ‘› )),
â€¢ ğœˆğ‘˜+1
1
ğ‘˜
ğ‘˜
ğ‘› + argmax
â€¢ ğœˆğ‘˜ğ‘›âˆ’ = (ğœˆğ‘˜+1
ğ‘¤âˆˆ[ğœˆ ğ‘› ,ğœˆ ğ‘› ] ğ‘§ğœˆ ğ‘› ;ğ‘¤ )/2, and
ğ‘˜+1 ğ‘˜

ğ‘˜+1

ğ‘›
ğ‘›
ğ‘›
â€¢ ğœˆ(ğ‘˜+1)
âˆ’ = [ğ‘¤ âˆ¶ ğ‘ (ğ‘¤; ğœˆ(ğ‘˜+1)+ ) = ğ›¿1 ].

25
(The resulting sequence will approach ğ‘¤ğ‘› , but instead of proving this, we can simply
note that if ğ‘£ğ‘˜ğ‘› â†’ ğ‘¤Ìƒ > ğ‘¤ğ‘› , we may let ğœˆğœ”ğ‘› = ğ‘¤Ìƒ and repeat the process indefinitely until
ğ‘¤ğ‘› is approached.) Then let îˆ½ğ‘› be the partition of (0, 1] consisting of (0, ğ‘¤ğ‘› ] and the
subintervals, open below and closed above, separated by {ğœˆğ‘˜ğ‘› , ğœˆğ‘˜ğ‘›âˆ’ }.
Let
ğ›¿2 â‰¡ (1 âˆ’ ğ›¿)

âˆ’

1âˆ’2ğ›½
3ğ›½ 2 âˆ’ 1,

and let {ğ‘ğœ… } denote the unique elements of {ğœˆğ‘˜ğ‘› , ğœˆğ‘˜ğ‘›âˆ’ } in descending order. Construct
Ìƒ < ğ›¿2 (ğ‘ğœ… âˆ’ ğ‘ğœ…+1 ) for all ğ‘„Ìƒ âˆˆ îˆ½Ìƒ ğ‘› â§µ [0, ğ‘¤ğ‘› ]
îˆ½Ìƒ ğ‘› by refining îˆ½ğ‘› so that, for each ğœ… â‰¥ 1, |ğ‘„|
Ìƒ â‰¤ ğ‘ğœ…+1 .
with max(ğ‘„)
Construct îˆ¼ ğ‘› by refining îˆ½Ìƒ ğ‘› so that
inf ğ‘¤âˆˆğ‘ƒ ğ´ğ‘›ğ‘¤
> (1 âˆ’ ğ›¿)1/3 âˆ€ğ‘ƒ âˆˆ îˆ¼ ğ‘› â§µ [0, ğ‘¤ğ‘› ].
supğ‘¤âˆˆğ‘ƒ ğ´ğ‘›ğ‘¤

(22)

ğ‘›
Finally, let îˆ¼ â‰¡ â‹€ğ‘
ğ‘›=0 îˆ¼ . We will now reallocate labor within each ğ‘ƒ âˆˆ îˆ¼ so that

the resulting allocation {ğ¿ğ‘›ğ‘¤ }(ğ›¿) is disjoint across workflows and at least (1âˆ’ğ›¿) as much
effective labor is allocated to each workflow as under {ğ¿ğ‘›ğ‘¤ }.
For each ğ‘ƒ âˆˆ îˆ¼, reallocate {ğ‹ğ‘›ğ‘ƒ } into |îˆºğ‘ƒ | sub-intervals as follows. Let ğ‘¤1 = inf(ğ‘ƒ).
Then for each ğœˆ = 1, ..., |îˆºğ‘ƒ |, pick an ğ‘› âˆˆ îˆºğ‘ƒ arbitrarily from among those not yet
chosen, and set ğ‘¤ğœˆ+1 such that ğ˜(ğ‘¤ğœˆ ,ğ‘¤ğœˆ+1 ] = ğ˜ğ‘›ğ‘ƒ . Allocate ğ¿ğ‘›(ğ›¿) across (ğ‘¤ğœˆ , ğ‘¤ğœˆ+1 ] in

proportion to ğ‘Œğ‘¤ subject to ğ‹ğ‘›(ğ›¿)
(ğ‘¤ ,ğ‘¤
ğœˆ

ğœˆ+1 ]

= ğ‹ğ‘›ğ‘ƒ :

ğ¿ğ‘›(ğ›¿)
ğ‘¤ = ğ‘Œğ‘¤ Ã—

ğ‹ğ‘›ğ‘ƒ
ğ˜(ğ‘¤ğœˆ ,ğ‘¤ğœˆ+1 ]

(23)

.

ğ‘› index the boundary point of partition îˆ½ğ‘› with ğ‘¤ âˆˆ
Given ğ‘› and ğ‘¤ > ğ‘¤ğ‘› , let ğœ…ğ‘¤
(ğ‘ğœ…ğ‘› +1 , ğ‘ğœ…ğ‘› ]. Given any reallocation {ğ¿Ì‚ ğ‘›ğ‘¤ } that maintains ğ‹Ì‚ ğ‘› = ğ‹ğ‘› for all ğ‘„Ìƒ âˆˆ îˆ½Ìƒ ğ‘› ,
ğ‘¤

ğ‘„Ìƒ

ğ‘¤

ğ‘„Ìƒ

ğ´Ì‚ ğ‘›ğ‘¤ can be lower-bounded by the value it would take in a reallocation in which (a) ğ‘›â€™s
Ìƒ â‰¤ ğ‘ğœ…ğ‘› +2 ,
labor in (ğ‘ğœ…ğ‘› +2 , ğ‘¤] were discarded and (b) within every ğ‘„Ìƒ âˆˆ îˆ½Ìƒ ğ‘› with max(ğ‘„)
ğ‘¤

ğ‘¤

Ìƒ Shift (a) multiplies ğ´ğ‘›ğ‘¤ by at least 1 âˆ’ ğ›¿1 , by
ğ‘›â€™s labor was shifted entirely to inf(ğ‘„).

26
construction of îˆ½ğ‘› . For each ğ‘„Ìƒ âˆˆ îˆ½Ìƒ ğ‘› , shift (b) replaces âˆ«ğ‘„Ìƒ (ğ‘¤ âˆ’ ğ‘£)âˆ’ğ›½ ğ¿ğ‘›ğ‘£ ğ‘‘ğ‘£ with
âˆ’ğ›½
âˆ’ğ›½ ğ‘›
Ìƒ âˆ’ğ›½ ğ‘›
âˆ« Ìƒ (ğ‘¤ âˆ’ inf(ğ‘„)) ğ¿ğ‘£ ğ‘‘ğ‘£ â‰¥ (1 + ğ›¿2 ) âˆ« Ìƒ (ğ‘¤ âˆ’ ğ‘£) ğ¿ğ‘£ ğ‘‘ğ‘£,
ğ‘„

ğ‘„

âˆ’

ğ›½2

so multiplies ğ‘›â€™s productivity at ğ‘¤ by at least (1 + ğ›¿2 ) 1âˆ’2ğ›½ . In particular,
ğ‘›
2/3 ğ‘›
ğ´ğ‘›(ğ›¿)
ğ‘¤ â‰¥ (1 âˆ’ ğ›¿1 )(1 âˆ’ ğ›¿2 )ğ´ğ‘¤ = (1 âˆ’ ğ›¿) ğ´ğ‘¤ .

(24)

Given ğ‘¤, let ğ‘› be the unique worker with ğ¿ğ‘›(ğ›¿)
ğ‘¤ > 0, and let ğ‘ƒ be the unique element
of îˆ¼ with ğ‘¤ âˆˆ ğ‘ƒ. By (23) and (24),
ğ‘›(ğ›¿)
2/3 ğ‘›
ğ‘›
ğ‘›
ğ‘Œğ‘¤(ğ›¿) = ğ´ğ‘›(ğ›¿)
ğ‘¤ ğ¿ğ‘¤ â‰¥ (1 âˆ’ ğ›¿) ğ´ğ‘¤ ğ‘Œğ‘¤ ğ‹ğ‘ƒ /ğ˜ğ‘ƒ .

Since ğ‹ğ‘›ğ‘ƒ /ğ˜ğ‘›ğ‘ƒ â‰¥ 1/ supğ‘£âˆˆğ‘ƒ ğ´ğ‘›ğ‘£ , by (22) we have
ğ´ğ‘›ğ‘¤ ğ‹ğ‘›ğ‘ƒ /ğ˜ğ‘›ğ‘ƒ â‰¥ (1 âˆ’ ğ›¿)1/3 .

(25)

So ğ‘Œğ‘¤(ğ›¿) â‰¥ (1 âˆ’ ğ›¿)ğ‘Œğ‘¤ .
Let ğœ ğ‘›(ğ›¿) denote the support of ğ¿ğ‘›(ğ›¿) . Choose an ğ‘› âˆˆ îˆºğ‘‰ with
ğœ‡(ğœ ğ‘›(ğ›¿) âˆ© ğ‘‰ ) < |ğ‘‰ |/|îˆºğ‘‰ |.

(26)

Let {ğ¿Ì„ ğ‘›ğ‘¤ }(ğ›¿) be a compression of {ğ¿ğ‘›ğ‘¤ }(ğ›¿) (i.e. an allocation with supports that partition the interval, maintaining the analog of (20)), and let upper bars indicate the corresponding supports, productivities, etc. ğœÌ„ ğ‘›(ğ›¿) will denote the supportâ€™s maximum.
WLOG assume that inf(ğœÌ„ ğ‘› ) = 0.
Let ğ‘£âˆ— âˆˆ ğœÌ„ ğ‘›(ğ›¿) denote the unique workflow with ğ‹Ì„ ğ‘›(ğ›¿)
= 1 âˆ’ ğ‹ğ‘›ğ‘‰+ , and let ğ‘‰Ì„ + â‰¡
ğ‘£âˆ—
(ğ›¿)
ğ‘›(ğ›¿)
âˆ—
(ğ‘£ , max(ğœÌ„ ğ‘› )]. Observe that for each ğ‘¤ âˆˆ ğ‘‰Ì„ + , there is an ğ‘¥(ğ‘¤) âˆˆ ğ‘‰+ with ğ¿Ì„ ğ‘¤ =
ğ‘›(ğ›¿)
Ì„ ğ‘›(ğ›¿)
ğ¿ğ‘›(ğ›¿)
ğ‘¥(ğ‘¤) and ğ‹ğ‘¤ = ğ‹ğ‘¥(ğ‘¤) . Let

ğ‘‹ â‰¡ (1 âˆ’

1
|ğ‘‰ | âˆˆ (0, 1).
|îˆºğ‘‰ | )

27
Due to the compression across ğ‘‰ in particular, we have for all ğ‘¤ âˆˆ ğ‘‰Ì„ + ,
ğ‘¥(ğ‘¤)
âˆ’ğ›½ ğ‘›(ğ›¿)
âˆ’ğ›½ ğ‘›(ğ›¿)
ğ¿ğ‘£ ğ‘‘ğ‘£ + âˆ«
(ğ‘¥(ğ‘¤) âˆ’ ğ‘£) ğ¿ğ‘£ ğ‘‘ğ‘£
0
inf(ğ‘‰ )
inf(ğ‘‰ )
ğ‘¥(ğ‘¤)
âˆ’ğ›½ ğ‘›(ğ›¿)
âˆ’ğ›½ ğ‘›(ğ›¿)
â‰¥ ğ‘‹ âˆ’ğ›½ âˆ«
(ğ‘¥(ğ‘¤) âˆ’ ğ‘£) ğ¿ğ‘£ ğ‘‘ğ‘£
(ğ‘¥(ğ‘¤) âˆ’ ğ‘£) ğ¿ğ‘£ ğ‘‘ğ‘£ + âˆ«
0
inf(ğ‘‰ )
inf(ğ‘‰ )
âˆ’ğ›½ ğ‘›(ğ›¿)
ğ‘›(ğ›¿)
â‰¥ (ğ‘‹ âˆ’ğ›½ âˆ’ 1) âˆ«
(ğ‘¥(ğ‘¤) âˆ’ ğ‘£) ğ¿ğ‘£ ğ‘‘ğ‘£ + ğ»ğ‘¥(ğ‘¤)
0
2
âˆ’ğ›½
ğ‘›
ğ‘›
â‰¥ (ğ‘‹ âˆ’ 1)ğ‹ğ‘‰âˆ’ + (1 âˆ’ ğ›¿) 3 ğ›¾ ğ»ğ‘¥(ğ‘¤)
by (24)
2ğ›¾
ğ‘‹ âˆ’ğ›½ âˆ’ 1 ğ‘›
ğ‘›
3
â‰¥[
ğ‹
+
(1
âˆ’
ğ›¿)
ğ»ğ‘¥(ğ‘¤)
ğ‘‰âˆ’
ğ‘›
]
maxğ‘¤ ğ»ğ‘¤
1
ğ‘‹ âˆ’ğ›½ âˆ’ 1 2ğ›¾1
ğ‘› ğ›¾ ğ‘›
ğ‘›
ğ‘‹ â‰¡ (1 +
â‰¥ (ğ‘‹ ) ğ»ğ‘¥(ğ‘¤) ,
>1
2 maxğ‘¤ ğ»ğ‘¤ğ‘› )

ğ»Ì„ ğ‘¤ğ‘›(ğ›¿) â‰¥ âˆ«

inf(ğ‘‰ )

(ğ‘¥(ğ‘¤) âˆ’ ğ‘£ âˆ’ ğ‘‹ )

for

1

ğ‘›

3

ğ›¿ â‰¤ ğ›¿ â‰¡ 1 âˆ’ (2 âˆ’ (ğ‘‹ ğ‘› ) ğ›¾ ) 2ğ›¾ ;

(27)

ğ‘›(ğ›¿)
ğ‘›(ğ›¿)
ğ‘› ğ‘›
so ğ´Ì„ ğ‘›(ğ›¿)
ğ‘¤ â‰¥ ğ‘‹ ğ´ğ‘¥(ğ‘¤) ; and so, because ğ¿Ì„ ğ‘¤ = ğ¿ğ‘¥(ğ‘¤) ,
ğ‘›
ğ‘ŒÌ„ğ‘¤(ğ›¿) â‰¥ ğ‘‹ ğ‘› ğ¿ğ‘›(ğ›¿)
ğ‘¥(ğ‘¤) ğ´ğ‘¥(ğ‘¤)

â‰¥ ğ‘‹ ğ‘› (1 âˆ’ ğ›¿)1/3 ğ‘Œ

by (23) and (25),

where ğ‘‹ ğ‘› is independent of ğ›¿. Thus ğ‘ŒÌ„ğ‘¤(ğ›¿) â‰¥ (ğ‘‹ ğ‘› )1/2 ğ‘Œ for
ğ‘›

3

ğ›¿ â‰¤ ğ›¿ ğ‘› â‰¡ min (ğ›¿ , 1 âˆ’ (ğ‘‹ ğ‘› )âˆ’ 2 ).
âˆ’1

1âˆ’2ğ›½

Assume ğ›¿ â‰¤ ğ›¿ ğ‘› . Let ğœ’ ğ‘› â‰¡ 1 âˆ’ (ğ‘‹ ğ‘› ) 4 1âˆ’ğ›½ âˆˆ (0, 1), and let
â§
ğœ’ ğ‘› ğ‹ğ‘›ğ‘‰+ /2
âª
âˆ—
âª
1
+
ğ¿Ì„ ğ‘›(ğ›¿)
ğ‘›
ğ‘¤ , ğ‘¤â‰¤ğ‘£ ;
âª
1âˆ’ğ‹
(
)
ğ‘‰+
âª
âª
âª
âª(1 âˆ’ ğœ’ ğ‘› )ğ¿Ì„ ğ‘›(ğ›¿)
ğ‘¤ âˆˆ ğ‘‰Ì„ + ;
ğ‘¤ ,
Ì‚ğ¿ğ‘›(ğ›¿)
=
â¨
ğ‘¤
âª
âª
ğœ‰2ğ‘› ğ‘Œ ,
ğ‘¤ âˆˆ (ğœÌ„ ğ‘›(ğ›¿) , ğœÌ„ ğ‘›(ğ›¿) + Î”ğ‘› ];
âª
âª
âª
âª
0,
ğ‘¤ > ğœÌ„ ğ‘›(ğ›¿) + Î”ğ‘› ,
âª
â©

(28)

28
where
ğ‘›

Î” â‰¡

ğœ’ ğ‘› ğ‹ğ‘›ğ‘‰+ /2
ğœ‰2ğ‘› ğ‘Œ

âˆ’ğ›¾

ğœ‰2ğ‘› â‰¡ (1 âˆ’ ğœ’ ğ‘› ğ‹ğ‘›ğ‘‰+ /2)

,

ğ‘›

/ğœ’ .

Multiplying labor within ğ‘‰Ì„ + by 1 âˆ’ ğœ’ ğ‘› multiplies productivity throughout ğ‘‰Ì„ + by
ğ›½

at least (1 âˆ’ ğœ’ ) 1âˆ’2ğ›½ , yielding
1âˆ’ğ›½

1

1

ğ‘ŒÌ‚ğ‘¤(ğ›¿) â‰¥ (1 âˆ’ ğœ’ ) 1âˆ’2ğ›½ ğ‘ŒÌ„ğ‘¤(ğ›¿) = (ğ‘‹ ğ‘› )âˆ’ 4 ğ‘ŒÌ„ğ‘¤(ğ›¿) â‰¥ (ğ‘‹ ğ‘› ) 4 ğ‘Œ âˆ€ğ‘¤ âˆˆ ğ‘‰Ì„ + .
Of the ğœ’ ğ‹ğ‘›ğ‘‰+ units of labor saved by the reallocation from ğ‘‰Ì„ + , half is allocated to
âˆ—
(0, ğ‘£âˆ— ] in proportion to ğ¿Ì„ ğ‘›(ğ›¿) . Letting ğœ‰1ğ‘› denote the coefficient on ğ¿Ì„ ğ‘›(ğ›¿)
ğ‘¤ in the ğ‘¤ â‰¤ ğ‘£
interval, we have
ğ‘ŒÌ‚ğ‘¤(ğ›¿) â‰¥ ğœ‰1ğ‘› ğ‘Œ (ğ›¿) âˆ€ğ‘¤ < ğ‘£âˆ— ,
where ğœ‰1ğ‘› > 1 is independent of ğ›¿.
The other half is allocated to an interval above ğœÌ„ ğ‘›(ğ›¿) . For all ğ‘¤ â‰¥ ğœÌ„ ğ‘›(ğ›¿) we must
ğ›¾
ğ‘› ğ‘›
âˆ’ğ›½ cannot
have ğ´Ì‚ ğ‘›(ğ›¿)
ğ‘¤ â‰¥ (1âˆ’ğœ’ ğ‹ /2) , because the penalty to distant learning (ğ‘¤ âˆ’ğ‘£)
ğ‘‰+

be less than 1 (at ğ‘¤ âˆ’ ğ‘£ = 1). So
ğ‘›
Ì‚ ğ‘›(ğ›¿)
Ì„ ğ‘›(ğ›¿) , ğœÌ„ ğ‘›(ğ›¿) + Î”ğ‘› ],
ğ´Ì‚ ğ‘›(ğ›¿)
ğ‘¤ ğ¿ğ‘¤ > ğ‘Œ /ğœ’ âˆ€ğ‘¤ âˆˆ (ğœ

where 1/ğœ’ ğ‘› > 1 is independent of ğ›¿.
Ì„ ğ‘›(ğ›¿) + Î”ğ‘› , 1]. Relative to
For ğ‘š â‰  ğ‘›, let {ğ¿Ì‚ ğ‘š(ğ›¿)
ğ‘¤ } be an optimal uniform partition of (ğœ
an optimal uniform partition of (ğœÌ„ ğ‘›(ğ›¿) , 1]â€”which would yield output weakly greater
than ğ‘ŒÌ„ğ‘¤(ğ›¿) â€”this partition assigns each ğ‘š â‰  ğ‘› an interval at most 1 âˆ’ Î” times as large.
So, by (17) and (18),
ğ‘ŒÌ‚ğ‘¤(ğ›¿) â‰¥ (1 âˆ’ Î”ğ‘› )2ğ›½âˆ’1 ğ‘ŒÌ„ (ğ›¿) âˆ€ğ‘¤ > ğœÌ„ ğ‘›(ğ›¿) + Î”ğ‘› ,
where Î” is independent of ğ›¿.
Denote

1

ğœ‰ ğ‘› â‰¡ min ((ğ‘‹ ğ‘› ) 4 , ğœ‰1ğ‘› , 1/ğœ’ ğ‘› , (1 âˆ’ Î”ğ‘› )2ğ›½âˆ’1 ) > 0.
We have established that for ğ›¿ â‰¤ ğ›¿ ğ‘› , ğ‘ŒÌ‚ğ‘¤(ğ›¿) â‰¥ ğœ‰ ğ‘› ğ‘Œ or ğ‘ŒÌ‚ğ‘¤(ğ›¿) â‰¥ ğœ‰ ğ‘› ğ‘Œ (ğ›¿) = (1 âˆ’ ğ›¿)ğœ‰ ğ‘› ğ‘Œ for all
ğ‘¤. So, if ğ›¿ < min(ğ›¿ ğ‘› , 1 âˆ’ 1/ğœ‰ ğ‘› ) for some ğ‘› satisfying (26) given ğ›¿, ğ‘ŒÌ‚ (ğ›¿) > ğ‘Œ . And given

29
any ğ›¿ > 0, there is some ğ‘› âˆˆ îˆºğ‘‰ satisfying (26). So for all ğ›¿ < minğ‘› (min(ğ›¿ ğ‘› , 1âˆ’1/ğœ‰ ğ‘› )),
ğ‘ŒÌ‚ (ğ›¿) > ğ‘Œ .
