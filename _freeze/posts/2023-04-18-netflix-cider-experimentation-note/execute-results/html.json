{
  "hash": "6663e65425977434f2e9c69378968e0a",
  "result": {
    "markdown": "---\ntitle: Four Experimentation Problems\nauthor: Tom Cunningham, [Integrity Institute](https://integrityinstitute.org/)\nexecute:\n  echo: false\n  cache: true # caches chunk output\ndate: 2023-10-13\nnumber-sections: true\nbibliography: inference.bib\ncsl: journal-of-development-economics.csl\nformat:\n   html:\n      toc: true\n      toc-depth: 2\n      toc-location: left\n      html-math-method:\n         method: mathjax\n         url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js\"\n         #     ^ this forces SVG instead of CHTML, otherwise xypic renders weird\n      include-in-header:\n         - text: |\n            <script>window.MathJax = {\n               loader: { load: [\"https://cdn.jsdelivr.net/gh/sonoisa/XyJax-v3@3.0.1/build/xypic.js\"]},\n               tex: {packages: {'[+]': ['xypic','bm']},\n                     macros: {  bm: [\"\\\\boldsymbol{#1}\", 1],\n                                ut: [\"\\\\underbrace{#1}_{\\\\text{#2}}\", 2],\n                                utt: [\"\\\\underbrace{#1}_{\\\\substack{\\\\text{#2}\\\\\\\\\\\\text{#3}}}\", 3] }\n               }\n            };\n            </script>\nengine: knitr\nreference-location: margin\nfigure-location: margin\n---\n\n<style>\n    h1 {  border-bottom: 4px solid black; color: black; }\n    h2 {  border-bottom: 1px solid #ccc;}\n</style>\n\n<!-- \n- http://tecunningham.github.io/posts/2023-04-18-netflix-cider-experimentation-note.html\n- Good writeup at 2023-01-12 Giorgio / Inference on Winners\n- **Leontief sandwich.**\n- **Everyone is implicitly Bayesian.** The choice of which treatment to run an experiment on, and which metrics to look at, reflects your priors. We are doing mostly *empirical Bayes* here. Empirical Bayes is half-baked bread, frequentist is just raw dough, completely inedible.\n-->\n\n\n#                                  Introduction\n\n<!-- **Tech firms have thousands of experiments running at a given time.** E.g. experiments changing user interface, changing weights in a ranking algorithm, changing price, changing the number of ads shown, or just canary experiments confirming that a new piece of code hasn't broken anything. -->\n\n**I describe a canonical experimentation situation and give recommendations for four common problems using common Bayesian framework:**\n\n::: {.column-margin}\n   The impetus for writing this up was Netflix's 2023 CIDER conference, many thanks to all participants especially Martin Tingley. Thanks to [J. Mark Hou](https://jmarkhou.com/about/) for comments.\n   ![](images/2023-10-13-11-56-04.png)\n:::\n\n\n1. **Setup:** The canonical tech problem is to choose a policy to maximize long-run user retention. Because the policy space is high-dimensional it's not feasible to run experiments on every alternative (there are trillions), instead most of the decision-making is done with human intuition based on observational data, and experiments are run to confirm those intuitions.\n\n2. **The inference problem.** The basic problem of experimentation is to estimate the true effect given the observed effect. The problem can become complicated when we have a set of different observed effects, e.g. across experiments, across metrics, across subgroups, or across time.\n      <!-- $$E[\\utt{t_1}{true effect}{on metric 1}\n      |\\utt{\\hat{t}_1,\\ldots,\\hat{t}_n}{observed effects}{on metrics 1...n}].$$ -->\n\n   Two common approaches are: (1) adjust confidence intervals (e.g. Bonferroni, always-valid, FDR-adjusted); (2) adjust point estimates based on the distribution (empirical Bayes). Both have significant drawbacks: my suggested approach is to let decision-makers make their own best-estimates of the true effects but provide them with an informative set of *benchmark* statistics so they can compare the results of any given experiment to the results from a reference group.\n\n3. **The extrapolation problem.** Given an effect on metric A what's our best estimate of the effect on metric B? This problem is common to observational inference, proximal goals, and extrapolation.\n\n   There are three approaches to solving this: (1) using raw priors; (2) using correlation across units (surrogacy); (3) using correlation across experiments (meta-analysis). I argue that approach #3 is generally the best option but reasonable care needs to be taken in interpreting the results.\n\n4. **The explore-exploit problem.** We would like to choose which experiments to run in an efficient and automated way. I think the technical solution is relatively clear but tech companies have struggled to implement it because good execution requires some discipline. I describe a simple algorithm that is not optimal but very simple and robust.\n\n5. **The culture problem.** Inside tech companies people keep misusing experiments and misinterpreting the results, especially (1) running under-powered experiments, (2) selectively choosing results, and (3) looking at correlations without thinking about identification. \n\n   A common response is to restrict access to only a subset of experiment resuts. However this often backfires because (1) it is difficult to formally specify the right subset; (2) it reinforces a perception that experimental results can be interpreted as best-estimates of true treatment effects; (3) it reinforces a norm of selecting experimental results as arguments for a desired outcome. I think a better alternative is to explicitly frame the problem as one of predicting the true effect given imperfect evidence, and benchmark peoples' prior performance in predicting the true effect of an intervention.\n\n   (This section is unfinished, I hope to add more).\n\n#                                  Setup\n\n**Firms choose their policy to maximize user retention.** As a simplified model companies are choosing policies to maximize long-run retention (or revenue). A policy is, for example, a recommendation algorithm, or notification algorithm, or the text and images used in an advertisement or the UX on a signup page. Notice that policies are very high dimensional: there are millions or billions of alternatives, while we usually run only a few experiments.^[In fact variation in the success of tech platforms is primarily due to variation in the inflow of new users, not due to variation in retention rates. However growth in new users is driven by the attractiveness of the product and retention is a good proxy for this.]\n\n**Experiments are not the primary source of causal knowledge.** People already have substantial knowledge about the effects of their decisions without either randomized experiments or natural experiments (IV, RDD, etc.). We built cathedrals, aeroplanes, welfare states, we doubled human life-expectancy, & WhatsApp grew to 1B users, all without randomized experiments. Formal causal inference *augments* our already substantial causal knowledge. Inside companies the primary way people learn about causal relationships is raw data (e.g. dashboards) and common-sense reasoning about human behaviour.\n\n**Experiments only solve the low-dimensional problem.** In most cases the dimensionality of the policy space is far higher than the dimensionality of experiment space, thus the responsibility for choosing policies is primarily human judgment, and then humans give a few variants to experiments to compare their performance.\n\n**Most questions related to experiments can be expressed as conditional expectations.** A good workhorse model of experimentation is the following. Suppose we have two metrics #1 and #2. Taking some set of experiments we can think of three joint distributions: the observed effects, the true effects, and the noise:^[For simplicitly assume the experiment doesn't have any effect on variances or covariances of outcomes, the effects are typically small enough that it doesn't matter.]\n\n   $$\\utt{\\binom{\\hat{t}_1}{\\hat{t}_2}}{observed}{effects}\n      =\\utt{\\binom{t_1}{t_2}}{true}{effects (ATE)}\n         +\\ut{\\binom{e_1}{e_2}}{noise}\n         $$\n\n   For simplicity we'll assume everything is normally distributed and has mean zero, then we get two very simple expressions for conditional expectations, and I'll argue that these conditional expectations serve as answers to almost all interesting experimentation questions:\n\n   $$\\begin{aligned}\n      E[t_1|\\hat{t}_1] &= \\utt{\\frac{\\sigma_{t1}^2}{\\sigma_{t1}^2+\\sigma_{e1}^2}}{signal-noise}{ratio}\\hat{t}_1 \n         && \\text{(posterior estimate of treatment effect, AKA shrinkage)} \\\\\n      E[t_2|\\hat{t}_1] &= \\utt{\\rho_{t}\\frac{\\sigma_{t2}}{\\sigma_{t1}}}{covariance}{of $t_1$ and $t_2$}\n            \\utt{\\frac{\\sigma_{t1}^2}{\\sigma_{t1}^2+\\sigma_{e1}^2}}{signal-noise}{ratio of $\\hat{t}_1$}\\hat{t}_1 \n         && \\text{(true effect on metric 2 given observed effect on metric 1)}\n   \\end{aligned}\n   $$\n\n   Once we have a clear expression in terms of conditional expectations we can add on additional considerations: nonlinearities, fat-tailed distributions, strategic problems, etc..\n\n\n   <!-- - Key point - conditional expectations ; observational inference condition can be empirically checked ; classical conditions are a waste of time ;  -->\n\n\n#                                  The Inference Problem\n\n**There are a number of experiment inference problems that we often find difficult.** We will discuss these as pure inference problems without worrying about strategic behaviour (e.g. peeking, cherry-picking).\n   \n   1. Estimate the treatment effect given the observed treatment effect.\n   2. Estimate the long-run treatment effect knowing the short-run observed effect.\n   3. Estimate the treatment effect, knowing the observed effect, and additionally the distribution of observed effects across some set of experiments.\n   4. Estimate the treatment effect on a subgroup, knowing the observed effect, and additionally the distribution of observed effects across all other subgroups.\n   \n**The textbook approach uses *p*-values.** A common approach (NHST) is to treat the true effect as equal to the observed effect if the p-value is below 0.05, and otherwise treat the true effect as zero. This leads to all sorts of well-known difficulties. \n\n**Empirical Bayes estimates are often imperfect.** We could instead calculate empirical-Bayes conditional expectations, $E[\\bm{t}|\\hat{\\bm{t}}]$, based on covariances from prior experiments, and treat those as the true effects. However the distribution of prior experiments is only a subset of the full information set available to the decision-maker, i.e. empirical Bayes is not Bayes, and very often there are idiosyncratic details about this particular experiment that are consequential.\n\n**My recommendation: report benchmark statistics.** The ideal decision process lets humans make a judgment about estimated treatment effects given three ingredients:\n   \n   1. **Raw estimate.** The point estimate and standard error.^[Equivalently, the point-estimate and p-value, or the upper and lower confidence bounds.]\n\n   2. **Benchmark statistic.** We should also report a statistic comparing this observed effect to observed effects of other similar treatments. There are many ways of benchmarking and I think they are all convey the same basic information, e.g. the empirical-bayes shrunk estimate (and there are various shrinkage estimators), the FDR-adjusted p-value, or the fraction of statistically significant experiments. We have to use judgment in defining what a \"similar\" experiment is, and it's important that we report to the end-user what class of similar experiments we're using and how many we have. For the remainder of the section I will assume we are reporting an empirical-bayes shrunk estimate.\n\n   3. **Idiosyncratic details.** We should additional report any information about this treatment relative to the benchmark class, that could be relevant to its effect on this metric. E.g. (1) suppose this experiment only affects iPhone users then it is rational to heavily discount any outcomes on Android use unless they are highly significant; (2) suppose this experiment is a direct replication of a prior experiment, then we will likely wish to shrink our estimates towards that prior experiment rather than towards the mean of all experiments.\n\n**Benchmarking solves all the problems above.** An empirical-Bayes shrunk estimate represents our best guess at the true treatment effect conditional on the experiment being drawn from a given reference class.\n\n**Useful shortcut: using the fraction of significant experiments to do shrinkage.** A convenient rule of thumb for doing empirical Bayes shrinkage is to use the fraction of experiments that are statistically significant in some class. If the fraction is zero then we should shrink all estimates to zero, if the fraction is 20% then we should shrink estimates by about 50%, and if the fraction is 1/2 then we should shrink estimates by about 20%. If everything's Gaussian and every experiment has the same $N$ then the optimal shrinkage factor is $1-(\\frac{1}{1.96}\\Phi^{-1}(\\frac{q}{2}))^2$, where $q$ is the fraction of stat-sig experiments.\n\n##       Strategic Problems\n\n**There are additionally some *strategic* problems in experiment interpretation.**\n\n   1. **Strategic stopping (\"peeking\").** An engineer will wait until an experiment has a high estimated impact, or low p-value, before presenting it for launch review. A common proposed remedy is that all experiments should be evaluated after the same length of time, or that engineers should pre-specify the length of experiments.\n   \n   2. **Selection of treatments (\"winners curse\").** An engineer will run a dozen variants and only present for launch review the best-performing one. A common proposed remedy is that every variant should be officially presented in launch reviews, even the poorly-performing ones.\n\n   3. **Selection of metrics (\"cherry picking\").** An engineer will choose to show the experiment results on the metrics that are favorable, not those that are unfavorable. A common proposed remedy is that the set of metrics should be standardized for all launches, or that the set of evaluation metrics should be pre-specified by the engineer (AKA a pre-analysis plan).\n\nI will argue that the commonly proposed remedies are highly imperfect fixes. These are complicated things to think about because the mix together issues of statistical inference and of strategic behaviour. In the discussion that follows I try to separate those out as clearly as possible.\n\n##       Strategic Stopping\n\n   **I will ignore dynamic effects.** For simplicity assume that all effects are constant, so the length of an experiment effectively determines just the sample size of that experiment. I.e. I will ignore time-dependent and exposure-dependent effects.\n\n   **Stopping rules are irrelevant to expected effect sizes.** Suppose an experiment has a given estimate. Does it matter to your estimate of the true causa effect if you learn that the experimenter chose the sample size $N$ by a data-dependent rule, e.g. continuing to collect data until the estimate was statistically significant? If you are estimating the true causal effect, $E[t|\\hat{t}]$ then it doesn't matter, your posterior will be identical either way.\n\n   **Stopping rules would be relevant if we made decisions based on statistical-significance.** A stopping rule would be relevant if we conditioned only on statistical-signficance instead of the full estimate. In other words the expected true effect, conditioning only on whether the estimated effect is significant, will depend on the distribution of experiments run. For example if people kept running experiments until they were significant then significant experiments would tend to have small effect sizes. However it is clearly bad practice to condition only on this binary piece of information when you have the full estimate, and if you have the full estimate then the stopping rule becomes irrelevant.\n\n   **The optimal stopping rule is data-dependent.** The discussion above took a stopping rule as given, we can also ask what's the efficient stopping rule. It's clear that a fixed length is inefficient: we should stop an experiment sooner if it does unexpectedly well or unexpectedly badly, in both of those cases the value of collecting more information has decreased because it's less likely to change our mind about a launch decision. Thus enforcing a static or pre-specific experiment length will lead to inefficient decision-making.\n\n   **Considering engineers' incentives.** Now consider the launch process as a game, with the engineers trying to persuade the director to launch their feature. Suppose the director's *ex post* optimal strategy is to launch if $E[t|\\hat{t}]>0$, and suppose the engineers get a bonus whenever their feature is launched. In equilibrium the engineers will keep their experiments running until $E[t|\\hat{t}]>0$, which will cause a skew distribution: the distribution of posteriors will show a cluster just above the threshold. The director's strategy is *ex post* optimal but it's not an efficient use of experimentation resources. In this game the director would likely wish to pre-commit to a different threshold which induces more efficient effort by engineers. However a more direct solution would be to align engineers' incentives with those of the director by rewarding them for their true impact, i.e. setting their bonuses proportional to $\\max\\{E[t|\\hat{t}],0\\}$, instead of discontinuously rewarding them for whether or not they launched.\n\n##       Selection of Treatments\n\n**If you learn an experiment is the top-performing variant it should change your asssessment.** Suppose we have a result $\\hat{t}_1$, and we are estimating the true treatment effect, $t_1$. If we learn that another variant has a lower treatment effect, $\\hat{t_1}>\\hat{t}_2$, then it is rational to update our assessment of $t_1$:\n\n   $$\\utt{E[t_1|\\hat{t}_1,\\hat{t}_1>\\hat{t}_2]}{assessment knowing}{it's winner}< \n      \\utt{E[t_1|\\hat{t}_1]}{assessment}{given outcome}\n      $$\n\n   This will hold whenever $Cov(t_1,t_2)>0$, i.e. when we have some shared source of uncertainty about the two treatment effects.[^error] We can write a model for this, however conditioning on this binary information (whether a variant is the winner) is not an efficient way of using the information at your disposal.\n\n   [^error]: Because $t_1$ and $t_2$ represent independent experiments we'll have $cov(e_1,e_2)=0$.\n\n**It's better to condition on the whole distribution.** In almost all cases we know much more than whether $\\hat{t}_1$ is the winner, we also know the value of $\\hat{t}_2$, and then this reduces simply to the empirical Bayes problem, i.e. we simply wish to estimate:\n      $$E[t_1|\\hat{t}_1,\\ldots,\\hat{t}_n],$$\n   and we can do that in the usual way.[^adjustment] E.g. if we have a Normal prior over treatment effects then we can estimate $\\sigma_t^2$ from $Var(\\hat{t})$ and $\\sigma_e^2$. Once we have conditioned on $\\sigma_t^2$ then it becomes irrelevant whether variant 1 is the winner or not, i.e.:\n      $$E[t_1|\\hat{t}_1,\\sigma_t^2]=E[t_1|\\hat{t}_1,\\sigma_t^2,\\hat{t}_1>\\hat{t}_2].$$\n\n   Put another way: the selection rule is irrelevant (just as the stopping rule is irrelevant) once we condition on the distribution of observed outcomes.\n\n**Implication: show the distribution.** If we are worried that engineers are selecting variants based on their outcomes then the simplest and cleanest fix is to calculate the distribution of variants and use that to discount any experiment results, either explicitly with an empirical Bayes estimator, or implicitly by showing the decision-maker the distribution.\n\n   [^adjustment]: @andrews2019inference describes some unbiased estimates for treatment effects conditional on them being winners. In general I would say this is an inefficient use of information, because we know much more about the distribution of treatment effects than just whether a specific variant is the winner. However that paper does argue that empirical Bayes estimates struggle when the sample-size is small or when we are estimating the tails of when variants are non-exchangeable, and in those cases the unbiased estimators may be useful.\n\n##       Selection of Metrics\n\n**Suppose engineers are selectively presenting the most favorable metrics.** Suppose there are two outcome metrics from a single experiment, and the engineer will present whichever is the most favorable. Knowing this fact should rationally affect your judgment of the treatment effect on the presented metric:\n   $$\\utt{E[t_1|\\hat{t}_1]}{assessment knowing}{only metric 1} > \n      \\utt{E[t_1|\\hat{t}_1,\\hat{t}_1>\\hat{t}_2]}{assessment knowing}{metric 1 beats metric 2}$$\n\n**Implication: engineers should present all outcome metrics.**  \n\n<!-- 3. **Alert thresholds.** We want to stop an experiment early if it has either a very good or very bad outcome, but it's difficult to know how to set the threshold in a principled way. -->\n\n\n##       On Launch Criteria\n\n**Choosing weights on metrics for a launch decisions involves many considerations:** network effects, noise, cross-metric proxy effects, and dynamic effects. In addition launch rules serve a bureaucratic role, and engineers will often want the launch rule to be public and without discretion.  To make clear decisions it's important to peel apart these layers, I recommend these steps:\n\n   1. **Choose a set of final metrics.** These are the metrics we would care about *if we had perfect knowledge of the experimental effect.* We can define tradeoffs between them, it's convenient to express those tradeoffs in terms of percentage changes, e.g. we might be indifferent between 1% DAU, 2% time/DAU, and 5% prevalence of bad content.^[Arguably revenue or profit is a more truly final metric, and these are just proxies, but these are probably close enough to final for most purposes.]\n\n   2. **Choose a set of proximal  metrics.** These are the metrics on which we are confident we can detect our experiment's effect, meaning the measured impact will be close to the true impact on these metrics (i.e. has a high signal-noise ratio). To determine whether a metric is moved we can use the fraction of a given class of experiments that have a statistically-significant effect on that metric: if the share is greater than 50% then we can be confident that the estimated effect is close to the true effect.\n\n   3. **Identify *conversion factors* between proximal and final metrics.** These tell us the best-estimate impact on final metrics given the impact on proximal metrics. Conversion factors can be estimated either from (a) long-running tuning experiments; (b) a meta-analysis of prior experiments with similar designs.\n\n      A final linear launch criteria can then be expressed as a set of conversion-factor weights applied to each of the proximal metrics.^[For derivation see @cunningham2019interpreting.]\n\n##                Comparing Launch Rules\n\n\n::: {.cell .column-margin hash='2023-04-18-netflix-cider-experimentation-note_cache/html/unnamed-chunk-1_04c8f358285592a6a2361d89d2e845f8'}\n::: {.cell-output-display}\n![](2023-04-18-netflix-cider-experimentation-note_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n::: {.cell .column-margin hash='2023-04-18-netflix-cider-experimentation-note_cache/html/unnamed-chunk-2_a9442cc3c64f94f93f4decf32d57aab4'}\n::: {.cell-output-display}\n![Ship if sum is positive](2023-04-18-netflix-cider-experimentation-note_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n::: {.cell .column-margin hash='2023-04-18-netflix-cider-experimentation-note_cache/html/unnamed-chunk-3_47a483d66cf52a8ba9e4ff68dd363e64'}\n::: {.cell-output-display}\n![](2023-04-18-netflix-cider-experimentation-note_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n**I find it useful to visualize different launch rules.** For simplicity suppose our utility function is linear: we have two metrics, 1 and 2, and we care about them equally:\n   $$U(t_1,t_2)=t_1+t_2.$$\n   But we only observe noisy estimates $\\hat{t}_1,\\hat{t}_2$. \n\n**@kohavi2020trustworthy recommend a stat-sig shipping rule.** They say (p105):\n\n   1. If no metrics are positive-significant then do not ship\n   2. If some are positive-significant and none are negative-significant then ship\n   3. If some are positive-significant and some are negative-significant then \"decide based on the tradeoffs.\n\n   I represent this in the first diagram (but I treat condition 3 as a non-ship). The dotted line represents $\\hat{t}_1+\\hat{t}_2=0$.\n   \n**The stat-sig shipping rule has strange consequences.** You can see that this rule will recommend shipping things even with *negative* face-value utility ($U(\\hat{t}_1,\\hat{t}_2)<0$), when there's a negative outcome on the relatively noisier metric. This will still hold if we evaluate utility with shrunk estimates, when there's equal proportional shrinkage on the two metrics, but if there's greater shrinkage on the noisier metric it will not hold.\n   \n\n**Linear shipping rules are better.** In the margin I illustrate (1) a rule to ship wherever the sum is positive; (2) a rule to ship wherever the sum is stat-sig positive. I have drawn the second assuming that $cov(\\hat{t}_1,\\hat{t}_2)=0$. With a positive covariance the threshold would be higher.\n\n\n\n::: {.cell .column-margin hash='2023-04-18-netflix-cider-experimentation-note_cache/html/unnamed-chunk-4_4582715b4b55aeb6823615c2ddfa3c9e'}\n::: {.cell-output-display}\n![](2023-04-18-netflix-cider-experimentation-note_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n**The Leontief sandwich.** I assumed above that our true utility function is linear. In fact tech companies often explicitly give nonlinear objective functions to teams, e.g.:\n   $$\\begin{aligned}\n      \\max_k &\\ A(k)\n         && \\text{(goal)} \\\\\n      \\text{s.t.} &\\ B(k)\\leq \\bar{B}\n         && \\text{(guardrail)}\n   \\end{aligned}$$\n\nThis is illustrated at right, the indifference curves are L-shaped so I'll call it a Leontief utility. Having Leontief preferences can cause some unintuitive decision-making, in particular the tradeoff between $A$ and $B$ will varies drastically depending on your location. One important observation is that if your goal is assessed at the end of some time-point (e.g. at the end of the half) then optimal launch decisions will depend on your future *expectations*, e.g. you'd be willing to launch a feature that boosts A at the cost of B only if you expect a future launch to make up that deficit in B.\n\nIn practice I think it's useful to think of this nonlinear objective function as sitting in the middle of the hierarchy of an organization, with approximately linear objective functions above and below it, i.e. a \"Leontief sandwich.\"\n\nAt the highest layer the CEO (or shareholders) care about all the metrics in way that is locally linear, i.e. they do not have sharp discontinuities in how they assess the company's health. At the lowest layer engineers and data scientists are trying to make individual changes that achieve the Org's overall goals, but because they only account for a small share of the overall org's impact they can treat their objectives as locally linear (& likewise in a value function we make linear tradeoffs between objectives because we're in such a small region). Finally even for orgs which have nonlinear objective functions it's often reasonable to think of the nonlinearities as \"soft\", e.g. if an org comes in slightly below a guardrail the punishment is slight, and if they come in above the guardrail then they will be rewarded. This softening makes the effective objective function much closer to linear, and so I think for many practical purposes it's reasonable to start with a linear objective function.\n\n\n\n#                                  The Extrapolation Problem\n\n**Many problems are predicting the effect one one metric (downstream) given the effect on another metric (upstream).** There are a variety of situations in which we cannot measure the effect on the downstream metric, either because it has high noise, or it is in the future:\n\n| upstream               | downstream                                       |\n| ---------------------- | ------------------------------------------------ |\n| short-run revenue      | long-run revenue                                 |\n| click                  | purchase                                         |\n| engagement on content  | response to survey (\"do you like this content?\") |\n| engagement on content  | retention                                        |\n| exposure to content    | retention                                        |\n| time on surface X      | time on all surfaces                             |\n| purchase               | repeat purchase                                  |\n| wait-time for delivery | retention                                        |\n| price                  | quantity purchased                               |\n\n   <!-- \n   **When does observational inference work well?** It will work well when the $e_1$ and $e_2$ vary in the same way.\n      - Will work when the upstream variable is quasirandom.\n      - Will work when \n\n      $$\\xymatrix{\n            *++[F:<15pt>]{unobserved}\\ar[d]\\ar@{.>}[dr]\n            & *++[F:<15pt>]{unobserved}\\ar[d]\n            \\\\ *++[F]\\txt{upstream}\n            & *++[F]\\txt{downstream}\n         }\n      $$    -->\n\n   <!-- **Observational inference has been disappointing.** Many projects in tech have tried to measure causal effects with observational inference but have been disappointing. [UNFINISHED]. -->\n\n   <!-- \n   - Someone built a system at LinkedIn for observational inference (OCELOT), included various algorithms (matching, ), had a review committee. But eventually petered out and defunded.\n   -->\n\nFor concreteness we will treat the problem of predicting the long-run (LR) effect of an experiment on DAU from its short-run (SR) estimated effects on all metrics:\n\n   $$E[\\utt{\\Delta\\text{DAU}_{LR}}{true long-run}{effect on DAU} |\n       \\utt{\\Delta \\widehat{\\text{DAU}}_{SR}, \\ldots, \\Delta\\widehat{\\text{engagement}}_{SR}}{estimated short-run effects}{}]$$\n\nThere are two obvious ways to calculate this:\n\n1. **Meta-analysis.** We can run a regression across prior experiments:\n   $$\\Delta\\widehat{\\text{DAU}}_{LR} \\sim \n       \\Delta \\widehat{\\text{DAU}}_{SR} + \\ldots + \\Delta\\widehat{\\text{engagement}}_{SR}$$\n   \n   However the coefficients will be biased if we use on the LHS the *observed* long-run DAU, instead of the *true* long-run DAU. This bias is often large, and in fact if you run a bunch of AA tests (where the causal effect is zero) you'll find strong significant relationships between short-run and long-run impacts. I discuss below ways in which to adjust for this bias.\n\n2. **Observational Inference.** We can run a regression across users:\n   $$\\text{DAU}_{LR} \\sim \n       \\text{DAU}_{SR} + \\ldots + \\text{engagement}_{SR}$$\n\n   We can look at what is most predictive of long-run DAU across users. The problem here is obviously endogeneity, and so it's worth spending time drawing a DAG and running robustness tests to carefully think through the sources of variation we're using.\n\n##             With Meta-Analysis\n\nWith $n$ metrics we can write the underlying model as:\n   $$\\utt{\\pmatrix{\\hat{t}_1\\\\\\vdots\\\\\\hat{t}_n}}{observed}{effects}\n      = \\utt{\\pmatrix{t_1\\\\\\vdots\\\\t_n}}{true}{effects}\n         +\\utt{\\pmatrix{e_1\\\\\\vdots\\\\e_n}}{noise}{(=user variation)}$$\n\nHere we are treating $\\Delta \\text{DAU}_{SR}$  and $\\Delta \\text{DAU}_{LR}$ as two different metrics, but for some experiments we only observe the first. We thus want to estimate the effect on long-run retention (DAU$_{LR}$) given short-run metrics.\n   $$E[\\Delta\\text{DAU}_{LR} |\n       \\Delta \\widehat{\\text{DAU}}_{SR}, \\ldots, \\Delta\\widehat{\\text{engagement}}_{SR}]$$\n\nwhere\n   $$\\begin{aligned}\n      \\Delta\\text{DAU}_{LR}   &= \\textit{true}\\text{ effect on long-run daily active users (AKA retention)}\\\\\n      \\Delta\\widehat{\\text{DAU}}_{SR} &= \\textit{estimated}\\text{ effect on short-run daily active users} \\\\\n      \\Delta\\widehat{\\text{engagement}}_{SR} &= \\textit{estimated}\\text{ effect on short-run engagement}\n   \\end{aligned}$$\n\n**Running a Regression will be Biased.** The obvious thing to do is run a regression across experiments:\n   $$\\Delta\\widehat{\\text{DAU}}_{LR} \\sim\n      \\Delta \\widehat{\\text{DAU}}_{SR} + \\ldots + \\Delta\\widehat{\\text{engagement}}_{SR}$$\n\nHowever this will be biased. The simplest way to demonstrate the bias is to show that even with AA tests (where there is zero treatment effect on either metric) we will still get a strong predictive relationship between the observed treatment effects on each of the two metrics:\n\n::: {.column-margin}\n![A simulated scatter-plot showing 20 experiments, with N=1,000,000, $\\sigma_{e1}^2=\\sigma_{e2}^2=1$, with correlation 0.8. The experiments are all AA-tests, i.e. there are no true treatment effects, yet a regression of $\\hat{t}_2$ on $\\hat{t}_1$ will consistently yield statistically-significant coefficients of around 0.8.](images/2022-04-08-09-34-41.png)\n:::\n\nThe bias is because in the regression our LHS variable is *estimated* retention ($\\Delta\\widehat{\\text{DAU}}_{LR}$ instead of $\\Delta\\text{DAU}_{LR}$), and the noise in that estimate will be correlated with the noise in the estimates of short-run metrics. In the linear bivariate case (where we have just one RHS variable) then we can write:\n   $$\\begin{aligned}\n      \\ut{\\frac{cov(\\hat{t}_2,\\hat{t}_1)}{var(\\hat{t}_1)}}{regression}\n      = \\utt{\\frac{cov(t_2,\\hat{t}_1)}{var(\\hat{t_1})}}{what we}{want to know}\n         + \\ut{\\frac{cov(e_2,e_1)}{var(\\hat{t}_1)}}{bias}\n   \\end{aligned}$$\n\nThe bias will be small if the short-run metrics have high signal-noise ratios (SNR), $\\frac{var(t_1)}{var(e_1)}\\gg 0$. A simple test for SNR ratio is the distribution of p-values: if most experiments are significant then the SNR is high. However in the typical case (1) $\\Delta \\widehat{\\text{DAU}}_{SR}$ is the best predictor of $\\Delta \\widehat{\\text{DAU}}_{LR}$; and (2) $\\Delta \\widehat{\\text{DAU}}_{SR}$ has a low signal-noise ratio (i.e. few outcomes are stat-sig). This means the results are hard to interpret, the bias is large.\n\n\n###          Adjusting for the Bias\n\nHere are some alternatives:\n\n1. **Run a regression just using the high-SNR metrics.** We could just drop $\\Delta\\widehat{\\text{DAU}}_{SR}$ as a regressor because of the bias. But in practice we lose a predictive power ($R^2$), so it's hard to know when this will be a good idea without an explicit model.\n\n2. **Adjust for bias in linear estimator.** If we want a linear estimator then we can estimate and adjust for the bias.\n   $$\\begin{aligned}\n      \\utt{\\frac{cov(t_2,\\hat{t}_1)}{var(\\hat{t_1})}}{BLUE for}{$t_2$ given $\\hat{t}_1$}\n         &= \\frac{cov(t_2,t_1)}{var(\\hat{t}_1)}\n         = \\ut{\\frac{cov(\\hat{t}_2,\\hat{t}_1)}{var(\\hat{t}_1)}}{regression result}\n            - \\utt{\\frac{cov(e_2,e_1)}{var(\\hat{t}_1)}}{observable}{variables}\n   \\end{aligned}$$\n\n   If everything is joint normal then the expectation is itself linear, and so this will be optimal. In practice the true distribution of effect-sizes is somewhat fat-tailed, which imply that the conditional expectation will be nonlinear in the observables. Nevertheless I think this is a good start. (One other complication is that the SNR is more complicated to calculate when experiments vary in their sample size).^[See @cunningham2019interpreting, and see @tripuraneni2023choosing for a slightly different setup with weaker assumptions.]\n\n3. **Use experiment splitting.** You can randomly assign users in each experiment to one or other sub-experiments. You now effectively have a set of *pairs* of experiments, each of which has experiments with identical treatment effects ($\\Delta \\text{DAU}_{LR}$) but independent noise. Thus you can run a regression with LHS from one split, and RHS from other split, and you'll get an unbiased estimate. Additionally you can easily fit a nonlinear model (@coey2019improving has details of how to do an experiment-splitting).\n\n4. **Run a regression just using the strongest experiments.** If the distribution of experiments is fat-tailed then the strongest experiments will have higher SNR, and so lower bias. A worry about this is that you're only estimating the relationship from outliers, so if there are nonlinearities you'll never know. At the same time the assumption of fat-tailed treatment-effects gives reason to believe the expectation will be nonlinear. (This is roughly how I interpret the @peysakhovich2018learning experiments-as-instruments paper. They propose using L0 regularization and experiment-splitting cross-validations, which I think effectively just selects the strongest experiments.)\n\n\n**Choosing a Reference Class.** It is important to think about the reference-class of experiments which we use to calibrate our estimates. The long-run DAU prediction can be though of as an empirical-bayes estimate, which is our best estimate conditional on the experiment being a random draw from this class of experiments.\n\nIn many cases a company's experiments will naturally fall into different classes: e.g. some have a very steep relationship between engagement and DAU, others have a very flat. It's important to both (1) visualize all the experiments, so that a reference-class can be chosen sensibly; (2) calculate the $R^2$ across experiments, so we can have some sense of confidence in our extrapolation.\n\n\n##            Observational Inference\n\n<!-- **Engagement and retention.** Returning to our application, we want to know the effect of an experiment on retention but we only measure the effect on short-run engagement. We can write this as:\n      $$\\xymatrix{\n         *+[F]{\\text{experiment}} \\ar[r] \\ar@{.>}@/_2pc/[rr]\n         & *+[F]{\\text{SR engagement}}\\ar[r]\n         & *+[F]{\\text{LR retention}} \n      }$$\n\nThe identifying assumptions:\n\n1. **Exclusion:** The effect of an experiment on LR retention is exclusively via its effects on SR engagement.\n2. **Unconfoundedness.** The correlation betweeen engagement and retention is exclusively due to the causal effect of engagement on retention.\n\nAs stated these are slightly weird assumptions: we don't really believe that clicking \"like\" on a post *causes* retention, instead it's more plausible that high-quality contnet causes both engagement and retention:\n      $$\\xymatrix@R=.5cm{\n         *+[F]\\txt{experiment} \\ar[r]\n      &  *+[F-:<6pt>]\\txt{timeline quality}\\ar[d] \\ar[r]\n      &  *+[F]\\txt{LR retention}\n      \\\\\n      &  *+[F]\\txt{SR engagement}\n      }$$\n\nWe can then think of factors which might violate this:\n\n1. If someone's in a good mood that might increase both engagement and retention, but not via timeline quality.\n2. If someone gets spooked about being tracked, that might lower engagement but not affect retention (and so cause attenuation).\n\nTo justify our identifying assumption we should be specific about what type of unobserved factors will affect propensity to engage and DAU. Some examples:\n\n1. Things that make you use Twitter more: a holiday, a new phone, bad weather.\n2. Random variation in tweets you see: some tweets you are more likely to engage, and may also cause you to return more often. -->\n\n**What we want to know:** Given the short-run effect of a content experiment on engagement we want to predict the long-run effect on DAU. We can start with a simple regression along these lines:\n   $$\\utt{\\text{DAU}_{u,t+1}}{long-run}{retention} \\sim \\utt{\\text{engagement}_{u,t}}{short-run}{engagement}$$\n\n**We could set up a DAG and discuss the surrogacy conditions.** The condition are that (1) all effects of an experiment on DAU are via short-run engagement; and (2) there is no unobserved factor which affects both SR engagement and LR DAU:\n\n$$\\xymatrix{\n      &  *+[F-:<6pt>]\\txt{unobserved}\\ar@{.>}[d] \\ar@{.>}[dr] \\\\\n         *+[F]{\\text{experiment}} \\ar[r] \\ar@{.>}@/_1pc/[rr]\n         & *+[F]{\\text{SR engagement}}\\ar[r]\n         & *+[F]{\\text{LR DAU}} \n      }$$\n\nIn fact we know that engagement doesn't *literally* lie on the causal chain, instead we think engagement is a good proxy for *content* which might lie on the causal chain.\n\nIn any case I find the following setup an easier way to think about the assumptions necessary for identification:\n\n**We can write it out a simple structural model as follows** (for compactness I leave out coefficients):\n\n$$\\begin{array}{rcccccccc}\n   \\text{engagement}_{u,t}\n      &=& \\utt{\\text{temperament}_{u}}{user-specific}{propensity to engage}  \n      &+& \\utt{\\text{mood}_{u,t}}{time-varying}{mood/holiday/etc.}\n      &+& \\utt{\\text{content}_{u,t}}{content seen}{on platform}\n      &+& \\utt{\\text{distractions}_{u,t}}{other platform effects}{e.g. messages, notifs}\\\\\n   \\text{DAU}_{u,t} \n      &=& \\text{temperament}_{u} \n      &+&\\text{mood}_{u,t}\n      &+&\\utt{\\sum_{s=1}^\\infty\\beta^s\\text{content}_{u,t-s}}{prior experience}{w content}\n      &+&\\text{distractions}_{u,t}\\\\\n\\end{array}$$\n\n\nSome general observations:\n\n   1. **We would get a more credible estimate if we could directly measure content quality.** E.g. if we could use the quality of the content available to the user on the RHS, instead of just their engagement on that content. This wouldn't get perfect identification but it would help.\n   2. **The relative shares of variation in the RHS is important.** If most of the variation in engagement is due to variation in content (i.e. high $R^2$ from content), then we don't need to worry much about confounding from other effects. We can think of introducing control variables as a way of increasing the share of varation in engagement due to content.\n   3. **We should control for distractions.** If we have measures of app-related events that don't affect content-seen but do affect engagement, e.g. notifications, messages, then we should use those as controls. This will increase the relative share of variation in engagement due to content.\n   4. **Controlling for pre-treatment outcomes changes variation used.** If we control for `engagement`$_{t-1}$ this will change the relative contribution of each factor in the variation of engagement. Specifically it will reduce the share of the terms with higher autocorrelation. Thus by definition `temperament` will reduce its contribution. However it's unclear whether `mood` or `content` has higher autocorrelation, and so controlling for pre-treatment could either increase or decrease the relative contribution of `content`. It's probably worth doing some simple decomposition of variation in engagement into (1) user, (2) content, and (3) mood (the residual), both statically and over time.\n   5. **Univariate linear prediction is usually pretty good.** In my experience you can get a fairly good prediction of most user-level metrics with a linear function of the lagged values. If you use a multivariate or nonlinear function you'll get a better fit but only by a small amount (one exception: when predicting discrete variables like DAU it's useful to use a continuous lagged variable like time-spent). So I'm skeptical that adding more regressors or adding nonlinearity will significantly change the estimates or the credibility of the estimates.\n   6. **Estimand is not $\\beta$ but $\\frac{1}{1-\\beta}$.** Suppose we see that 1 unit of engagement causes a certain increase in DAU over the following weeks. We then want to apply that estimate to an experiment which *permanently* increases engagement by 1 unit. We thus should take the integral over all the subsequent DAU effects. In the simple exponential case the effect of a shock at period $t$ on DAU at period $t+s$ will be $\\beta^s$, and so the cumulative effect on all subsequent periods will be $1+\\beta+\\beta^2+\\ldots=\\frac{1}{1-\\beta}$.\n   7. **Autocorrelation in content makes things messier.** If there is significant autocorrelation in content then the interpretation of `DAU~engagement` is more difficult. E.g. if we see that engagement on $t$ is correlated with DAU on $t+1$ this could be because either (1) content on $t$ content caused the DAU on $t+1$, or (2) good content on $t$ is correlated with good content on $t+1$, which in turn causes DAU on $t+1$. I don't think controlling for pre-treatment levels or trends solves this.\n\n#                                  The Explore-Exploit Problem\n\n**What experiments should you run?** The prior sections have been just about interpretation of existing experiments, we can now turn to the choice of which experiment to run. The space of all possible experiments is immensely high dimensional and thus most of this process uses human judgment. However in some cases we can reduce the space to a small number of dimensions and use an algorithm to explore that space. We can call this process a \"bandit\" or \"explore exploit\" or \"adaptive experimentation\" or \"gradient descent\" problem (though gradient descent is typically pure exploration with no exploitation).\n\n**Typical cases for explore-exploit:**\n\n   - Tuning parameters on a recommendation algorithm to maximize retention.\n   - Tuning parameters on video or audio streaming to maximize satisfaction and retention.\n   - Tuning parameters on ad bidding to maximize net profit.\n   - Exploring different components of quality in recommendations:\n      - Content quality\n      - Producer quality\n      - User-topic interest\n      \n      In each case showing some content that is *less* interesting to the user, but in return for learning more information.\n\n**Tuning projects have a high failure rate.** I should say that I am not an expert on explore-exploit algorithms and many others have deeper professional experience than I do. However I have seen multiple tuning projects either abandoned because of complexity, or fail to find a retuning which yields a non-trivial impact on metrics. Speaking broadly I think the problems were overly-complicated designs, under-powered experiments, lags in effects, and improper use of short-term proxies for long-term outcomes. \n\n   <!-- 1. Projects to systematically explore quality in recommendations are difficult. These type of projects are difficult to evaluate because their impact is often lagged (you trade-off short-run engagement for long-run engagement) or non-local (you trade-off the target user's engagement for the community's engagement). This means estimating the impact is . -->\n\n\n::: {.cell .column-margin hash='2023-04-18-netflix-cider-experimentation-note_cache/html/unnamed-chunk-5_d787600e0664e1877e6737eba6b82cfc'}\n::: {.cell-output-display}\n![If $\\beta_i^*$ is already close to the global optimum then there will be not much loss from perturbing some users because the loss function should be flat in that neighborhood.](2023-04-18-netflix-cider-experimentation-note_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n**Recommendation: a simple tuning algorithm using weather stations.** Here is a crude but easy-to-execute method for dynamically optimizing parameters. It's less efficient than other algorithms but it's easy to describe, easy to implement (it uses the existing AB-test system), and easy to visualize and see that it's working as intended. In short: for each parameter we set up two permanent \"weather stations\" treatments: 1/3 of users get a slightly higher value, and 1/3 of users get a slightly lower value.\n\n   Suppose we have $n$ parameters to tune $(\\beta_1,\\ldots,\\beta_n)$: we run $n$ orthogonal experiments, each of which partitions the all users into 3 equal-sized buckets, with either (1) $\\beta_n=\\beta_n^*$ , (2) $\\beta_n=\\beta_n^*-\\varepsilon_n$, (3) $\\beta_n=\\beta_n^*+\\varepsilon_n$, where $\\beta_n^*$ is the current production level of $\\beta$. If $n=2$ then users would be assigned as such:\n\n\n::: {.cell .column-margin hash='2023-04-18-netflix-cider-experimentation-note_cache/html/unnamed-chunk-6_36089c3bcedfbd7dfad4471881ce77d9'}\n::: {.cell-output-display}\n![If we start at a point above the global optimum then the \"low\" group benefits and the \"high\" group suffers, but we can see that any short-term cost will be outweighed by long-term benefit.](2023-04-18-netflix-cider-experimentation-note_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n   |                         | $\\beta_1-\\varepsilon_1$ | $\\beta_1$ | $\\beta_1+\\varepsilon_1$ |\n   | ----------------------: | ----------------------- | --------- | ----------------------- |\n   | $\\beta_2-\\varepsilon_2$ | 1/9                     | 1/9       | 1/9                     |\n   |               $\\beta_2$ | 1/9                     | 1/9       | 1/9                     |\n   | $\\beta_2+\\varepsilon_2$ | 1/9                     | 1/9       | 1/9                     |\n\n   The size of the perturbations $\\varepsilon_i$ are easy to adjust dynamically as the data comes in: we can start small and keep increasing until we see a stat-sig difference in the outcome. We monitor the trajectory of each bucket continuously, and once/month make a formal decision about whether to adjust the production parameters, e.g. increasing $\\beta_n$ to $\\beta_n+\\varepsilon_n$ or lowering it to $\\beta_n-\\varepsilon_n$. When interpreting these experiments it is important to monitor the full trajectory of outcomes over time, ideally a visualization will show a large matrix of trajectories, with one cell for each combination of experiment-bucket and metric. The shipping criteria can be a pre-specific weighted average of metrics or .\n   \n**We can use the data generated to explore other aspects:** (1) whether there are significant interaction effects between the different experiments (e.g. if the users who have both increasing $\\beta_1$ and $\\beta_2$ have a different effect), and (2) whether there are significant heterogeneities in outcomes across subgroups.\n\n**This is the simplest general framework I know of for continuous optimization of a set of parameters.** I think that simplicity is by far the most important criterion: I have seen a long history of optimization projects get tangled in complexity and fail. Because of the past history of failures I think it's crucial to do the simplest and most transparent thing at each point until you have a steady rhythm and track record of making progress.\n\n**The hard work is the choice of parameters to tune.** Once you have a small set of parameters to tune it's not too hard to find the global optimum. However in typical problems there are thousands or millions or billions of possible parameters, how should you choose which ones to tune?\n\n<!-- \n- (Meta's AX is used for hyperparameter tuning, but not that much for product experiments)\n-->\n\n<!-- #                                  The Culture Problem [UNFINISHED]\n\n**Tool-makers don't trust tool-users.** Some common themes:\n\n   - If you give experimenters a lot of metrics they'll choose the subset which support their preferred decision.\n\n   - If you give product leaders a goal on a metric that is a proxy for quality they'll increase the metric and meet the goal but in a way that doesn't increase quality.\n\n   - If you let data scientists use observational-causal-inference tools they'll use them indiscriminately, hardly spending any time to think about whether the exogeneity assumption hold in their cases.\n\n**There are three broad approaches:**\n\n1. Put restrictions on experimenters to prevent them from misinterpreting experiment results.\n2. Change incentives for experimenters to prevent them from misusing experiment results.\n3. Educate experiments so they use experiment results better. -->\n\n<!-- **Taking culture as fixed.** People are trying to develop.\n**Tech companies are full of quack doctors.** infested with dousing people ; witch hunters ; quack doctors ; medicine men.  -->\n\n<!-- \n#                                Appendix\n\n**Frequentist solutions to selection problems.** Frequentists (e.g. @andrews2019inference) talk about the problem of \"multiple comparisons\" or \"winner's curse\" but those aren't the problems they're solving. If you look at the actual applications they're all where each experiment has a prior which is tight relative to the confidence intervals (AKA underpowered experiments). In these cases it's well-known that frequensti estimates are badly behaved: they tend to over-estimate the true effect and have high rates of false positives. This would still be true even if you have only *one* underpowered experiment.\n\nThe problem is that frequentists don't realize that they're running underpowered experiments (becuase they don't like to think about priors), instead they prefer situation where priors are locally uniform. But in situations where you have many experiments or many outcomes then you are likely to have low-powered experiments, and so they come up with correction that approximate empirical Bayes inference.\n\nThey say that this is specific to the case of running many experiments but it's not: the same problems hold if you run a single under-powered experiment. Conversely, if you're running many high-powered experiments then these adjustments are unnecessary.\n\nPut another way: @andrews2019inference paper tries to find estimators that are unbiased conditional on being a winner ($E[y1|t1,y1>y2]$ and $E[max{y1,y2}|t1,t2]$). But this is throwing away information, it's just using discrete information (whether you're a winner) when you have a rich continuous dataset, from which you can estimate the full distribution of treatments, and so estimate $E[t1|y1,y2]$.\n -->\n\n\n#                                References",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}