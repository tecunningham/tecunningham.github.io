{
  "hash": "0bcfd96f8978393236b403e49660344d",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"LLM Time-Saving, Demand Theory, and Task Activation\"\nauthor: \"Tom Cunningham (METR)\"\ndate: today\ndraft: true\ncitation: true\nreference-location: document\nbibliography: ai.bib\nformat:\n  html:\n    toc: true\n    toc-depth: 3\nexecute:\n  echo: false\n  warning: false\n  error: false\n  cache: true\n---\n\n\n<!-- https://tecunningham.github.io/posts/2025-12-17-llm-time-saving-demand-theory-substitution.llm.html -->\n\n<!-- Author note: review tools/test_llm_time_saving_qmd.py for tests and run it after edits. -->\n\n::: {#3ddcbbff .cell results='asis' execution_count=1}\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n<details class=\"validation-checklist\">\n<summary>Validation Checks</summary>\n\n```text\nValidation Checks\n✅ Programmatic: proof structure (collapsed)\n✅ Programmatic: Mermaid syntax lint (offline)\n✅ (8/8) Programmatic: bibliography tests\n  ✅ Duplicate citekeys\n  ✅ One field per line + trailing comma\n  ✅ Source locator present (url/doi/eprint) (225/225)\n  ✅ Abstract length <= 500 words (72/72)\n  ✅ abstract_source only when abstract is present (63/63)\n  ✅ arXiv eprints use arxiv.org/pdf/<id>.pdf URLs (24/24)\n  ✅ text_url has local text archive (16/16)\n  ✅ bibclean lint\n✅ Programmatic: citekeys resolve in posts/ai.bib\n⏳ LLM-assisted: quality + citation plausibility gate (optional)\n```\n</details>\n\n:::\n:::\n\n\n<style>\n  dl {display: grid;}\n  dt {grid-column-start: 1; width: 18em;}\n  dd {grid-column-start: 2; margin-left: 1.25em;}\n  details.validation-checklist {\n    background: #f5f5f5;\n    border: 1px solid #777;\n    border-radius: 6px;\n    padding: 0.5em 0.75em;\n    margin-bottom: 1em;\n  }\n  details.validation-checklist > summary {\n    cursor: pointer;\n  }\n</style>\n\n<!--\nEditing note: definition lists + multiline math\n\nIn Quarto, a definition list item starts with exactly one leading colon:\n\nTerm (a short claim)\n: First paragraph of the definition.\n  Continuation lines must be indented (e.g. two spaces).\n  $$\n  Multiline display math goes here.\n  $$\n\nDo NOT prefix continuation lines with additional ':' characters; that often breaks math rendering.\n-->\n\n## Results-first summary\n\n**Speedups do not mechanically translate into aggregate productivity.**\n: Once LLMs change the relative time costs of tasks, optimal time allocation changes, and the aggregate lift depends on substitution and (often) task activation.\n\n**Overall lift is not pinned down by a share and a conditional speedup alone.**\n: If tasks that get sped up initially take share $s$ of time and get speedup factor $A$ when you do them, then an arithmetic share calculation gives $1+s(A-1)$. That expression is correct only for a knife-edge estimand where you hold the task mix fixed. If you let the person reoptimize, $s$ is endogenous and the total lift depends on substitution.\n\n**Allowing reallocation makes the estimand a time-allocation object.**\n: Let $t=(t_1,\\dots,t_n)$ be time allocated across tasks with $\\sum_i t_i=1$. Let $A=(A_1,\\dots,A_n)$ be task-specific productivity (effective output per unit time on each task). Let the output index be\n  $$\n  y(A_1 t_1,\\dots,A_n t_n),\n  $$\n  where $y(\\cdot)$ is increasing in each argument. The substitution-adjusted productivity level is\n  $$\n  V(A)\\equiv \\max_{t\\ge 0}\\; y(A_1 t_1,\\dots,A_n t_n)\\quad\\text{s.t.}\\quad \\sum_i t_i\\le 1.\n  $$\n  An LLM changes productivity from $A^0$ to $A^1$, and the lift is $V(A^1)/V(A^0)$.\n\n**Substitutability governs how much time share moves when a task becomes cheap.**\n: In a continuous model, if tasks are close substitutes then time shifts strongly toward the sped-up tasks and the aggregate lift can be much larger than $1+s(A-1)$. If tasks are strong complements, time shares are stable and the aggregate lift is closer to the share calculation. In parametric cases this is summarized by an elasticity; outside parametric cases it is a demand-curve object.\n\n**Observing post-LLM shares can identify the elasticity in a CES benchmark.**\n: In a two-good CES setting, observing both pre and post time shares for an AI-affected bundle, together with the relevant speedup, identifies the substitution elasticity via a logit-share formula. This makes post-LLM shares especially informative when you want a point estimate.\n\n**With unit-demand task choice, you can bound the lift using pre and post task baskets.**\n: If tasks are lumpy and you either do a task or not, and you observe the time required for each task pre and post AI, then you can bound the aggregate lift by evaluating the pre-AI chosen basket at post-AI time prices (a Laspeyres-style bound) and the post-AI chosen basket at post-AI time prices (a Paasche-style bound). The discrete section formalizes this.\n\n## Setup: time allocation and task productivity\n\n**The canonical setup is time allocation with task productivity.**\n: You choose time shares $t=(t_1,\\dots,t_n)$ with $\\sum_i t_i=1$, and productivity enters as the vector $A=(A_1,\\dots,A_n)$ in the output index $y(A_1 t_1,\\dots,A_n t_n)$.\n\n**Index-number results can be imported by reinterpreting productivity as inverse prices.**\n: If you define a time price $p_i\\equiv 1/A_i$ for an efficiency unit of task $i$, then changes in $A$ can be analyzed with standard expenditure-function and price-index tools. I use that equivalence in the continuous section because it is the cleanest way to import known results about exact indices, Tornqvist approximations, and compensated-demand integrals.\n\n**Two margins matter.**\n: The continuous model treats tasks as divisible and focuses on intensive reallocation. The discrete model treats the task set as lumpy (setup time or unit demand) and focuses on which tasks get activated.\n\n**Continuous intensive margin means smooth substitution.**\n: Choose time shares $t_i$ and allow interior reallocation.\n\n**Discrete extensive margin means task activation and selection.**\n: Choose which tasks to do at all (unit demand or setup costs).\n\n**Separating the two margins keeps assumptions and data requirements explicit.**\n: The logic, formulas, and data requirements diverge sharply between intensive reallocation and extensive-margin activation, so I treat them separately.\n\n## Estimation cheat sheet\n\n**This section maps measured productivity changes into a lift under common assumptions.**\n: It is a lookup table for turning observed time changes or productivity multipliers into a substitution-adjusted lift $V(A^1)/V(A^0)$.\n\n### Two-good reduction (AI-affected bundle vs the rest)\n\n**A two-good reduction collapses tasks into an AI-affected bundle and the rest.**\n: For back-of-envelope calculations, define two bundles: good 2 is the set of tasks whose productivity rises by a factor $A$ when AI is allowed, and good 1 is everything else (normalized). Let $s_0$ be the pre-AI time share on good 2, and suppose only good 2 gets the productivity multiplier $A>1$. Common benchmarks are:\n\n| Assumption about substitution | Implied output gain $V(A^1)/V(A^0)$ | Notes |\n|---|---|---|\n| Perfect complements (Amdahl-style) | $\\dfrac{1}{(1-s_0)+s_0/A}$ | Fixed proportions; AI only relaxes the bottleneck |\n| Cobb-Douglas ($\\varepsilon=1$) | $A^{s_0}$ | Share is constant; log change is $s_0\\ln A$ |\n| CES elasticity $\\varepsilon$ | $\\left((1-s_0)+s_0\\,A^{\\varepsilon-1}\\right)^{\\frac{1}{\\varepsilon-1}}$ | Requires $\\varepsilon$ (or enough data to infer it) |\n| Perfect substitutes | $A$ | You can reallocate everything to the sped-up bundle |\n\n**In the multi-task case, a Tornqvist/Divisia log-index is a convenient approximation.**\n: A common approximation is\n  $$\n  \\Delta\\ln V \\;\\approx\\; \\sum_i \\bar t_i\\,\\ln\\left(\\frac{A_i^1}{A_i^0}\\right),\\qquad \\bar t_i \\equiv \\tfrac12(t_{i,0}+t_{i,1}),\n  $$\n  which uses average (pre/post) time shares (see @caves1982indexnumbers).\n\n**Anthropic-style numbers span a wide range under different substitution assumptions.**\n: If $s_0=0.10$ and $A=5$ (a 5x multiplier when AI is used), the implied gain depends heavily on the benchmark.\n\n| Benchmark | Gain factor | Percent |\n|---|---:|---:|\n| Complements bound | $1/(0.9+0.02)\\approx 1.087$ | $+8.7\\\\%$ |\n| Cobb-Douglas | $5^{0.1}\\approx 1.174$ | $+17.4\\\\%$ |\n| CES, $\\varepsilon=2$ | $0.9+0.1\\\\cdot 5 = 1.4$ | $+40\\\\%$ |\n| Substitutes bound | $5$ | $+400\\\\%$ |\n\n**The spread is the point.**\n: Large conditional speedups do not translate to a unique aggregate lift without additional structure or data.\n\n### If you observe pre and post time shares, you can infer an elasticity (in CES)\n\n**In a two-good CES benchmark, post-AI shares identify the elasticity.**\n: In the same two-good CES setting, let $s_1$ be the post-AI time share on good 2. Then\n  $$\n  \\operatorname{logit}(s_1)-\\operatorname{logit}(s_0)=(\\varepsilon-1)\\ln A,\n  $$\n  so\n  $$\n  \\varepsilon \\;=\\; 1 + \\frac{\\operatorname{logit}(s_1)-\\operatorname{logit}(s_0)}{\\ln A}.\n  $$\n  Here $\\operatorname{logit}(s)\\equiv \\ln\\!\\left(\\frac{s}{1-s}\\right)$.\n  This is often the cleanest way to use both pre and post shares: estimate $\\varepsilon$, then plug into the CES gain formula. If you do not believe CES globally, treat this as local to the observed price change.\n\n### Estimation flowchart (what to do with your data)\n\n\n```{mermaid}\nflowchart TD\n  A[\"Goal: estimate productivity lift<br/>V(A_new)/V(A_old)<br/>for fixed time endowment\"] --> B{\"Are tasks mostly divisible<br/>at the relevant margin?\"}\n\n  B -->|Yes: continuous| C{\"Are speedups small<br/>or broad-based?\"}\n  C -->|Yes| D[\"Use share-weighted log changes:<br/>d ln V ~= sum_i t_i d ln A_i<br/>(Tornqvist/Divisia)\"]\n  C -->|No| E{\"Do you have enough data to pin down substitution?<br/>e.g. CES elasticity, demand system\"}\n  E -->|Yes| F[\"Compute an exact/parametric index:<br/>map p_i=1/A_i; use e(p,1)<br/>CES closed form when applicable\"]\n  E -->|No| G[\"Trace a demand curve:<br/>vary effective AI cost/strength;<br/>estimate compensated shares; integrate\"]\n\n  B -->|No: setup or unit-demand| H[\"Model extensive margin:<br/>which tasks get activated?\"]\n  H --> I[\"Unit-demand tasks: bound the lift<br/>Laspeyres lower bound: pre-AI basket at post-AI time prices<br/>Paasche upper bound: post-AI basket at post-AI time prices\"]\n```\n\n\n### What you can estimate with what data\n\n| What you can measure | Recommended move | What you can credibly report |\n|---|---|---|\n| One big conditional multiplier $A$ + a baseline share $s_0$ | Report complements/substitutes bounds; add CES sensitivity | A wide interval for $V(A^1)/V(A^0)$ unless you assume $\\varepsilon$ |\n| Pre and post shares $(s_0,s_1)$ for an AI bundle + a multiplier $A$ | Use the logit formula to estimate $\\varepsilon$; plug into CES gain | A model-based point estimate (local to the observed change) |\n| Multiple randomized “AI price/quality” arms + observed shares | Estimate share response vs $\\ln A$; integrate shares over the change | An “area under the (compensated) demand curve” estimate for large changes |\n| Choice/activation data (tasks attempted) under multiple AI prices | Model activation thresholds / setup costs explicitly | Decomposition into intensive and extensive margins |\n\n## Continuous (intensive-margin) model\n\n**Homotheticity lets you treat productivity changes as inverse price changes.**\n: Write effective task output as $z_i\\equiv A_i t_i$. Then\n  $$\n  V(A)=\\max_{z\\ge 0}\\; y(z)\\quad\\text{s.t.}\\quad \\sum_i \\frac{z_i}{A_i}\\le 1.\n  $$\n  Define time prices $p_i\\equiv 1/A_i$. This turns the constraint into $\\sum_i p_i z_i\\le 1$, so standard expenditure-function and index-number results apply.\n  In particular, define the unit-expenditure function $P(p)\\equiv e(p,1)$. Under homotheticity, the productivity level is\n  $$\n  V(A)=\\frac{1}{P(1/A)}.\n  $$\n  This is the classic index-number framing applied to time prices (see @caves1982indexnumbers).\n\n**EV and CV translate productivity changes into welfare changes in time units.**\n: Let $A^0\\to A^1$ and define $p^k\\equiv 1/A^k$. Equivalent and compensating variation (measured in time) are\n  $$\n  EV=e(p^0, V(A^1))-1,\\qquad CV=e(p^1, V(A^0))-1.\n  $$\n  Under homotheticity,\n  $$\n  EV=\\frac{P(p^0)}{P(p^1)}-1,\\qquad CV=\\frac{P(p^1)}{P(p^0)}-1,\n  $$\n  which is a clean way to translate time-saving into a welfare measure (see @hausman1981exact).\n\n**Small changes admit a share-weighted approximation.**\n: Let $t_i(A)$ denote optimal time shares. For small changes,\n  $$\n  d\\ln V \\;\\approx\\;\\sum_i t_i(A)\\,d\\ln A_i.\n  $$\n  This is the time-allocation analog of share-weighted Hulten-style logic (see @hulten1978growth).\n\n**Large changes require integrating compensated shares.**\n: When gains are large, constant-elasticity shortcuts are dangerous. Using Hicksian (compensated) shares $s_i^H(p)$,\n  $$\n  d\\ln P(p)=\\sum_i s_i^H(p)\\,d\\ln p_i.\n  $$\n  For a single changing price (equivalently, a single changing productivity component),\n  $$\n  \\ln\\frac{P(p^1)}{P(p^0)}=\\int s_2^H(p_2)\\,d\\ln p_2,\n  $$\n  i.e., exact welfare is an area-under-the-compensated-demand-curve object (see @willig1976consumerssurplus).\n\n**CES yields closed-form formulas useful for back-of-envelope work.**\n: For a CES aggregator in $z$,\n  $$\n  y(z)=\\left(\\sum_i \\alpha_i z_i^{\\frac{\\sigma-1}{\\sigma}}\\right)^{\\frac{\\sigma}{\\sigma-1}},\\quad\\sigma>0,\n  $$\n  the associated price index and (compensated) shares have closed form. In a two-good reduction where only good 2 gets a productivity multiplier $A$ and its ex-ante time share is $s_0$,\n  $$\n  \\frac{V(A^1)}{V(A^0)}=\\left((1-s_0)+s_0\\,A^{\\varepsilon-1}\\right)^{\\frac{1}{\\varepsilon-1}},\\qquad \\varepsilon\\equiv\\sigma.\n  $$\n  This is the clean continuous benchmark (see @caves1982indexnumbers).\n\n#### Two-good CES yields closed-form gain and share shift\n\n**The output gain has a closed form in terms of $s_0,A,\\varepsilon$.**\n: In a two-good CES benchmark where only good 2 gets a multiplier $A$ (so $p_2'=p_2/A$),\n  $$\n  \\frac{V(A^1)}{V(A^0)}=\\left((1-s_0)+s_0\\,A^{\\varepsilon-1}\\right)^{\\frac{1}{\\varepsilon-1}}.\n  $$\n\n**The post-AI share shift pins down $\\varepsilon$ when $s_1$ is observed.**\n: The post-AI time share is\n  $$\n  s_1=\\frac{s_0\\,A^{\\varepsilon-1}}{(1-s_0)+s_0\\,A^{\\varepsilon-1}}\n  \\quad\\Leftrightarrow\\quad\n  \\operatorname{logit}(s_1)-\\operatorname{logit}(s_0)=(\\varepsilon-1)\\ln A.\n  $$\n\n::: {.callout-note collapse=\"true\"}\n## Proof (structured)\n\n1. *Given* CES unit-expenditure (time price) index\n   $$\n   P(p)=\\left(\\alpha_1^{\\varepsilon}p_1^{1-\\varepsilon}+\\alpha_2^{\\varepsilon}p_2^{1-\\varepsilon}\\right)^{\\frac{1}{1-\\varepsilon}},\n   \\qquad V(A)=\\frac{1}{P(1/A)}.\n   $$\n2. *Let* $p_2' = p_2/A$ with $p_1$ fixed. Then\n   $$\n   \\frac{V(A^1)}{V(A^0)}=\\frac{P(p)}{P(p')}=\n   \\left(\n     \\frac{\\alpha_1^{\\varepsilon}p_1^{1-\\varepsilon}+\\alpha_2^{\\varepsilon}p_2^{1-\\varepsilon}}\n          {\\alpha_1^{\\varepsilon}p_1^{1-\\varepsilon}+\\alpha_2^{\\varepsilon}(p_2/A)^{1-\\varepsilon}}\n   \\right)^{\\frac{1}{\\varepsilon-1}}.\n   $$\n3. *Define* the ex-ante share\n   $$\n   s_0\\equiv\\frac{\\alpha_2^{\\varepsilon}p_2^{1-\\varepsilon}}\n                  {\\alpha_1^{\\varepsilon}p_1^{1-\\varepsilon}+\\alpha_2^{\\varepsilon}p_2^{1-\\varepsilon}}.\n   $$\n   Substituting into Step 2 yields the claimed gain formula.\n4. *For shares,* note CES time shares satisfy\n   $$\n   \\frac{s}{1-s}=\\frac{\\alpha_2^{\\varepsilon}p_2^{1-\\varepsilon}}{\\alpha_1^{\\varepsilon}p_1^{1-\\varepsilon}}.\n   $$\n   Therefore,\n   $$\n   \\frac{s_1/(1-s_1)}{s_0/(1-s_0)}\n   =\n   \\frac{(p_2')^{1-\\varepsilon}}{p_2^{1-\\varepsilon}}\n   =\n   \\left(\\frac{p_2/A}{p_2}\\right)^{1-\\varepsilon}\n   =\n   A^{\\varepsilon-1},\n   $$\n   which rearranges to the stated closed form for $s_1$ and the logit identity.\n5. QED.\n:::\n\n## Discrete (extensive-margin) model\n\n**Task activation makes selection central.**\n: The continuous model assumes you always do some of each task. That is wrong when tasks are lumpy, have setup costs, or are unit-demand. In those cases, LLMs can create newly affordable tasks, meaning the major effect is selection (which tasks get done at all), not intensive reallocation.\n\n### Unit-demand formulation\n\n**A unit-demand formulation treats tasks as lumpy goods.**\n: Let each task have payoff $u_i$ and required time $w_i$, with decision $q_i\\in\\{0,1\\}$. Then\n  $$\n  \\max_{q\\in\\{0,1\\}^n}\\sum_i u_i q_i\\quad\\text{s.t.}\\quad \\sum_i w_i q_i\\le 1.\n  $$\n\n**Productivity multipliers can turn tasks on at extensive-margin thresholds.**\n: If AI multiplies productivity for task $i$ by $A_i$, and time required is inversely proportional to productivity, then $w_i$ falls to $w_i/A_i$. That change can turn tasks on once a threshold is crossed. This is the basic lumpy-choice phenomenon: tasks that were too time-expensive become attractive after AI. A CES elasticity is not a good summary statistic in this regime.\n\n**Pre and post task baskets give Laspeyres and Paasche bounds on the lift.**\n: Suppose you observe (i) the pre-AI chosen task vector $q^0$ and the post-AI chosen task vector $q^1$, and (ii) the per-task time requirements before and after AI, denoted $w_i^0$ and $w_i^1$. Define the time cost of a basket as $T^k(q)\\equiv \\sum_i w_i^k q_i$.\n  Consider the fixed-basket lifts\n  $$\n  G_L \\equiv \\frac{T^0(q^0)}{T^1(q^0)} \\qquad\\text{and}\\qquad\n  G_P \\equiv \\frac{T^0(q^1)}{T^1(q^1)}.\n  $$\n  Under standard revealed-preference logic for a cost-of-living index (with time as the numeraire), the true time-price index lies between Paasche and Laspeyres, so the productivity lift lies between their inverses:\n  $$\n  G_L \\;\\le\\; \\frac{V(A^1)}{V(A^0)} \\;\\le\\; G_P.\n  $$\n  Intuitively, $G_L$ is a conservative hold-the-old-basket-fixed estimate (lower bound), while $G_P$ allows both the basket and effective prices to shift (upper bound).\n\n**Tasks with high \"overhead shares\" get larger effective multipliers.**\n: A useful way to think about the tasks-you-previously-avoided condition is to decompose time cost into a baseline component plus an overhead component (search, reading, drafting, refactoring). For example, write\n  $$\n  w_i=a_i+b_i,\n  $$\n  and suppose AI primarily reduces overhead, $b_i' = b_i/A^{O}$ while $a_i$ is unchanged. Then\n  $$\n  w_i' = a_i + \\frac{b_i}{A^{O}},\\qquad\n  A_i \\equiv \\frac{w_i}{w_i'} = \\frac{a_i+b_i}{a_i+b_i/A^{O}},\n  $$\n  so $A_i$ is increasing in the overhead share $b_i/(a_i+b_i)$. Tasks you previously avoided plausibly have high overhead shares, so they get larger effective multipliers and are more likely to cross activation thresholds.\n\n### Setup-cost variant (bridging discrete and continuous)\n\n**Setup costs create lumpy activation even when within-task intensity is divisible.**\n: Add a fixed setup time $\\phi_i$ and allow continuous within-task time allocations:\n$$\n\\max_{q,t}\\; y(A_1 t_1,\\dots,A_n t_n)\\quad\\text{s.t.}\\quad \\sum_i \\phi_i q_i + \\sum_i t_i \\le 1,\\; t_i=0\\;\\text{if }q_i=0.\n$$\n\n**With setup costs, large shocks mostly expand the active set.**\n: If $\\phi_i=0$, we recover the continuous model. If $\\phi_i>0$, large productivity changes mostly expand the active set $\\{i:q_i=1\\}$, not the intensive shares.\n\n### Worked example (discrete, not continuous)\n\n**Discrete choice does not identify a single time-savings without more structure.**\n: Suppose you can pick one task (unit demand). Task A yields value $u_A=10$ and takes 1 hour. Task B yields $u_B=12$ and takes 2 hours. Without LLMs you choose A. Now an LLM speeds up task B so it takes 1 hour, and you switch to B. The observed switch is consistent with a wide range of time-equivalent gains:\n\n| Bound interpretation | Time-equivalent gain |\n|---|---:|\n| Upper bound (extra value worth a full hour) | 1 hour |\n| Lower bound (extra value is only a small quality bump) | 0 hours |\n\n**Selection effects require a model or bounds, not just observed switching.**\n: The observed reallocation does not identify a precise time-savings without modeling discrete choice. This is why constant-elasticity summaries can be weak in the activation regime.\n\n### Newly activated tasks\n\n**Definition of newly activated tasks.**\n: Call a task newly activated if you would not do it at baseline time prices but you do once its time cost drops.\n  Examples include literature reviews you previously would not attempt, custom data visualizations, and long-form proofreading or refactoring.\n\n**These show up as extensive-margin choices, not smooth intensities.**\n: In a unit-demand or setup-cost model, these tasks show up as newly activated $q_i=1$ choices, not as marginal changes in time allocations. This is why AI share of time can jump even if underlying preferences are stable: the feasible set changed.\n\n## Applications\n\n### Application 1: from query-level time savings to an aggregate lift (Anthropic)\n\n**Anthropic-style log studies estimate conditional time savings on observed AI conversations.**\n: @anthropic2025estimatingproductivitygains estimates time savings from Claude conversations by comparing time required with vs without AI for a sample of tasks drawn from usage logs.\n\n**Back-of-envelope inputs can be summarized as a baseline share and a productivity multiplier.**\n: Illustratively, the inputs are:\n\n| Quantity | Symbol | Value |\n|---|---:|---:|\n| Baseline time share on the AI-affected bundle | $s_0$ | $10\\\\%$ |\n| Productivity multiplier when AI is used | $A$ | $5$ |\n\n**Even in a two-good continuous approximation, the implied lift depends sharply on substitution.**\n: The table below shows how different assumptions about substitutability map the same inputs into very different lifts.\n\n| Assumption about substitution | What it means in this note | Assumption on $\\varepsilon$ | Implied lift when $s_0=10\\%$ and $A=5$ |\n|---|---|---|---|\n| Strong complements (Amdahl/Leontief-style) | Time shares do not move much when a subset of tasks speeds up | effectively $\\varepsilon\\to 0$ | about $+8.7\\%$ |\n| Cobb-Douglas benchmark | Unit elasticity, moderate reallocation | $\\varepsilon=1$ | about $+17.4\\%$ |\n| CES example with stronger substitution | Time shifts strongly toward the sped-up bundle | $\\varepsilon=2$ | about $+40\\%$ |\n\n**The key empirical object is the share response to an effective price change.**\n: The identification problem is not only how big $A$ is when AI is used, but how time shares respond when relative time prices change. In the CES benchmark that response is summarized by $\\varepsilon$; outside CES, it is a demand-curve object.\n\n**Endogenous task mix can make pre-AI shares a poor guide.**\n: If AI reduces time costs more for tasks you previously avoided (high search/reading/writing overhead), then the pre-AI share $s_0$ understates the mass of tasks that become attractive post-AI.\n\n**Quality adjustment matters whenever \"time saved\" is partly quality.**\n: Some measured time savings are quality improvements (or vice versa). If your output index treats quality as part of output, you need a quality measure to map time changes into the output index $y(\\cdot)$.\n\n### Application 2: interpreting “uplift” RCTs (METR / open-source dev)\n\n**Uplift RCTs often identify intensive effects on a fixed task set.**\n: @becker2025uplift is an RCT where tasks are assigned to allow vs disallow AI tools, and the outcome is completion time (with additional self-reports and robustness checks). The headline is that AI access increases completion time on average in their setting.\n\n**A simple back-of-envelope converts time changes into a productivity multiplier.**\n: If allowing AI increases completion time by 19%, then the implied productivity multiplier is $A \\approx 1/1.19 \\approx 0.84$ (a slowdown). Holding output constant, that is about a 16% productivity hit on that task population.\n\n**Fixed-task designs identify an $A_i$ distribution but not substitution.**\n: With a fixed task set (as in many RCTs), the main object you learn is a task-level time change (an $A_i$ distribution if you convert time changes into productivity multipliers), holding task composition fixed. Mapping that into $V(A^1)/V(A^0)$ requires additional assumptions or variation that moves the task mix.\n\n**Aggregate interpretations require an explicit output and quality index.**\n: Translating time changes into an aggregate productivity index still requires an output/quality index: are we holding output constant (pure time saved), holding time constant (more output), or letting both adjust? In practice you often want quality-adjusted output per unit time.\n\n**If AI changes which tasks are attempted, the design must measure selection.**\n: If AI access changes which tasks are attempted, then the design has to either fix tasks (to isolate intensive effects) or explicitly allow selection and measure it (to capture the extensive margin).\n\n## Experimental design\n\n**Design choices mostly trade off identification of substitution against control of selection.**\n: Designing studies that map $\\{A_i\\}$ into an aggregate lift $V(A^1)/V(A^0)$ is mostly about measuring substitution and separating intensive vs extensive margins.\n\n**Decide the estimand before collecting data.**\n: Same tasks, faster (hold task outputs fixed) is different from best use of a fixed time budget (maximize $y(A_1 t_1,\\dots,A_n t_n)$). The latter matches the substitution estimand in this note.\n\n**Use complementary protocols to separate intensive and extensive margins.**\n: A simple way to separate margins is to run both a fixed-task and a time-budget protocol:\n\n| Protocol | What is held fixed | What it identifies |\n|---|---|---|\n| Fixed-task protocol | A pre-specified task list | Task-level time/quality changes (intensive margin) |\n| Time-budget protocol | A fixed time endowment and objective | Substitution and selection (task mix + output index) |\n\n**Measure quality explicitly and specify how it enters the output index.**\n: Use blinded human evaluation, unit tests, or objective metrics; pre-register how quality enters $y(\\cdot)$ (e.g. threshold vs continuous scoring).\n\n**Trace a demand curve by randomizing effective AI price/quality.**\n: Randomize not only AI allowed, but the effective cost or strength of AI (token budgets, latency, model quality tiers, usage quotas). This gives variation to estimate how shares move with relative time prices (the object behind $\\varepsilon$ or, more generally, Hicksian shares).\n\n**Handle learning, spillovers, and dynamics.**\n: Use cross-over designs, washout periods, or between-subject randomization where appropriate; measure experience and allow for dynamic treatment effects.\n\n## Related literature (more explicit)\n\n**This section is a reading map keyed to the objects used in the note.**\n: The goal is to point to canonical references for price indices, time allocation, discrete activation, and empirical measurement, without mixing too much bibliographic detail into the thesis lines.\n\n### Index numbers and exact welfare from price changes (continuous case)\n\n**Expenditure-function cost-of-living indices.**\n: @konus1939trueindex defines a cost-of-living index as the ratio of minimum expenditures needed to reach a fixed utility level at two price vectors. This is exactly the object $P(p)=e(p,1)$ (with time prices instead of money prices) that corresponds here to $V(A)=1/P(1/A)$ under homotheticity.\n\n**Exact/superlative indices and share-weighted approximations.**\n: @diewert1976exact formalizes when index numbers (including Tornqvist/Divisia-type log-share formulas) are exact for flexible functional forms and motivates using share-weighted log changes as a local approximation.\n\n**Duality and productivity measurement via index numbers.**\n: @caves1982indexnumbers is a central reference for exact index-number measurement of input, output, and productivity, and for the duality framing used in the continuous model here.\n\n**Large changes and compensated-demand integrals.**\n: @willig1976consumerssurplus and @hausman1981exact connect equivalent/compensating variation and consumer surplus to integrals of compensated demand, clarifying why large AI shocks call for area under Hicksian demand rather than constant-elasticity shortcuts.\n\n**Demand systems for practical welfare calculations.**\n: @deaton1980aids introduces AIDS, a workhorse integrable demand system used to estimate demand and welfare effects of price changes in practice.\n\n### Time allocation, information costs, and \"overhead\" time\n\n**Time allocation as a core economic problem.**\n: @becker1965allocation frames time allocation as a core economic problem, and is a natural ancestor of treating task durations as time prices.\n\n**Time requirements and shadow prices.**\n: @deserpa1971time emphasizes that goods require time to consume/produce and develops shadow-price implications close to the time-price interpretation here.\n\n**Information/search costs as a mechanism for time savings.**\n: @stigler1961information treats costly search as central; empirically, many LLM speedups look like reductions in search/reading/writing overhead rather than reductions in core task time.\n\n**Characteristics and implicit prices.**\n: @lancaster1966consumer gives an alternative lens: tasks bundle characteristics (information, drafting, formatting, reasoning), and AI shifts the implicit prices of those characteristics.\n\n### Discrete activation, selection, and welfare with lumpy choices\n\n**Welfare tools for discrete choice.**\n: @smallrosen1981welfare is a classic reference for welfare analysis with discrete choice, directly relevant for unit-demand tasks and activation thresholds.\n\n**Applied discrete-choice estimation.**\n: @train2003discretechoice is a standard applied reference for estimating random-utility/discrete-choice models when you want to quantify substitution and welfare.\n\n### Task-based technological change and aggregation\n\n**Task-based technological change.**\n: @autor2003skill and @acemoglu2011handbook popularize task-based views of technology, conceptually aligned with treating tasks as goods with relative time prices.\n\n**Small-shock share-weighted logic (Hulten-style).**\n: @hulten1978growth is the backbone for why $\\Delta\\ln V\\approx \\sum_i t_i\\,\\Delta\\ln A_i$ can be reasonable for small or broad shocks.\n\n**Beyond small shocks: nonlinearities and large changes.**\n: @baqaee2019macro is a canonical reference on nonlinearities and large shocks and connects directly to why large AI shocks motivate demand-curve integration and careful treatment of selection.\n\n**Micro-to-macro aggregation from technology evidence.**\n: @oberfield2021micro is a useful reference on linking micro evidence on technology to macro implications.\n\n**CES as a workhorse benchmark.**\n: @dixit1977monopolistic sits behind many closed-form substitution formulas, including the two-good CES benchmark used above.\n\n### Empirical AI productivity and usage measurement\n\n**Log-based time-savings evidence.**\n: @anthropic2025estimatingproductivitygains illustrates the appeal (and limits) of measuring conditional time savings and baseline shares in observational usage data.\n\n**Fixed-task RCT evidence.**\n: @becker2025uplift exemplifies randomized access to AI tools with completion-time outcomes, highlighting the importance of quality measurement and task selection.\n\n**Early gen-AI productivity evidence across settings.**\n: @noy2023generative and @brynjolfsson2023generative are widely cited early studies with evidence on productivity and heterogeneity.\n\n**Usage composition and task mix.**\n: @chatterji2025chatgpt documents usage patterns and task composition for ChatGPT, relevant when the task mix is endogenous.\n\n**Valuing time savings and digital services.**\n: @varian2011economic and @collis2025welfare are representative of work valuing digital services and time savings, complementary to the demand-theory framing here.\n\n",
    "supporting": [
      "2025-12-17-llm-time-saving-demand-theory-substitution.llm_files"
    ],
    "filters": [],
    "includes": {}
  }
}