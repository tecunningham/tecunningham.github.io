{
  "hash": "a523458823eb733fd32fd3dfe3576cc6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"LLM Time-Saving, Demand Theory, and Task Activation\"\nauthor: \"Tom Cunningham (METR)\"\ndate: today\ndraft: true\ncitation: true\nreference-location: document\nbibliography: ai.bib\nformat:\n  html:\n    toc: true\n    toc-depth: 3\nexecute:\n  echo: false\n  warning: false\n  error: false\n  cache: true\n---\n\n<!-- https://tecunningham.github.io/posts/2025-12-17-llm-time-saving-demand-theory-substitution.llm.html -->\n<!-- Author note: review tools/test_llm_time_saving_qmd.py for tests and run it after edits. -->\n\n::: {#2e4a0ba0 .cell results='asis' execution_count=2}\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n<details class=\"validation-checklist\">\n<summary>Validation Checks</summary>\n\n```text\nValidation Checks\n✅ Programmatic: proof structure (collapsed)\n✅ Programmatic: Mermaid syntax lint (offline)\n✅ [7/7] Programmatic: bibliography tests\n  ✅ Duplicate citekeys\n  ✅ One field per line + trailing comma\n  ✅ Source locator present (url/doi/eprint) (225/225)\n  ✅ Abstract length <= 500 words (72/72)\n  ✅ abstract_source only when abstract is present (63/63)\n  ✅ arXiv eprints use arxiv.org/pdf/<id>.pdf URLs (24/24)\n  ✅ text_url has local text archive (16/16)\n✅ Programmatic: citekeys resolve in posts/ai.bib\n⏳ LLM-assisted: quality + citation plausibility gate (optional)\n```\n</details>\n\n:::\n:::\n\n\n<style>\n  dl {display: grid;}\n  dt {grid-column-start: 1; width: 18em;}\n  dd {grid-column-start: 2; margin-left: 1.25em;}\n  details.validation-checklist {\n    background: #f5f5f5;\n    border: 1px solid #777;\n    border-radius: 6px;\n    padding: 0.5em 0.75em;\n    margin-bottom: 1em;\n  }\n  details.validation-checklist > summary {\n    cursor: pointer;\n  }\n</style>\n\n<!--\nEditing note: definition lists + multiline math\n\nIn Quarto, a definition list item starts with exactly one leading colon:\n\nTerm (a short claim)\n: First paragraph of the definition.\n  Continuation lines must be indented (e.g. two spaces).\n  $$\n  Multiline display math goes here.\n  $$\n\nDo NOT prefix continuation lines with additional ':' characters; that often breaks math rendering.\n-->\n\n#        Summary\n\nWe want to know the effect of AI-speedups on output.\n: \n\nMany good:\n\n- General Hicksian demand\n- General CES\n- For fixed allocation\n- Lower and upper bounds from observing $A$ or $A'$.\n\nTwo-good case.\n\n$$\\frac{1}{(1-t_2)+t_2/A'}\\;\\le\\;\\frac{v'}{v}\\;\\le\\;(1-t_2')+A't_2'$$\n\n```mermaid\ngraph TD\n  %% Assumptions\n  A1[\"A1: Tasks & time endowment\"]\n  A2[\"A2: Productivities; z_i = A_i t_i\"]\n  A3[\"A3: y is monotone, concave, homogeneous\"]\n  A4[\"A4: Differentiability (Shephard/envelope)\"]\n  C1[\"C1: CES aggregator\"]\n  T1[\"T1: Activation costs (discrete activation)\"]\n\n  %% Propositions / corollaries\n  P1[\"Prop 1: Time allocation ⇔ time-price formulation\"]\n  P2[\"Prop 2: V(A)=1/P(1/A)\"]\n  P3[\"Prop 3: V(A')/V(A)=P(p)/P(p')\"]\n  P4[\"Prop 4: Laspeyres/Paasche-type bounds\"]\n  P5[\"Prop 5: EV/CV in time units\"]\n  P6[\"Prop 6: Time shares = Hicksian shares\"]\n  P7[\"Prop 7: d ln V = Σ t_i d ln A_i\"]\n  C71[\"Cor 7.1: First-order baseline-share approximation\"]\n  P8[\"Prop 8: Exact integral for large changes\"]\n  C81[\"Cor 8.1: Single-component integral\"]\n  P9[\"Prop 9: Törnqvist/Divisia trapezoid approximation\"]\n  P10[\"Prop 10: CES price index and shares\"]\n  P11[\"Prop 11: Two-good CES gain closed form\"]\n  P12[\"Prop 12: CES share response / elasticity identification\"]\n  C121[\"Cor 12.1: CES limiting cases\"]\n  P13[\"Prop 13: Activation ⇒ possible non-differentiability\"]\n\n  %% Dependencies (edges indicate proof dependence)\n  A1 --> P1\n  A2 --> P1\n\n  A3 --> P2\n  P1 --> P2\n\n  P2 --> P3\n\n  A3 --> P6\n  P1 --> P6\n  P2 --> P6\n\n  A3 --> P5\n  P2 --> P5\n\n  A3 --> P4\n  P3 --> P4\n  P6 --> P4\n\n  A3 --> P7\n  A4 --> P7\n  P2 --> P7\n  P6 --> P7\n\n  P7 --> C71\n  P7 --> P8\n  P8 --> C81\n  P8 --> P9\n\n  C1 --> P10\n  A4 --> P10\n\n  P3 --> P11\n  P10 --> P11\n  P6 --> P11\n\n  P10 --> P12\n  P6 --> P12\n\n  P11 --> C121\n\n  T1 --> P13\n  ```\n\n\n\n#        Setup\n\nYou allocate time between tasks.\n: Let $t=(t_1,\\dots,t_n)$ be time allocated across tasks with $\\sum_i t_i=1$. Let $A=(A_1,\\dots,A_n)$ be task-specific productivity (effective output per unit time on each task). Let the output index be\n   $$y(A_1 t_1,\\dots,A_n t_n),$$\n   where $y(\\cdot)$ is increasing in each argument.\n  \n    We assume people allocate time across tasks optimally, so the substitution-adjusted productivity level is:\n    $$V(A)\\equiv \\max_{t\\ge 0}\\; y(A_1 t_1,\\dots,A_n t_n)\\quad\\text{s.t.}\\quad \\sum_i t_i\\le 1.$$\n   An LLM changes productivity from $A$ to $A'$, and so the lift is $V(A')/V(A)$.\n\n\nThis is identical to a consumption problem.\n: Write effective task output as $z_i\\equiv A_i t_i$. Then\n  $$\n  V(A)=\\max_{z\\ge 0}\\; y(z)\\quad\\text{s.t.}\\quad \\sum_i \\frac{z_i}{A_i}\\le 1.\n  $$\n  Define time prices $p_i\\equiv 1/A_i$. This turns the constraint into $\\sum_i p_i z_i\\le 1$, so standard expenditure-function and index-number results apply. Define the unit-expenditure function $P(p)\\equiv e(p,1)$. Under homotheticity, the productivity level is\n  $$\n  V(A)=\\frac{1}{P(1/A)}.\n  $$\n  This is the classic index-number framing applied to time prices (see @caves1982indexnumbers).\n\nEV and CV translate productivity changes into welfare changes in time units.\n: Let $A\\to A'$ and define $p\\equiv 1/A$ and $p'\\equiv 1/A'$. Equivalent and compensating variation (measured in time) are\n  $$\n  EV=e(p, V(A'))-1,\\qquad CV=e(p', V(A))-1.\n  $$\n  Under homotheticity,\n  $$\n  EV=\\frac{P(p)}{P(p')}-1,\\qquad CV=\\frac{P(p')}{P(p)}-1,\n  $$\n  which is a clean way to translate time-saving into a welfare measure (see @hausman1981exact).\n\nSmall changes admit a share-weighted approximation.\n: Let $t_i(A)$ denote optimal time shares. For small changes,\n  $$\n  d\\ln V \\;\\approx\\;\\sum_i t_i(A)\\,d\\ln A_i.\n  $$\n  This is the time-allocation analog of share-weighted Hulten-style logic (see @hulten1978growth).\n\nLarge changes require integrating compensated shares.\n: When gains are large, constant-elasticity shortcuts are dangerous. Using Hicksian (compensated) shares $s_i^H(p)$ at fixed output,\n  $$\n  d\\ln P(p)=\\sum_i s_i^H(p)\\,d\\ln p_i.\n  $$\n  For a single changing price (equivalently, a single changing productivity component),\n  $$\n  \\ln\\frac{P(p')}{P(p)}=\\int s_2^H(p_2)\\,d\\ln p_2,\n  $$\n  i.e., exact welfare is an area-under-the-compensated-demand-curve object (see @willig1976consumerssurplus).\n\nCES yields closed-form formulas useful for back-of-the-envelope work.\n: For a CES aggregator in $z$,\n  $$\n  y(z)=\\left(\\sum_i \\alpha_i z_i^{\\frac{\\sigma-1}{\\sigma}}\\right)^{\\frac{\\sigma}{\\sigma-1}},\\quad\\sigma>0,\n  $$\n  the associated price index and (compensated) shares have closed form. In a two-good reduction where only good 2 gets a productivity multiplier $A_2$ and its ex-ante time share is $t_2$,\n  $$\n  \\frac{V(A')}{V(A)}=\\left((1-t_2)+t_2\\,A_2^{\\varepsilon-1}\\right)^{\\frac{1}{\\varepsilon-1}},\\qquad \\varepsilon\\equiv\\sigma.\n  $$\n  This is the clean continuous benchmark (see @caves1982indexnumbers).\n\nIn the multi-task case, a Tornqvist/Divisia log-index is a convenient approximation.\n: A common approximation (using optimal time shares) is\n  $$\n  \\Delta\\ln V \\;\\approx\\; \\sum_i \\bar t_i\\,\\ln\\left(\\frac{A_i'}{A_i}\\right),\\qquad \\bar t_i \\equiv \\tfrac12(t_i+t_i'),\n  $$\n  which uses average (pre/post) time shares (see @caves1982indexnumbers).\n\n\n#     Two Good Case\n\nCollapse tasks into an AI-affected bundle and the rest.\n: Define two bundles: good 2 is the set of tasks whose productivity rises by a factor $A_2$ when AI is allowed, and good 1 is everything else (normalized). Let $t_2$ be the pre-AI time share on good 2, and suppose only good 2 gets a productivity multiplier $A'_2>1$, with $A_1=1$.\n\nThe output gain has a closed form in terms of $t_2,A_2,\\varepsilon$.\n: In a two-good CES benchmark where only good 2 gets a multiplier $A_2$ (so $p_2'=p_2/A_2$),\n  $$\n  \\frac{V(A')}{V(A)}=\\left((1-t_2)+t_2\\,A_2^{\\varepsilon-1}\\right)^{\\frac{1}{\\varepsilon-1}}.\n  $$\n\nThe post-AI share shift pins down $\\varepsilon$ when $t_2'$ is observed.\n: The post-AI time share is\n  $$\n  t_2'=\\frac{t_2\\,A_2^{\\varepsilon-1}}{(1-t_2)+t_2\\,A_2^{\\varepsilon-1}}\n  \\quad\\Leftrightarrow\\quad\n  \\operatorname{logit}(t_2')-\\operatorname{logit}(t_2)=(\\varepsilon-1)\\ln A_2.\n  $$\n\n::: {.callout-note collapse=\"true\"}\n## Proof (structured)\n\n1. *Given* CES unit-expenditure (time price) index\n   $$\n   P(p)=\\left(\\alpha_1^{\\varepsilon}p_1^{1-\\varepsilon}+\\alpha_2^{\\varepsilon}p_2^{1-\\varepsilon}\\right)^{\\frac{1}{1-\\varepsilon}},\n   \\qquad V(A)=\\frac{1}{P(1/A)}.\n   $$\n2. *Let* $p_2' = p_2/A_2$ with $p_1$ fixed. Then\n   $$\n   \\frac{V(A')}{V(A)}=\\frac{P(p)}{P(p')}=\n   \\left(\n     \\frac{\\alpha_1^{\\varepsilon}p_1^{1-\\varepsilon}+\\alpha_2^{\\varepsilon}p_2^{1-\\varepsilon}}\n         {\\alpha_1^{\\varepsilon}p_1^{1-\\varepsilon}+\\alpha_2^{\\varepsilon}(p_2/A_2)^{1-\\varepsilon}}\n   \\right)^{\\frac{1}{\\varepsilon-1}}.\n   $$\n3. *Define* the ex-ante share\n   $$\n   t_2\\equiv\\frac{\\alpha_2^{\\varepsilon}p_2^{1-\\varepsilon}}\n                  {\\alpha_1^{\\varepsilon}p_1^{1-\\varepsilon}+\\alpha_2^{\\varepsilon}p_2^{1-\\varepsilon}}.\n   $$\n   Substituting into Step 2 yields the claimed gain formula.\n4. *For shares,* note CES time shares satisfy\n   $$\n   \\frac{t_2}{1-t_2}=\\frac{\\alpha_2^{\\varepsilon}p_2^{1-\\varepsilon}}{\\alpha_1^{\\varepsilon}p_1^{1-\\varepsilon}}.\n   $$\n   Therefore,\n   $$\n   \\frac{t_2'/(1-t_2')}{t_2/(1-t_2)}\n   =\n   \\frac{(p_2')^{1-\\varepsilon}}{p_2^{1-\\varepsilon}}\n   =\n   \\left(\\frac{p_2/A_2}{p_2}\\right)^{1-\\varepsilon}\n   =\n   A_2^{\\varepsilon-1},\n   $$\n   which rearranges to the stated closed form for $t_2'$ and the logit identity.\n5. QED.\n:::\n\n## Two-good reduction (AI-affected bundle vs the rest)\n\nCommon benchmarks are:\n\n| Assumption about substitution                          | Implied output gain $V(A')/V(A)$                                            | Notes                                                    |\n| ------------------------------------------------------ | --------------------------------------------------------------------------- | -------------------------------------------------------- |\n| CES elasticity $\\varepsilon$                           | $\\left((1-t_2)+t_2\\,A_2^{\\varepsilon-1}\\right)^{\\frac{1}{\\varepsilon-1}}$   | Requires $\\varepsilon$ (or enough data to infer it)      |\n| $\\varepsilon\\rightarrow0$ (perfect complements/Amdahl) | $\\dfrac{1}{(1-t_2)+t_2/A_2}$                                                | Fixed proportions; $t_2$ is the fixed-proportion share   |\n| $\\varepsilon=1$ (Cobb-Douglas/Hulten)                  | $A_2^{t_2}$                                                                 | Share is constant; log change is $t_2\\ln A_2$            |\n| $\\varepsilon\\rightarrow\\infty$ (perfect substitutes)   | $A_2$                                                                       | Assumes the other bundle is not required for output      |\n\n\nAnthropic-style numbers span a wide range under different substitution assumptions.\n: If $t_2=0.10$ and $A_2=5$ (a 5x multiplier when AI is used), the implied gain depends heavily on the benchmark.\n\n| Benchmark | Gain factor | Percent |\n|---|---:|---:|\n| Complements bound | $1/(0.9+0.02)\\approx 1.087$ | $+8.7\\%$ |\n| Cobb-Douglas | $5^{0.1}\\approx 1.174$ | $+17.4\\%$ |\n| CES, $\\varepsilon=2$ | $0.9+0.1\\cdot 5 = 1.4$ | $+40\\%$ |\n| Substitutes bound | $5$ | $+400\\\\%$ |\n\nThe spread is the point.\n: Large conditional speedups do not translate to a unique aggregate lift without additional structure or data.\n\n## If you observe pre and post time shares, you can infer an elasticity (in CES)\n\nIn a two-good CES benchmark, post-AI shares identify the elasticity.\n: In the same two-good CES setting, let $t_2'$ be the post-AI time share on good 2. Then\n  $$\n  \\operatorname{logit}(t_2')-\\operatorname{logit}(t_2)=(\\varepsilon-1)\\ln A_2,\n  $$\n  so\n  $$\n  \\varepsilon \\;=\\; 1 + \\frac{\\operatorname{logit}(t_2')-\\operatorname{logit}(t_2)}{\\ln A_2}.\n  $$\n  Here $\\operatorname{logit}(x)\\equiv \\ln\\!\\left(\\frac{x}{1-x}\\right)$.\n  This is often the cleanest way to use both pre and post shares: estimate $\\varepsilon$, then plug into the CES gain formula. This uses Hicksian shares (or Marshallian shares under homotheticity and a fixed time endowment). If you do not believe CES globally, treat this as local to the observed price change.\n\nTask activation makes selection central.\n: The continuous model assumes you always do some of each task. That is wrong when tasks are lumpy, have setup costs, or are unit-demand. In those cases, LLMs can create newly affordable tasks, meaning the major effect is selection (which tasks get done at all), not intensive reallocation.\n\n\n\n## Application 1: from query-level time savings to an aggregate lift (Anthropic)\n\nAnthropic-style log studies estimate conditional time savings on observed AI conversations.\n: @anthropic2025estimatingproductivitygains estimates time savings from Claude conversations by comparing time required with vs without AI for a sample of tasks drawn from usage logs.\n\nBack-of-the-envelope inputs can be summarized as a baseline share and a productivity multiplier.\n: Illustratively, the inputs are:\n\n| Quantity | Symbol | Value |\n|---|---:|---:|\n| Baseline time share on the AI-affected bundle | $t_2$ | $10\\%$ |\n| Productivity multiplier when AI is used | $A_2$ | $5$ |\n\nEven in a two-good continuous approximation, the implied lift depends sharply on substitution.\n: The table below shows how different assumptions about substitutability map the same inputs into very different lifts.\n\n| Assumption about substitution | What it means in this note | Assumption on $\\varepsilon$ | Implied lift when $t_2=10\\%$ and $A_2=5$ |\n|---|---|---|---|\n| Strong complements (Amdahl/Leontief-style) | Time shares do not move much when a subset of tasks speeds up | effectively $\\varepsilon\\to 0$ | about $+8.7\\%$ |\n| Cobb-Douglas benchmark | Unit elasticity, moderate reallocation | $\\varepsilon=1$ | about $+17.4\\%$ |\n| CES example with stronger substitution | Time shifts strongly toward the sped-up bundle | $\\varepsilon=2$ | about $+40\\%$ |\n\nThe key empirical object is the share response to an effective price change.\n: The identification problem is not only how big $A_2$ is when AI is used, but how time shares respond when relative time prices change. In the CES benchmark that response is summarized by $\\varepsilon$; outside CES, it is a demand-curve object.\n\nEndogenous task mix can make pre-AI shares a poor guide.\n: If AI reduces time costs more for tasks you previously avoided (high search/reading/writing overhead), then the pre-AI share $t_2$ understates the mass of tasks that become attractive post-AI.\n\nQuality adjustment matters whenever \"time saved\" is partly quality.\n: Some measured time savings are quality improvements (or vice versa). If your output index treats quality as part of output, you need a quality measure to map time changes into the output index $y(\\cdot)$.\n\n## Application 2: interpreting “uplift” RCTs (METR / open-source dev)\n\nUplift RCTs often identify intensive effects on a fixed task set.\n: @becker2025uplift is an RCT where tasks are assigned to allow vs disallow AI tools, and the outcome is completion time (with additional self-reports and robustness checks). The headline is that AI access increases completion time on average in their setting.\n\nA simple back-of-the-envelope converts time changes into a productivity multiplier.\n: If allowing AI increases completion time by 19%, then the implied productivity multiplier is $A \\approx 1/1.19 \\approx 0.84$ (a slowdown). Holding output constant, that is about a 16% productivity hit on that task population.\n\nFixed-task designs identify an $A_i$ distribution but not substitution.\n: With a fixed task set (as in many RCTs), the main object you learn is a task-level time change (an $A_i$ distribution if you convert time changes into productivity multipliers), holding task composition fixed. Mapping that into $V(A')/V(A)$ requires additional assumptions or variation that moves the task mix.\n\nAggregate interpretations require an explicit output and quality index.\n: Translating time changes into an aggregate productivity index still requires an output/quality index: are we holding output constant (pure time saved), holding time constant (more output), or letting both adjust? In practice you often want quality-adjusted output per unit time.\n\nIf AI changes which tasks are attempted, the design must measure selection.\n: If AI access changes which tasks are attempted, then the design has to either fix tasks (to isolate intensive effects) or explicitly allow selection and measure it (to capture the extensive margin).\n\n# Related Literature\n\n\n## Index numbers and exact welfare from price changes\n\nExpenditure-function cost-of-living indices.\n: @konus1939trueindex defines a cost-of-living index as the ratio of minimum expenditures needed to reach a fixed utility level at two price vectors. This is exactly the object $P(p)=e(p,1)$ (with time prices instead of money prices) that corresponds here to $V(A)=1/P(1/A)$ under homotheticity.\n\nExact/superlative indices and share-weighted approximations.\n: @diewert1976exact formalizes when index numbers (including Tornqvist/Divisia-type log-share formulas) are exact for flexible functional forms and motivates using share-weighted log changes as a local approximation.\n\nDuality and productivity measurement via index numbers.\n: @caves1982indexnumbers is a central reference for exact index-number measurement of input, output, and productivity, and for the duality framing used in the continuous model here.\n\nLarge changes and compensated-demand integrals.\n: @willig1976consumerssurplus and @hausman1981exact connect equivalent/compensating variation and consumer surplus to integrals of compensated demand, clarifying why large AI shocks call for area under Hicksian demand rather than constant-elasticity shortcuts.\n\nDemand systems for practical welfare calculations.\n: @deaton1980aids introduces AIDS, a workhorse integrable demand system used to estimate demand and welfare effects of price changes in practice.\n\n## Time allocation, information costs, and \"overhead\" time\n\nTime allocation as a core economic problem.\n: @becker1965allocation frames time allocation as a core economic problem, and is a natural ancestor of treating task durations as time prices.\n\nTime requirements and shadow prices.\n: @deserpa1971time emphasizes that goods require time to consume/produce and develops shadow-price implications close to the time-price interpretation here.\n\nInformation/search costs as a mechanism for time savings.\n: @stigler1961information treats costly search as central; empirically, many LLM speedups look like reductions in search/reading/writing overhead rather than reductions in core task time.\n\nCharacteristics and implicit prices.\n: @lancaster1966consumer gives an alternative lens: tasks bundle characteristics (information, drafting, formatting, reasoning), and AI shifts the implicit prices of those characteristics.\n\n## Discrete activation, selection, and welfare with lumpy choices\n\nWelfare tools for discrete choice.\n: @smallrosen1981welfare is a classic reference for welfare analysis with discrete choice, directly relevant for unit-demand tasks and activation thresholds.\n\nApplied discrete-choice estimation.\n: @train2003discretechoice is a standard applied reference for estimating random-utility/discrete-choice models when you want to quantify substitution and welfare.\n\n## Task-based technological change and aggregation\n\nTask-based technological change.\n: @autor2003skill and @acemoglu2011handbook popularize task-based views of technology, conceptually aligned with treating tasks as goods with relative time prices.\n\nSmall-shock share-weighted logic (Hulten-style).\n: @hulten1978growth is the backbone for why $\\Delta\\ln V\\approx \\sum_i t_i\\,\\Delta\\ln A_i$ can be reasonable for small or broad shocks.\n\nBeyond small shocks: nonlinearities and large changes.\n: @baqaee2019macro is a canonical reference on nonlinearities and large shocks and connects directly to why large AI shocks motivate demand-curve integration and careful treatment of selection.\n\nMicro-to-macro aggregation from technology evidence.\n: @oberfield2021micro is a useful reference on linking micro evidence on technology to macro implications.\n\nCES as a workhorse benchmark.\n: @dixit1977monopolistic sits behind many closed-form substitution formulas, including the two-good CES benchmark used above.\n\n## Empirical AI productivity and usage measurement\n\nLog-based time-savings evidence.\n: @anthropic2025estimatingproductivitygains illustrates the appeal (and limits) of measuring conditional time savings and baseline shares in observational usage data.\n\nFixed-task RCT evidence.\n: @becker2025uplift exemplifies randomized access to AI tools with completion-time outcomes, highlighting the importance of quality measurement and task selection.\n\nEarly gen-AI productivity evidence across settings.\n: @noy2023generative and @brynjolfsson2023generative are widely cited early studies with evidence on productivity and heterogeneity.\n\nUsage composition and task mix.\n: @chatterji2025chatgpt documents usage patterns and task composition for ChatGPT, relevant when the task mix is endogenous.\n\nValuing time savings and digital services.\n: @varian2011economic and @collis2025welfare are representative of work valuing digital services and time savings, complementary to the demand-theory framing here.\n\n",
    "supporting": [
      "2025-12-17-llm-time-saving-demand-theory-substitution.llm_files"
    ],
    "filters": [],
    "includes": {}
  }
}