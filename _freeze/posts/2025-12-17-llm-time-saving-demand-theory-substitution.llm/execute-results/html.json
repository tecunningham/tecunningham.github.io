{
  "hash": "a1a9af3221f4391b5ab75b0835944ccf",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"LLM Time-Saving, Demand Theory, and Task Activation\"\nauthor: \"Tom Cunningham (METR)\"\ndate: today\ndraft: true\ncitation: true\nreference-location: document\nbibliography: ai.bib\nformat:\n  html:\n    toc: true\n    toc-depth: 3\nexecute:\n  echo: false\n  warning: false\n  error: false\n  cache: true\n---\n\n\n<!-- https://tecunningham.github.io/posts/2025-12-17-llm-time-saving-demand-theory-substitution.llm.html -->\n\n<!-- LLM note: review tools/test_llm_time_saving_qmd.py for tests and run it after edits. -->\n\n::: {#46225106 .cell results='asis' execution_count=2}\n\n::: {.cell-output .cell-output-display .cell-output-markdown}\n<details class=\"validation-checklist\">\n<summary>Validation Checks</summary>\n\n- ✅ Programmatic: required sections present (8/8)\n- ✅ Programmatic: proof structure (collapsed)\n- ✅ Programmatic: estimation flowchart present\n- ✅ Programmatic: diagrams present\n- ✅ Programmatic: applications present\n- ✅ Programmatic: experimental design present\n- ✅ Programmatic: bibliography tests\n  - ✅ Duplicate citekeys\n  - ✅ One field per line + trailing comma\n  - ✅ Source locator present (url/doi/eprint) (218/218)\n  - ✅ Abstract length &lt;= 500 words (72/72)\n  - ✅ abstract_source only when abstract is present (63/63)\n  - ✅ arXiv eprints use arxiv.org/pdf/&lt;id&gt;.pdf URLs (24/24)\n  - ✅ text_url has local text archive (16/16)\n  - ✅ bibclean lint\n- ✅ Programmatic: citekeys resolve in `posts/ai.bib`\n- ⏳ LLM-assisted: quality + citation plausibility gate (optional)\n</details>\n:::\n:::\n\n\n<style>\n   details.validation-checklist {\n      background: #f5f5f5;\n      border: 1px solid #777;\n      border-radius: 6px;\n      padding: 0.5em 0.75em;\n      margin-bottom: 1em;\n   }\n   details.validation-checklist > summary {\n      cursor: pointer;\n   }\n   .callout-header {\n      cursor: pointer;\n   }\n   dl {display: grid;}\n   dt {grid-column-start: 1; width: 10em;}\n   dd {grid-column-start: 2; margin-left: 2em;}\n</style>\n\n## Results-first summary\n\nWe want a productivity lift from LLM time-saving that allows for substitution: after an LLM changes which tasks are cheap in time, you reallocate time across tasks (and possibly activate tasks you previously skipped). This is a classic demand-theory problem, with *time* as the budget and *tasks* as the goods.\n\n- **Estimand.** Let $p_i$ be time per unit of task-output $i$, and let $u(x)$ be an output index (effective work accomplished). For a unit time endowment,\n  $$\n  v(p)\\;=\\;\\max_{x\\ge 0} u(x)\\quad\\text{s.t.}\\quad \\sum_i p_i x_i\\le 1.\n  $$\n  If an LLM yields speedups $\\beta_i$ so $p_i' = p_i/\\beta_i$, the productivity gain is $v(p')/v(p)$.\n\n- **Continuous (intensive-margin) benchmark.** Under standard regularity and (especially) homotheticity, $v(p)=1/P(p)$ where $P(p)=e(p,1)$ is the unit time-expenditure function (a time price index). Small changes obey a Divisia/Tornqvist approximation $\\Delta\\ln v\\approx \\sum_i s_i\\,\\Delta\\ln\\beta_i$. Large changes require integrating *compensated* shares (“area under Hicksian demand”). @caves1982indexnumbers; @hausman1981exact; @willig1976consumerssurplus\n\n- **Discrete (extensive-margin) reality.** Many tasks have setup time, are lumpy, or are “unit-demand” (you either do them or not). A particularly important empirical condition is: **LLMs reduce time costs more for tasks you previously avoided** (high reading/search/writing overhead). Under that condition, “AI-affected time share” is endogenous, and ex-ante shares can badly misstate the productivity lift unless you model (or measure) the extensive margin.\n\n- **Practical upshot.** With only a baseline share $s_0$ on an “AI bundle” and a speedup $\\beta$, the implied gain spans a wide range (complements lower bound to substitutes upper bound). To get a point estimate you need either (i) a demand system / elasticity that maps $s_0,\\beta\\mapsto v(p')/v(p)$, or (ii) experimental variation that traces out how time allocation responds as the effective AI cost changes.\n\n## Setup: time prices and speedups\n\n**Objects.** Tasks $i=1,\\dots,n$, outputs $x_i\\ge 0$, time endowment normalized to $1$, time prices $p_i>0$ (time per unit of task output), and LLM speedups $\\beta_i>0$ so $p_i' = p_i/\\beta_i$.\n\nWe interpret $u(x)$ as an *output index* or “effective work accomplished,” with time prices defining the budget:\n$$\n\\sum_i p_i x_i \\le 1.\n$$\n\nThe important split is:\n\n1. **Continuous intensive margin:** choose continuous $x_i$ (smooth substitution).\n2. **Discrete extensive margin:** choose which tasks to activate (unit demand or setup costs).\n\nI treat these separately because the logic, formulas, and data requirements diverge.\n\n## Estimation cheat sheet\n\nThis section is a “lookup table” for turning *measured* speedups into a productivity lift $v(p')/v(p)$ under common assumptions.\n\n### Two-good reduction (AI-affected bundle vs the rest)\n\nFor many back-of-envelope calculations, you can collapse tasks into:\n\n- **good 2:** the set of tasks whose time cost drops by $\\beta$ when AI is allowed,\n- **good 1:** everything else (normalized).\n\nLet $s_0$ be the **pre-AI time share** on good 2 (i.e. $s_0 \\equiv p_2 x_2 / (p_1 x_1+p_2 x_2)$ evaluated at baseline prices), and suppose only $p_2$ changes to $p_2' = p_2/\\beta$.\n\nThen common benchmarks are:\n\n| Assumption about substitution | Implied output gain $v(p')/v(p)$ | Notes |\n|---|---|---|\n| Perfect complements (Amdahl-style) | $\\dfrac{1}{(1-s_0)+s_0/\\beta}$ | Fixed proportions; AI only relaxes the bottleneck |\n| Cobb-Douglas ($\\varepsilon=1$) | $\\beta^{s_0}$ | Share is constant; log change is $s_0\\ln\\beta$ |\n| CES elasticity $\\varepsilon$ | $\\left((1-s_0)+s_0\\,\\beta^{\\varepsilon-1}\\right)^{\\frac{1}{\\varepsilon-1}}$ | Requires $\\varepsilon$ (or enough data to infer it) |\n| Perfect substitutes | $\\beta$ | You can reallocate everything to the sped-up bundle |\n\nIn the multi-task case, a common log-index approximation is the Tornqvist/Divisia form\n$$\n\\Delta\\ln v \\;\\approx\\; \\sum_i \\bar s_i\\,\\ln \\beta_i,\\qquad \\bar s_i \\equiv \\tfrac12(s_i^0+s_i^1),\n$$\nwhich uses average (pre/post) time shares. @caves1982indexnumbers\n\n**Anthropic-style numbers as a sanity check.** If $s_0=0.10$ and $\\beta=5$ (80% time reduction when AI is used), the implied gain ranges from:\n\n- complements bound: $1/(0.9+0.02)\\approx 1.087$ (8.7%),\n- Cobb-Douglas: $5^{0.1}\\approx 1.174$ (17.4%),\n- CES with $\\varepsilon=2$: $0.9+0.1\\cdot 5 = 1.4$ (40%),\n- substitutes bound: $5$ (400%).\n\nThe spread here is the point: large conditional speedups do *not* translate to a unique aggregate lift without additional structure or data.\n\n### If you observe pre and post time shares, you can infer an elasticity (in CES)\n\nIn the same two-good CES setting, let $s_1$ be the **post-AI** time share on good 2. Then\n$$\n\\operatorname{logit}(s_1)-\\operatorname{logit}(s_0)=(\\varepsilon-1)\\ln \\beta,\n$$\nso\n$$\n\\varepsilon \\;=\\; 1 + \\frac{\\operatorname{logit}(s_1)-\\operatorname{logit}(s_0)}{\\ln\\beta}.\n$$\n\nHere $\\operatorname{logit}(s)\\equiv \\ln\\!\\left(\\frac{s}{1-s}\\right)$.\n\nThis is often the cleanest way to use both pre and post shares: estimate $\\varepsilon$, then plug into the CES gain formula. (And if you do *not* believe CES globally, you should treat this as local-to-the-observed price change.)\n\n### Estimation flowchart (what to do with your data)\n\n\n```{mermaid}\nflowchart TD\n  A[Goal: estimate productivity lift v(p')/v(p)\\nfor fixed time endowment] --> B{Are tasks mostly divisible\\nat the relevant margin?}\n\n  B -->|Yes (continuous)| C{Are speedups small\\nor broad-based?}\n  C -->|Yes| D[Use share-weighted log changes:\\nΔ ln v ≈ Σ s_i Δ ln β_i\\n(Tornqvist/Divisia)]\n  C -->|No| E{Do you have enough data to pin down substitution?\\n(e.g. CES elasticity, demand system)}\n  E -->|Yes| F[Compute an exact/parametric index:\\nP(p)=e(p,1), v=1/P\\n(CES closed form when applicable)]\n  E -->|No| G[Trace a demand curve:\\nvary effective AI cost/strength;\\nestimate compensated shares; integrate]\n\n  B -->|No (setup costs / unit demand)| H[Model extensive margin:\\nwhich tasks get activated?]\n  H --> I[Report both:\\n(i) intensive effect on existing tasks,\\n(ii) activation/selection effect]\n```\n\n\n### What you can estimate with what data\n\n| What you can measure | Recommended move | What you can credibly report |\n|---|---|---|\n| One big conditional speedup $\\beta$ + a baseline share $s_0$ | Report complements/substitutes bounds; add CES sensitivity | A wide interval for $v(p')/v(p)$ unless you assume $\\varepsilon$ |\n| Pre and post shares $(s_0,s_1)$ for an AI bundle + a speedup $\\beta$ | Use the logit formula to estimate $\\varepsilon$; plug into CES gain | A model-based point estimate (local to the observed change) |\n| Multiple randomized “AI price/quality” arms + observed shares | Estimate share response vs $\\ln\\beta$; integrate shares over the change | An “area under the (compensated) demand curve” estimate for large changes |\n| Choice/activation data (tasks attempted) under multiple AI prices | Model activation thresholds / setup costs explicitly | Decomposition into intensive and extensive margins |\n\n## Continuous (intensive-margin) model\n\n### Primal, dual, and the time price index\n\nThe primal problem is\n$$\nv(p)\\;=\\;\\max_{x\\ge 0} u(x) \\quad\\text{s.t.}\\quad \\sum_i p_i x_i\\le 1.\n$$\n\nDefine the expenditure function\n$$\ne(p,\\bar u)=\\min_{x\\ge 0} \\Big\\{\\sum_i p_i x_i: u(x)\\ge \\bar u\\Big\\}.\n$$\n\nIf $u(\\cdot)$ is homothetic and degree-1 homogeneous, then $e(p,\\bar u)=\\bar u\\,e(p,1)$. Define the **unit time price index**\n$$\nP(p)\\equiv e(p,1)\\quad\\Rightarrow\\quad v(p)=\\frac{1}{P(p)}.\n$$\n\nThis is the classic index-number framing applied to time prices. @caves1982indexnumbers\n\n### EV/CV in time units\n\nLet $p^0\\to p^1$ and $u^k=v(p^k)$. Equivalent and compensating variation (measured in *time*) are\n$$\nEV=e(p^0,u^1)-1,\\qquad CV=e(p^1,u^0)-1.\n$$\n\nUnder homotheticity,\n$$\nEV=\\frac{P(p^0)}{P(p^1)}-1,\\qquad CV=\\frac{P(p^1)}{P(p^0)}-1.\n$$\n\nThis is the cleanest way to translate LLM time savings into a welfare measure. @hausman1981exact\n\n### Small changes (share-weighted)\n\nLet $t_i(p)\\equiv p_i x_i^*(p)$ be optimal time shares. For small changes in time prices,\n$$\nd\\ln v\\;=\\;-d\\ln P\\;\\approx\\;\\sum_i t_i\\,d\\ln \\beta_i.\n$$\n\nThis is the time-allocation analog of share-weighted Hulten-style approximations. @hulten1978growth\n\n### Large changes (area under compensated demand)\n\nWhen LLM gains are large, constant-elasticity approximations are dangerous. Using Hicksian (compensated) shares $s_i^H(p)$,\n$$\nd\\ln P(p)=\\sum_i s_i^H(p)\\,d\\ln p_i.\n$$\n\nFor a single changing price $p_2$,\n$$\n\\ln\\frac{P(p^1)}{P(p^0)}=\\int s_2^H(p_2)\\,d\\ln p_2,\n$$\n\ni.e., exact welfare is the **area under the compensated demand curve**. @willig1976consumerssurplus\n\n### CES specialization (closed-form)\n\nFor a CES aggregator\n$$\nu(x)=\\left(\\sum_i \\alpha_i x_i^{\\frac{\\sigma-1}{\\sigma}}\\right)^{\\frac{\\sigma}{\\sigma-1}},\\quad\\sigma>0,\n$$\n\nthe price index and time shares are\n$$\nP(p)=\\left(\\sum_i \\alpha_i^{\\sigma}p_i^{1-\\sigma}\\right)^{\\frac{1}{1-\\sigma}},\\qquad\n t_i(p)=\\frac{\\alpha_i^{\\sigma}p_i^{1-\\sigma}}{\\sum_j \\alpha_j^{\\sigma}p_j^{1-\\sigma}}.\n$$\n\nIn the two-task case, if task 2 speeds up by $\\beta$ and its ex-ante share is $s_0$, then\n$$\n\\frac{y'}{y}=\\left((1-s_0)+s_0\\,\\beta^{\\varepsilon-1}\\right)^{\\frac{1}{\\varepsilon-1}},\\qquad \\varepsilon\\equiv\\sigma.\n$$\n\nThis is the clean continuous benchmark. @caves1982indexnumbers\n\n#### Proposition: two-good CES gain and share shift\n\n**Claim.** In a two-good CES benchmark where only good 2 speeds up by $\\beta$ (so $p_2'=p_2/\\beta$), the output gain and share shift can be written in terms of the ex-ante share $s_0$:\n\n1. **Output gain**\n   $$\n   \\frac{v(p')}{v(p)}=\\left((1-s_0)+s_0\\,\\beta^{\\varepsilon-1}\\right)^{\\frac{1}{\\varepsilon-1}}.\n   $$\n2. **Post-AI share**\n   $$\n   s_1=\\frac{s_0\\,\\beta^{\\varepsilon-1}}{(1-s_0)+s_0\\,\\beta^{\\varepsilon-1}}\n   \\quad\\Leftrightarrow\\quad\n   \\operatorname{logit}(s_1)-\\operatorname{logit}(s_0)=(\\varepsilon-1)\\ln\\beta.\n   $$\n\n::: {.callout-note collapse=\"true\"}\n## Proof (structured)\n\n1. *Given* CES unit-expenditure (time price) index\n   $$\n   P(p)=\\left(\\alpha_1^{\\varepsilon}p_1^{1-\\varepsilon}+\\alpha_2^{\\varepsilon}p_2^{1-\\varepsilon}\\right)^{\\frac{1}{1-\\varepsilon}},\n   \\qquad v(p)=\\frac{1}{P(p)}.\n   $$\n2. *Let* $p_2' = p_2/\\beta$ with $p_1$ fixed. Then\n   $$\n   \\frac{v(p')}{v(p)}=\\frac{P(p)}{P(p')}=\n   \\left(\n     \\frac{\\alpha_1^{\\varepsilon}p_1^{1-\\varepsilon}+\\alpha_2^{\\varepsilon}p_2^{1-\\varepsilon}}\n          {\\alpha_1^{\\varepsilon}p_1^{1-\\varepsilon}+\\alpha_2^{\\varepsilon}(p_2/\\beta)^{1-\\varepsilon}}\n   \\right)^{\\frac{1}{\\varepsilon-1}}.\n   $$\n3. *Define* the ex-ante share\n   $$\n   s_0\\equiv\\frac{\\alpha_2^{\\varepsilon}p_2^{1-\\varepsilon}}\n                  {\\alpha_1^{\\varepsilon}p_1^{1-\\varepsilon}+\\alpha_2^{\\varepsilon}p_2^{1-\\varepsilon}}.\n   $$\n   Substituting into Step 2 yields the claimed gain formula.\n4. *For shares,* note CES time shares satisfy\n   $$\n   \\frac{s}{1-s}=\\frac{\\alpha_2^{\\varepsilon}p_2^{1-\\varepsilon}}{\\alpha_1^{\\varepsilon}p_1^{1-\\varepsilon}}.\n   $$\n   Therefore,\n   $$\n   \\frac{s_1/(1-s_1)}{s_0/(1-s_0)}\n   =\n   \\frac{(p_2')^{1-\\varepsilon}}{p_2^{1-\\varepsilon}}\n   =\n   \\left(\\frac{p_2/\\beta}{p_2}\\right)^{1-\\varepsilon}\n   =\n   \\beta^{\\varepsilon-1},\n   $$\n   which rearranges to the stated closed form for $s_1$ and the logit identity.\n5. **QED.**\n:::\n\n## Discrete (extensive-margin) model: task activation\n\nThe continuous model assumes you always do *some* of each task. That is wrong when tasks are lumpy, have setup costs, or are unit-demand. In those cases, LLMs can create **newly affordable tasks**, meaning the major effect is *selection*, not *intensive* time reallocation.\n\n### Unit-demand formulation\n\nLet each task have payoff $u_i$ and required time $w_i(p)$, with decision $q_i\\in\\{0,1\\}$. Then\n$$\n\\max_{q\\in\\{0,1\\}^n}\\sum_i u_i q_i\\quad\\text{s.t.}\\quad \\sum_i w_i(p) q_i\\le 1.\n$$\n\nSpeedups change $w_i$ by $\\beta_i$, which can **turn tasks on** once a threshold is crossed. This is exactly the “Cadillac tasks” phenomenon: tasks that were too time-expensive become attractive after the LLM. The usual CES elasticity is not a good summary in this regime.\n\nOne useful empirical way to think about the “tasks you previously avoided” condition is to decompose time cost into a baseline component plus an “overhead” component (search, reading, drafting, refactoring). For example, write\n$$\nw_i=a_i+b_i,\n$$\nand suppose AI primarily reduces overhead, $b_i' = b_i/\\beta^{O}$ while $a_i$ is unchanged. Then\n$$\nw_i' = a_i + \\frac{b_i}{\\beta^{O}},\\qquad\n\\beta_i \\equiv \\frac{w_i}{w_i'} = \\frac{a_i+b_i}{a_i+b_i/\\beta^{O}},\n$$\nso $\\beta_i$ is increasing in the overhead share $b_i/(a_i+b_i)$. Tasks you previously avoided plausibly have high overhead shares, so they get larger effective speedups and are more likely to cross activation thresholds.\n\n### Setup-cost variant (bridging discrete and continuous)\n\nAdd a fixed setup time $\\phi_i$ and a continuous intensity $x_i$:\n$$\n\\max_{q,x}\\;u(x)\\quad\\text{s.t.}\\quad \\sum_i \\phi_i q_i + \\sum_i p_i x_i \\le 1,\\; x_i=0\\;\\text{if }q_i=0.\n$$\n\nIf $\\phi_i=0$, we recover the continuous model. If $\\phi_i>0$, large LLM speedups mostly expand the active set $\\{i:q_i=1\\}$, not the intensive shares.\n\n### Worked example (discrete, not continuous)\n\n**Example.** Suppose you can pick *one* task (unit-demand). Task A yields value $u_A=10$ and takes 1 hour. Task B yields $u_B=12$ and takes 2 hours. Without LLMs you choose A. Now an LLM speeds up task B so it takes 1 hour. You switch to B.\n\n- **Upper bound on time-equivalent gain:** 1 hour (if the extra value $u_B-u_A$ is “worth” a full hour).\n- **Lower bound:** 0 hours (if the extra value is just a small quality bump).\n\nSo the *observed* reallocation does not identify a precise time-savings without modeling discrete choice. This is why constant-elasticity summaries can be weak in the activation regime.\n\n### Newly activated tasks (\"Cadillac tasks\")\n\nCall a task *newly activated* if you would not do it at baseline prices but you do once its time cost drops. (In earlier conversations I called these “Cadillac tasks.”)\n\n- literature reviews you previously would not attempt,\n- custom data visualizations,\n- long-form proofreading or refactoring.\n\nIn a unit-demand or setup-cost model, these tasks show up as **newly activated $q_i=1$ choices**, not as marginal increases in $x_i$. That is why “AI share of time” can jump even if your underlying preferences are stable: the feasible set changed.\n\n## Applications\n\n### Application 1: from query-level time savings to an aggregate lift (Anthropic)\n\n@anthropic2025estimatingproductivitygains estimate time savings from Claude conversations by comparing time required with vs without AI for a sample of tasks drawn from usage logs.\n\nSuppose the headline inputs (illustrative, in the spirit of the writeup) are:\n\n- Claude is used for $s_0=10\\%$ of baseline work (measured pre-AI / from logs),\n- conditional time reduction is 80%, i.e. speedup $\\beta=5$ when AI is used.\n\nThen even in the *continuous two-good* approximation, the implied aggregate lift depends sharply on substitution:\n\n- complements/Amdahl bound: $+8.7\\%$,\n- Cobb-Douglas: $+17.4\\%$,\n- CES with $\\varepsilon=2$: $+40\\%$.\n\nThe key empirical question is therefore not only “how big is $\\beta$ when AI is used?” but also “how does the *time share* respond as the relative time price changes?” (In CES that’s summarized by $\\varepsilon$; outside CES, you need the whole demand curve.)\n\nTwo concrete complications show up immediately in a log-based setting:\n\n1. **Endogenous task mix (extensive margin).** If AI reduces time costs more for tasks you previously avoided (high search/reading/writing overhead), then the pre-AI share $s_0$ understates the mass of tasks that become attractive post-AI.\n2. **Quality-adjusted output.** Some measured “time savings” are quality improvements (or vice versa). If your output index treats quality as part of output, you need a quality measure to map time changes into $u(x)$.\n\n### Application 2: interpreting “uplift” RCTs (METR / open-source dev)\n\n@becker2025uplift is an RCT where tasks are assigned to allow vs disallow AI tools, and the outcome is completion time (with additional self-reports and robustness checks). The striking headline in that paper is that AI access *increases* completion time on average in their setting.\n\nFrom the demand-theory perspective:\n\n- A simple back-of-envelope translation: if allowing AI increases completion time by 19%, then the implied “speedup” is $\\beta \\approx 1/1.19 \\approx 0.84$ (a slowdown). Holding output constant, that’s about a 16% productivity hit on that task population.\n- With a **fixed task set** (as in many RCTs), the main object you learn is a task-level time-price change $p_i\\to p_i'$ (a $\\beta_i$ distribution), holding task composition fixed.\n- Translating that into an **aggregate productivity index** still requires an output/quality index: are we holding output constant (pure time saved), holding time constant (more output), or letting both adjust? In practice you often want “quality-adjusted output per unit time.”\n- If, in future settings, AI access changes *which* tasks are attempted, then the design has to either (i) fix tasks (to isolate intensive effects) or (ii) explicitly allow selection and measure it (to capture the extensive margin).\n\n## Experimental design (estimating substitution and selection)\n\nDesigning studies that map $\\{\\beta_i\\}$ into an aggregate lift $v(p')/v(p)$ is mostly about (a) measuring substitution, and (b) separating intensive vs extensive margins.\n\n1. **Decide the estimand up front.** “Same tasks, faster” (hold $x$ fixed) is different from “best use of a fixed time budget” (maximize $u(x)$). The latter matches the substitution estimand in this note.\n2. **Run two complementary protocols.**\n   - **Fixed-task protocol (intensive margin):** randomize AI availability within a pre-specified task list; measure time and quality on each task.\n   - **Time-budget protocol (substitution/selection):** give participants a fixed time budget and a menu (or open-ended objective); randomize AI availability; measure the output index achieved under each condition.\n3. **Measure quality explicitly.** Use blinded human evaluation, unit tests, or objective metrics; pre-register how quality enters $u(x)$ (e.g. threshold vs continuous scoring).\n4. **Trace a demand curve by varying effective AI “price.”** Randomize not only “AI allowed,” but the *effective cost* of using AI (token budgets, latency, model quality tiers, usage quotas). This gives variation to estimate how shares move with relative time prices (the object behind $\\varepsilon$ or, more generally, Hicksian shares).\n5. **Handle learning and spillovers.** Use cross-over designs, washout periods, or between-subject randomization where appropriate; measure experience and allow for dynamic treatment effects.\n\n## Diagrams\n\n### Threshold diagram (discrete activation)\n\n::: {#ff360f31 .cell execution_count=3}\n\n::: {.cell-output .cell-output-display}\n![Discrete activation: speedups switch on tasks](2025-12-17-llm-time-saving-demand-theory-substitution.llm_files/figure-html/cell-3-output-1.png){width=471 height=336 fig-align='center'}\n:::\n:::\n\n\n## Checklist for the desiderata\n\n- **Bibliography validity:** citations are included and checked against `ai.bib`. See the validation checklist at the top.\n- **Citation faithfulness:** the optional LLM-assisted check flags suspicious claim-to-citation mismatches (also in the checklist).\n- **Proofs:** proofs are structured, collapsed by default, and include numbered steps plus a QED marker.\n- **Cheat sheet:** includes a two-good benchmark table and an estimation flowchart.\n- **Legible diagrams:** Mermaid estimation flowchart + a discrete activation figure.\n- **Applications:** explicit discussion of Anthropic time-savings and METR uplift-style RCTs.\n- **Experimental design:** concrete protocol suggestions for estimating substitution and selection.\n\n## Related literature (more explicit)\n\nThis note touches several literatures that are often cited separately. Here is a reading map that matches the objects used above (price indices, time allocation, discrete activation, and empirical measurement).\n\n### Index numbers and exact welfare from price changes (continuous case)\n\n- **\"True\" cost-of-living index and the expenditure function.** Konus defines the cost-of-living index as the ratio of minimum expenditures needed to reach a fixed utility level at two price vectors. This is exactly the object $P(p)=e(p,1)$ (with time prices instead of money prices) that makes $v(p)=1/P(p)$ under homotheticity. @konus1939trueindex\n- **Exact/superlative indices and share-weighted approximations.** Diewert formalizes when standard indices (including Tornqvist/Divisia-type log-share formulas) are \"exact\" for flexible demand/production forms, and motivates using share-weighted log changes as a local approximation. @diewert1976exact\n- **Productivity measurement via index numbers.** Caves-Christensen-Diewert is a central reference for exact index-number measurement of input, output, and productivity and for the duality-based framing used in the continuous model here. @caves1982indexnumbers\n- **Exact welfare for large changes.** Willig and Hausman connect equivalent/compensating variation and consumer surplus to integrals of compensated demand, clarifying why large AI shocks call for \"area under Hicksian demand\" rather than constant-elasticity shortcuts. @willig1976consumerssurplus; @hausman1981exact\n- **Estimating demand systems.** Deaton-Muellbauer's AIDS is a workhorse integrable demand system used to estimate (Marshallian/Hicksian) demand and welfare effects of price changes in practice. @deaton1980aids\n\n### Time allocation, information costs, and \"overhead\" time\n\n- **Time as a scarce resource.** Becker's allocation-of-time framework treats time as a fundamental constraint; it is the natural ancestor of interpreting task durations as \"prices\" in a time budget. @becker1965allocation\n- **Time requirements as part of preferences/technology.** DeSerpa emphasizes that goods require time to consume/produce and develops shadow-price implications; this is close to the time-price interpretation here. @deserpa1971time\n- **Search and information as economic objects.** Stigler's economics-of-information treats costly search as central; empirically, many LLM speedups look like reductions in search/reading/writing overhead rather than reductions in \"core\" task time. @stigler1961information\n- **Goods as characteristics.** Lancaster's characteristics model is a useful alternative interpretation: tasks can be seen as bundles of characteristics (information, drafting, formatting, reasoning), and AI shifts the implicit \"prices\" of those characteristics. @lancaster1966consumer\n\n### Discrete activation, selection, and welfare with lumpy choices\n\n- **Welfare with discrete choice.** Small-Rosen is a classic reference on how to do welfare analysis when choices are discrete; it is the natural toolkit for unit-demand tasks and activation thresholds. @smallrosen1981welfare\n- **Estimation tools for discrete choice.** Train is the standard applied reference for estimating random-utility/discrete-choice models (including simulation methods) when you want to quantify substitution and welfare. @train2003discretechoice\n\n### Task-based technological change and aggregation\n\n- **Tasks as the primitive.** Autor-Levy-Murnane (and Acemoglu-Autor) popularize task-based views of technology's effects, which is conceptually aligned with treating tasks as \"goods\" with relative prices in time. @autor2003skill; @acemoglu2011handbook\n- **Small-shock aggregation (Hulten logic).** Hulten's share-weighted result is the backbone of why $\\\\Delta\\\\ln v\\\\approx \\\\sum_i s_i\\\\,\\\\Delta\\\\ln\\\\beta_i$ can be reasonable for small or broad shocks. @hulten1978growth\n- **When share-weighted formulas fail.** Baqaee-Farhi is a canonical reference on nonlinearities and large shocks (\"beyond Hulten\"); it connects directly to why large AI shocks motivate demand-curve integration and careful treatment of selection. @baqaee2019macro\n- **Micro-to-macro technology measurement.** Oberfield-Raval is a useful reference on linking micro evidence on technology to macro implications. @oberfield2021micro\n- **CES as a workhorse.** Dixit-Stiglitz is the canonical CES/variety reference that sits behind many closed-form substitution formulas (including the two-good CES benchmark used above). @dixit1977monopolistic\n\n### Empirical AI productivity and usage measurement\n\n- **Task-level time savings from logs.** Anthropic's Claude-log approach illustrates the appeal (and limits) of measuring conditional time savings and baseline shares in observational usage data. @anthropic2025estimatingproductivitygains\n- **RCT \"uplift\" estimates.** Becker et al. (METR) is an example of randomized access to AI tools with completion time outcomes, highlighting the importance of quality measurement and task selection. @becker2025uplift\n- **Other early gen-AI productivity evidence.** Noy-Zhang and Brynjolfsson-Li-Raymond are widely cited early studies of gen-AI impacts (often with quality or heterogeneity considerations). @noy2023generative; @brynjolfsson2023generative\n- **What people do with chatbots.** Chatterji et al. document usage patterns and task composition for ChatGPT, which matters for mapping micro speedups to aggregate lifts when the task mix is endogenous. @chatterji2025chatgpt\n- **Valuing time/information services.** Varian and Collis-Brynjolfsson are representative of the broader literature on valuing digital services and time savings, complementary to the demand-theory framing here. @varian2011economic; @collis2025welfare\n\n",
    "supporting": [
      "2025-12-17-llm-time-saving-demand-theory-substitution.llm_files"
    ],
    "filters": [],
    "includes": {}
  }
}