{
  "hash": "d5b6ed6b17a4b1e142f11a81a990f309",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: A Model of ChatGPT\nauthor:\n  - name: Tom Cunningham\ncitation: true\ndate: 2025-06-14\ndate-modified: last-modified\nfig-align: center\nfig-height: 1\nbibliography: ai.bib\nreference-location: margin\nengine: knitr\neditor:\n  render-on-save: true\nformat:\n   pdf:\n      include-in-header:\n         - text: |\n            \\usepackage[all]{xy}\n            \\newcommand{\\bm}[1]{\\boldsymbol{#1}}\n            \\newcommand{\\ut}[2]{\\underbrace{#1}_{\\text{#2}}}\n            \\newcommand{\\utt}[3]{\\underbrace{#1}_{\\substack{\\text{#2}\\\\\\text{#3}}}}\n            \\newcommand{\\bmatrix}[1]{\\begin{bmatrix}#1\\end{bmatrix}}\n            \\usepackage{enumitem}\n            \\usepackage{sectsty}\n            \\sectionfont{\\sectionrule{0pt}{0pt}{-4pt}{1pt}}\n            \\subsectionfont{\\sectionrule{0pt}{0pt}{-4pt}{.1pt}}\n\n   html:\n      toc: true\n      toc-depth: 2\n      toc-location: left\n      html-math-method:\n         method: mathjax\n         url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js\"\n         #     ^ this forces SVG instead of CHTML, otherwise xypic renders weird\n      include-in-header:\n         - text: |\n            <script>window.MathJax = {\n                     loader: { load: ['[custom]/xypic.js'],\n                                 paths: {custom: 'https://cdn.jsdelivr.net/gh/sonoisa/XyJax-v3@3.0.1/build/'}},\n                  tex: {packages: {'[+]': ['xypic']},\n                     macros: {\n                        bm: [\"\\\\boldsymbol{#1}\", 1],\n                        bmatrix: [\"\\\\begin{bmatrix}#1\\\\end{bmatrix}\", 1],\n                        smallmatrix: [\"\\\\begin{smallmatrix}#1\\\\end{smallmatrix}\", 1],\n                        ut: [\"\\\\underbrace{#1}_{\\\\text{#2}}\", 2],\n                        utt: [\"\\\\underbrace{#1}_{\\\\substack{\\\\text{#2}\\\\\\\\\\\\text{#3}}}\", 3]\n                     }}};\n            </script>\n            <style>\n               h1 {  border-bottom: 4px solid black; }\n               h2 {  border-bottom: 1px solid gray; padding-bottom: 0px; color: black; }\n               dl { margin-bottom: 0px; }\n               dt strong { font-weight: bold; }\n               dd { margin-left: 20px; }\n               .cell-output-display p {padding: 0 0 0cm 0; margin: 0 0 0 0;}\n            </style>\n---\n\n\n\n\nThis note gives a simple model of human and AI ability to answer questions.\n: Each question $\\bm{q}$ is a high-dimensional vector of bits, with a true scalar answer $a$. Each agent tries to estimate the answer by interpolating among previous questions $(\\bm{q}^i,a^i)_{i=1,\\ldots,n}$ that they have encountered.\n\nThis model gives a series of general implications:\n\n: 1. **The quality of answer to a new question depends on the distance from the training set.** Suppose a human and a computer both have to answer some question $q$, then the agent who performs better will be the agent where $q$ has a smaller projection onto their training set $Q$.\n\n: 2. **The average quality of answers will decrease linearly with $n$.**\n\n: 2. **The value of getting advice from another agent depends on distance between the two training sets.** (TBC)\n\n: 3. **An agent whose training set is the union of two other agents' training sets will outperform them.** This is for a couple of reasons:\n   1. Averaging many noisy labels lets the computer beat a single noisy human label.\n   2. Averaging many labels lets the computer *interpolate* in a superior way to humans.\n\n\nInterpreted as a model of ChatGPT, this gives a set of predictions.\n\n: We can interpret this model as one of ChatGPT use. The human has some history of questions ($Q^H$), and they have observed answer for each.  We will describe the computer's training set ($Q^C$) as \"questions and answers on the internet.\" Of course ChatGPT's training procedure is more complicated, we discuss it in more detail below. A human will invoke ChatGPT if and only if the expected improvement in the answer exceeds some cost. \n\n: 1. ChatGPT will be used for questions which are closer (in question-space) to questions on the internet, than questions in the user's own experience.\n: 2. ChatGPT will b\n\n**Things to add:**\n\n1. **Dimensionality.**\n\n\n#           Model\n\nThe world is characterized by a set of $p$ weights, $\\bm{w}$. \n\nAll agents have Gaussian priors over those weights:\n   $$\\bm{w}\\sim N(\\bm{0},\\sigma^2I_p)$$\n\nEach agent observes a matrix $Q$ of questions, each question has $p$ binary parameters:\n\n   $$\\begin{aligned}\n      Q      &\\in \\{-1,1\\}^{n\\times p}\n         && \\text{($n$ questions, each has $p$ binary parameters)}\\\\\n      \\bm{w} &\\sim N(0,\\Sigma) \n         && (p\\times 1\\text{ vector of true parameters of the world)}\\\\\n      \\ut{\\bm{a}}{$n\\times1$}   &= \\ut{Q}{$n\\times p$}\\ut{\\bm{w}}{$p\\times1$}\n         && \\text{(answers provided by the world)}\\\\\n   \\end{aligned}\n   $$\n\nWe can also write this out in matrix form:\n\n   $$\\begin{aligned}\n      Q &= \\bmatrix{q_1^1 & \\ldots & q^1_p \\\\ & \\ddots & \\\\ q^n_1 & \\ldots & q^n_p}\n         && \\text{(matrix of $n$ questions, each with $p$ parameters)} \\\\\n      \\bm{w}'  &= \\bmatrix{w_1 \\ldots w_p}\n         && \\text{(vector of $p$ unobserved weights)}\\\\\n      \\bm{a}    &= \\bmatrix{a^1 \\\\ \\vdots \\\\ a^n} \n         = \\bmatrix{q_1^1 w_1 + \\ldots q_p^1w_p \\\\ \\vdots \\\\ q_1^n w_1 + \\ldots q_p^n w_p}\n         && \\text{(vector of $n$ observed answers)}\\\\\n   \\end{aligned}\n   $$\n\n**Training Data.** Each agent $i$ has access to a set of observations, or \"training data,\" which consists of a set of questions $Q_i$ and their corresponding answers $\\bm{a}_i$.\n   \\begin{aligned}\n      \\mathcal{D}_i = \\{ (Q_i, \\bm{a}_i) \\}\n   \\end{aligned}\n\n#           Propositions\n\nProposition 1 (Bayesian posterior).\n: The agent's posterior mean and variance will be:\n   $$\\begin{aligned}\n      \\hat{\\bm w}&= \\Sigma Q^{\\top}(Q\\Sigma Q^{\\top})^{-1}\\bm a\\\\\n      \\Sigma_{\\mid a} &=\\Sigma-\\Sigma Q^{\\top}(Q\\Sigma Q^{\\top})^{-1}Q\\Sigma\n   \\end{aligned}\n   $$\n\n<details><summary>Proof (via Gaussian conditioning)</summary>\nThe derivation follows from the standard formula for conditional Gaussian distributions. We begin by defining the joint distribution of the weights $\\bm{w}$ and the answers $\\bm{a}$.\n\nThe weights and answers are jointly Gaussian:\n\\[\n   \\begin{pmatrix} \\bm{w} \\\\ \\bm{a} \\end{pmatrix} \\sim N\\left(\n      \\begin{pmatrix} \\bm{0} \\\\ \\bm{0} \\end{pmatrix},\n      \\begin{pmatrix} \n         \\Sigma & \\Sigma Q' \\\\\n         Q\\Sigma & Q\\Sigma Q'\n      \\end{pmatrix}\n   \\right)\n\\]\nwhere the covariance terms are derived as follows:\n- $Cov(\\bm{w}, \\bm{w}) = \\Sigma$ (prior covariance)\n- $Cov(\\bm{a}, \\bm{a}) = Cov(Q\\bm{w}, Q\\bm{w}) = Q Cov(\\bm{w}, \\bm{w}) Q' = Q\\Sigma Q'$\n- $Cov(\\bm{w}, \\bm{a}) = Cov(\\bm{w}, Q\\bm{w}) = Cov(\\bm{w}, \\bm{w})Q' = \\Sigma Q'$\n\nThe conditional mean $E[\\bm{w}|\\bm{a}]$ is given by the formula:\n\\[\nE[\\bm{w}|\\bm{a}] = E[\\bm{w}] + Cov(\\bm{w},\\bm{a})Var(\\bm{a})^{-1}(\\bm{a} - E[\\bm{a}])\n\\]\nSubstituting the values from our model ($E[\\bm{w}] = \\bm{0}$, $E[\\bm{a}] = \\bm{0}$):\n\\[\n\\hat{\\bm{w}} = \\bm{0} + (\\Sigma Q')(Q\\Sigma Q')^{-1}(\\bm{a} - \\bm{0}) = \\Sigma Q'(Q\\Sigma Q')^{-1}\\bm{a}\n\\]\nThis gives us the posterior mean of the weights. The posterior covariance is given by:\n\\[\nVar(\\bm{w}|\\bm{a}) = Var(\\bm{w}) - Cov(\\bm{w},\\bm{a})Var(\\bm{a})^{-1}Cov(\\bm{a},\\bm{w}) = \\Sigma - \\Sigma Q'(Q\\Sigma Q')^{-1}Q\\Sigma\n\\]\n∎\n</details>\n\nProposition 2 (Expected error).\n: The expected squared error for a new question $\\bm q$ is:\n  $$ \\mathbb{E}[(\\bm q'(\\bm w - \\hat{\\bm w}))^2] = \\bm q' \\Sigma_{\\mid a} \\bm q $$\n  For an isotropic prior where $\\Sigma = \\sigma^2 I$, the error is proportional to the squared distance of $\\bm q$ from the subspace spanned by the previously seen questions $Q$:\n  $$ \\mathbb{E}[(\\bm q'(\\bm w - \\hat{\\bm w}))^2] = \\sigma^2 \\|(I-P_Q)\\bm q\\|^2 $$\n  where $P_Q$ is the projection matrix onto the row-span of $Q$.\n\n<details><summary>Proof</summary>\nThe first part is proven in Proposition 1's derivation of the posterior covariance. For the isotropic case, we substitute $\\Sigma = \\sigma^2 I$ into the expression for $\\Sigma_{\\mid a}$:\n$$\\begin{aligned}\n   \\Sigma_{\\mid a} &= \\sigma^2 I - (\\sigma^2 I) Q'(Q(\\sigma^2 I)Q')^{-1}Q(\\sigma^2 I) \\\\\n                   &= \\sigma^2 I - \\sigma^4 Q'( \\sigma^2 Q Q')^{-1} Q \\\\\n                   &= \\sigma^2 I - \\sigma^2 Q'(Q Q')^{-1} Q \\\\\n                   &= \\sigma^2 (I - P_Q)\n\\end{aligned}\n$$\nwhere $P_Q = Q'(QQ')^{-1}Q$ is the projection matrix onto the row-span of $Q$. The error is then:\n$$\\begin{aligned}\n   \\mathbb{E}[(\\bm q'(\\bm w - \\hat{\\bm w}))^2] &= \\bm q' (\\sigma^2 (I - P_Q)) \\bm q \\\\\n                                             &= \\sigma^2 \\bm q'(I - P_Q) \\bm q \\\\\n                                             &= \\sigma^2 \\|(I - P_Q)\\bm q\\|^2\n\\end{aligned}\n$$\nThe vector $(I - P_Q)\\bm q$ is the component of $\\bm q$ orthogonal to the row-span of $Q$. Its squared norm is the squared projection distance. ∎\n</details>\n\nProposition 3 (Zero Error Condition).\n: With an isotropic prior, the expected error for a new question $\\bm q$ is zero if and only if $\\bm q$ is in the row-span of the training questions $Q$.\n  $$ \\mathbb{E}[(\\bm q'(\\bm w - \\hat{\\bm w}))^2] = 0 \\iff \\bm q \\in \\operatorname{rowspan}(Q) $$\n\n<details><summary>Proof</summary>\nFrom Proposition 2, the expected error with an isotropic prior is $\\sigma^2 \\|(I-P_Q)\\bm q\\|^2$. Since $\\sigma^2 > 0$, the error is zero if and only if $\\|(I-P_Q)\\bm q\\|^2 = 0$. This is true if and only if $(I-P_Q)\\bm q = \\bm 0$, which means $\\bm q = P_Q \\bm q$. This condition holds if and only if $\\bm q$ is in the subspace onto which $P_Q$ projects, which is the row-span of $Q$. ∎\n</details>\n\nProposition 4 (Noise-averaging).\n: If a human supplies one noisy label per question, $\\bm{a}_{\\mathrm H} = Q\\bm{w} + \\bm{\\epsilon}_{\\mathrm H}$ with $\\bm{\\epsilon}_{\\mathrm H} \\sim N(0, s_{\\mathrm H}^{2}I)$, and the computer receives $m\\ge2$ i.i.d. copies of that label, its training data has answers $\\bar{\\bm{a}} = \\frac{1}{m}\\sum_{j=1}^m \\bm{a}_{\\mathrm H,j}$, giving an effective noise variance of $s_{\\mathrm C}^{2}=s_{\\mathrm H}^{2}/m$. For every new question $q$, the computer's expected error is lower:\n\\[\n   \\mathbb E[(q(\\bm w-\\widehat{\\bm w}_{\\mathrm C}))^{2}]<\\mathbb E[(q(\\bm w-\\widehat{\\bm w}_{\\mathrm H}))^{2}].\n\\]\n\n<details><summary>Proof</summary>\nThe expected squared error for an agent $i$ on a new question $q$ is $q'\\Sigma_{\\mid i}q$, where $\\Sigma_{\\mid i}$ is the posterior covariance. With noisy observations, the posterior covariance is $\\Sigma - \\Sigma Q'(Q\\Sigma Q' + s_i^2I_n)^{-1}Q\\Sigma$. Since $s_C^2 < s_H^2$, the term $(Q\\Sigma Q' + s_C^2I_n)^{-1}$ is smaller than $(Q\\Sigma Q' + s_H^2I_n)^{-1}$ in the positive definite sense, which makes the computer's posterior covariance smaller, and thus its expected error is lower for any $q$. ∎\n</details>\n\nProposition 5 (Information loss from partial notes).\n: Let a human observe questions and answers $(Q_{\\mathrm H}, \\bm{a}_{\\mathrm H})$, but only record a subset of questions $Q_{\\mathrm C} \\subset Q_{\\mathrm H}$ for the computer's training data. For any new question $q$ that is not in the span of the computer's questions, $q\\notin\\operatorname{rowspan}(Q_{\\mathrm C})$, the computer's mean-squared error is at least as large as the human's:\n\\[\n   \\mathrm{MSE}_{\\mathrm C}(q)\\;\\ge\\;\\mathrm{MSE}_{\\mathrm H}(q).\n\\]\n\n<details><summary>Proof</summary>\nThe mean squared error is given by $q'\\Sigma_{\\mid \\mathcal{D}}q$, where $\\Sigma_{\\mid \\mathcal{D}}$ is the posterior covariance. Since the human's training set $Q_H$ contains more information than the computer's $Q_C$, the human's posterior covariance will be smaller (in the positive definite sense) than the computer's. The human's posterior is conditioned on a larger row-space $\\operatorname{rowspan}(Q_H) \\supset \\operatorname{rowspan}(Q_C)$, hence its predictive variance is smaller or equal along any direction. For $q \\notin \\operatorname{rowspan}(Q_C)$, the inequality is generally strict. ∎\n</details>\n\nProposition 6 (Super-human aggregation).\n: Let two humans $A,B$ observe disjoint question sets $(Q_A, \\bm{a}_A)$ and $(Q_B, \\bm{a}_B)$ and write *all* of their questions. A computer trained on the union of the datasets $Q_{\\mathrm C}=\\begin{psmallmatrix}Q_A\\\\Q_B\\end{psmallmatrix}$ will have a mean squared error no larger than either human for any question $q$:\n\\[\n   \\mathrm{MSE}_{\\mathrm C}(q)\\;\\le\\;\\min\\bigl\\{\\mathrm{MSE}_{A}(q),\\,\\mathrm{MSE}_{B}(q)\\bigr\\}\\quad\\forall q.\n\\]\n\n<details><summary>Proof</summary>\nThe computer's question set $Q_C$ contains both $Q_A$ and $Q_B$, so $\\operatorname{rowspan}(Q_C)$ contains both $\\operatorname{rowspan}(Q_A)$ and $\\operatorname{rowspan}(Q_B)$. As in Proposition 5, a larger question set leads to a smaller or equal posterior covariance. Therefore, the computer's MSE, which is determined by its posterior covariance, will be less than or equal to that of either human for any question $q$. ∎\n</details>\n\nProposition 7 (Novel-question competence).\n: Under the set-up of Proposition 6, there exist questions lying outside each human's span but inside $\\operatorname{rowspan}(Q_{\\mathrm C})$ on which the computer's predictive variance is *zero* while both humans are uncertain.\n\n<details><summary>Proof</summary>\nThe expected error on a new question $q$ is $q'\\Sigma_{\\mid \\mathcal{D}}q$. If $q$ is in the row-span of the training questions $Q$, and the prior is isotropic, $\\Sigma = \\sigma^2I$, then the posterior covariance on that question is zero. Specifically, the error is $\\sigma^2||(I-P_Q)q||^2$, where $P_Q$ is the projection matrix onto the row-span of $Q$. If $q$ is in the span, $P_Qq=q$, so the error is zero. Since there can be questions in $\\operatorname{rowspan}(Q_C)$ that are not in $\\operatorname{rowspan}(Q_A)$ or $\\operatorname{rowspan}(Q_B)$, the computer can answer these questions with zero error, while the humans cannot. ∎\n</details>\n\nProposition 8 (De-gradation from speculative writing).\n: If each human now labels *every* possible question using their own posterior mean, $\\hat{\\bm{a}}_A = Q\\hat{\\bm{w}}_A$ and $\\hat{\\bm{a}}_B = Q\\hat{\\bm{w}}_B$, and the computer is trained on the average of these labels, $\\bm{a}_C = \\frac{1}{2}(\\hat{\\bm{a}}_A + \\hat{\\bm{a}}_B)$, its error on the original questions $Q_A$ and $Q_B$ will generally be higher than the individual human errors.\n\n<details><summary>Proof</summary>\nThe computer's weights, $\\hat{\\bm{w}}_C$, will be an estimate based on the average of the two human posteriors. The computer is now trying to predict the average of two biased estimates of the true weights. Averaging two biased estimators generally increases the mean squared error, unless their biases are perfectly aligned. Since the humans' weights are estimated from different data, their biases will point in different directions, and the computer's error will be a compromise that is worse than either specialist on their own domain of expertise. ∎\n</details>\n\nProposition 9: Perfect Imitation.\n: If one human records all their observations then the computer will perfectly imitate them. Suppose that there is one human and they write down all of their observations, $\\hat{Q}=Q$. Because the computer and human have the same priors, and observe the same data, then they will therefore end up with the same estimated weights ($\\hat{\\bm{w}}=\\bar{\\bm{w}}$), and so the computer will answer every question exactly as the human does, though neither knows the truth ($\\bar{\\bm{w}}\\neq\\bm{w}$).\n\n<details>\n<summary>Proof</summary>\n\nLet the human's training data be $(Q, \\bm{a})$. The human's posterior estimate of the weights is given by Proposition 1:\n\\[\n\\hat{\\bm{w}}_{\\mathrm H} = \\Sigma Q^{\\top}(Q\\Sigma Q^{\\top})^{-1}\\bm a.\n\\]\nThe computer is trained on the same data, so its training set is also $(Q, \\bm{a})$. Its posterior estimate is therefore:\n\\[\n\\hat{\\bm{w}}_{\\mathrm C} = \\Sigma Q^{\\top}(Q\\Sigma Q^{\\top})^{-1}\\bm a.\n\\]\nThe agents' estimates are identical, $\\hat{\\bm{w}}_{\\mathrm H} = \\hat{\\bm{w}}_{\\mathrm C}$. For any new question $\\tilde{q}$, the human's predicted answer is $\\tilde{a}_{\\mathrm H} = \\tilde{q}'\\hat{\\bm{w}}_{\\mathrm H}$ and the computer's is $\\tilde{a}_{\\mathrm C} = \\tilde{q}'\\hat{\\bm{w}}_{\\mathrm C}$. Since their weight estimates are identical, their answers will be too. ∎\n</details>\n\nProposition 10: Tacit Knowledge Advantage.\n: If humans have tacit knowledge, then computers can outperform in choosing a question to maximize the answer. We can model tacit knowledge with two separate sets of human weights:\n   \n   $$\\begin{aligned}\n      \\hat{\\bm{w}}^T   &= \\text{tacit knowledge}\\\\\n      \\hat{\\bm{w}}^E &= \\text{explicit knowledge}\\\\\n   \\end{aligned}\n   $$\n   \nWhen the human encounters a new question $\\tilde{\\bm{q}}$ they will use their tacit knowledge to form an estimate of the answer, $\\hat{a}=\\tilde{\\bm{q}}'\\hat{\\bm{w}}^T$. But they have limited ability to introspect about that capacity, and so when asked how they make their judgments they can report only $\\hat{\\bm{w}}^E$. For simplicity assume tacit knowledge is perfect ($\\hat{\\bm{w}}^T=\\bm{w}$), and explicit knowledge is imperfect ($\\hat{\\bm{w}}^E\\neq \\bm{w}$).\n\n<details>\n<summary>Proof</summary>\n\nThe task is to find a question $\\bm{q}$ that maximizes the true answer $a = \\bm{q}'\\bm{w}$. The optimization problem is $\\bm{q}^* = \\arg\\max_{\\bm{q}\\in\\{-1,1\\}^p} \\bm{q}'\\bm{w}$.\n\nThe human can only use their explicit knowledge, $\\hat{\\bm{w}}^E$, to search for the best question. Thus, the human solves:\n\\[\n\\bm{q}_{\\mathrm H} = \\arg\\max_{\\bm{q}\\in\\{-1,1\\}^p} \\bm{q}'\\hat{\\bm{w}}^E.\n\\]\nThe value they achieve is $\\bm{q}_{\\mathrm H}'\\bm{w}$.\n\nThe computer is trained on the human's answers, which are generated by their tacit knowledge, $\\hat{\\bm{w}}^T$. Assuming the computer has enough data to learn this perfectly, its estimate of the weights is $\\bar{\\bm{w}} = \\hat{\\bm{w}}^T$. The proposition states that tacit knowledge is perfect, so $\\hat{\\bm{w}}^T = \\bm{w}$, which means the computer learns the true weights, $\\bar{\\bm{w}} = \\bm{w}$.\n\nThe computer can then solve the true optimization problem:\n\\[\n\\bm{q}_{\\mathrm C} = \\arg\\max_{\\bm{q}\\in\\{-1,1\\}^p} \\bm{q}'\\bar{\\bm{w}} = \\arg\\max_{\\bm{q}\\in\\{-1,1\\}^p} \\bm{q}'\\bm{w} = \\bm{q}^*.\n\\]\nThe value the computer achieves is $\\bm{q}_{\\mathrm C}'\\bm{w}$.\n\nBy definition of the maximum, $\\bm{q}_{\\mathrm C}'\\bm{w} \\ge \\bm{q}_{\\mathrm H}'\\bm{w}$. The inequality is strict if the human's explicit knowledge is imperfect ($\\hat{\\bm{w}}^E \\neq \\bm{w}$), which would cause them to choose a suboptimal $\\bm{q}_{\\mathrm H}$. ∎\n</details>\n\nProposition 11: Projection Distance.\n: Suppose two people, $A$ and $B$, have observed different training sets $Q^A$ and $Q^B$, then we can characterize their expected error for the answer of a new question $q$ (where $q\\not\\in Q^A$, $q\\not\\in Q^B$). The expected squared error is exactly the squared length of the component of $q$ that is **orthogonal** to the set of questions that person $i$ has already encountered. It is zero if $q$ is contained in the span of their past questions and grows with the distance of $q$ from that span.\n\n<details>\n<summary>Proof</summary>\n\nLet the true weights be $\\bm{w}\\sim N(0,\\Sigma)$ and let the two individuals observe $\\bm a^i = Q^i\\bm w\\;(i\\in\\{A,B\\})$. Their posterior mean is  \n$$\\hat{\\bm w}^{\\,i}= \\Sigma(Q^i)'\\!\\bigl(Q^i\\Sigma(Q^i)'\\bigr)^{-1}\\bm a^i$$\nand their posterior covariance is  \n$$\\Sigma_{\\mid i}= \\Sigma-\\Sigma(Q^i)'\\!\\bigl(Q^i\\Sigma(Q^i)'\\bigr)^{-1}Q^i\\Sigma.$$\n\nBoth people answer the fresh question by $\\hat a_i = q'\\hat{\\bm w}^{\\,i}$, while the truth is $a=q'\\bm w$. Conditioning on the training set we therefore have  \n$$\n   \\mathbb{E}\\!\\left[(a-\\hat a_i)^2\\;\\middle|\\;Q^i\\right]\n      =\\mathbb{E}\\!\\left[(q'(\\bm w-\\hat{\\bm w}^{\\,i}))^2\\;\\middle|\\;Q^i\\right]\n      = q'\\Sigma_{\\mid i}q.\n$$\n\nIf we specialise to $\\Sigma=\\sigma^{2}I_p$ then $\\Sigma_{\\mid i}= \\sigma^{2}\\!\\bigl(I-P^{i}\\bigr)$ with the orthogonal projector $P^{i}= (Q^i)'\\bigl(Q^i(Q^i)'\\bigr)^{-1}Q^i$ onto the row–span of $Q^i$. Hence  \n$$\n   \\mathrm{err}_i(q)\\equiv\\mathbb{E}\\!\\left[(a-\\hat a_i)^2\\;\\middle|\\;Q^i\\right]\n   =\\sigma^{2}\\bigl\\|(I-P^{i})\\,q\\bigr\\|^{2}.\n$$\n\nPerson $A$ is expected to be more accurate than $B$ precisely when  \n$$\n    q'\\Sigma_{\\mid A}q \\;<\\; q'\\Sigma_{\\mid B}q\n    \\qquad\\bigl(\\text{equivalently } \n    \\|(I-P^{A})q\\| < \\|(I-P^{B})q\\|\\bigr).\n$$\n∎\n</details>\n\n\n#       Additional Observations\n\nThese are a few miscellaneous results additional results that helped me with intuition for the working of this model.\n\n**With one observation and two weights.** Suppose $n=1, p=2$, then we have:\n   $$\\begin{aligned}\n      Q  &= \\bmatrix{q_1 & q_2} \\\\\n      \\bm{a}'  &= \\bmatrix{a} \\\\\n      \\bm{w}'  &= \\bmatrix{w_1 & w_2 } \\\\\n      \\Sigma &= \\bmatrix{\\sigma_1^2 & \\rho \\\\ \\rho & \\sigma_2^2}\\\\\n      \\Sigma Q' &= \\bmatrix{ \\sigma_1^2q_1 + \\rho q_2 \\\\ \\rho q_1 + \\sigma_2^2 q_2 } \\\\\n      Q\\Sigma Q' &= \\bmatrix{ \\sigma_1^2q_1^2 + 2\\rho q_1q_2 + \\sigma_2^2 q_2^2} \\\\\n      \\hat{\\bm{w}}=\\Sigma Q'(Q\\Sigma Q')^{-1}\\bm{a}\n         &= \\bmatrix{ \\frac{\\sigma_1^2q_1 + \\rho q_2}{\\sigma_1^2q_1^2 + 2\\rho q_1q_2 + \\sigma_2^2 q_2^2} \\\\ \n                  \\frac{\\rho q_1 + \\sigma_2^2 q_2}{\\sigma_1^2q_1^2 + 2\\rho q_1q_2 + \\sigma_2^2 q_2^2}} a\n   \\end{aligned}\n   $$\n\nWe can normalize $q_1=q_2=1$, then we have:\n   $$\\hat{w}_1 = \\frac{\\sigma_1^2+\\rho}{\\sigma_1^2+2\\rho+\\sigma_2^2}a,$$\n   Here we are dividing up responsibility for the answer ($a$) into the contributions of each component, nice and simple.\n\n**With two observations and one weight.** Here we're *over-identified*. \n   $$\\begin{aligned}\n      Q  &= \\bmatrix{q^1 \\\\ q^2} \\\\\n      \\bm{a}  &= \\bmatrix{a^1 \\\\ a^2} \\\\\n      \\bm{w}  &= \\bmatrix{w } \\\\\n      \\Sigma &= \\bmatrix{\\sigma^2 }\\\\\n      \\Sigma Q' &= \\bmatrix{ \\sigma^2 q^1 & \\sigma^2 q^2 } \\\\\n      Q\\Sigma Q' &= \\bmatrix{ \\sigma^2 q^1q^1 & \\sigma^2q^1q^2 \\\\ \\sigma^2q^1q^2 & \\sigma^2q^2q^2}\n         && \\text{(this matrix doesn't have an inverse)}\n   \\end{aligned}\n   $$\n\n\n**With noise.** Suppose we only observe the answers with random noise, then we have\n   $$\\begin{aligned}\n      \\ut{\\bm{a}}{$n\\times1$}   &= \\ut{Q}{$n\\times p$}\\ut{\\bm{w}}{$p\\times1$}\n         + \\ut{\\bm{e}}{$n\\times 1$} \\\\\n      \\bm{e} &\\sim N(\\bm{0},s^2I_n) && \\text{(i.i.d. noise with variance $s^2$)}\\\\\n      Cov(\\bm{w},\\bm{a})   &= \\Sigma Q' \\\\\n      Var(\\bm{a}) &= Q\\Sigma Q' + s^2I_n \\\\\n      E[\\bm{w}|Q,\\bm{a}]   &= \\Sigma Q'(Q\\Sigma Q' + s^2I_n)^{-1}\\bm{a}\n   \\end{aligned}\n   $$\n\n**Compare to Bayesian linear regression.**  We can compare this result to Bayesian linear regression (e.g. [Wikipedia](https://en.wikipedia.org/wiki/Bayesian_linear_regression)):\n   $$\\begin{aligned}\n      \\bar{\\beta}  &= \\Sigma Q'(Q\\Sigma Q' + s^2I_n)^{-1}\\bm{a}\n         && \\text{(our result)} \\\\\n      \\tilde{\\beta} &= (Q'Q+s^{2}\\Sigma^{-1})^{-1}Q'\\bm{a}\n         && \\text{(standard Bayesian linear regression)}\\\\\n   \\end{aligned}\n   $$\n\n   I *believe* that these can be shown to be equivalent by the [matrix inversion lemma](https://en.wikipedia.org/wiki/Woodbury_matrix_identity), though I haven't confirmed this. There is a concise proof in an online note from Utah State University.\n\n**Extension: quadratic forms.** Instead of answers being linear in question-features ($a=q'w$) we could suppose they're quadratic, $a=q'Wq$, with $W$ a matrix having dimension $p^2$. I am not sure whether we could still get an analytic solution for posteriors. One way to visualise $W$ is that each bit in $q$ adds an \"L\" (a row and a column) to the matrix, and $a$ is the sum of the cells where both the row and the column are activated.\n\n**Extension: binary answers.** In some cases it is natural to think of the answer, $a$, as binary instead of continuous. We might be able to reinterpret the model with $a$ representing the log-odds ratio of a binary outcome. Alternatively there might be a way of having a beta-binomial conjugate prior over the probability of $a$.\n\n#           Related Literature\n\n##          Agrawal et al. (2018) \"Exploring the Impact of Artificial Intelligence: Prediction versus Judgment\"\n\nhttps://www.nber.org/system/files/working_papers/w24626/w24626.pdf\n\n##          Kleinberg et al. (2017) \"Human Decisions and Machine Predictions\"\n\n\n\n\n\n\n#           References",
    "supporting": [
      "2025-06-14-model-of-ai_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": null,
    "postProcess": false
  }
}