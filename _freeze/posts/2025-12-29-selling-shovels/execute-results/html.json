{
  "hash": "14e9df3b2cab10cb6f9131534d03422b",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Selling Shovels\nengine: knitr\ndraft: true\n---\n\nClaim: labs will switch from selling shovels to using the shovels themselves.\n: As the models get stronger labs will stop selling access to the models, and instead use the models themselves to create or co-create artefacts (algorithms, videos, scientific discoveries), which they can directly sell, or take a revenue-share from.\n\nExamples:\n:     - Create a movie\n      - Predict stock prices & trade on them\n      - Find new more efficient algorithms\n      - Optimize a widely-used technology\n      - Make a scientific discovery\n\nObservations on modeling this:\n:     1. _Probably we can ignore the inference costs._ -- seems like main implications are just from value side, & ignoring cost.\n      1. _Distinction between local vs global value._ -- (not quite rivalness or excludability, but correlation in value).\n      2. _Distinction between large problems and small problem._\n\nTheory on residual rights.\n: Suppose there are two factors in creating some good, it's efficient to assign ownership (residual rights) to the factor which has less-observable inputs (\"the party whose investment is most important and most difficult to specify in a contract\").\n\nWho could I get to coauthor?\n: Erik B; Andrey; Philip Trammel; Basil; _Andy Haupt_.\n\n      Hi Phil, Basil, Andy. Merry christmas! I've written up a one-page note on an issue I think is important, & I'm looking for a coauthor, in case I could tempt any of you. I'm personally persuaded this is going to be a big deal, & I feel I haven't seen public discussion of this issue, so keen to get this out somewhere. (& FWIW my expectation is partly informed by my industry experience.)\n\n##       Models\n\nSmall problems vs big problems (demand curve).\n:     - The old model can solve a lot of $1 problems, the new model can solve a few $1M problems.\n      - Can think of this as steepening of the demand curve, so it becomes optimal to raise the price. But by itself this doesn't have implications for rent-vs-own or buy-vs-build.\n\nGlobal value vs local value.\n:     - Suppose old models solve *local* problems, & new models solve *global* problems (nonrival).\n      - Suppose the total surplus generated in each case is the same.\n      - Village example: (1) you dig a well and sell the water; (2) you build a library and sell access.\n      - If you sell the inputs to build a nonrival good, then can't charge a premium for it.\n\nUnobservable inputs.\n:     - Suppose output depends on inputs from humans and from the AI, what's the efficient allocation of ownership of the outputs?\n      - Oliver Hart says \"ownership should be allocated to the party whose investment is most important and most difficult to specify in a contract.\"\n      - For existing models it's difficult to specify the human inputs in a contract.\n      - However as new models get better it's easier to specify them.\n\nHuman-level vs superhuman (M&M model).\n:     - Each human has a knowledge set, allows them to produce certain goods.\n      - Assume symmetric demand across goods (and σ>1), pins down sorting of people to goods.\n      - Old AI gives people ability to make existing goods.\n      - New AI gives people ability to make new goods.\n\nVertical integration.\n:  \n\n\n(note: Dixit-Stiglitz love-of-variety CES has $\\sigma>1$, i.e. gross substitutes, otherwise you get pathological cases)\n\n##       ownership / residual rights\n\nAlchian and Demsetz (1972):\n\n: > \"“The essential role of the firm is to economize on the costs of measuring marginal productivities. ... the monitor must be the residual claimant.\"\n\nFama and Jensen (1983):\n\n: > \"Efficient organization assigns residual claims to those with the greatest incentives to monitor and control.\n\nHart (1995):\n\n: > **\"Ownership should be allocated to the party whose investment is most important and most difficult to specify in a contract.** ... The owner of an asset has the right to decide on its use in all contingencies not specified in the contract, and hence ownership confers bargaining power when investments are non-verifiable.\"\n\nExamples:\n\n: > \"If a fisherman rents a boat, then when unforeseen contingencies arise the owner of the boat has control. If instead the fisherman owns the boat, then he has control over its use in all circumstances not specified in the contract.\"\n\n: > \"When a worker’s actions are hard to specify, it may be optimal for the firm to hire the worker and own the output, rather than contract for a finished good.\"\n\n\n##       post on METR slack\n\nI feel that in 2026 we're going to get a blast of capabilities from a direction we're not expecting. Specifically using a lot of inference to optimize on a single task:\n\n- Create a movie\n- Predict stock prices\n- Optimize algorithms\n- Optimize technology\n- Solve scientific problems\n\nYou want to spend a ton of tokens going deep on a single problem, and you can have a semi-custom architecture for this. Labs wouldn't want to *sell* this ability to customers, it makes more economic sense to *use* the technology internally and then sell the product you've create with it.\n\nThis makes me a bit more anxious about AI R&D ability not being publicly observable, because AI R&D is a type of optimization problem, & it seems likely that labs will postpone releasing this technology publicly.\n\n##        shovels metaphor\n\nSuppose you have a lot of shovels:\n\n1. The valley is full of gold flakes everywhere, then you make money selling shovels to other people.\n2. The valley has one large gold nugget burried deep, then you use the shovels yourself to dig it out yourself.\n\noriginal quote, but not clear provenance:\n\n> \"When Everybody Is Digging for Gold, It’s Good To Be in the Pick and Shovel Business\"\n\n\n\n## 2025-12-24 |                      monopoly in general equilibrium\n\n1. **TLDR: as monopolist you can now choose the price vector, allows you to get bigger share of endowments.**\n\n2. **In an Edgeworth box, the other guy's offer curve is now your budget constraint.**\n\n3. **If e=1 (Cobb-Douglas), offer curves will be straight lines:**\n\n   ![](images/2025-12-24-06-14-37.png)\n   - I can get half his loaves, while giving away almost zero fishes.\n\n4. **If gross complements you can get everything from the other guy.**\n\n![](images/2025-12-24-06-20-29.png)\n\n5. **If gross substitutes you can only get a little extra surplus.**\n\n![](images/2025-12-28-06-38-42.png)\n\n[ChatGPT diagrams](https://chatgpt.com/share/695141da-a83c-8013-8366-e1caa9c5e9c1)\n\n--------------------------------------------\n\n[Lones Smith notes on monopoly in general equilibrium](https://www.lonessmith.com/wp-content/uploads/2020/09/7-GE-in-Exchange-Economies-1.pdf)\n\n   - Monopolist \"seeks his highest indifference curve on Iris’s TOC [trade offer curve]\"\n   - Nice point that monopolist can do even better with a two-part tariff. \n\n\n# 2026-01-28 |                      model\n\nTwo nice models:\n\n1. **Expanding varieties:** old LLMs give you access to others' varieties, new LLMs create new varieties.\n\n2. **Lowering costs:** old LLMs bring you to the cost frontier, new LLMs lower costs beyond the frontier.\n\n##             Models\n\nExpanding varieties (old LLMs share existing knowledge, new LLMs create new knowledge).\n:  - Knowledge is how to make a specific variety of a good, like a Stiglitz *varieties* model. \n   - Knowledge is *symmetric* -- each variety is equally substitutable.\n   - Old LLMs: people learn the ability to make existing varieties.\n   - New LLMs: people create new varieties.\n   - (this is basically Venn diagram model).\n   - Implications:\n      1. New varieties are more valuable than existing varieties -> you can charge monopoly price.\n\nLowering costs / recipes.\n: \n   _In each domain each firm has a recipe,_ which has some cost $c$. Over time they learn new recipes and lower the cost. At any point some firm has the lowest cost.\n\n   _Old LLMs give you access to existing recipes,_ you lower your cost towards the world frontier. Implication: (1) LLMs used by those behind frontier; (2) LLMs used for things outside your domain of expertise.\n\n   _New LLMs create new recipes,_ pushing the cost down. \n\nRecipe model in a landscape.\n:  - r \\in R, recipe\n   - c(r) = cost for that recipe, lower is better\n   - each person knows a set of recipes, & one is the *best* recipe, i.e. lowest cost.\n   - old LLMs make private knowledge public\n   - new LLMs, you can spend tokens to find new recipes.\n\nRandom landscape (balls from urn).\n: Kortum model of returns to effort, how fat are the tails.\n\nRugged landscape (fourier).\n: \n\nRegular landscape (fourier).\n: \n\n\n##          What I want to get:\n\nNew LLMs spend more on test-time compute.\n: Model old LLMs as lookup tables, they want to spend a lot on pre-training. But .\n: relatively much more on test-time compute, for a single problem.\n\n1. **Labs prefer to sell to fewer customers.**\n   - Extreme version: suppose there's Bertrand competition in the final market, then a single knowledge-owner will earn large rents, but two knowledge-owners will earn zero rents.\n\n2. **Benchmarks shouldn't be on.**\n\n\n##          Observations\n\nThere are only a few deep problems.\n: There's a large set of problems, but they can be mapped down into a much smaller set.\n   - Logic problem: .\n   - Knowledge problem:\n\nLLM benchmarks should now be on *world* frontier open problems.\n:  1. Instead of reproducing an existing variety, you have to create a new variety.\n   2. Instead of approaching an existing score, you need to create a new high score on some world record.\n\nThere will be an intellectual property land grab.\n: If we discover this new technology (technology for making technologies), and you can lay claim to discoveries, then whoever gets there first will be able to appropriate huge amounts of the surplus.\n\nAI has been providing private goods, now it's providing public goods.\n: It's been solving individual problems.\n\n\n\n##       Model of LLMs for Discovery\n\nIt's useful to distinguish between two types of LLMs:\n:   1. **LLMs that share existing knowledge (old LLMs)**-- they are trained on human-produced and human-judged data.\n    1. **LLMs that discover new knowledge (new LLMs)** -- they are trained against new data directly from the real world, e.g. math, verifiable problems, computer use, actions in the world.\n\nThe two types have quite distinct economic implications.\n:   1. **Knowledge-sharing LLMs will be used by *non-experts*,** and will be widely available.\n    1. **Knowledge-creating LLMs will will be used by *experts*,** and will be closely held by the labs, or sold with exclusive licenses.\n\n\n\nOld LLMs share existing knowledge.\n:     Traditionally LLMs are trained with human judgment as the ground truth, using labels from paid raters, or from LLM users. As a consequence they can answer questions and solve problems up to the limits of human expertise but rarely beyond.\n\n      An implication: they will be used by people outside their areas of expertise, and by firms that are followers, to catch up to the frontier.\n      \n      As a consequence they decrease knowledge rents -- people and firms whose value is from their existing knowledge.\n      \n      They increase home production (you can solve problems yourself instead of paying for it), and so decrease GDP.\n      \n      They decrease the returns to innovation (and news-gathering), because new knowledge diffuses more quickly.\n      \n<!-- (more speculative) they decrease firm size, because you don't need an in-house specialist anymore. -->\n\nNew LLMs discover new knowldge.\n:     They will be mostly used by *leader* firms instead of followers, and will be used *inside* their area of expertise instead of at the fringers.\n      \n      The returns to LLM use don't diminish so fast, you keep spending on inference until you hit diminishing returns on new knowledge creation.\n      \n      The demand for new knowledge is much less elastic than the demand for exiting knowledge. Selling knowledge to one person is much more valuable than selling to two people. Thus labs will prefer to restrict output, e.g. by selling the knowledge to just one firm, instead of selling the ability to generate knowledge.\n      \n      Our benchmarks for new LLMs will be qualitatively different. Instead of seeing if they can answer questions which we already know the answer to, we want them to answer *new* questions, e.g. Erdos problems, or setting records on optimization benchmarks.\n\n      Examples of knowledge-creating LLMs:\n         - Create a movie\n         - Predict stock prices\n         - Optimize algorithms\n         - Optimize technology\n         - Solve scientific problems\n\n\nA simple model with recipes:\n:   1. **Baseline: everyone buys from the person who knows the best recipe.** Everyone has a unit of labor. There's one consumption good, but various recipes for producing it, $r\\in R$, which determine the labor-cost of producing the good, $c(r)$. In equilibrium the person who knows the lowest-cost recipe ($c_1$) will sell the good in return for others' labor. Their margins are equal to the difference to the next-lowest-cost recipe, $c_2-c_1$ (assume Bertrand competition).\n    1. **Knowledge-sharing LLMs eliminate rents.** Now you invent a knowledge-sharing LLM, which can reveal the lowest-cost known recipe, $c_1$. You cannot make substantial profits from this knowledge: once two producers have the same cost then margins will be driven to zero. Assuming the recipe does diffuse, total output remains the same but the surplus is now distributed equally. If we additionally assumed some trade cost $\\delta$ then the knowledge will have value equal to $\\delta$, but notably there's no value to *exclusively* license your LLM.\n    2. **Knowledge-creating LLMs generate additional surplus.** Next we introduce a knowledge-creating LLM, which generates a new recipe $c_0<c_1$. The inventor can monetize this either by producing the good themselves or licensing the recipe to a single producer. Now exclusivity is important: if they sold the recipe to *two* producers then profits will be driven to zero, and the value of the recipe will be zero. In equilibrium total output increases, the extra surplus is split between consumers and the owner of the new recipe.\n\n    This model can be extended to multiple goods: if people have Cobb-Douglas preferences across goods, then they will spend a fixed fraction of labor on each, and so each goods market can be treated as independent.\n\n|                         | LLM that shares knowledge | LLM creates knowledge  |\n| ----------------------- | ------------------------- | ---------------------- |\n|                         |                           |                        |\n| Superhuman performance? | Only in special cases     | Often                  |\n| Who uses it?            | Followers in an industry  | Leaders in an industry |\n| What types of use?      | Specializations           | Non-specializations    |\n|                         |                           |                        |\n\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2025-12-29-selling-shovels_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n#                       notes\n\n[Naveen Rao, Databricks VP, Feb 2025](https://x.com/NaveenGRao/status/1886544584588619840)\n\n   > \"Prediction: all closed AI model providers will stop selling APIs in the next 2-3 years. Only open models will be available via APIs.\n\n   > \"Why? For an open model service, the value prop is clear...it's hard to build a scalable service to access the model and the model is commodity. The race-to-the bottom happened with the commodity already (model). Let AI app builders iterate on great UIs for apps upon scalable services with commodity capabilities\n\n   > \"Closed model providers are trying to build non-commodity capabilities and they need great UIs to deliver those. It's not just a model anymore, but an app with a UI for a purpose.\n\n   > \"If closed models are available via API, all it does is create competition for the app the closed provider is building. The secret sauce is capabilities + UI.\"\n\nAlexander Doria [\"The Model is the Product\"](https://vintagedata.org/blog/posts/model-is-the-product)\n\n   (says that models will do eveyrhting, you don't need a wrapper)\n   (thanks to Elsa Jiang for the reference)\n\n(story that OpenAI will share revenues with clients for developing new drugs)\n\n- https://sean.heelan.io/2026/01/18/on-the-coming-industrialisation-of-exploit-generation-with-llms/\n\nSarah Friar, Jan 2026, [\"A business that scales with the value of intelligence\"](https://openai.com/index/a-business-that-scales-with-the-value-of-intelligence/)\n\n> \"As intelligence moves into scientific research, drug discovery, energy systems, and financial modeling, new economic models will emerge. Licensing, IP-based agreements, and outcome-based pricing will share in the value created. That is how the internet evolved. Intelligence will follow the same path.\"\n\nTTT-Discover\n\n1. *\"Standard AI learns to imitate.  We introduce a new framework that trains AI to make new discoveries in science + engineering.\"* -- https://x.com/james_y_zou/status/2014404929566490848\n   \n2. \"How to get AI to make discoveries on open scientific problems? Most methods just improve the prompt with more attempts. But the AI itself doesn't improve. With test-time training, AI can continue to learn on the problem it’s trying to solve: http://test-time-training.github.io/discover.pdf\" -- https://x.com/mertyuksekgonul/status/2014394903850311797\n   \n3. *\"Test-time training is inevitable. We’re heading toward models that truly learn from experience: TTT for LLM memory (TTT-E2E), and now for open scientific problems.\"* -- https://x.com/karansdalal/status/2014429596847112605\n\n> \"We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős’ minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to 2×faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers.\"\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}