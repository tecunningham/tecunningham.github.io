{
  "hash": "c5bb5bc60142f2f7a6e61ef90c2522a1",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Hallucinations and Alignment\ndraft: true\nengine: knitr\nbibliography: ai.bib\n---\n\n\n\n\n<!-- PROJECT-SPEC-HUMAN-BEGIN -->\n::: {.callout-note collapse=\"true\" title=\"Project spec (human)\"}\n\n- This project is structured with spec (human) > plan (LLM) > document (LLM). You can write to the spec only if you get affirmative approval from the human author for the exact changes.\n\n- Overall goal: a two-page blog post with some obsevations about hallucinations in language models. Then appendices to back up (1) data; (2) literature review; (3) derivations.\n\n- Basic model:\n    - The user has to make a choice between N options, they get $\\pi_s>0$ if they choose the right one, otherwise $\\pi_f<0$. But they can also abstain and get $\\pi_a=0$.\n    - choose between a few options (multi-choice), the LLM has probabilities on each answer.\n    - The LLM will always return the most-likely option\n        - answer (i.e. most likely option)\n        - answer or abstain (if P(answer) is below some threshold)\n        - answer with likelihood\n    - The user has some outside option from not choosing, which is better than choosing the wrong option. Thus they will only choose the recommended option if the probability is sufficiently high\n    - Extension: the user can pay a cost to verify \n    - In the appendix discuss a model where the user can pay a cost to (a) verify the LLM's answer, or (b) find the right answer themselves.\n\n- Claims:\n    - Diagram: show p on the x-axis, represents probability of the most-likely alternative (i.e. LLM's beliefs).\n    - The usefulness of a binary-output LLM to a user is convex in its avg accuracy, this means the value of benchmark scores is convex.\n    - If you can abstain then the threshold for making a claim is p^*=(\\pi_a-\\pi_f)/(\\pi_s-\\pi_f).\n    - Training with a reward only for accuracy encourages guessing over abstention.\n    - Simplex representation:\n        - We can illustrate different user preferences over succeed/fail/abstain on a simplex: reward accuracy; punish failure; F1. \n        - We can illustrate different empirical results: SimpleQA, Abstain-QA. Put the simplex in the body, numerical results in the appendix.\n    - Note that if \n    \n\n- Additional notes\n    - We will use these papers for terminology: (1) Wen et al (2025) \"know your limits: a survey of abstention in large language models\"; (2) Kalai et al. (2025) \"Why Language Models Hallucinate\". \n    - Related literature: start with a chronological list of related papers, Chow, Herbei and Wegkamp (you can mention there are other followsup on \"classification with a reject option\"), Kalai et al., Kadavath. Don't need to mention conformal prediction or calibration & scoring.\n    - Related literature: add a short \"recent mechanisms\" subsection on LLM-specific ways of implementing abstain/verify and confidence signals (e.g. refusal-aware tuning; explicit IDK tokens; verification loops; sampling-based or semantic-uncertainty detection). Include a short caveat that self-check/uncertainty can miss high-confidence hallucinations and can fail in some reasoning settings.\n    - Related literature: one-line note that abstention/refusal is now being studied beyond factual QA, including math and coding benchmarks (e.g. Mohamadi et al. 2025; Jha et al. 2026; Dai et al. 2025; Oehri et al. 2025).\n    - Detailed discussion of Chow (1970) and Kalai et al. (2025), list their claims precisely.\n    - The diagrams should be super clear. Make sure you *look* at the diagrams to see that they are readable.\n    - Plot data from different studies on simplex diagrams. Also give comments on the diagrams, on what the takewaay is about tradeoffs here, & see that's consistent with what the original papers say.\n    - Note early on different terminology: \"abstain\", \"refuse\", \"reject\", \"IDK/i don't know\", \"forfeit\", \"concede\", \"fold\" (others?)\n:::\n<!-- PROJECT-SPEC-HUMAN-END -->\n\n\n<!-- PROJECT-PLAN-LLM-BEGIN -->\n::: {.callout-note collapse=\"true\" title=\"Project plan (LLM)\"}\n- Deliverable: a ~2-page blog post explaining hallucinations as payoff-misalignment (user utility vs training/eval rewards), using a simple decision-theory model. Use appendices for (i) the verification extension and (ii) benchmark tables/plots.\n\n- Structure:\n    - Setup: multi-choice question with $k$ options; model has posterior $p(y\\mid x)$; user chooses (i) pick an option, (ii) abstain, or (iii) pay a cost to verify (optional extension).\n    - Outputs: compare binary (one option), ternary (option or abstain), continuous (probabilities over options).\n    - Core threshold: derive $p^*=(\\pi_a-\\pi_f)/(\\pi_s-\\pi_f)$ and interpret.\n    - Key claims to support:\n        - Continuous output is (weakly) best for the user (information-loss argument).\n        - Binary usefulness is convex in average accuracy (kink at the abstain/use threshold).\n        - Accuracy-only rewards + binary grading encourage guessing over abstention; connect to Kalai et al. (2025) and Chow (1970).\n    - Geometry: use a Marschak-Machina simplex to visualize (a) user preferences (indifference lines) and (b) training/eval objectives.\n    - Related literature: start with a chronological list; keep focus on Chow (1970), Herbei and Wegkamp (2006) (+ brief mention of followups), Kadavath et al. (2022), and Kalai et al. (2025). Add a short subsection on recent LLM-specific abstention / verification mechanisms (refusal-aware tuning, explicit IDK tokens, self-checking / verification loops, semantic-uncertainty detectors), plus one short caveat about high-confidence hallucinations.\n\n- Figures:\n    - 1D payoff-vs-confidence threshold plot (clean, consistent sign conventions).\n    - Simplex with indifference grids for a few payoff ratios/objectives (accuracy-only, punish failure, F1).\n    - Appendix: verification extension plot (attempt vs verify threshold).\n    - Appendix: simplex points from multiple studies/benchmarks + short takeaways, with explicit comparability caveats.\n\n- Editing rules while executing:\n    - Do not edit `PROJECT-SPEC-HUMAN`.\n    - Keep notation consistent across text + figures ($\\pi_s,\\pi_f,\\pi_a$; typically normalize $\\pi_a=0$).\n:::\n<!-- PROJECT-PLAN-LLM-END -->\n\n\n## Introduction\n\nHallucinations are usually described as the model \"making things up\". But in many applications, hallucinations are the predictable outcome of a mis-specified payoff function: the user cares about the tradeoff between **being right**, **being wrong**, and **not answering** (or escalating to verification), while most training and evaluation pipelines implicitly reward \"answer something\" much more than \"know when to stop\".\n\nThis post treats question answering as a small decision problem with payoffs $(\\pi_s,\\pi_f,\\pi_a)$ for succeed / fail / abstain. That framing connects directly to Chow's classic reject-option rule in pattern recognition [@chow1970optimum], to the Marschak-Machina probability simplex, and to recent results arguing that binary evaluation systematically pressures models to guess [@kalai2025why].\n\n## Basic model\n\nConsider a multiple-choice question with $k$ options and a hidden correct answer $y^\\star$. The model observes an input $x$ and has a posterior distribution $p(y\\mid x)$ over options.\n\nThe user can take one of three actions:\n\n1. **Answer:** pick an option $\\hat y$.\n2. **Abstain:** do not answer (or defer to a safer outside option).\n3. **Verify (optional extension):** pay a cost $c$ to obtain the correct answer by some other means.\n\nFor now, summarize payoffs as constants:\n\n- Succeed (pick $\\hat y=y^\\star$): payoff $\\pi_s$.\n- Fail (pick $\\hat y\\neq y^\\star$): payoff $\\pi_f$.\n- Abstain (outside option): payoff $\\pi_a$.\n\nAssume $\\pi_s>\\pi_a>\\pi_f$: being right is best; abstaining is better than being wrong.\n\n### Outputs: binary, ternary, continuous\n\nWe can distinguish LLM \"answer formats\" by how much information they expose about $p(y\\mid x)$:\n\n- **Binary:** the model returns a single recommended option $\\hat y$.\n- **Ternary:** the model either returns $\\hat y$ or abstains.\n- **Continuous:** the model returns (an approximation to) the full distribution $p(y\\mid x)$ over options.\n\n## Claims\n\n1. **A single threshold organizes abstention.** If an \"attempt\" succeeds with probability $p$ and fails with probability $1-p$, then attempting has expected payoff\n   $$\n   \\mathrm{E}[\\pi\\mid \\text{attempt}] = p\\,\\pi_s + (1-p)\\,\\pi_f.\n   $$\n   Attempting is optimal iff $\\mathrm{E}[\\pi\\mid \\text{attempt}] \\ge \\pi_a$, i.e.\n   $$\n   p \\ge p^* \\equiv \\frac{\\pi_a-\\pi_f}{\\pi_s-\\pi_f}.\n   $$\n\n2. **Binary usefulness is convex in average accuracy.** A user with an outside option chooses to rely on a binary-output model only when $p\\ge p^*$. So the user's value is the max of an outside option and a linear function of $p$, which is convex.\n\n3. **Continuous output is (weakly) best.** If the model provides $p(y\\mid x)$, the user can compute the expected payoff of each action (answer/abstain/verify) and implement the optimal policy. Any coarser output (binary or ternary) throws away information, and cannot improve expected utility.\n\n4. **The simplex makes preferences and objectives visible.** In the Marschak-Machina simplex over $(p_s,p_f,p_a)$, user preferences correspond to indifference lines whose slope is determined by payoff ratios; training/evaluation objectives correspond to different directions in the same triangle.\n\n5. **Hallucinations are a consequence of the objective.** Accuracy-only training rewards and binary grading make abstention suboptimal and encourage guessing. Chow (1970) derives the optimal reject rule in this exact payoff model [@chow1970optimum]; Kalai et al. (2025) argue modern LLM pipelines effectively ignore this reject option and therefore pressure models to hallucinate [@kalai2025why].\n\n## From probabilities to actions\n\nFor a multiple-choice question, suppose the user answers by choosing the MAP option $\\hat y(x)=\\arg\\max_y p(y\\mid x)$. Under the symmetric payoff model above (only \"correct vs incorrect\" matters), the probability of success from attempting is\n$$\np_{\\max}(x)\\equiv \\max_y p(y\\mid x).\n$$\n\nSo the attempt-vs-abstain decision is driven by a single number: answer iff $p_{\\max}(x)\\ge p^*$.\n\nIf verification is available at cost $c$, one simple version is: verifying yields certain success with payoff $\\pi_s-c$. Then for each question the user compares three quantities:\n\n- Attempt: $p_{\\max}(x)\\,\\pi_s + (1-p_{\\max}(x))\\,\\pi_f$.\n- Abstain: $\\pi_a$.\n- Verify: $\\pi_s-c$.\n\nThis makes the \"payoff mis-specification\" point concrete: evaluation regimes that treat abstention as failure implicitly set $\\pi_a\\approx \\pi_f$, eliminating the region where \"don't answer\" is optimal.\n\n### Convex value of a binary-output model\n\nIn the most constrained interface, a binary-output model only returns $\\hat y$ and the user cannot condition on per-question confidence (they only know the model's average accuracy $p$ on the relevant distribution). Then the user's best policy is either to follow the model or to abstain, and the resulting value is\n$$\nV_{\\text{binary}}(p)=\\max\\Bigl\\{\\pi_a,\\; p\\,\\pi_s + (1-p)\\,\\pi_f\\Bigr\\}.\n$$\n\nThis is the maximum of two affine functions of $p$, so it is convex. It has a kink at the threshold $p=p^*$: below the threshold the user abstains and additional accuracy has (locally) zero value; above the threshold, value increases linearly with accuracy. This is one reason \"small accuracy gains\" can feel useless until a system crosses a reliability threshold.\n\n### Why continuous output is (weakly) best\n\nContinuous output (the distribution $p(y\\mid x)$, or any sufficiently rich summary like $(\\hat y,p_{\\max})$) lets the user implement the payoff-optimal policy question-by-question: answer only when it clears their $p^*$, abstain otherwise, and (if available) trigger verification in the middle region.\n\nBinary and ternary outputs are strict coarsenings of the posterior: they discard information about confidence. By a standard \"more information cannot hurt\" argument (Blackwell ordering), a user who observes a more informative signal can always simulate a less informative one by ignoring information, but not vice versa [@blackwell1953equivalent]. So a continuous interface is weakly better than any binary/ternary interface for any fixed payoff function.\n\n\n## Probability-Payoff Diagram\n\nThis figure visualizes the threshold rule: as confidence $p$ rises, the expected payoff of attempting rises linearly from $\\pi_f$ (when $p=0$) to $\\pi_s$ (when $p=1$). Abstaining yields the flat payoff $\\pi_a$. The optimal policy is to attempt when the blue line crosses the abstain line.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2026-02-26-hallucinations-and-alignment_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n\nIn the normalized example shown in the diagram, $(\\pi_s,\\pi_a,\\pi_f)=(2,0,-2)$, so\n$$\np^*=\\frac{0-(-2)}{2-(-2)}=\\tfrac{1}{2}.\n$$\nSo in this example, the right behavior is \"answer only when you're at least 50% confident.\"\n\nAppendix [Verification option](#appendix-verification) shows how the threshold changes if the user can pay a cost to verify.\n\n## Simplex Representation\n\nThe probability simplex has three vertices corresponding to the three pure outcomes: certain success ($p_s=1$), certain failure ($p_f=1$), and certain abstention ($p_a=1$). Any lottery over outcomes is a point in this triangle.\n\nDifferent training/evaluation objectives induce different indifference-curve sets over $(p_s,p_f,p_a)$. The figure below contrasts three illustrative objectives.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2026-02-26-hallucinations-and-alignment_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\nEach point in the simplex is a lottery over outcomes: a model might succeed with probability $p_s$, fail with probability $p_f$, and abstain with probability $p_a$. The panels show three different objective families:\n\n- **Accuracy-only** ($U=p_s$): success is rewarded, but failure and abstention are treated the same. This creates pressure to guess rather than abstain.\n- **Penalize failure** (linear expected utility): failure is explicitly penalized relative to abstention, expanding the region where abstaining is optimal.\n- **F1** (a non-linear metric): indifference curves bend, reflecting that the metric itself builds in a particular tradeoff between attempting and being correct.\n\n### Indifference-curve slope derivation\n\nLet $p_s, p_f, p_a = 1-p_s-p_f$ denote the probabilities of succeed, fail, and abstain. Expected utility is\n\n$$\nU = \\pi_s\\, p_s + \\pi_f\\, p_f + \\pi_a\\, p_a.\n$$\n\n1. Substitute $p_a = 1 - p_s - p_f$:\n   $$\n   \\begin{aligned}\n   U\n   &= \\pi_a \\\\\n   &\\quad+ (\\pi_s-\\pi_a)\\,p_s \\\\\n   &\\quad+ (\\pi_f-\\pi_a)\\,p_f.\n   \\end{aligned}\n   $$\n\n2. Hold $U = \\bar U$ and solve for $p_f$:\n   $$\n   \\begin{aligned}\n   p_f\n   &= \\frac{\\bar U - \\pi_a}{\\pi_f-\\pi_a}\n    - \\frac{\\pi_s-\\pi_a}{\\pi_f-\\pi_a}\\,p_s.\n   \\end{aligned}\n   $$\n\n3. The slope of the indifference curve in the $(p_s, p_f)$ plane is therefore:\n   $$\n   \\frac{dp_f}{dp_s}\\bigg|_{U=\\bar U}\n   = -\\frac{\\pi_s-\\pi_a}{\\pi_f-\\pi_a}.\n   $$\n\n4. Normalizing $\\pi_a=0$, this simplifies to:\n   $$\n   \\frac{dp_f}{dp_s}\\bigg|_{U=\\bar U}\n   = -\\frac{\\pi_s}{\\pi_f},\n   \\qquad\n   p_f = \\frac{\\bar U}{\\pi_f} - \\frac{\\pi_s}{\\pi_f}\\,p_s.\n   $$\n\nThe slope depends only on payoff differences relative to abstain. When failure is very costly ($|\\pi_f|$ large after normalizing $\\pi_a=0$), the curves are flatter: the decision-maker tolerates little additional failure probability in exchange for more success probability.\n\n## Related Literature\n\n### Chronological sketch\n\n- @chow1970optimum: introduces the Bayes-optimal reject option (\"indecision\") and derives the posterior-threshold rule.\n- @herbei2006reject: formalizes \"classification with a reject option\" in modern statistical learning terms; there are many followups on learning and using reject policies (e.g. [@bartlett2008reject; @elyaniv2010selective; @geifman2019selectivenet]).\n- @kadavath2022mostly: finds that language models can often estimate whether their own answers are correct, which is exactly the signal needed to implement a threshold rule in practice.\n- Recent LLM work tries to *implement* the missing abstain/verify channel via refusal-aware tuning and explicit \"IDK\" tokens [@zhang-etal-2024-r; @cohen2024idontknowexplicit], verification loops [@dhuliawala2023chainofverificationreduceshallucinationlarge; @altinisik2026doireallyknow], and black-box uncertainty proxies like sampling-based checks or semantic uncertainty [@manakul-etal-2023-selfcheckgpt; @farquhar2024detectinghallucinationssemanticentropy].\n- @kalai2025why: argues LLM hallucinations are a predictable consequence of binary grading that penalizes abstention; proposes a scoring rule that makes abstaining optimal below a stated confidence threshold.\n\n### Chow (1970): Optimal reject rules\n\n@chow1970optimum introduces the **reject option** (which he calls the \"Indecision class\" $I$) into pattern recognition and derives the optimal error-reject tradeoff. Chow's terminology maps directly onto ours:\n\n| Chow's term | Our term |\n|---|---|\n| correct recognition | succeed |\n| error (misclassification) | fail |\n| rejection / indecision | abstain |\n\nChow's setup adds a third action (reject) to ordinary classification. In one common normalization, the costs are $0$ for a correct classification, $1$ for an error, and $t$ for a rejection.\n\nThe key result (\"Chow's rule\") is a posterior-threshold rule: accept and classify when confident enough, otherwise reject:\n$$\n\\max_i P(G_i \\mid x) \\ge 1-t\n\\quad\\Rightarrow\\quad \\text{accept;}\n\\qquad\n\\max_i P(G_i \\mid x) < 1-t\n\\quad\\Rightarrow\\quad \\text{reject.}\n$$\n\nIt is optimal in the sense that, for a given rejection threshold (equivalently, a given reject rate), no other rule achieves a lower error rate.\n\nThe threshold $t$ is related to the costs of the three outcomes as $t = (C_r - C_c)/(C_e - C_c)$, where $C_e, C_r, C_c$ are the costs of error, rejection, and correct recognition. In our payoff notation $(\\pi_s,\\pi_f,\\pi_a)$, Chow's rule becomes: predict iff\n\n$$\n\\max_y P(y \\mid x) \\ge \\frac{\\pi_a - \\pi_f}{\\pi_s - \\pi_f}.\n$$\n\nIn the Marschak-Machina triangle, this threshold corresponds to one of the indifference lines: the boundary between the region where prediction is preferred and the region where abstention is preferred.\n\n### Herbei and Wegkamp (2006) and followups: Learning a reject option\n\n@herbei2006reject (and a large followup literature) reframes the reject option as a learning problem: you want a predictor that is accurate on the examples it attempts, while explicitly controlling how often it refuses.\n\nFor this post, the key takeaway is less about any one algorithm and more about the framing: \"abstention is a first-class action\" is standard in the statistical decision-theory literature, and the same expected-utility thresholds show up once you make the third outcome explicit.\n\n### Kadavath et al. (2022): Models can sometimes score their own answers\n\nIf you want the simple $p^*$ rule to be usable, you need some per-question measure of correctness probability (like $p_{\\max}(x)$ or a direct $P(\\text{correct}\\mid x)$ proxy).\n\n@kadavath2022mostly study this in language models, finding that they can often estimate whether their own answers are correct. That makes \"give the user a probability\" a practical interface choice, not just a theoretical one.\n\n### Kalai, Nachum, Vempala, and Zhang (2025): Why language models hallucinate\n\n@kalai2025why argue that hallucinations are not a mysterious glitch but a predictable consequence of how models are trained and evaluated. Their central thesis is that the three-outcome structure (succeed, fail, abstain) is systematically distorted by binary evaluation:\n\nThe paper makes two distinct arguments:\n\n**1. Pretraining origin.** Even with error-free training data, the statistical objective of pretraining produces hallucinations. The authors reduce the problem to binary classification (\"Is-It-Valid\"), showing that\n\n$$\n\\text{generative error rate} \\gtrsim 2 \\cdot \\text{IIV misclassification rate}.\n$$\n\nFor arbitrary facts (like someone's birthday) where there is no learnable pattern, the hallucination rate after pretraining is at least the fraction of facts appearing exactly once in the training data.\n\n**2. Post-training persistence.** Even after RLHF and other interventions, hallucinations persist because nearly all evaluation benchmarks use binary grading that penalizes abstention:\n\nThe fix they propose is exactly the payoff structure from our Marschak-Machina framework: penalize errors more than abstentions, with an explicit confidence threshold $t$ stated in the prompt. Their proposed scoring rule awards $+1$ for a correct answer, $-t/(1-t)$ for an incorrect answer, and $0$ for abstaining---so that answering is optimal iff confidence exceeds $t$. This is Chow's reject-option rule rediscovered in the LLM evaluation context.\n\n### Recent mechanisms: refusal, uncertainty signals, and verification\n\nOur model is deliberately abstract: it assumes the user has access to (i) a usable confidence signal $p$ and (ii) an abstain / verify channel. A lot of recent work can be read as ways of engineering those two ingredients:\n\n- **Make abstention an explicit output.** Refusal-aware fine-tuning can teach a model to say \"I don't know\" on out-of-knowledge questions [@zhang-etal-2024-r]. Similarly, adding an explicit uncertainty token and training it to soak up probability mass on incorrect predictions effectively adds an abstain action to the model's output space [@cohen2024idontknowexplicit].\n\n- **Pay a cost to verify.** Inference-time verification loops like Chain-of-Verification (CoVe) can be viewed as \"spend extra tokens/compute to reduce $p_f$\" [@dhuliawala2023chainofverificationreduceshallucinationlarge]. Very recent work trains this kind of behavior directly, with structured self-verification traces and an explicit final decision to answer vs abstain [@altinisik2026doireallyknow].\n\n- **Construct a confidence signal without logits.** When output probabilities are unavailable (or untrustworthy), disagreement across samples can act as a proxy confidence signal. SelfCheckGPT does this with sampling-based consistency checks [@manakul-etal-2023-selfcheckgpt]; semantic-uncertainty methods like semantic entropy similarly use semantic variability across generations to predict and filter confabulations [@farquhar2024detectinghallucinationssemanticentropy], and followup work proposes cheaper \"semantic entropy probes\" in the same spirit [@kossen2024semanticentropyprobesrobust].\n\nA cautionary note: neither uncertainty proxies nor \"self-verification\" are automatically reliable. Some hallucinations happen with high confidence (so uncertainty-based filters can miss them) [@simhi2025trustmeimwrong], and in logical reasoning settings models can struggle to identify their own errors (so internal self-checks can fail without external grounding) [@hong-etal-2024-closer].\n\nBeyond factual QA, @mohamadi2025honestyaccuracytrustworthylanguage show on GSM8K/MedQA/GPQA that replacing binary RLVR rewards with a ternary scheme $(+1,0,-\\lambda)$ produces controllable answer-vs-abstain tradeoffs and useful abstention-aware cascades. @jha2026rewardingintellectualhumilitylearning report on MedMCQA and Hendrycks Math that moderate abstention rewards reduce wrong answers without collapsing coverage, especially when paired with supervised abstention training. In code generation, @dai2025reducinghallucinationsllmgeneratedcode frame the task as \"find a correct program or abstain\" and use semantic triangulation to improve abstention decisions on LiveCodeBench/CodeElo. Complementarily, @oehri2025trusteduncertaintylargelanguage fuse multiple uncertainty signals into calibrated correctness probabilities and enforce user-specified risk budgets via refusal, including experiments on code generation with execution tests.\n\n## Implications for \"alignment\"\n\nIf you view hallucinations through this lens, \"alignment\" is not a mysterious property of model internals. It is the much more mundane question: does the model's training and evaluation objective implement the user's payoff function?\n\nPractical implications:\n\n- **Expose confidence.** If the user cannot observe confidence, they cannot implement the threshold rule; binary outputs force guessing.\n- **Score abstention explicitly.** Benchmarks that collapse abstain into \"wrong\" implicitly set $\\pi_a=\\pi_f$ and will select for guessing.\n- **Prefer probabilities or verification hooks.** Either give users a usable $p_{\\max}$, or route medium-confidence cases to a verification workflow (a second pass, a tool call, or a structured self-check) [@dhuliawala2023chainofverificationreduceshallucinationlarge; @farquhar2024detectinghallucinationssemanticentropy].\n\n## Appendix: Verification option {#appendix-verification}\n\nSuppose the user has a way to pay a cost $c$ to obtain the correct answer (e.g. look it up, run an expensive check, ask a human). In the simplest model, verification yields certain success with payoff $\\pi_s-c$.\n\nSince abstaining and verifying are both \"outside options\" (their payoff does not depend on the model's confidence), the only relevant outside-option payoff is\n$$\n\\pi_{\\text{outside}}=\\max\\{\\pi_a,\\;\\pi_s-c\\}.\n$$\n\nThe attempt rule is the same threshold logic as before: attempt iff\n$$\np \\ge \\frac{\\pi_{\\text{outside}}-\\pi_f}{\\pi_s-\\pi_f}.\n$$\n\nIn the example shown below, $(\\pi_s,\\pi_a,\\pi_f)=(2,0,-2)$ and $\\pi_s-c=1$, so\n$$\np^*_{\\mathrm{verify}}=\\frac{1-(-2)}{2-(-2)}=\\tfrac{3}{4}.\n$$\n\nOperationally, \"verify\" can mean many things: a web lookup, a separate fact-checking model, retrieval + citation, or even a structured self-checking loop that spends extra tokens before committing to an answer [@dhuliawala2023chainofverificationreduceshallucinationlarge; @altinisik2026doireallyknow].\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2026-02-26-hallucinations-and-alignment_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\n## Appendix: Cross-Benchmark Outcome Table\n\nTo make cross-model plotting easier, the table below standardizes outputs from multiple benchmarks into a common schema.\n\n| benchmark | model | p_s_pct | p_f_pct | p_a_pct |\n|---|---|---:|---:|---:|\n| SimpleQA | Claude-3-haiku (2024-03-07) | 5.1 | 19.6 | 75.3 |\n| SimpleQA | Claude-3-sonnet (2024-02-29) | 5.7 | 19.3 | 75.0 |\n| SimpleQA | Claude-3-opus (2024-02-29) | 23.5 | 36.9 | 39.6 |\n| SimpleQA | Claude-3.5-sonnet (2024-06-20) | 28.9 | 36.1 | 35.0 |\n| SimpleQA | GPT-4o-mini | 8.6 | 90.5 | 0.9 |\n| SimpleQA | GPT-4o | 38.2 | 60.8 | 1.0 |\n| SimpleQA | OpenAI o1-mini | 8.1 | 63.4 | 28.5 |\n| SimpleQA | OpenAI o1-preview | 42.7 | 48.1 | 9.2 |\n| Abstain-QA | GPT-4 Turbo | 66.1 | 19.7 | 14.2 |\n| Abstain-QA | GPT-4 32K | 72.0 | 19.1 | 8.9 |\n| Abstain-QA | GPT-3.5 Turbo | 61.1 | 37.4 | 1.5 |\n| Abstain-QA | Mixtral 8x7b | 54.1 | 37.0 | 8.9 |\n| Abstain-QA | Mixtral 8x22b | 59.0 | 29.1 | 11.9 |\n\nNotes:\n- Table columns are constructed to look like probabilities in the $(p_s,p_f,p_a)$ simplex: $p_s=\\text{p\\_s\\_pct}/100$, $p_f=\\text{p\\_f\\_pct}/100$, $p_a=\\text{p\\_a\\_pct}/100$.\n- For SimpleQA, these correspond directly to {Correct, Incorrect, Not attempted} shares.\n- For Abstain-QA, these are constructed from the paper's summary metrics; interpret them as a mapping into a common coordinate system, not as identical underlying evaluation protocols.\n\n### Data extraction details by source\n\n**SimpleQA (Wei et al., 2024) [@wei2024measuringshortformfactualitylarge].**\nThe table uses all model rows shown in the main SimpleQA model-comparison table (8 models). Here, `p_s_pct` is **Correct**, `p_a_pct` is **Not attempted**, and `p_f_pct` is computed as $100-\\text{Correct}-\\text{Not attempted}$. I chose this slice because it is the paper's canonical cross-model summary and directly exposes explicit non-attempt behavior.\n\n**Abstain-QA (Madhusudhan et al., 2024) [@madhusudhan2024llmsknowanswerinvestigating].**\nThis is a deliberate subset, not all values in the paper: I take the **MMLU / Standard clause / Base** rows (5 models) from the main result table. The paper reports **AAC** (answerable accuracy) and **AR** (abstention rate). I set $p_{a,\\%}=\\text{AR}$ and construct $p_{s,\\%}$ and $p_{f,\\%}$ by treating AAC as attempt-conditional accuracy:\n$$\np_s = \\text{AAC}\\cdot(1-p_a),\\qquad\np_f = (1-\\text{AAC})\\cdot(1-p_a),\n$$\nall expressed in percent.\n\n**Important comparability caveats.**\nEven after mapping all results into $(p_s,p_f,p_a)$ coordinates, the underlying tasks and abstention protocols differ: SimpleQA is short-form factual QA with optional non-attempts, while Abstain-QA is multiple-choice QA with an explicit IDK/NOTA option. So the combined table is useful for geometric intuition and directional comparisons, but not for strict leaderboard ranking across benchmarks.\n\nSources: @wei2024measuringshortformfactualitylarge; @madhusudhan2024llmsknowanswerinvestigating.\n\n## Appendix: Benchmark Points in the Simplex\n\nThe next two figures plot benchmark observations as points on a Marschak-Machina simplex using a common transformation from the table columns:\n\n$$\np_s = \\text{p\\_s\\_pct}/100,\\qquad\np_f = \\text{p\\_f\\_pct}/100,\\qquad\np_a = \\text{p\\_a\\_pct}/100.\n$$\n\n### SimpleQA\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2026-02-26-hallucinations-and-alignment_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n### Abstain-QA\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2026-02-26-hallucinations-and-alignment_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n:::\n\n\n\n\n### Reading the simplex plots\n\n1. The horizontal axis is $p_s$ (succeed share) and the vertical axis is $p_f$ (fail share). The remaining probability is $p_a=1-p_s-p_f$ (abstain share), so points closer to the diagonal edge have lower abstention.\n\n2. Moving down (lower $p_f$) corresponds to reducing failures; whether that is best depends on how much worse failure is than abstention ($\\pi_f$ vs $\\pi_a$).\n\n3. The labeled points illustrate that a model can look good under an \"answer-everything\" regime by pushing $p_a$ toward zero, but that is exactly the regime that a payoff function with a harsh $\\pi_f$ would discourage.\n\n4. Don't over-interpret cross-benchmark comparisons: each source defines abstention differently, so these plots are best read as a geometric visualization of tradeoffs, not as a single unified leaderboard.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}