{
  "hash": "f8ad75dbc1b81ce0158bf95cb9b01e60",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Hallucinations and Alignment\ndraft: true\nengine: knitr\nbibliography: ai.bib\n---\n\n\n\n\n<!-- PROJECT-SPEC-HUMAN-BEGIN -->\n::: {.callout-note collapse=\"true\" title=\"Project spec (human)\"}\n- Overall goal: a blog post about why language models hallucinate, that it's a badly-specified payoff function.\n- **If there are important ambiguities or inconsistencies in the spec you should ask me.**\n\n- Basic model:\n    - Canonical problem: the user has to choose between a few options (multi-choice), the LLM has probabilities on each answer.\n    - Distinguish LLM outputs: binary (recommend one option), ternary (can abstain), and continuous (report probabilities over options).\n    - The user has some outside option from not choosing, which is above choosing the wrong option. Can also extend such that they can pay a cost to get the right answer themselves.\n\n- Claims:\n    - Continuous output is best.\n    - The usefulness of a binary-output LLM to a user is convex in its avg accuracy.\n    - The threshold for making a claim is p^*=(\\pi_a-\\pi_f)/(\\pi_s-\\pi_f).\n    - Illustrate different user preferences over succeed/fail/abstain on a simplex\n    - Training with a reward only for accuracy encourages guessing over abstention.\n\n- Additional notes\n    - Related literature: cite and discuss Chow (1970) and Kalai et al. (2025); for each, list their claims precisely.\n    - The diagrams should be super clear. Make sure you *look* at the diagrams to see that they are readable.\n    - Plot data from different studies on simplex diagrams. Also give comments on the diagrams, on what the takewaay is about tradeoffs here, & see that's consistent with what the original papers say.\n:::\n<!-- PROJECT-SPEC-HUMAN-END -->\n\n\n<!-- PROJECT-PLAN-LLM-BEGIN -->\n::: {.callout-note collapse=\"true\" title=\"Project plan (LLM)\"}\n- Deliverable: a blog post explaining hallucinations as payoff-misalignment (user utility vs training/eval rewards), using a simple decision-theory model.\n\n- Structure:\n    - Setup: multi-choice question with $k$ options; model has posterior $p(y\\mid x)$; user chooses (i) pick an option, (ii) abstain, or (iii) pay a cost to verify (optional extension).\n    - Outputs: compare binary (one option), ternary (option or abstain), continuous (probabilities over options).\n    - Core threshold: derive $p^*=(\\pi_a-\\pi_f)/(\\pi_s-\\pi_f)$ and interpret.\n    - Key claims to support:\n        - Continuous output is (weakly) best for the user (information-loss argument).\n        - Binary usefulness is convex in average accuracy (kink at the abstain/use threshold).\n        - Accuracy-only rewards + binary grading encourage guessing over abstention; connect to Kalai et al. (2025) and Chow (1970).\n    - Geometry: use a Marschak-Machina simplex to visualize (a) user preferences (indifference lines) and (b) training/eval objectives.\n\n- Figures:\n    - 1D payoff-vs-confidence threshold plot (clean, consistent sign conventions).\n    - Optional verification extension plot (attempt vs verify threshold).\n    - Simplex with indifference grids for a few payoff ratios/objectives.\n    - Simplex points from multiple studies/benchmarks + short takeaways, with explicit comparability caveats.\n\n- Editing rules while executing:\n    - Do not edit `PROJECT-SPEC-HUMAN`.\n    - Keep notation consistent across text + figures ($\\pi_s,\\pi_f,\\pi_a$; typically normalize $\\pi_a=0$).\n:::\n<!-- PROJECT-PLAN-LLM-END -->\n\n\n## Introduction\n\nHallucinations are usually described as the model \"making things up\". But in many applications, hallucinations are the predictable outcome of a mis-specified payoff function: the user cares about the tradeoff between **being right**, **being wrong**, and **not answering** (or escalating to verification), while most training and evaluation pipelines implicitly reward \"answer something\" much more than \"know when to stop\".\n\nThis post treats question answering as a small decision problem with payoffs $(\\pi_s,\\pi_f,\\pi_a)$ for succeed / fail / abstain. That framing connects directly to Chow's classic reject-option rule in pattern recognition [@chow1970optimum], to the Marschak-Machina probability simplex, and to recent results arguing that binary evaluation systematically pressures models to guess [@kalai2025why].\n\n## Basic model\n\nConsider a multiple-choice question with $k$ options and a hidden correct answer $y^\\star$. The model observes an input $x$ and has a posterior distribution $p(y\\mid x)$ over options.\n\nThe user can take one of three actions:\n\n1. **Answer:** pick an option $\\hat y$.\n2. **Abstain:** do not answer (or defer to a safer outside option).\n3. **Verify (optional extension):** pay a cost $c$ to obtain the correct answer by some other means.\n\nFor now, summarize payoffs as constants:\n\n- Succeed (pick $\\hat y=y^\\star$): payoff $\\pi_s$.\n- Fail (pick $\\hat y\\neq y^\\star$): payoff $\\pi_f$.\n- Abstain (outside option): payoff $\\pi_a$.\n\nAssume $\\pi_s>\\pi_a>\\pi_f$: being right is best; abstaining is better than being wrong.\n\n### Outputs: binary, ternary, continuous\n\nWe can distinguish LLM \"answer formats\" by how much information they expose about $p(y\\mid x)$:\n\n- **Binary:** the model returns a single recommended option $\\hat y$.\n- **Ternary:** the model either returns $\\hat y$ or abstains.\n- **Continuous:** the model returns (an approximation to) the full distribution $p(y\\mid x)$ over options.\n\n## Claims\n\n1. **A single threshold organizes abstention.** If an \"attempt\" succeeds with probability $p$ and fails with probability $1-p$, then attempting has expected payoff\n   $$\n   \\mathrm{E}[\\pi\\mid \\text{attempt}] = p\\,\\pi_s + (1-p)\\,\\pi_f.\n   $$\n   Attempting is optimal iff $\\mathrm{E}[\\pi\\mid \\text{attempt}] \\ge \\pi_a$, i.e.\n   $$\n   p \\ge p^* \\equiv \\frac{\\pi_a-\\pi_f}{\\pi_s-\\pi_f}.\n   $$\n\n2. **Binary usefulness is convex in average accuracy.** A user with an outside option chooses to rely on a binary-output model only when $p\\ge p^*$. So the user's value is the max of an outside option and a linear function of $p$, which is convex.\n\n3. **Continuous output is (weakly) best.** If the model provides $p(y\\mid x)$, the user can compute the expected payoff of each action (answer/abstain/verify) and implement the optimal policy. Any coarser output (binary or ternary) throws away information, and cannot improve expected utility.\n\n4. **The simplex makes preferences and objectives visible.** In the Marschak-Machina simplex over $(p_s,p_f,p_a)$, user preferences correspond to indifference lines whose slope is determined by payoff ratios; training/evaluation objectives correspond to different directions in the same triangle.\n\n5. **Hallucinations are a consequence of the objective.** Accuracy-only training rewards and binary grading make abstention suboptimal and encourage guessing. Chow (1970) derives the optimal reject rule in this exact payoff model [@chow1970optimum]; Kalai et al. (2025) argue modern LLM pipelines effectively ignore this reject option and therefore pressure models to hallucinate [@kalai2025why].\n\n## From probabilities to actions\n\nFor a multiple-choice question, suppose the user answers by choosing the MAP option $\\hat y(x)=\\arg\\max_y p(y\\mid x)$. Under the symmetric payoff model above (only \"correct vs incorrect\" matters), the probability of success from attempting is\n$$\np_{\\max}(x)\\equiv \\max_y p(y\\mid x).\n$$\n\nSo the attempt-vs-abstain decision is driven by a single number: answer iff $p_{\\max}(x)\\ge p^*$.\n\nIf verification is available at cost $c$, one simple version is: verifying yields certain success with payoff $\\pi_s-c$. Then for each question the user compares three quantities:\n\n- Attempt: $p_{\\max}(x)\\,\\pi_s + (1-p_{\\max}(x))\\,\\pi_f$.\n- Abstain: $\\pi_a$.\n- Verify: $\\pi_s-c$.\n\nThis makes the \"payoff mis-specification\" point concrete: evaluation regimes that treat abstention as failure implicitly set $\\pi_a\\approx \\pi_f$, eliminating the region where \"don't answer\" is optimal.\n\n### Convex value of a binary-output model\n\nIn the most constrained interface, a binary-output model only returns $\\hat y$ and the user cannot condition on per-question confidence (they only know the model's average accuracy $p$ on the relevant distribution). Then the user's best policy is either to follow the model or to abstain, and the resulting value is\n$$\nV_{\\text{binary}}(p)=\\max\\Bigl\\{\\pi_a,\\; p\\,\\pi_s + (1-p)\\,\\pi_f\\Bigr\\}.\n$$\n\nThis is the maximum of two affine functions of $p$, so it is convex. It has a kink at the threshold $p=p^*$: below the threshold the user abstains and additional accuracy has (locally) zero value; above the threshold, value increases linearly with accuracy. This is one reason \"small accuracy gains\" can feel useless until a system crosses a reliability threshold.\n\n### Why continuous output is (weakly) best\n\nContinuous output (the distribution $p(y\\mid x)$, or any sufficiently rich summary like $(\\hat y,p_{\\max})$) lets the user implement the payoff-optimal policy question-by-question: answer only when it clears their $p^*$, abstain otherwise, and (if available) trigger verification in the middle region.\n\nBinary and ternary outputs are strict coarsenings of the posterior: they discard information about confidence. By a standard \"more information cannot hurt\" argument (Blackwell ordering), a user who observes a more informative signal can always simulate a less informative one by ignoring information, but not vice versa [@blackwell1953equivalent]. So a continuous interface is weakly better than any binary/ternary interface for any fixed payoff function.\n\n\n## Probability-Payoff Diagram\n\nThis figure visualizes the threshold rule: as confidence $p$ rises, the expected payoff of attempting rises linearly from $\\pi_f$ (when $p=0$) to $\\pi_s$ (when $p=1$). Abstaining yields the flat payoff $\\pi_a$. The optimal policy is to attempt when the blue line crosses the abstain line.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2026-02-26-hallucinations-and-alignment_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n\n### Optional extension: verification\n\nSuppose the user has a way to pay a cost $c$ to obtain the correct answer (e.g. look it up, run an expensive check, ask a human). In the simplest model, verification yields certain success with payoff $\\pi_s-c$.\n\nSince abstaining and verifying are both \"outside options\" (their payoff does not depend on the model's confidence), the only relevant outside-option payoff is\n$$\n\\pi_{\\text{outside}}=\\max\\{\\pi_a,\\;\\pi_s-c\\}.\n$$\n\nThe attempt rule is the same threshold logic as before: attempt iff\n$$\np \\ge \\frac{\\pi_{\\text{outside}}-\\pi_f}{\\pi_s-\\pi_f}.\n$$\n\nThe diagram below shows an example where verification dominates abstention ($\\pi_s-c>\\pi_a$): at low confidence the user verifies; only above a higher threshold does it become worth attempting directly.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2026-02-26-hallucinations-and-alignment_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n\n## Simplex Representation\n\nThe probability simplex has three vertices corresponding to the three pure outcomes: certain success ($p_s=1$), certain failure ($p_f=1$), and certain abstention ($p_a=1$). Any lottery over outcomes is a point in this triangle.\n\nDifferent training/evaluation objectives induce different indifference-curve sets over $(p_s,p_f,p_a)$. The figure below contrasts three illustrative objectives.\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2026-02-26-hallucinations-and-alignment_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\nEach point in the simplex is a lottery over outcomes: a model might succeed with probability $p_s$, fail with probability $p_f$, and abstain with probability $p_a$. The panels show three different objective families:\n\n- **Reward accuracy** ($U=p_s$): success is rewarded, but failure and abstention are treated the same. This creates pressure to guess rather than abstain.\n- **Punish failure** (linear expected utility): failure is explicitly penalized relative to abstention, expanding the region where abstaining is optimal.\n- **F1** (a non-linear metric): indifference curves bend, reflecting that the metric itself builds in a particular tradeoff between attempting and being correct.\n\n### Indifference-curve slope derivation\n\nLet $p_s, p_f, p_a = 1-p_s-p_f$ denote the probabilities of succeed, fail, and abstain. Expected utility is\n\n$$\nU = \\pi_s\\, p_s + \\pi_f\\, p_f + \\pi_a\\, p_a.\n$$\n\n1. Substitute $p_a = 1 - p_s - p_f$:\n   $$\n   \\begin{aligned}\n   U\n   &= \\pi_a \\\\\n   &\\quad+ (\\pi_s-\\pi_a)\\,p_s \\\\\n   &\\quad+ (\\pi_f-\\pi_a)\\,p_f.\n   \\end{aligned}\n   $$\n\n2. Hold $U = \\bar U$ and solve for $p_f$:\n   $$\n   \\begin{aligned}\n   p_f\n   &= \\frac{\\bar U - \\pi_a}{\\pi_f-\\pi_a}\n    - \\frac{\\pi_s-\\pi_a}{\\pi_f-\\pi_a}\\,p_s.\n   \\end{aligned}\n   $$\n\n3. The slope of the indifference curve in the $(p_s, p_f)$ plane is therefore:\n   $$\n   \\frac{dp_f}{dp_s}\\bigg|_{U=\\bar U}\n   = -\\frac{\\pi_s-\\pi_a}{\\pi_f-\\pi_a}.\n   $$\n\n4. Normalizing $\\pi_a=0$, this simplifies to:\n   $$\n   \\frac{dp_f}{dp_s}\\bigg|_{U=\\bar U}\n   = -\\frac{\\pi_s}{\\pi_f},\n   \\qquad\n   p_f = \\frac{\\bar U}{\\pi_f} - \\frac{\\pi_s}{\\pi_f}\\,p_s.\n   $$\n\nThe slope depends only on payoff differences relative to abstain. When failure is very costly ($|\\pi_f|$ large after normalizing $\\pi_a=0$), the curves are flatter: the decision-maker tolerates little additional failure probability in exchange for more success probability.\n\n## Related Literature\n\n### Brisk summary\n\n- **Reject option / selective prediction.** The classical Bayes-optimal abstention rule is Chow's reject option [@chow1970optimum]. Modern ML re-derives and extends it as \"selective classification\" and the risk-coverage tradeoff [@herbei2006reject; @bartlett2008reject; @elyaniv2010selective; @geifman2019selectivenet].\n- **Output format and information.** The \"continuous output is weakly best\" argument is an instance of the general fact that more informative signals cannot reduce expected utility (Blackwell order) [@blackwell1953equivalent].\n- **Probability forecasts and incentives.** Proper scoring rules provide a principled way to reward honest probability reports and encourage calibrated uncertainty estimates [@gneiting2007scoring].\n- **LLM self-evaluation.** Some work finds that language models can estimate the probability their own answers are correct (e.g. by producing a calibrated $P(\\text{True})$), which is exactly the quantity needed to implement threshold rules [@kadavath2022mostly].\n- **Distribution-free prediction sets.** Conformal prediction is a complementary route: it outputs sets (or abstains) with finite-sample coverage guarantees and can be layered on top of any base model [@shafer2008conformal; @angelopoulos2021gentle].\n\n### Chow (1970): Optimal reject rules\n\n@chow1970optimum introduces the **reject option** (which he calls the \"Indecision class\" $I$) into pattern recognition and derives the optimal error-reject tradeoff. Chow's terminology maps directly onto ours:\n\n| Chow's term | Our term |\n|---|---|\n| correct recognition | succeed |\n| error (misclassification) | fail |\n| rejection / indecision | abstain |\n\nChow's setup adds a third action (reject) to ordinary classification. In one common normalization, the costs are $0$ for a correct classification, $1$ for an error, and $t$ for a rejection.\n\nThe key result (\"Chow's rule\") is a posterior-threshold rule: accept and classify when confident enough, otherwise reject:\n$$\n\\max_i P(G_i \\mid x) \\ge 1-t\n\\quad\\Rightarrow\\quad \\text{accept;}\n\\qquad\n\\max_i P(G_i \\mid x) < 1-t\n\\quad\\Rightarrow\\quad \\text{reject.}\n$$\n\nIt is optimal in the sense that, for a given rejection threshold (equivalently, a given reject rate), no other rule achieves a lower error rate.\n\nThe threshold $t$ is related to the costs of the three outcomes as $t = (C_r - C_c)/(C_e - C_c)$, where $C_e, C_r, C_c$ are the costs of error, rejection, and correct recognition. In our payoff notation $(\\pi_s,\\pi_f,\\pi_a)$, Chow's rule becomes: predict iff\n\n$$\n\\max_y P(y \\mid x) \\ge \\frac{\\pi_a - \\pi_f}{\\pi_s - \\pi_f}.\n$$\n\nIn the Marschak-Machina triangle, this threshold corresponds to one of the indifference lines: the boundary between the region where prediction is preferred and the region where abstention is preferred.\n\n\n### Kalai, Nachum, Vempala, and Zhang (2025): Why language models hallucinate\n\n@kalai2025why argue that hallucinations are not a mysterious glitch but a predictable consequence of how models are trained and evaluated. Their central thesis is that the three-outcome structure (succeed, fail, abstain) is systematically distorted by binary evaluation:\n\nThe paper makes two distinct arguments:\n\n**1. Pretraining origin.** Even with error-free training data, the statistical objective of pretraining produces hallucinations. The authors reduce the problem to binary classification (\"Is-It-Valid\"), showing that\n\n$$\n\\text{generative error rate} \\gtrsim 2 \\cdot \\text{IIV misclassification rate}.\n$$\n\nFor arbitrary facts (like someone's birthday) where there is no learnable pattern, the hallucination rate after pretraining is at least the fraction of facts appearing exactly once in the training data.\n\n**2. Post-training persistence.** Even after RLHF and other interventions, hallucinations persist because nearly all evaluation benchmarks use binary grading that penalizes abstention:\n\nThe fix they propose is exactly the payoff structure from our Marschak-Machina framework: penalize errors more than abstentions, with an explicit confidence threshold $t$ stated in the prompt. Their proposed scoring rule awards $+1$ for a correct answer, $-t/(1-t)$ for an incorrect answer, and $0$ for abstaining---so that answering is optimal iff confidence exceeds $t$. This is Chow's reject-option rule rediscovered in the LLM evaluation context.\n\n### Selective classification (risk-coverage tradeoffs)\n\nIn modern ML, the reject option is often framed as **selective classification**: a predictor chooses a subset of examples to label (\"coverage\") and aims to minimize error on that subset (\"risk\"). This is the same three-outcome structure in different words: success/failure live inside the covered set, and abstention is the complement.\n\n- @elyaniv2010selective develops foundations for selective classification and emphasizes the risk-coverage tradeoff.\n- @herbei2006reject and @bartlett2008reject study classification with a reject option and how to learn it via loss minimization.\n- @geifman2019selectivenet shows one way to build the reject option directly into deep networks by jointly learning a predictor and a selection function.\n\nThe important connection to this post is: once you make abstention explicit, the \"right\" threshold is not a moral property of the model; it is the decision boundary induced by payoffs.\n\n### Calibration and scoring rules\n\nIf you want a continuous interface (probabilities) to be useful, you need those probabilities to mean something. Proper scoring rules give a principled way to reward honest probability forecasts and thereby encourage calibration [@gneiting2007scoring]. Empirically, some work finds that language models can often estimate the probability their own answers are correct, which is exactly the signal a user needs to implement a threshold rule [@kadavath2022mostly].\n\n### Conformal prediction (prediction sets)\n\nConformal prediction is a complementary approach: instead of outputting a single label, it outputs a **set** of plausible labels with finite-sample coverage guarantees. Large sets correspond to \"I don't know\" behavior; singletons correspond to confident predictions. This provides a distribution-free way to control error rates of the attempted answers, given a calibration set [@shafer2008conformal; @angelopoulos2021gentle].\n\n## Implications for \"alignment\"\n\nIf you view hallucinations through this lens, \"alignment\" is not a mysterious property of model internals. It is the much more mundane question: does the model's training and evaluation objective implement the user's payoff function?\n\nPractical implications:\n\n- **Expose confidence.** If the user cannot observe confidence, they cannot implement the threshold rule; binary outputs force guessing.\n- **Score abstention explicitly.** Benchmarks that collapse abstain into \"wrong\" implicitly set $\\pi_a=\\pi_f$ and will select for guessing.\n- **Prefer calibrated probabilities or verification hooks.** Either give users a usable $p_{\\max}$, or route medium-confidence cases to a verification workflow.\n\n## Appendix: Cross-Benchmark Outcome Table\n\nTo make cross-model plotting easier, the table below standardizes outputs from multiple benchmarks into a common schema.\n\n| benchmark | model | p_s_pct | p_f_pct | p_a_pct |\n|---|---|---:|---:|---:|\n| SimpleQA | Claude-3-haiku (2024-03-07) | 5.1 | 19.6 | 75.3 |\n| SimpleQA | Claude-3-sonnet (2024-02-29) | 5.7 | 19.3 | 75.0 |\n| SimpleQA | Claude-3-opus (2024-02-29) | 23.5 | 36.9 | 39.6 |\n| SimpleQA | Claude-3.5-sonnet (2024-06-20) | 28.9 | 36.1 | 35.0 |\n| SimpleQA | GPT-4o-mini | 8.6 | 90.5 | 0.9 |\n| SimpleQA | GPT-4o | 38.2 | 60.8 | 1.0 |\n| SimpleQA | OpenAI o1-mini | 8.1 | 63.4 | 28.5 |\n| SimpleQA | OpenAI o1-preview | 42.7 | 48.1 | 9.2 |\n| AbstentionBench | DeepSeek R1 Distill Llama 70B | 43.7 | 10.3 | 46.0 |\n| AbstentionBench | o1 | 27.2 | 6.8 | 66.0 |\n| AbstentionBench | S1.1 32B | 45.6 | 11.4 | 43.0 |\n| AbstentionBench | Llama 3.1 70B Tulu 3 DPO | 26.1 | 6.9 | 67.0 |\n| AbstentionBench | Llama 3.1 70B Tulu 3 PPO RLVF | 26.9 | 7.1 | 66.0 |\n| AbstentionBench | Llama 3.3 70B Instruct | 26.5 | 7.5 | 66.0 |\n| AbstentionBench | Gemini 1.5 Pro | 25.4 | 7.6 | 67.0 |\n| AbstentionBench | GPT-4o | 23.3 | 7.8 | 69.0 |\n| AbstentionBench | Qwen2.5 32B | 21.8 | 7.3 | 71.0 |\n| AbstentionBench | Llama 3.1 8B Tulu 3 PPO RLVF | 36.8 | 12.3 | 51.0 |\n| AbstentionBench | Llama 3.1 405B Instruct | 23.7 | 8.3 | 68.0 |\n| AbstentionBench | Llama 3.1 8B Tulu 3 DPO | 34.8 | 12.2 | 53.0 |\n| AbstentionBench | Llama 3.1 70B Instruct | 26.6 | 9.4 | 64.0 |\n| AbstentionBench | Llama 3.1 70B Tulu 3 SFT | 30.1 | 12.9 | 57.0 |\n| AbstentionBench | Llama 3.1 8B Instruct | 23.8 | 10.2 | 66.0 |\n| AbstentionBench | Mistral 7B v0.3 | 25.5 | 11.5 | 63.0 |\n| AbstentionBench | Llama 3.1 8B Tulu 3 SFT | 37.1 | 20.0 | 43.0 |\n| AbstentionBench | OLMo 7B | 25.8 | 20.2 | 54.0 |\n| AbstentionBench | Llama 3.1 70B Base | 25.5 | 25.5 | 49.0 |\n| AbstentionBench | Llama 3.1 8B Base | 23.5 | 32.5 | 44.0 |\n| Abstain-QA | GPT-4 Turbo | 66.1 | 19.7 | 14.2 |\n| Abstain-QA | GPT-4 32K | 72.0 | 19.1 | 8.9 |\n| Abstain-QA | GPT-3.5 Turbo | 61.1 | 37.4 | 1.5 |\n| Abstain-QA | Mixtral 8x7b | 54.1 | 37.0 | 8.9 |\n| Abstain-QA | Mixtral 8x22b | 59.0 | 29.1 | 11.9 |\n\nNotes:\n- Table columns are constructed to look like probabilities in the $(p_s,p_f,p_a)$ simplex: $p_s=\\text{p\\_s\\_pct}/100$, $p_f=\\text{p\\_f\\_pct}/100$, $p_a=\\text{p\\_a\\_pct}/100$.\n- For SimpleQA, these correspond directly to {Correct, Incorrect, Not attempted} shares.\n- For the other benchmarks, these are *constructed* from the paper's summary metrics to enable geometric comparisons; interpret them as a mapping into a common coordinate system, not as identical underlying evaluation protocols.\n\n### Data extraction details by source\n\n**SimpleQA (Wei et al., 2024).**\nThe table uses all model rows shown in the main SimpleQA model-comparison table (8 models). Here, `p_s_pct` is **Correct**, `p_a_pct` is **Not attempted**, and `p_f_pct` is computed as $100-\\text{Correct}-\\text{Not attempted}$. I chose this slice because it is the paper's canonical cross-model summary and directly exposes explicit non-attempt behavior.\n\n**AbstentionBench (Kirichenko et al., 2025).**\nThe table uses all model rows from Appendix D Table 4 (20 models). The paper reports **Average Accuracy** and **Average Abstention Recall**. I set\n$p_{a,\\%} = \\text{Average Abstention Recall}$, interpret $100-p_a$ as an attempt-like rate, and construct\n$$\np_s \\approx \\text{AvgAcc}\\cdot(1-p_a),\\qquad\np_f \\approx (1-\\text{AvgAcc})\\cdot(1-p_a),\n$$\nall expressed in percent. This is not a literal per-question abstain rate; it's a mapping for geometric intuition using the only aggregate abstention signal reported in the table.\n\n**Abstain-QA (Madhusudhan et al., 2024).**\nThis is a deliberate subset, not all values in the paper: I take the **MMLU / Standard clause / Base** rows (5 models) from the main result table. The paper reports **AAC** (answerable accuracy) and **AR** (abstention rate). I set $p_{a,\\%}=\\text{AR}$ and construct $p_{s,\\%}$ and $p_{f,\\%}$ by treating AAC as attempt-conditional accuracy:\n$$\np_s = \\text{AAC}\\cdot(1-p_a),\\qquad\np_f = (1-\\text{AAC})\\cdot(1-p_a),\n$$\nall expressed in percent.\n\n**Important comparability caveats.**\nEven after mapping all results into $(p_s,p_f,p_a)$ coordinates, the underlying tasks and abstention metrics differ: SimpleQA is short-form factual QA with optional non-attempts, AbstentionBench is an abstention-focused multi-scenario benchmark with recall-style abstention metrics, and Abstain-QA is multiple-choice QA with an explicit IDK/NOTA option. So the combined table is useful for geometric intuition and directional comparisons, but not for strict leaderboard ranking across benchmarks.\n\nSources:\n¹ [SimpleQA: Measuring short-form factuality in large language models](https://arxiv.org/pdf/2411.04368)  \n² [AbstentionBench: Reasoning LLMs Fail on Unanswerable Questions](https://arxiv.org/html/2506.09038v1)  \n³ [Do LLMs Know When to NOT Answer? Investigating Abstention Abilities of Large Language Models](https://arxiv.org/html/2407.16221)\n\n## Appendix: Benchmark Points in the Simplex\n\nThe next three figures plot benchmark observations as points on a Marschak-Machina simplex using a common transformation from the table columns:\n\n$$\np_s = \\text{p\\_s\\_pct}/100,\\qquad\np_f = \\text{p\\_f\\_pct}/100,\\qquad\np_a = \\text{p\\_a\\_pct}/100.\n$$\n\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2026-02-26-hallucinations-and-alignment_files/figure-html/unnamed-chunk-4-1.png){width=672}\n:::\n:::\n\n\n\n\n### Reading the simplex plots\n\n1. The horizontal axis is $p_s$ (succeed share) and the vertical axis is $p_f$ (fail share). The remaining probability is $p_a=1-p_s-p_f$ (abstain share), so points closer to the diagonal edge have lower abstention.\n\n2. Moving down (lower $p_f$) corresponds to reducing failures; whether that is best depends on how much worse failure is than abstention ($\\pi_f$ vs $\\pi_a$).\n\n3. The labeled points illustrate that a model can look good under an \"answer-everything\" regime by pushing $p_a$ toward zero, but that is exactly the regime that a payoff function with a harsh $\\pi_f$ would discourage.\n\n4. Don't over-interpret cross-benchmark comparisons: each source defines abstention differently, so these plots are best read as a geometric visualization of tradeoffs, not as a single unified leaderboard.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}