{
  "hash": "c9ac0abe505c1501a1c721dec5afdc43",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Hallucinations and Alignment\ndraft: true\nengine: knitr\nbibliography: ai.bib\n---\n\n\n\n\n## TikZ Diagrams\n\n### Probability-Payoff Diagram\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2026-02-26-hallucinations-and-alignment_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n\n\n### Marschak-Machina Diagram\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](2026-02-26-hallucinations-and-alignment_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n### Indifference-Curve Slope Derivation\n\nLet $p_r := p_{\\text{right}}$, $p_w := p_{\\text{wrong}}$, and $p_a := p_{\\text{abstain}} = 1 - p_r - p_w$.  \nSuppose expected utility is\n$$\nU = \\pi_{\\text{succeed}} p_r + \\pi_{\\text{fail}} p_w + \\pi_{\\text{abstain}} p_a.\n$$\nSubstitute $p_a = 1 - p_r - p_w$:\n$$\nU = \\pi_{\\text{abstain}}\n  + (\\pi_{\\text{succeed}}-\\pi_{\\text{abstain}})p_r\n  + (\\pi_{\\text{fail}}-\\pi_{\\text{abstain}})p_w.\n$$\nHolding utility fixed at level $\\bar U$ gives the indifference curve:\n$$\np_w\n= \\frac{\\bar U-\\pi_{\\text{abstain}}}{\\pi_{\\text{fail}}-\\pi_{\\text{abstain}}}\n- \\frac{\\pi_{\\text{succeed}}-\\pi_{\\text{abstain}}}{\\pi_{\\text{fail}}-\\pi_{\\text{abstain}}}\\,p_r.\n$$\nSo the slope in the $(p_r,p_w)$ diagram is\n$$\n\\frac{dp_w}{dp_r}\\Big|_{U=\\bar U}\n= -\\frac{\\pi_{\\text{succeed}}-\\pi_{\\text{abstain}}}{\\pi_{\\text{fail}}-\\pi_{\\text{abstain}}}.\n$$\nThis makes explicit that the slope depends on payoff differences relative to abstain: changing $\\pi_{\\text{succeed}}$, $\\pi_{\\text{fail}}$, or $\\pi_{\\text{abstain}}$ changes the slope.\n\nIf we normalize $\\pi_{\\text{abstain}}=0$, this becomes\n$$\n\\frac{dp_w}{dp_r}\\Big|_{U=\\bar U}\n= -\\frac{\\pi_{\\text{succeed}}}{\\pi_{\\text{fail}}},\n$$\nwith indifference lines\n$$\np_w = \\frac{\\bar U}{\\pi_{\\text{fail}}} - \\frac{\\pi_{\\text{succeed}}}{\\pi_{\\text{fail}}}\\,p_r.\n$$\n\n\n### Chow (1970): Reject Option\n\nChow analyzes optimal classification when \"reject/abstain\" is allowed as a third action alongside class predictions @chow1970optimum.\nThe key result is a posterior-threshold rule: predict only when posterior confidence is high enough; otherwise abstain.\n\nWith symmetric 0-1 classification payoff (correct payoff $1$, wrong payoff $0$) and abstain payoff $\\pi_{\\text{abstain}} \\in (0,1)$, the rule is:\n$$\n\\max_y P(y \\mid x) \\ge \\pi_{\\text{abstain}}\n\\quad\\Longrightarrow\\quad\n\\text{predict } \\arg\\max_y P(y \\mid x),\n$$\nand abstain otherwise.\n\nAn equivalent loss version (mistake loss $1$, reject loss $d$) is:\n$$\n\\max_y P(y \\mid x) \\ge 1-d\n\\quad\\Longrightarrow\\quad\n\\text{predict}.\n$$\n\nIn the Marschak-Machina triangle, this is exactly a comparison of expected prediction payoff against $\\pi_{\\text{abstain}}$, so Chow's threshold boundary corresponds to an indifference line.\n\nRecent theory in the LLM setting proves a complementary point: even calibrated language models must hallucinate at a nonzero rate, which motivates explicit abstain/verification policies rather than prediction-only objectives @kalai2024calibrated.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}