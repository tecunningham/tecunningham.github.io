{
  "hash": "2af12597cbfe276233547c833e7cc569",
  "result": {
    "markdown": "---\ntitle: How Much has Social Media affected Polarization?\nauthor: Tom Cunningham\ndate: 2023-08-04\nexecute:\n  echo: false\n  cache: true # caches chunk output\nfig-align: center\nbibliography: social-media.bib\n#csl: nature.csl\nreference-location: margin\n#citation-location: margin\nformat:\n   html:\n      toc: true\n      toc-depth: 2\n      toc-location: left\n      code-fold: true\n      html-math-method:\n         method: mathjax\n         url: \"https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg-full.js\"\n         #     ^ this forces SVG instead of CHTML, otherwise xypic renders weird\n      include-in-header:\n         - text: |\n            <script>window.MathJax = {\n                     loader: { load: ['[custom]/xypic.js'],\n                                 paths: {custom: 'https://cdn.jsdelivr.net/gh/sonoisa/XyJax-v3@3.0.1/build/'}},\n                  tex: {packages: {'[+]': ['xypic']},\n                     macros: {\n                        bm: [\"\\\\boldsymbol{#1}\", 1],\n                        ut: [\"\\\\underbrace{#1}_{\\\\text{#2}}\", 2],\n                        utt: [\"\\\\underbrace{#1}_{\\\\substack{\\\\text{#2}\\\\\\\\\\\\text{#3}}}\", 3]\n                     }}};\n            </script>\n\nengine: knitr\ndraft: true\neditor:\n   render-on-save: true\n---\n\n<style>\n    h1 {  border-bottom: 4px solid black;}\n    h2 {  border-bottom: 1px solid gray; padding-bottom: 0px; font-size: 14px; color: black; }\n</style>\n\n<!-- https://tecunningham.github.io/posts/2023-07-27-meta-2020-elections-experiments.html -->\n\n<!-- <span style=\"background:yellow;\">==Draft, don't circulate yet please.==</span> -->\n\n#                    Summary\n\n\n::: {.column-margin}\n   ![](images/2023-08-04-13-59-52.png)\n   Thanks to Dean Eckles, Solomon Messing, Jeff Allen, & Brandon Silverman for discussion which led to this post. I put together the [spreadsheet summary of results](https://docs.google.com/spreadsheets/d/1_96kEzP9MFLcBFppVV0Bl7O3Cv9hQFxKArwS2zVCtXE/edit#gid=0) with Dean and Solomon.\n:::\n\n**Changing Facebook's feed ranking algorithm for 1.5 months moves a user's affective polarization by less than 0.03 standard deviations.** This is small compared to a growth of 1.1 standard deviations in nationwide affective polarization over the last 40 years.^[@allcott2019trends]\n\n**What can we learn from this?** In this note I discuss how we can extrapolate these estimates to other effects of interest, specifically the aggregate impact of social media on the US over the last 20 years.\n\n**Small effects in experiments are consistent with large effects in aggregate.** The aggregate contribution of social media to polarization will differ from these experimental estimates in a number of ways I discuss below: depth, breadth, duration, timing, and category. My rough attempts to account for these considerations make me think the aggregate effect is likely 10 or 20 times larger than the effects that would be measured in these experiments, and so small measured effects are consistent with large aggregate effects.\n\nPut simply: these experiments measure the effect of reducing exposure of an individual user (not their friends and family) to political content on Facebook by 15% for 1.5 months, and occurred in a period after Facebook had already sharply reduced the amount of partisan content circulating. Thus we should expect them to measure only a small fraction of the cumulative impact of social media, and in fact these results are consistent with social media being the sole driver of the growth of polarization in the US.\n\n**Nevertheless other evidence implies that social media has probably not made a huge contribution to US polarization.** If we wish to evaluate the balance of evidence relating social media to polarization there are many other sources which are probably more informative than these experiments. I give a rough sketch below and it seems to me social media probably does not account for a majority share, mainly because (1) polarization had been growing for 20 years prior to social media's introduction, and much of the growth since 2014 was in people without internet access; (2) a lot of partisan discourse continues to spread outside of social media, e.g. through cable TV and talk radio; (3) other countries do not show a similar increase in affective polarization.\n\n\n\n<!-- methodology: filling out cells -->\n\n#                    The Experiments\n\n**Last week's papers reported the results of three experiments on Facebook's News Feed.** The experiments were run between September and December 2020, and half-way through participants were asked about their feelings towards members of their own party and the opposing party, e.g. *\"how warm do you feel about Republicans on a scale of 0-100?\"*[^timing] The answers were aggregated to make an index of \"affective polarization\":\n   $$\\xymatrix@R=0em@C=6em{\n      *+[F:<5pt>]\\txt{rank items on News\\\\Feed chronologically}  \\ar[dr] & \\\\\n      *+[F:<5pt>]\\txt{remove reshares\\\\on News Feed}  \\ar[r] & \n         *+[F:<5pt>]\\txt{affective\\\\polarization\\\\survey}\\\\\n      *+[F:<5pt>]\\txt{downrank likeminded\\\\items on News Feed}  \\ar[ur]\n      }\n   $$\n\n**All three experiments found effects on polarization of less than 0.03 standard deviations (SDs).** The 95% confidence intervals on affective polarization are approximately $\\pm$ 0.03 SDs, and the effect-sizes are all smaller than that (i.e. they do not estimate a significant effect). Dean Eckles, Solomon Messing, and myself put together a [spreadsheet summary](https://docs.google.com/spreadsheets/d/1_96kEzP9MFLcBFppVV0Bl7O3Cv9hQFxKArwS2zVCtXE/edit#gid=0) of the results from all the experiments reported so far, along with other results from the literature on political effects of media.\n\nThey also measured effects on a number of other off-platform outcomes: removing reshares did lower news knowledge by 0.07 standard deviations, but all other outcomes (factual discernment, issue polarization, perceived legitimacy, self-reported turnout) were not significant, and had similar-sized confidence intervals.\n\n<!-- Over the last 20 years affective polarization in the US has increased by roughly 0.5 SD, so if we compare the raw effects then our CIs would say FB could not have contributed more than 1/25th of this increase. -->\n\n\n<!-- **All three experiments found effects on polarization of less than 0.03 standard deviations (SDs).** The estimated effect sizes were all smaller than the width of the 95% confidence intervals, around 0.03 SDs. -->\n\n\n <!-- @guess2023chronological concludes: \n \n   > \"these findings suggest that social media algorithms may not be the root cause of phenomena such as increasing political polarization.\"\n   -->\n\n   <!-- (@boxell2022PolarizationTrends estimate that US polarization has increased by around 0.3 SDs since 2014, when Facebook and other social media companies became widespread). -->\n\n[^timing]: Although the treatments ran for 3 months (24 Sep--23 Dec 2020), the survey responses were collected during the experiment and the average survey measure was measured after around 1.5 months of treatment: see Figure S2 in the Supplementary Appendix.\n\n#                    Extrapolating to the Cumulative Effect of Social Media\n\n**Many people have interpreted these results as implying that social media has not had much effect on overall polarization.** E.g. one of the experimental papers says:^[@guess2023chronological]\n \n   > \"these findings suggest that social media algorithms may not be the root cause of phenomena such as increasing political polarization.\"\n\n**Here I try to extrapolate from these experiments to the long-run aggregate effect of social media.** The comparison is between two extremes but there are a lot of other intermediate estimands that we could alternatively use, e.g. the effect of permanantly disabling just Facebook for everybody, or the effect of temporarily disabling all social networks for an individual user.\n\n   These are difficult judgment calls. I have tried my best to be neutral and discuss evidence on either side but it's likely I'm forgetting some important considerations.\n\n**I work through five ways in which the experimental results will differ from the aggregate impact on social media:**\n\n   1. **Depth.** Whether changing one feature or disabling the app entirely.\n   2. **Breadth.** Whether changing the experience for one user or for all users.\n   3. **Duration.** Whether changing the experience for 1.5 months or for the whole history of social media.\n   4. **Timing.** Whether changing the experience in Oct 2020, or the average effect over 2014-2020.\n   5. **Category.** Whether changing the experience just for Facebook or for all social media.\n\nI try to give quantitative estimates for each of these five differences, and it makes me think that having tight confidence intervals on the effects of the experiments (plus or minus 0.03 SDs) is still consistent with broad confidence intervals on the aggregate effect of social media (plus or minus 1 SDs).\n\n\n**(1) *Depth*: the experiments have small effects on exposure.** Each of the experiments reported have effects on overall Facebook time-spent of less than 25%, and on exposure to political material of less than 15%. Thus the effect of complete withdrawal from Facebook seems likely to be at least 2X larger than measured by any of these experiments. The most natural causal path from Facebook use to polarization is exposure to partisan or misleading political media. An additional experiment was run which deactivated peoples' accounts but the results from that experiment are not yet public (as of August 4).\n\n   | effects on metric[^fn]      | time-spent | political impression | cross-cutting impressions | untrustworthy impressions |\n   | --------------------------- | ---------- | -------------------- | ------------------------- | ------------------------- |\n   | baseline                    | ?          | 14pp                 | 21pp                      | 3pp                       |\n   |                             |            |                      |                           |                           |\n   | - rank chronologically      | -21%       | +12%                 | -10%                      | +60%                      |\n   | - remove reshares           | -5%        | -14%                 | -3%                       | -32%                      |\n   | - downrank likeminded posts | -0%        | -5%                  | +7%                       | ?                         |\n\n   [^fn]: [source data](https://docs.google.com/spreadsheets/d/1_96kEzP9MFLcBFppVV0Bl7O3Cv9hQFxKArwS2zVCtXE/edit#gid=0).\n\n   <!-- (Note the deactivation experiment reported in @allcott2020welfare reduces their measure of \"polarization\" by 0.16 SD significantly, however this mainly driven by responses to a question about \"exposure to congenial news\", while the responses to the \"affective polarization\" do not move statistically significantly). -->\n\n**(2) *Breadth*: Experiments exclude network effects.** The effects of social media on polarization likely work not just through direct exposure but also downstream via peoples' interactions with friends and families, in both online and offline conversations. Thus it seems likely the aggregate effect could be 2X or larger than the individual effect.\n\n**(3) *Duration*: the experiments only measure short-run effects.** These experiments measured the effect of a News Feed change on polarization after around 1.5-2 months, while most American adults have been using Facebook for perhaps 10 years.^[The polarization survey was run during the experiments, I believe the average outcome was mesaured around 1.5 months after the start of the experiment.] It is hard to judge how quickly we should expect polarization attitudes to respond to treatment, and I have not found useful academic literature. The national polarization trends documented in @allcott2019trends seem fairly stable despite a volatile news cycle suggesting attitudes change relatively slowly. If the half-life of adjustment was 1.5 months (which seems quite short to me) then the effects measured in these experiments would be half of the long-run effect. It seems likely that the effects of exposure do not decay at a constant rate: there is a short-run component that decays quickly (the effect of salience), and a long-run component that decays slowly.^[I could not find any discussion of dynamic effects of these experiments in any of the 3 papers or supplementary appendices, I think this is a shame because (1) there is good reason to expect the cumulative treatment effects will dramatically change over time, (2) the experiments are sufficiently well-powered that dynamic effects should be easy to observe.]\n\n **(4) *Timing*: Experiments were run during the lead-up to the 2020 election.** The experiments ran between September and November 2020. If we compare this to the average experience on Facebook over the previous decade there are reasons to expect relatively smaller effects on polarization:\n   \n   - Following the 2016 elections facebook invested very heavily in integrity systems reducing prevalence of many types of bad content by factors of between 2X and 10X, especially misinformation and hyperpartisan political content. In May 2020 Guy Rosen [claimed](https://about.fb.com/news/2020/05/investments-to-fight-polarization/) that Facebook had made \"a number of important steps to reduce the amount of content that could drive polarization on our platform\" over the prior years.\n   - @allcott2019trends estimates that exposure to misinformation on Facebook (measured by data on engagement with domains known to host misinformation) grew over 2015 and 2016, roughly doubling, then fell over 2017 and 2018, roughly halving. The data ends at the end of 2018 but I believe the trend would continue downward.\n      ![](images/2023-08-02-16-19-16.png)\n   \n   - Meta's Community Standards reports show a decline in prevalence of most types of harmful content by a factor of between 2 and 5 over roughly 2017 to 2022 (see chart [here](https://tecunningham.github.io/posts/2023-01-31-social-media-suspensions-data.html#meta-facebook-instagram)). \n   - Prior to and during the 2020 election Facebook implemented a series of extra \"break the glass\" measures with the effect of suppressing extreme or fringe political content. \n   - During election seasons there tends to be significantly more political content circulating. This might mean that Facebook would have a relatively larger impact on polarization in this period. However if the influence of social media on polarization depends on the *share* of exposure to partisan or polarizing content (rather than the level) then the effect would be the same in election season as outside election season.\n\n   In fact 2020 Facebook has further reduced the prevalence of political and fringe content since 2020:\n   \n   - The share of politics on News Feed was reduced by 50%.^[[The WSJ](https://www.wsj.com/articles/facebook-politics-controls-zuckerberg-meta-11672929976) reported that in late 2021 *\"Mr. Zuckerberg and the board chose the most drastic [option], instructing the company to demote posts on “sensitive” [(politics and health)] topics as much as possible in the newsfeed that greets users when they open the app\"*, and that in 2022 *\"politics accounts for less than 3% of total content views in users’ newsfeed, down from 6% around the time of the 2020 election.\"* The article reports that these experiments reduced daily visitation (daily active users) by 0.2%.]\n   - Prevalence of hate-speech fell by factor of 5 between 2020 and 2022 (from 0.1% to 0.02%). ([ref](https://transparency.fb.com/data/community-standards-enforcement/hate-speech/facebook/)).\n   - Engagement on US right-wing politics pages has fallen by factor of 4 from 2021-2022 ([ref](https://fwiwnewsletter.substack.com/p/has-facebook-dialed-down-the-conservative)).\n   - Prevalence of engagement-bait among the top 20 most-viewed posts went from 100% to 5% between 2021Q3 to 2022Q3 ([ref](https://www.wsj.com/articles/facebooks-most-popular-posts-were-trash-here-is-how-it-cleaned-up-11669140034)).\n\n**(5) *Category*: The experiments affected only Facebook, but in 2020 Facebook probably accounted for around 1/4 of all partisan political content that people are exposed to on social media.** If we include YouTube, TikTok, Instagram, Twitter, Snapchat, Reddit, and the long tail of niche social networks.\n\n**(6) *Population*: The experiments measure outcomes only on Facebook users.** I believe that the \"population average treatment effects\" reported in the papers are weighted to match the Facebook-using population, not the voting population. This would be a reason for the experimental effect-size to be larger than the aggregate effect. I would guess around 2/3 of the US adult population is active once/month on Facebook, and so the aggregate effect-size could be smaller by that factor.\n\n**Putting it all together.** If factors 1-5 each contributed a 2X amplification, as well as factor 6 contributing a 2/3 shrinkage, then the cumulative effect of social media on polarization until 2020 would be 20X larger than the experimentally-measured effect, i.e. effective confidence intervals would be 0.6 SDs instead of 0.03 SDs. In other words these experiments would not be sufficiently well-powered to rule out social media being responsible for the *entire* growth of polarization since 2014.\n\n#                    Other Evidence on Media and Polarization [UNFINISHED]\n\n<!-- **Other experiments on social media and polarization.** The only other thorough experiment  -->\n\n**Trends in affective polarization.** @boxell2022PolarizationTrends document affective polarization across a dozen countries, 1978-2020:\n\n   ![](images/2023-07-27-15-03-32.png)\n\n   1. In the US affective polarization index increased from around 25 to 50, *\"an increase of 1.08 standard deviations as measured in the 1978 distribution.\"*  (I'm not sure if the SD increased).\n\n   2. Across the world there's no clear trend: some countries increased, other countries decreased. This weakens the simple argument that polarization has increased at the same time as social media use.\n\n   3. In the US the trend seems to be entirely due to increasing negative feelings about the opposing party:\n      ![](images/2023-08-03-12-51-42.png)\n\n   The US timeseries can be seen [online](https://electionstudies.org/data-tools/anes-guide/anes-guide.html?chart=affective_polarization_parties) from the [ANES](https://electionstudies.org).\n\n**Observational data finds that much of the growth in polarization in the US was among people who were not online.** @boxell2017greater say \n\n   > \"the growth in polarization in recent years [1996-2012] is largest for the demographic groups least likely to use the internet and social media\"\n\n**Ballpark estimates for share of exposure.** These are extremely crude estimates of affective polarization scores (from @boxell2022PolarizationTrends) and average time spent exposed to various types of content.\n\n|                              | 2004 | 2016 | 2020 |\n| ---------------------------- | :--: | :--: | :--: |\n| affective polarization score |  40  |  45  |  50  |\n|                              |      |      |      |\n| minutes/day/adult            |      |      |      |\n| - media                      | 300  | 300  | 300  |\n| - media civic                |  15  |  15  |  15  |\n| - media civic partisan       |  5   |  8   |  6   |\n| - Facebook                   |  0   |  60  |  60  |\n| - Facebook civic             |  0   |  6   |  3   |\n| - Facebook civic partisan    |  0   |  3   |  1   |\n|                              |      |      |      |\n\n**Content on Meta platforms.** @guess2023chronological has data from the control group in their 2020 experiments:\n\n   | Share of Impressions               | Facebook | Instagram |\n   | ---------------------------------- | -------- | --------- |\n   | Political content                  | 14%      | 5%        |\n   | Political news content             | 6%       | -         |\n   | Content from untrustworthy sources | 3%       | 1%        |\n   | Uncivil content                    | 3%       | 2%        |\n\n[Pew 2022](https://www.pewresearch.org/journalism/fact-sheet/news-platform-fact-sheet/?tabId=tab-4ef8dece-845a-4b25-8637-ceb3114503c5) has data on where people get their news from:\n\n   |               | pct adults regularly get news from |\n   | ------------- | ---------------------------------- |\n   | television    | 65%                                |\n   | news websites | 63%                                |\n   | search        | 60%                                |\n   | social media  | 50%                                |\n   | radio         | 47%                                |\n   | print         | 33%                                |\n   | podcasts      | 23%                               |\n\n**Radio show popularity.** Around half of the top 20 most-listened radio shows in the US are conservative talk, with around 90M weekly listeners (this is double-counting overlapping users). [Data from 2021](https://en.wikipedia.org/wiki/List_of_most-listened-to_radio_programs).\n\n**Television.** Fox News is Cable TV's most-watched network with around 5M regular viewers. ([source from 2016](http://www.adweek.com/tvnewser/2016-ratings-fox-news-channel-is-cable-tvs-most-watched-network/315009)).\n\n**Time spent on social media.** [Statista](https://www.statista.com/statistics/433871/daily-social-media-usage-worldwide/): Average time-spent 150 minutes/day/person on social networks\n\n**The academic literature has identified other possible causes of polarization.** Some potential causes: southern realignment, 1968 changes to the primary system, the Obama presidency, the tea party movement (though each of these could be in part proximal causes). Martin & Yurcoglu (2017) argue that a large part of recent growth is due to cable news: \n   > \"the cable news channels can explain an increase in political polarization of similar size to that observed in the US population over [2000-2008]. ... In absolute terms, however, this increase is fairly small.\"\n\nSee also Haidt and Bail's long document [Social Media and Political Dysfunction: A Collaborative Review](https://docs.google.com/document/d/1vVAtMCQnz8WVxtSNQev_e1cGmY9rnY96ecYuAj6C548/edit#heading=h.96bogdklzo1j)\n\n**Notes on the effect of Facebook deactivation on polarization in @allcott2020welfare:**\n\n- Unlike the questions used in typical population surveys, the questions were explicitly about their feelings during the period of the experiment, e.g. *\"Thinking back over the last 4 weeks, how warm or cold did you feel towards the parties and the president on the feeling thermometer?\"* \n\n- By far the largest effect was on the \"congenial news exposure\" question: *\"over the last 4 weeks how often did you see news that made you better understand the point of view of the Democrat (Republican) party?\"* The score was the difference between the answer for their own party vs the other-side party. It seems to me both that (1) deactivating Facebook would quite mechanically affect one's exposure to such news, and (2) this wouldn't normally be called a measure of \"polarization\" in the literature. The paper mentions in a footnote that *\"the effect on the political polarization index is robust to excluding each of the seven individual component variables,\"* but it turns out that removing \"congenial news exposure\" halves the effect-size and shifts the p-value from 0.00 to 0.09 (i.e. from very significant to non-significant).\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}