{
  "hash": "90894bcce0ce9f8fc03d605608bbf90a",
  "result": {
    "markdown": "---\ntitle: Ranking by Engagement\ndate: 2023-05-08\nauthor: Tom Cunningham, [Integrity Institute](https://integrityinstitute.org/)\n#institute: \"This should be orange\"\naliases:\n  - 2023-04-28-ranking-by-engagement.html\nexecute:\n  echo: false\n  cache: true # caches chunk output\nfig-align: center\nbibliography: social-media.bib\nreference-location: margin\n#citation-location: margin\nformat:\n   html:\n      include-in-header:\n         - text: |\n            <script>window.MathJax = {\n               loader: { load: [\"https://cdn.jsdelivr.net/gh/sonoisa/XyJax-v3@3.0.1/build/xypic.js\"]},\n               tex: {packages: {'[+]': ['xypic','bm']},\n                     macros: {  bm: [\"\\\\boldsymbol{#1}\", 1],\n                                ut: [\"\\\\underbrace{#1}_{\\\\text{#2}}\", 2],\n                                utt: [\"\\\\underbrace{#1}_{\\\\substack{\\\\text{#2}\\\\\\\\\\\\text{#3}}}\", 3] }\n               }\n            };\n            </script>\nengine: knitr\neditor:\n   render-on-save: true\n---\n\n<style>\n   h1 {  border-bottom: 4px solid black; }\n   h2 {  border-bottom: 1px solid #ccc; }\n   h3 { font-weight: bold; font-size: 1em; /*margin-left: -1em;*/ }\n   h3~p { margin-left: 2em; }\n   h3~ul { margin-left: 2em; }\n   h3~ol { margin-left: 2em; }\n   body {counter-reset: h3counter; }\n   h3::before {\n      content: counter(h3counter) \". \";\n      counter-increment: h3counter;\n   }\n   \n   dt { font-weight: normal; }\n   dt strong { font-weight: bold; }\n   dd { margin-left: 20px; }\n   a { color: black; }\n   /* dl { display: grid; grid-auto-flow: row;}\n   dt { grid-column-start: 1; }\n   dd { grid-column-start: 2; } */\n</style>\n\n<!-- \n - New github link: https://tecunningham.github.io/posts/2023-04-28-ranking-by-engagement.html\n - Old github link: http://tecunningham.github.io/2023-04-28-ranking-by-engagement.html \n - Integrity institute tweet: https://twitter.com/Integrity_Inst/status/1655697509988069376\n - My tweet thread: https://twitter.com/testingham/status/1656295725653979136\n - Hacker News: https://news.ycombinator.com/item?id=35927507\n - Blog post with Luke: https://docs.google.com/document/d/1uUAdAE21xvXCT_xmRDFXQHL14pYVMxQMpP3vweiHzA4/\n-->\n\n::: {.column-margin}\n   <!-- tom.cunningham@gmail, [@testingham](https://twitter.com/testingham). I worked at FB for 5 years, and Twitter for 1 year, now affiliated with the [Integrity Institute](https://integrityinstitute.org). This note entirely based on public information.  -->\n   Thanks to comments from Jeff Allen, Jacquelyn Zehner, David Evan Harris, Jonathan Stray, and others. If you find this note useful for your work send me an email and tell me :).\n:::\n\n\n\n<!-- \nto add:\n- the joint distribution is endogenous. As your classifier of spam gets better then spam will change.\n- alternative: rank by quality (against objective standard) or by provenance.\n- Technically need a condition on covariance of quality and engagement and retentiveness; \n- Limitations of this framework: not accounting for diversity rules, explore-exploit.\n- User controls generally have minor impact\n- Predicting engagement on public content: embeddings are standard practice (notes from Karan)\n- Suboptimal ranking: Spence distortion\n- Note that sensitive content is mainly driven by peer-to-peer sharing, rather than by ranking.\n- Add: Even Oldridge (2022) [Recommender Systems, Not Just Recommender Models](https://medium.com/nvidia-merlin/recommender-systems-not-just-recommender-models-485c161c755e)\n  - Pinterest paper: https://arxiv.org/pdf/1702.07969.pdf\n    - \"The gold standard for testing changes is online A/B experimentation, where we evaluate ranking primarily by their impact on save propensity. ... We found that PR AUC metrics are extremely predictive of closeups and clickthroughs in A/B experiments, but we had difficulty predicting the save behavior using offline evaluation.\"\n\n\nsend to: Evan Rose, Ian Ross, David Vickrey, \nsent to: Andrew Hall, Panos, Sam, Franz, Luca, Akos, Jonathan Stray, WhatsApp Twitter Health group, Dean Eckles, Delia\n\nSee also:\n- tecunningham.github.io/_drafts/2021-05-28-quality-problem.md\n- integrity/2019-10-18-bad-content-slides.md -- work with Evan and Andrew Hall\n- (inauthentic notes?)\n-->\n\n::: {.column-margin}\n![](images/2023-06-13-15-11-29.png)\n:::\n\n**Six observations on ranking by engagement on social media platforms:**\n\n1. **Platforms rank content primarily by the predicted probability of engagement.** Platforms choose for each user the items they are predicted to click on, or reply to, or to retweet, etc.^[In this note I'm using \"engagement\" to refer to individual actions not user-level metrics like time-spent or DAU.]\n\n2. **Platforms rank by engagement because it increases user retention.** In experiments which compare engagement-ranked feeds to unranked feeds (\"chronological\" feeds) the users with engagement-ranked feeds consistently show substantially higher long-run retention (DAU) and time-spent. Platforms care about engagement not in itself but as a means to an end, and when faced with a tradeoff between engagement and retention would choose retention.\n\n   <!-- 2. **Ranking by engagement increases user retention.** In experiments which compare engagement-ranked feeds to unranked feeds (\"chronological\" feeds) the users with engagement-ranked feeds consistently show substantially higher long-run retention (DAU) and time-spent. -->\n\n   <!-- 1. **Platforms rank by engagement in order to increase retention.** Leadership are generally willing to sacrifice a significant amount of engagement in return for a small increase in retention. -->\n\n3. **Engagement is negatively related to quality.** The content with the highest predicted engagement very often has low scores by various measures of objective quality: clickbait, spam, scams, misleading headlines, and misinformation. Intuitively this is because engagement only measures immediate appeal, and the most appealing content is often the most disappointing. Low quality content typically *hurts* retention, and as a consequence platforms often supplement their engagement-based ranking algorithms with a range of proxies for content quality.\n\n   <!-- 4. **Ranking by engagement has many pathologies.** Despite the positive overall relationship between engagement and retention many platforms have struggled with pockets of content that have high engagement but low quality and which ultimately hurt retention. Facebook struggled with engagement bait (e.g. posts that say \"like if you agree\"), and continues to struggle with misleading and inauthentic content. YouTube struggled with videos that promised big payoffs but failed to deliver. Platforms have tried to address these problems with a mixture of (1) defining measures of objective quality, e.g. employing professional raters and training classifiers; (2) shifting to deeper measures of engagement, e.g. using the length of watch-time on a video instead of just the act of clicking on a video. -->\n\n4. **Sensitive content is often both engaging and retentive.** Engagement-ranked feeds often increase the prevalence of various types of \"sensitive\" content: nudity, bad language, abuse, hate speech, hyper-partisan politics, etc.. However unlike low-quality content, reducing the prevalence of sensitive content often hurts retention, implying that sensitivity is positively correlated with retention.\n\n5. **Sensitive content is often preferred by users.** Platforms have tried out many experiments with asking users directly for their preferences over content. The results have been mixed, and platforms have often been disappointed to find that users express fairly positive attitudes towards content that the platform considers sensitive.\n\n6. **Platforms don't want sensitive content but don't want to be seen to be removing it.** Platform decision-makers often have principled reasons for limiting the distribution of certain types of sensitive content. Additionally there are instrumental reasons: sensitive content attracts negative attention from the media, advertisers, app stores, politicians, regulators, and investors. But platforms are also hesitant to directly target this content, especially when it has some political dimension. As a consequence platforms often target sensitive content indirectly by using proxies, and they prefer to justify their decision-making by appealing to user preferences or to user retention.\n\n\n::: {.cell .column-margin layout-align=\"center\" hash='2023-04-28-ranking-by-engagement_cache/html/unnamed-chunk-1_79670ea5161c8fbc0e908fc5903dc350'}\n::: {.cell-output-display}\n![](2023-04-28-ranking-by-engagement_files/figure-html/unnamed-chunk-1-1.png){fig-align='center' width=288}\n:::\n:::\n\n**In an appendix I formalize the argument.** I show that all these observations can be expressed as covariances between different properties of content, e.g. between the retentiveness, predicted engagement rates, and other measures of content quality. From those covariances we can derive Pareto frontiers and visualize how platforms are trading-off between different outcomes.\n\n<br /><br /><br />\n\n#        Argument in Detail\n\n###   I will bucket attributes of content into five types\n\n   1. *Engagement:* the predicted probability of a user clicking, commenting, retweeting, etc., on a specific piece of content.\n   2. *Retentiveness:* the causal contribution of seeing the content on a specific user's long-term retention (e.g. DAU). Unlike the other attributes this can never be directly observed, only inferred from experiments.\n   3. *Quality:* some objective measure of quality, e.g. whether fact-checked, whether the headline is misleading, whether the linked website has a high ad-load, whether the source is trustworthy, etc..\n   4. *Sensitivity:* whether the content could be offensive, harmful, corrosive -- e.g. nudity, bad language, abuse, hate speech.\n   5. *Preference:* the user's response to a survey question, e.g. \"do you want to see more of this type of content?\"\n\n   Note that \"quality\" and \"sensitivity\" apply to individual pieces of content, while the other three attributes apply to relationship between a user and a piece of content.\n\n\n###   Social media platforms rank their content primarily by predicted engagement\n\n   The core ranking model for most social platforms is a weighted average of predicted engagement rates.^[I believe this is true for almost all platforms with personalized recommendations including YouTube, Netflix, and Amazon. An excellent reference for how recommenders work, with illustrations and links, is Thorburn, Bengani, & Stray (2022) [\"How Platform Recommenders Work\"](https://medium.com/understanding-recommenders/how-platform-recommenders-work-15e260d9a15a)]\n\n   However ranking functions also include hundreds of other tweaks incorporating non-engagement features, upranking or downranking content depending on, for example, the media type (photo/text/video), the relationship between the user and the author (whether you follow this person), various predictions of  of objective quality (classifiers predicting whether the content is spam, offensive, adult, misinformation, etc.), or other features (network centrality, off-platform popularity, etc.). They also often have some diversity rules to prevent the content that is shown from being too similar.^[For simplicity the rest of the discussion treats the causal effect of content as purely separable.]\n\n   Ranking by popularity is common for other media: we look at lists of bestsellers, most popular, highest grossing, most watched, or top charting. Attention is limited and it would be inefficient to offer people a random selection of everything that's available.\n\n###   Predicted engagement rates are mostly historical engagement rates\n\n   In many cases the most important predictors of whether a user will engage with a piece of content are (1) this user's historical rate of engagement on similar pieces of content (e.g. content from the same author, or of the same media-type); (2) other users' rate of engagement on this piece of content. Platforms do use more complicated models (embeddings and neural nets), those models typically are most valuable for qualitatively new types of content, when you have relatively sparse historical data either on the user or on the item.[^twoTowers]\n\n   [^twoTowers]: See discussion below of Covington et al. (2016) which describes an architecture commonly used in recommenders.\n\n###   Platforms care primarily about long-run retention, engagement is a means to that end\n\n   The outcome that leadership care about the most is long-run retention, measured with metrics like Daily Active Users (DAU).^[Most publicly-traded platforms report in their quarterly earnings just one non-financial metric: the number of active users (DAU/MAU/mDAU,  etc.). I do not know of any company that publicly reports a metric of aggregate engagement.] They would generally sacrifice substantial amounts of engagement in return for DAU. They also would sacrifice substantial short-term DAU if it could be shown with confidence that it would lead to higher long-term DAU.\n\n   This point is often unclear because many changes to ranking (as measured in experiments) move engagement and retention in the same direction, and move short-run and long-run metrics in the same direction, meaning that we cannot easily tell which metric is decisive. Individual teams are often given targets to increase short-term engagement but that is mainly because that metric is easier to measure. \n   \n   <!-- Arguably there are misalignments of incentives such that lower-level employees tend to ship features that increase short-run engagement at the expense of long-run retention. -->\n\n###   Engagement-ranked feeds have substantially higher long-term retention and time-spent than chronologically-ranked feeds\n\nUsers who are given engagement-ranked feeds in experiments typically have higher long-term DAU by single-digit percentages (1%-9%), and higher long-term time-spent by double-digit percentages (10%-99%). Accounting for network effects makes the aggregate difference even larger.^[@huszar2022twitter note that since Twitter introduced a ranked timeline in 2016 they maintained an experiment with 1% of users with a chronological feed.].\n\n- *Meta 2020 experiments with chronological ranking showed a 20% time-spent decline on Facebook and a 10% decline on Instagram.* The experiments ran for 3 months and they reported the average effect over the whole period, it is likely that the time-spent effects continued declining. The effects on DAU were not reported. See @guess2023chronological.\n- *A Facebook 2018 experiment showed a 3% decline in time-spent after 10 days.* The effects on time-spent seemed to be linearly trending down at the time the analysis was posted. Engagement (MSI) declined by about 20%, and politics impression reduced by 15% (the share of politics impressions is more complicated to calculate but seems unambiguously down).  The effects on DAU were not reported ([source](https://www.bigtechnology.com/p/facebook-removed-the-news-feed-algorithm?s=09).). The experiment was not on a purely chronological feed: they retained \"diversity rules, client-side ranking, read-state logging, comment-bumping\" as well as integrity ranking rules.\n\n   <!-- A lot of work inside platforms is experimenting with varying the weights on different engagement types to try to increase user retention. -->\n\n###      Engaging content is often low quality\nDespite the positive relationship between engagement and retention, many studies have found that highly-engaging content is has lower-than-average quality:\n\n-  *In summer 2016 half of Facebook's most-seen posts related to the US election were misinformation.* As [reported](https://www.buzzfeednews.com/article/craigsilverman/viral-fake-election-news-outperformed-real-news-on-facebook) by Craig Silverman at Buzzfeed. This exceeds the *average* rate of misinformation, i.e. the most-engaging posts have a much lower-than-average quality.\n-  *In 2019 Facebook's top group and pages were run by troll farms.* A series of internal analyses by by Jeff Allen  ([subsequently leaked](https://s3.documentcloud.org/documents/21063547/oct-2019-facebook-troll-farms-report.pdf)) found that a substantial share of Facebook's top pages, groups, and posts, were run by \"troll farms,\" whose main tactic was reposting copied content that had high engagement rates.  <!-- Troll farms' pages, when combined, were  included Facebook's top overall page, top Christian American page, top African American page, 2nd largest Native American page, and 5th largest Womens' page. -->\n-  *In 2019 Facebook's high-quality content received lower engagement.* In the late 2010s Facebook maintained an internal \"quality\" score for content (FUSS=\"Feed Unified Scoring System\"). A data scientist's analysis from 2019 ([subsequently leaked, see p.10](https://s3.documentcloud.org/documents/21063547/oct-2019-facebook-troll-farms-report.pdf)) found that low-quality content had significantly higher predicted engagement rates.^[These correlations can be difficult to interpret: suppose there is no correlation between engagement and quality in the pool of all available content, there will nevertheless be a negative correlation among the subset of content that is *seen* if the ranking algorithm penalizes low-quality content, meaning low-engagement low-quality content will never be shown to users. It is unclear from the document whether the correlation is among content that is available, or content that is seen.] \n-  *In 2021 Facebook's most-viewed posts were very low quality.* Since early 2021 FB has been releasing a dataset of their 20 most-viewed links and posts. An [Integrity Institute analysis](https://lookerstudio.google.com/u/0/reporting/28bc32fd-a067-4b4a-9be0-637e8c9bd917/page/0Z3mC?s=g7_EWEyFjrc) by Jeff Allen has found that each quarter 60-80% of the posts fail some basic checks, either \"the account behind it is anonymous, is posting unoriginal content, using spammy page or group networks, or if the post or link violated Facebook's community standards.\"\n- *In 2020 Twitter's ranking has mixed effects on political content.* Huszar et al. (2021) compare political content of users who are randomized to ranked vs chronological feeds. They report (1) for political parties, ranked feed tends to amplify the right-wing parties somewhat more than left-wing parties (but the same does not hold for individual politicians); (2) for US media sources, ranked feed amplifies \"sources that are more partisan compared to ones rated as center\".^[Bakshy et al. (2015) found that Facebook's feed-ranking doesn't substantially change the share of cross-cutting (across-the-aisle) content seen.]\n- *In 2020 FB and IG chronological-ranking experiments showed a 5-15% increase in political content.* @guess2023chronological found that replacing feed-ranking algorithms with simple chronological ranking (i.e., the most-recent posts are shown first) for 3 months (1) the share of impressions that were classified as political increased by 15% on Facebook and by 5% on Instagram, the share that were classified as \"political news\" increased by 40% on Facebook.\n\n\n\n###   Many platforms have found that increasing quality helps retention\n\nPlatforms have tried to address quality problems by defining measures of objective quality:\n\n   - *Facebook uses many heuristics and classifiers to identify various types of low-quality content:* Facebook identifies and downranks, among other things, engagement bait, links that go to ad-farms, scraped content, titles that withhold information, and titles that exaggerate information. In each case these types of content would generate high engagement but give users a bad experience, and in most cases experiments confirmed that dowranking these types of content increases long-run user retention.\n   - *Facebook uses some metadata features to identify low-quality content.* E.g. Facebook calculates the [\"click gap\"](https://www.cnbc.com/2019/04/10/facebook-click-gap-google-like-approach-to-stop-fake-news-going-viral.html) (the amount of organic traffic a website gets) and [\"broad trust\"](https://www.wired.com/story/how-facebook-wants-to-improve-the-quality-of-your-news-feed/) (diversity of engagement across users).\n\n      <!-- I couldn't find any public mention of network centrality -->\n   - *YouTube has introduced a series of quality adjustments to ranking:* E.g. downranking [\"sensationalistic tabloid content\"](https://blog.youtube/inside-youtube/on-youtubes-recommendation-system/) and upranking [\"authoritative content\"](https://blog.youtube/inside-youtube/on-youtubes-recommendation-system/).\n\n   Some companies have also shifted engagement weights to put relatively more weight on \"deeper\" measures of engagement:\n\n   - *In 2012 YouTube switched from maximizing clicks to maximizing watch-time.* They found it led to a short-term decrease in clicks but a long-term [increase in retention](https://blog.youtube/inside-youtube/on-youtubes-recommendation-system/). I believe Netflix similarly has invested a lot of time in developing \"deep\" measures of engagement.\n\n   <!-- 1. **Engagement is a less-good signal for retention for some types of content.** In general people tend to engage with content that interests or entertains them, and for that reason engagement will tend to correlate with retentiveness. However there are a number of classes of content where the connection between engagement and value comes loose: (1) some content explicitly asks users to engage, called engagement bait (\"like if you agree\", \"comment to vote\"), and so will generate engagement in excess of its retentive value; (2) some users will engage with content that they dislike, e.g. leaving a negative reaction or comment. Platforms are generally aware that they ought to be penalizing content that generates non-retentive engagement, but sometimes move slowly as these projects can be difficult to validate. -->\n\n###      Quality also helps the producer ecosystem.\n\n   There is an additional reason for prioritizing the quality of content independent of the direct effect on user retention: because prioritizing high-quality content helps foster a long-run community of creators.\n\n   A central fact about social media is that it relies on a tremendous amount of uncompensated labor. Most content is created for the joy of creation, with little realistic expectation of financial return. The impulse to post clearly relies on a delicate social perception or norm, and a platform could inadvertantly break this spell. I think speaking loosely Facebook mismanaged their public-content ecosystem in this way: they alienated creators in a variety of ways, especially by allowing copied content to proliferate, and high-quality creators instead posted to Instagram, Twitter, YouTube, and Tik Tok. Facebook leadership tried to attract creators with various monetary incentives but they often backfired: creators who are financially-motivated are often not the creators you want.\n\n###   Engagement measures *immediate* quality, and hence is a poor proxy for the quality of factual claims\n\n   Engagement necessarily measures the immediate reaction of a user to a piece of content, and thus ranking by predicted engagement will surface content that *appears* to be good. This is fine when there is no hidden aspect to quality, e.g. for jokes and pictures which mostly be judged in the moment. However if we rank informational content by predicted engagement it will tend to surface the claims that are the most sensational or intriguing independent of whether they are true.\n\n   If apples were sold only by how they looked, and not by how they tasted, then we would be offered delicious-looking and bland-tasting apples.\n\n   I believe this basic mechanism explains why internet platforms typically have higher rates of exaggerated, misleading, or false content compared to traditional media (newspapers, television, etc.). Traditional media do not publish whichever headlines would maximize short-run sales because that would harm long-run sales. This also explains why platforms have found that they can substantially improve retention by building proxies for quality. \n\n###   A negative relationship between engagement and quality can be caused by unscrupulous publishers\n\n   Suppose each publisher can produce a fixed number of headlines, which will vary in (1) the headline's propensity to be clicked, and (2) whether the headline is true. There are two types of publishers:\n\n   1. Honest publishers: they choose the most-engaging headlines from within the subset that are true.\n   2. Dishonest publishers: they choose the most-engaging headlines from the universe of all possible headlines (irrespective of truth).\n\n   In this world the most-engaging headlines will be disproportionately false compared to the average headline. In the long run consumers will learn some skepticism, and to discount headlines in proportion to how clickable they seem, but they are unlikely to learn to discriminate perfectly. There's always a chance that an intriguing headline will be true, and so the negative correlation would persist in equilibrium.[^model]\n      \n   [^model]: A more formal version: each consumer sees a single headline with observed signal $s$ and chooses whether to click. The payoff from clicking is $s$ if it's from an honest publisher, zero otherwise, and there's some stochastic outside option so the consumer's probability of clicking is continuously increasing in the expected payoff from clicking. Honest publishers report their signals drawn from $f_H(s)$, dishonest publishers choose any signal. In equilibrium the dishonest publishers' signal distribution, $f_D(s)$, must be such that all signals with non-zero mass have have an equal click-through rate, meaning there is some $\\kappa$ such that for every $s>\\kappa$, $f_D(s)=\\frac{s-\\kappa}{\\kappa}f_H(s)$. Thus low click-through-rate headlines ($s<\\kappa$) are all true, but high click-through rate headlines ($s>\\kappa$) all have some share which are false. Qualitatively: if a headline is not very interesting, then you believe it; but if it's interesting then you discount exactly inversely to how interesting it is. In this model we have (1) retentiveness (consumer surplus) is increasing with engagement; (2) quality (truth) is decreasing with engagement; (3) retentiveness (consumer surplus) would be higher if you rank by both engagement and quality (e.g. by removing false stories).\n\n###      Platforms have been slow in improving the quality of ranked content\n\n   I discuss above some examples of Facebook's slowness in addressing problems with the quality of content. I think this slowness is for two mains reasons. First, predicting engagement is a well-defined technical problem with a track record of success while evaluating content-quality is much more open-ended and difficult to validate. Hard-headed engineers often argue that a user's preferences are revealed in their engagement and that evaluating quality is paternalistic. Secondly, platforms are nervous of being opinionated about objective quality because they don't wish to take sides on politically delicate issues. In 2016 Facebook was criticized for using human judgment in determining what topics are \"trending\", and in the wake of that criticism many projects which involved human judgment were shut down and replaced with automatic systems. Then in 2020 engineers on News Feed were told to avoid using words such as \"trust\" or \"quality\" or \"authority\", and to instead use language that referred only to user preferences.\n\n   <!-- 9.  **Ranking by engagement *changes* the relationship between engagement and retention.** Once you start ranking by engagement this creates an incentive for content producers to create highly-engaging content. The net effect is that engagement will become less informative about retention.  -->\n\n   <!-- We see this change in correlations prominently in spam: when we find a behaviour that is highly predictive of spam, and add it to our spam classifier, then the predictiveness of that feature rapidly becomes worse because spammers learn to avoid using it. -->\n\n###   Sensitive content is often both engaging and retentive\n\n   I have defined \"sensitive\" content to include nudity, bad language, abuse, hate speech, hyper-partisan politics, etc.. Sensitive content often has higher-than-average engagement rates, and when content is demoted this often hurts retention, implying that sensitivity is positively correlated with retentiveness.\n\n   - A 2023 academic study by [Beknazar-Yuzbashev et al.](https://drive.google.com/file/d/1HYiBOGLNM91RBiqBlxKFvjDhJAuCxe60/view) found that filtering the 7% most-toxic content on Facebook reduced overall Facebook content consumed by 20%.\n\n###    Platforms don't want to show sensitive content\n \n   Platforms are clearly prepared to pay a cost to reduce the prevalence of sensitive content, both in terms of retentiveness (DAU), and in the monetary cost of engineers, labelers, and computation.^[Platforms often spend around 5% of their total costs on content moderation, despite the prevalence of sensitive content and the effects on retention typically being closer to 0.1% or less.]\n\n   The platforms have many reasons for avoiding sensitive content, independent of its effect on retention, but internally there is often an ambiguity about the contribution of different reasons. In part decisions are driven by a feeling of moral duty to not amplify content that is harmful. However there are also many instrumental reasons, because sensitive content often causes friction with advertisers, app stores, regulators, media, employees and investors.\n\n   Misinformation is a somewhat special case. From what has been discussed before, misinformation can be expected to reduce retention because it's not true. However misinformation is very often related to sensitive issues, e.g. partisan politics, race relations, vaccines, and often reports falsehoods that support the viewer's political prejudices.\n\n###      Platforms avoid directly penalizing sensitive content\n\n   Platforms are caught in a double bind: there are strong pressures to reduce the amount of sensitive content on their platform, but there is also a pressure not to be seen to be making judgments about the objective value or harm of content. They want a garden with no weeds but they also wish to have clean hands. This often causes a bifurcation between the nominal reason for a policy and the real reason. Some examples:\n   \n  - *Platforms often downrank engagement patterns because they correlate with sensitive content.* It is common to downrank posts which features a specific engagement pattern (e.g. certain types of sharing, certain types of downstream attributes), and the downranking is justified internally based on the correlation with measures of quality or sensitivity, e.g. misinformation, or hate speech, or hyperpartisan content. This is odd because it would seem to be more efficient to target the sensitive content directly, i.e. instead of downranking the proxy, use the proxy as a feature in a classifier, and downrank based on the classifier output. However platforms avoid this approach in part because they are nervous about the perception of being perceiving as judges of the quality of content.\n   \n  - *Platforms speak about sensitivity rules as if they were adopted to serve the interests of their users.* Google's Jigsaw group has an influential set of definitions of content quality, their [definition](https://support.perspectiveapi.com/s/about-the-api-attributes-and-languages?language=en_US)  of a \"toxic\" comment is *\"a rude, disrespectful, or unreasonable comment that is likely to make people leave a discussion.\"* This definition is worded to presuppose that rude comments cause lower retention. The definition thus allows a platform to talk about their toxicity classifiers as if they were solely serving the interests of the users exposed to toxic language.\n\n  - *Survey questions are chosen based on their correlation with measures of sensitive content.* Platforms will often try out multiple different wordings of a survey question and decide which one to use by comparing the results with their internal measures of content quality and sensitivity, leading to survey questions that are somewhat awkwardly worded (e.g. asking people \"is this good for the world?\").\n\n###   Subjective user ratings of quality have a mixed relationship with objective measures of quality\n\n   Platforms have often tried to collect explicit user feedback about quality, e.g. asking \"was this worth your time?\", \"do you want to see more of this?\", \"was this informative?\". In my experience, for most such questions, responses are highly correlated with engagement, but often show a negative correlation with objective measures of quality. E.g. people often rate misinformation as \"informative\" and \"worth my time.\"\n\n   Nevertheless some of these initiatives have had success in raising both objective quality and retention, e.g. Facebook recently launched a prompt asking \"would you like to see more posts like this?\" The signal from this prompt apparently increases both retention and many objective measures of quality.\n\n   <!-- This finding is often disappointing to the platform: platforms don't like to have low-quality content on their platforms (misinfo, misleading, hateful, partisan), but they also wish to avoid making substantive judgments about objective quality as it seems paternalistic and politically sensitive. Because of this tension platforms have continued to experiment with different wordings of questions, leading them to eventually find questions which correlate with objective quality but are somewhat awkwardly worded (e.g. asking people \"is this good for the world?\" and using the answer as a signal of subjective quality). -->\n\n###   Platforms additionally care about engagement because of network effects\n\n   I said above that platforms care about engagement primarily insofar as it's a proxy for retention, however there is an additional reason to pay attention to engagement. When one user engages (likes, comments, retweets) this increases the value of the platform to all the other users, and so has an indirect positive effect on retention. For this reason platforms are generally willing to sacrifice some retention in return for engagement, as measured in an experiment, if the sacrifice is sufficiently small.\n\n#           Technical Appendix: Expressed as a Covariance Matrix\n\n\n**We can express most of the argument above with a covariance matrix.** Given a user we can give scores to each piece of content with respect to the five attributes defined above. Then we can give a reasonable characterization of the platform ranking problem with the following covariance matrix:\n\n|                   | retentiveness | engagement | quality | sensitivity | preference |\n| :---------------: | :-----------: | :--------: | :-----: | :---------: | :--------: |\n| **retentiveness** |    &nbsp;     |     +      |    +    |      0      |     +      |\n|  **engagement**   |    &nbsp;     |   &nbsp;   |    -    |      +      |     +      |\n|    **quality**    |    &nbsp;     |   &nbsp;   | &nbsp;  |      0      |     0      |\n|  **sensitivity**  |    &nbsp;     |   &nbsp;   | &nbsp;  |   &nbsp;    |     +      |\n|  **preference**   |    &nbsp;     |   &nbsp;   | &nbsp;  |   &nbsp;    |   &nbsp;   |\n\n**Given this covariance matrix, we can draw Pareto frontiers and indifference curves.** Each Pareto frontier represents the set of achievable tradeoffs between two outcomes. I explain below how elliptical Pareto frontiers and linear indifference curves can be derived from the covariance matrix if we assume that everything is distributed joint Normally.\n\n**Retentiveness and engagement.** We can draw a Pareto frontier between retention and engagement as below. We do not directly observe the retentiveness of content, but we know that ranking content by engagement (i.e. choosing the farthest right-hand point on the Pareto frontier) increases retention relative to an unranked feed, so we can infer that retentiveness and engagement are positively correlated, thus the Pareto ellipse must be upward-sloping.\n\n\n::: {.cell layout-align=\"center\" hash='2023-04-28-ranking-by-engagement_cache/html/unnamed-chunk-2_72d01dd2d92d83f155d995d659fe1b90'}\n::: {.cell-output-display}\n![](2023-04-28-ranking-by-engagement_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=288}\n:::\n:::\n\n\n**Engagement and quality.** As discussed above, we often see that (1) measures of content quality have zero or negative correlation with engagement, (2) downranking low-quality content (equivalently, upranking high-quality content) increases retention. This is somewhat surprising because engagement and retention have a positive correlation, meaning the three correlations are not transitive.\n\n\n::: {.cell layout-align=\"center\" hash='2023-04-28-ranking-by-engagement_cache/html/unnamed-chunk-3_d91a652143616fce6f027c965e9b4782'}\n::: {.cell-output-display}\n![](2023-04-28-ranking-by-engagement_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=336}\n:::\n:::\n\n\n\n   We illustrate the relationship with retention here with three lines representing different levels of retention, effectively these are indifference curves of a platform that is trying to maximize retention.\n\n**Engagement and sensitivity.** Next consider \"sensitive\" attributes. We often see that more sensitive content has higher engagement rates, shown below as an upward-tilt to the Pareto frontier. In addition experiments that penalize sensitive content often have a negative effect on retention: this could be either due to a positive partial correlation between engagement and retentiveness, or a positive partial correlation between sensitivity and retentiveness. But in either case it seems that sensitive content does not have a strong negative effect on retention.\n\n   Despite these facts, most platforms still put substantial penalties on sensitive content, either directly or indirectly (as discussed above), and they pay a price in terms of both engagement and retention.\n\n\n::: {.cell layout-align=\"center\" hash='2023-04-28-ranking-by-engagement_cache/html/unnamed-chunk-4_3eb22aba80b71510789e4450ff460016'}\n::: {.cell-output-display}\n![](2023-04-28-ranking-by-engagement_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=288}\n:::\n:::\n\n\n**Preference and sensitivity.** Finally consider a direct measure of user preference over content, e.g. asking users \"is this informative?\" or \"would you like to see more like this?\" In general user preference correlates relatively well with engagement, but it also offers incremental value for predicting retentiveness, in other words adding an additional term to the ranking function to predict user preference tends to increase retention.\n\n   However as discussed above, projects which collect survey questions are often focussed on the sensitivity of content rather than its retentiveness, and in that respect their findings are often mixed. Below we illustrate a case in which ranking by preference increases retentiveness but does not lower the amount of sensitive content (which platforms often desire). However platforms will offer try out many different wordings of survey questions, and each question will have somewhat different correlations.\n\n\n::: {.cell layout-align=\"center\" hash='2023-04-28-ranking-by-engagement_cache/html/unnamed-chunk-5_76ef17ca710f7f5ae64474e6ea79e7f0'}\n::: {.cell-output-display}\n![](2023-04-28-ranking-by-engagement_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=288}\n:::\n:::\n\n\n\n<!-- **Describing the relationships between properties:**\n\n- Engagement is positively correlated with retentiveness, which is why it's a good way to rank content.\n- However engagement is also correlated with bad things (spam/clickbait, misinfo/hate).\n- Identifying and removing spam/clickbait will harm engagement but improve retentiveness.\n- However identifying and removing misinfo/hate will harm both engagement and retentiveness.\n- When we ask users what they want (\"user preference\") we typically find their answers are correlated with retention and engagement, and they want to see misinfo/hate. However when the question is worded differently, the correlations can change.\n- Platform decision-making seems to put most weight on retentiveness, but are prepared to sacrifice some retentiveness in exchange for reducing misinfo/hate. -->\n\n<!-- **Note: covariances can be hard to measure.** Platforms can calculate most of the covariances above relatively easily. However it's important to calculate the covariances using *all* content, not just content that's seen: suppose there is no correlation between A and B in the pool of content available, but your ranking function has positive weights on signals A and B, then the subset of content that is shown will have a negative correlation between A and B. Additionally the covariances with *retention* cannot be measured offline, we need to run experiments, and the experiments will identify the *partial correlations* (i.e. the effect on retention when we marginally increase the weight on one of the components.) -->\n\n##             Formal Observations\n\nHere I describe a few formal properties of a model of ranking based on a joint-normal distribution of attributes. I have a longer writeup with proofs of these results which I hope to publish soon, I am happy to share a draft on request.\n\n1. **The covariance between item attributes will determine a Pareto frontier among outcomes.** Suppose we know the joint distribution of attributes and we can choose a subset with share $p$ of the distribution (e.g. a fixed number of impressions given a pool of possible stories to show), and we want to calculate the average value of each attribute in the subset of content shown to the user. Then we can describe the Pareto frontier over subsets, i.e. the set of realized average outcomes, and it will be a function of the covariances among attributes over pieces of content. With 2 attributes the Pareto frontier will be an ellipse with shape exactly equal to an isoprobability curve from the joint density.\n\n   The shape of the ellipse has a simple interpretation. If two attributes are positively correlated then the Pareto frontier will be *tight* meaning there is little tradeoff, i.e. we will have similar aggregate outcomes independent of the relative weights put on each outcome in ranking. If instead two attributes are negatively correlated then the Pareto frontier will be *loose* meaning outcomes will vary a lot with the relative weights used in ranking.\n\n   Our assumption that the share $p$ is fixed is equivalent to assuming that any ranking rule will get the same number of impressions. This assumption obviously has some tension with *retentiveness* being an outcome variable: if some ranking rule has low retentiveness, then we would expect lower impressions. Accounting for this would make the Pareto frontier significantly more complicated to model, for simplicity we can interpret every attribute except retentiveness as a short-run outcome. Alternatively we could interpret them as relative instead of absolute outcomes, e.g. as engagement/impression or engagement/DAU.\n\n2. **Improving a classifiers will stretch the Pareto frontier.** As a classifier gets better the average prediction will stay the same but the variance will increase, meaning the Pareto frontier will stretch out, and given a linear indifference curve we can derive the effect on outcomes.\n\n3. **The joint distribution plus utility weights will determine ranking weights.** If we observe only some outcomes then we can calculate the conditional expectation for other outcomes. Typically we want to know retentiveness, and we can write the conditional expectation as follows:\n   $$E[\\text{retentiveness}|\n      \\text{engagement},\\ldots,\\text{user preference}].$$\n   This expectation has a closed-form solution when the covariance matrix is joint normal. When we have just two signals, for example engagement and quality, we can write:\n\n   \\begin{aligned}\n      E[r|e,q] &= \\frac{1}{1-\\gamma^2}(\\rho_e-\\gamma\\rho_q)e +\n                  \\frac{1}{1-\\gamma^2}(\\rho_q-\\gamma\\rho_e)q\\\\\n      r     &= \\text{retentiveness}\\\\\n      e     &= \\text{engagement (predicted)}\\\\\n      q     &= \\text{quality (predicted)}\\\\\n      \\rho_{e}     &= \\text{covariance of engagement and retentiveness}\\\\\n      \\rho_{q}     &= \\text{covariance of quality and retentiveness}\\\\\n      \\gamma     &= \\text{covariance of engagement and quality}\n   \\end{aligned}\n\n   Note that the slope of the iso-retentiveness line in $(e,q)$-space will be $-\\frac{\\rho_e-\\gamma\\rho_q}{\\rho_q-\\gamma\\rho_e}$.\n\n4. **Experiments which vary ranking weights tell us about covariances.** We can write findings from experiments as follows. First, suppose we find that retention is higher when ranked by engagement than when unranked, this can be written:\n      \\begin{aligned}\n         \\utt{E[r|e>e^*]}{ranked by}{engagement} &> \\ut{E[r]}{unranked}\n      \\end{aligned}\n   Here $e^*$ is chosen such that $P(e>e^*)=p$ for some $p$, representing the share of potential inventory that the user consumes. This implies that engagement must positively correlate with retentiveness, $\\rho_e>0$.\n   \n   Next we can express that retention is higher when we put some weight $\\beta$ on quality:\n\n   \\begin{aligned}\n      \\utt{E[r|e+\\beta q>\\kappa^*]}{ranked by}{engagement and quality} &> \\utt{E[r|e>e^*]}{ranked by}{engagement}\n   \\end{aligned}\n   Here $\\kappa^*$ is chosen such that $P(e+\\beta q > \\kappa^*)=P(e>e^*)=p$. If $\\beta$ is fairly small then we can infer that the iso-retentiveness line is downward-sloping, implying:\n      $$\\frac{\\rho_e-\\gamma\\rho_q}{\\rho_q-\\gamma\\rho_e}>0.$$\n\n   This implies that both engagement and quality have the same sign. I don't think they both can be negative, so they both must be positive:\n      \\begin{aligned}\n         \\rho_e - \\gamma \\rho_q &> 0 \\\\\n         \\rho_q - \\gamma \\rho_e &> 0.\n      \\end{aligned}\n\n5. **I think it's reasonable to treat preferences as locally linear.** To have a well-defined maximization problem (with an interior solution) we need either nonlinear preferences or a nonlinear Pareto frontier. It's always easier to treat things as linear when you can, so a relevant question is which of these two is closer to linear? Internally companies often treat their preferences as nonlinear, e.g. setting specific goals and guardrails, but those are always flexible and often have justifications as incentive devices. Typical metric changes are small, only single-digit percentage points, over that range the Pareto frontier does show significant diminishing returns while (it seems to me) value to the company does not.\n\n\n<!-- --------------------------------------------\n\n**The correlations determine a Pareto frontier.** Suppose each attribute is expressed as a probability, and the probabilities are all Normally distributed. Then given a fixed $N$ pieces of content we can .\n\nthe ranking function.** Given a correlation matrix and a utility function we can derive a linear ranking function. E.g. suppose we care only about retention, then for each piece of content we can calculate:\n   $$E[\\text{retentiveness}|\\text{engagement},\\ldots,\\text{user preference}]=\\Sigma_{r,x}\\Sigma_{x}^{-1}\\bm{x}.$$\n\n\nIdeally platforms want to rank every piece of content by retentiveness. However retentiveness is the one attribute that they cannot directly observe: they can only try experiments with different mixtures of content-types, and this is how we have backed out the correlations in this matrix.\n\n**This covariance matrix determines the Pareto frontier.** Given the covariances among these properties we can determine the Pareto frontier, i.e. the potential tradeoff that between different outcomes (retention, engagement, etc.). If we assume that everything's Normally distributed then the Pareto frontier will be an ellipse. It's difficult to draw a 5-dimensional Pareto frontier but we can illustrate a couple of two-dimensional slices.\n\n\n**Engagement and spamminess.** Suppose we additionally observe spamminess, we now wish to choose content that maximizes retentiveness but we have two signals. We know the *individual* predictive power of these two signals: engagement is a positive predictor for retention, and spamminess is a negative predictor for retention. However when we have both of these signals we need to additionally take into account the correlation between them:\n   $$\\begin{aligned}\n      E[\\text{retentiveness}|\\text{engagement},\\text{spamminess}]\n         & \\propto (\\rho_{e,r}-\\rho_{s,r}\\rho_{e,s})\\text{engagement}\n         + (\\rho_{s,r}-\\rho_{e,r}\\rho_{e,s})\\text{spamminess} \\\\\n      \\rho_{e,r} &>0 \\ \\ \\text{(correlation of engagement and retentiveness)} \\\\\n      \\rho_{s,r} &<0 \\ \\ \\text{(correlation of spamminess and retentiveness)} \\\\\n      \\rho_{e,s} &>0 \\ \\ \\text{(correlation of engagement and spamminess)}\n   \\end{aligned}\n   $$\n\n   In this case we can see that adding a measure of spamminess will *increase* the weight we put on engagement. This can be seen by observing that the indirect effect ($-\\rho_{s,r}\\rho_{e,s}$) has a positive sign. Intuitively, if we hold fixed spamminess, then engagement becomes a purer and more valuable signal of retentiveness.\n\n   Because there are three attributes it is difficult to visualize either the joint distribution or the Pareto frontier. However we can visualize just engagement and spamminess, and overlay lines to represent expected retentiveness (AKA indifference curves):\n\n\n::: {.cell layout-align=\"center\" hash='2023-04-28-ranking-by-engagement_cache/html/unnamed-chunk-6_d8b9249c6a92ecc6fe2c0e01794bd1c2'}\n::: {.cell-output-display}\n![caption](2023-04-28-ranking-by-engagement_files/figure-html/unnamed-chunk-6-1.png){fig-align='center' width=288}\n:::\n:::\n\n\n   Here we can see that to maximize retention we'll choose a point on the Pareto frontier that sacrifices some engagement, in return for a decrease in spamminess. -->\n\n<!-- ```{tikz}\n\\begin{tikzpicture}[scale=6]\n   \\draw[<->] (0,1) node[above,align=center] {quality of\\\\content} -- (0,0)\n         -- (1,0) node[right,align=center]{engagement\\\\with content};\n\n   \\draw[rotate around={45:(.5,.5)},fill=black,opacity=.1] (.5,.5)\n      ellipse (.2 and .4);\n   \\draw[gray,dashed] (.5,.75)--(1.2,1.2) node[right,black] {joint density of engagement \\& quality};\n\n   \\draw[rotate around={45:(.5,.5)},black] (.5,.5)\n      ellipse (.1 and .2);\n   \\draw[gray,dashed] (.58,.58)--(1.2,.9) node[right,black] {Pareto frontier (ellipse) showing tradeoff of engagement and quality};\n\\end{tikzpicture}\n``` -->\n\n<!-- ```{tikz}\n#| fig-width: 3\n\\begin{tikzpicture}[scale=6]\n   \\draw[<->] (0,1) node[above] {$X_2$}\n         --(0,0) --(1,0) node[below]{$X_1$};\n   \\tikzset{ partial ellipse/.style args={#1:#2:#3}{\n         insert path={+ (#1:#3) arc (#1:#2:#3)} } }\n         \n   % indifference\n   \\draw[rotate around={45:(1,1)},line width=2,color=pink] (1,1) \n      [partial ellipse=150:210:.5 and .6];\n\n   \\draw[rotate around={45:(.5,.5)},line width=2,gray,dashed] (.5,.5) \n      ellipse (.2 and .25);\n   \\draw[rotate around={45:(.5,.5)},line width=2] (.5,.5) \n      [partial ellipse=-55:55:.2 and .25];\n\n   \\fill (.64,.64) circle[radius=0.5pt];\n\n   \\node[align=center] at (.6,-.4)\n      {Black: achievable combinations of\\\\ $X_1$ and $X_2$, AKA \"Pareto frontier\".\\\\\n      Pink: indifference curve of objective fn.\\\\\n      Dashed: full Pareto frontier,\\\\\n      including $w_1<0$ and $w_2<0$.};\n\\end{tikzpicture}\n``` -->\n\n<!-- \n\n**Additional observations:**\n\n1. **Companies don't understand their own ranking algorithms.** Ranking algorithms are typically huge spaghetti-code monsters, with thousands of tweaks, and often companies have built simulations to help understand their own algorithms.\n\n- ranking based on short-run engagement will tend to *superficial* content, food that looks better than it tastes; in equilibrium signals will corrode; but if you can get *deeper* quality measures then you can align incentives, e.g. PANDA\n\n- [Note on relationship between FUSS quality and pEngagement](https://s3.documentcloud.org/documents/21063547/oct-2019-facebook-troll-farms-report.pdf)\n\n-->\n\n<!-- \nsuggested by jonathan stray:\n\nCiampaglia et al. (2018, Scientific Reports) *\"How algorithmic popularity bias hinders or promotes quality\"*\n\nThey have a model of quality and ranking-by-popularity but I found it hard to follow the intuitions, and their observations about comparative statics seem to be mostly from simultions.\n\n- Quality q_i is the probability that an item is selected by a user when not exposed to the popularity of the item.\n- At each time step:\n   - with probability , an item is selected based on its popularity (diminishing function of rank, with exponent .)\n   - with probability 1-, chosen proportional to quality\n \n -->\n\n#             Appendix: Literature on Ranking and Recommendation\n\n\n##            Overviews of Recommender Systems (chronological)\n\n\n**Adomavicius and Tuzhilin (2005) \"Toward the Next Generation of Recommender Systems: A Survey of the State-of-the-Art and Possible Extensions\"**\n~  An influential overview of recommender systems (14,000 citations!). The canonical example is recommending movies to get the highest predicted rating. They use \"rating\" as similar to \"engagement\". A more recent survey is [Roy and Dutta (2022)](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00592-5).\n\n**Davidson et al. (2010) [\"The YouTube Video Recommendation System\"](https://www.inf.unibz.it/~ricci/ISR/papers/p293-davidson.pdf)**\n   \n~   > \"[videos] are scored and ranked using ... signals [which] can be broadly categorized into three groups corresponding to three different stages of ranking: 1) video quality, 2) user specificity and 3) diversification.\"\"\n\n~   > \"The primary metrics we consider include click through rate (CTR), long CTR (only counting clicks that led to watches of a substantial fraction of the video), session length, time until first long watch, and recommendation coverage (the fraction of logged in users with recommendations).\"\n\n~   They say recommendations are good because they have high click-through rate.\n   - [A blog post from 2012](https://blog.youtube/news-and-events/youtube-now-why-we-focus-on-watch-time/) discusses a switch from views to watch time: *\"Our video discovery features were previously designed to drive views. This rewarded videos that were successful at attracting clicks, rather than the videos that actually kept viewers engaged. (Cleavage thumbnails, anyone?)\"*\n\n**Gomez-Uribe and Hunt (2015) [\"The Netflix Recommender System: Algorithms, Business Value, and Innovation\"](https://dl.acm.org/doi/abs/10.1145/2843948)**\n~  Clearly states that they evaluate AB tests using engagement, but it is regarded as an imperfect proxy for retention:\n~ > *\"we have observed that improving engagementthe time that our members spend viewing Netflix contentis strongly correlated with improving retention. Accordingly, we design randomized, controlled experiments ... to compare the medium-term engagement with Netflix along with member cancellation rates across algorithm variants. Algorithms that improve these A/B test metrics are considered better.\"*\n\n**Covington et al. (2016) [\"Deep Neural Networks for YouTube Recommendations\"](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/45530.pdf)**\n\n   This paper proposed a very influential architecture for content recommendation (the paper has 3000 citations). They say:\n   \n   > \"Our final ranking objective is constantly being tuned based on live A/B testing results but is generally a simple function of expected watch time per impression. Ranking by click-through rate often promotes deceptive videos that the user does not complete (clickbait) whereas watch time better captures engagement\" \n\n**Lada, Wang, & Yan (2021, FB Blog) [How does news feed predict what you want to see?](https://tech.facebook.com/engineering/2021/1/news-feed-ranking/)**\n\n**Thorburn, Bengani, & Stray (2022, Understanding Recommenders) [\"How Platform Recommenders Work\"](https://medium.com/understanding-recommenders/how-platform-recommenders-work-15e260d9a15a)**\n\n~   This is an excellent short article with description and illustration of the stages in building a slate of content: moderation, candidate generation, ranking, and reranking. Includes links to many posts from platforms describing their systems.\n\n~   ![](images/2023-07-31-15-45-00.png)\n\n~   ![](images/2023-07-31-16-09-47.png)\n\n**Arvin Narayanan (2023) [\"Understanding Social Media Recommendation Algorithms\"](https://knightcolumbia.org/content/understanding-social-media-recommendation-algorithms)**\n\n~   A good overview of recommendation algorithms, with an in-depth discussion of Facebook's MSI. \n\n~   Criticisms of social media recommendation: (1) harm users because \"implicit-feedback-based feeds cater to our basest impulses,\" (2) harm creators because \"engagement optimization ... is a fickle overlord,\" (3) harms society because \"social media platforms are weakening institutions by undermining their quality standards and making them less trustworthy. While this has been widely observed in the case of news ... my claim is that every other institution is being affected, even if not to the same degree.\"\n\n~   The technical part of the essay is excellent but I found some of the arguments about harm and social effects hard to follow. \n\n\n##             Proposals for Change (chronological)\n\n**Andrew Mauboussin (2022, SurgeAI) [\"Moving Beyond Engagement: Optimizing Facebook's Algorithms for Human Values\"](https://www.surgehq.ai/blog/what-if-social-media-optimized-for-human-values)**\n~  Says that the problem is *\"the most engaging content is often the most toxic.\"* They propose using human raters, e.g. ask people \"did this post make you feel closer to your friends and family on a 1-5 scale?\" They label a small set of FB posts as a proof of concept.\n\n**Bengani, Stray, & Thorburn (2022,Medium) [\"Whats Right and Whats Wrong with Optimizing for Engagement\"](https://medium.com/understanding-recommenders/whats-right-and-what-s-wrong-with-optimizing-for-engagement-5abaac021851)**   \n~  They define engagement as \"a set of user behaviors, generated in the normal course of interaction with the platform, which are thought to correlate with value to the user, the platform, or other stakeholders.\" Reviews evidence for good and bad effects of ranking by engagement. <!-- Jonathan says \"this was quite influential when published\" -->\n\n**Ovadya & Thorburn (2023). [Bridging Systems: Open Problems for Countering Destructive Divisiveness across Ranking, Recommenders, and Governance](https://doi.org/10.48550/arXiv.2301.09976)**\n\n**Stray, Iyer, Larrauri (2023) [\"The Algorithmic Management of Polarization and Violence on Social Media\"](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4429558)**\n\n~   1. Our overall goal should be to minimize \"destructive conflict\".\n~   2. The major lever used has been content moderation: changing the visibility of content based on semantic criteria (e.g. downranking toxic, disallowing hate speech).\n~   3. However we should put relatively more work on system design, e.g. adding friction or changing the mechanics of sharing or engagement-based ranking. In part because there's a robust correlation between content that causes destructive conflict and content that is engaging.\n\n**Milli, Pierson and Garg (2023) [Choosing the Right Weights: Balancing Value, Strategy, and Noise in Recommender Systems](https://arxiv.org/abs/2305.17428)**\n\n~   I find the model a little hard to follow.\n   <!-- I wrote to Smitha about this: I think the model is a bit incoherent. -->\n\n   <!-- > \"model in which two producers compete for the attention of one user. The recommender system ranks producers based on a linear combination of predictions of k behaviors. However, producers can strategically adapt their items to increase the probability of different user behaviors. User utility depends on being shown a high value producer, and a producers utility is the probability they are ranked highly minus their costs of strategic manipulation.\"\n\n   E.g. considering weights on like, RT, and reply: (1) which one is closer to true user preference; (2) which has higher signal-noise ratio (); (3) .\n\n   - Two items, i\\in\\{-1,1\\}, and k behaviours (e.g. like, comment). We have prediction of each, y\\in\\mathbb{R}^k. Final score is \\bm{w}^T\\bm{y}. Predictions are . -->\n\n**Lubin & Gilbert (2023) [\"Accountability Infrastructure: How to implement limits on platform optimization to protect population health\"](https://arxiv.org/abs/2306.07443)**\n\n~  A very wide-ranging and loose discussion of issues related to ranking content. Makes an analogy with 19th century measures to control public health. I think the main proposal is that firms come up with metrics to measure their effect on social problems such as mental health, and regularly report on how they're doing. They suggest requirements for platforms of different sizes:\n\n~  |       |                                                                                                                                                                                                                   |\n   | ----- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n   | 1M+   | Submitted plan for metrics and methods for evaluation of potential structural harms                                                                                                                               |\n   | 10M+  | Consistent data collection on potential structural harms                                                                                                                                                          |\n   | 50M+  | Quarterly, enforceable assessments on product aggregate effects on structural harms, with breakouts for key subgroups                                                                                             |\n   | 100M+ | Monthly, enforceable assessments on product aggregate effects as well as targeted assessments of specific product rollouts for any subproduct used by at least 50 million users, with breakouts for key subgroups |\n\n#           Appendix: Taxonomy of Metrics\n\nThis is meant to be a parsimonious taxonomy of metrics used in a recommender. They are organized by the the types of entity they apply to. For simplicity I omit aggregations (e.g. a user's like rate is just the average over likes over user-item pairs), and I omit predictions (e.g. an item's pToxic is just the prediction of whether a paid rater would rate the item as toxic).\n\n| **entity**    | **type of metric**   | **metric**                                         |\n| ------------- | -------------------- | -------------------------------------------------- |\n| user          | activity             | login (DAU/DAP)                                    |\n|               |                      | time spent                                         |\n|               |                      |                                                    |\n|               | evaluation           | survey (\"are you satisfied?\")                      |\n|               |                      |                                                    |\n| item          | paid rater           | policy-violating (\"does this violate policy?\")     |\n|               |                      | quality evaluation (\"does this fit quality defn?\") |\n|               |                      |                                                    |\n|               | objective features   | recency                                            |\n|               |                      | item contains author info                          |\n|               |                      | item contains link                                 |\n|               |                      |                                                    |\n| producer      | objective features   | off-platform popularity                            |\n|               |                      |                                                    |\n|               | graph statistics     | network centrality                                 |\n|               |                      |                                                    |\n| user-item     | social interaction   | like (heart/fav/emoji reaction)                    |\n|               |                      | comment (reply)                                    |\n|               |                      | reshare (retweet/forward)                          |\n|               |                      | downstream interactions                            |\n|               |                      |                                                    |\n|               | interest signal      | linger (time spent watching)                       |\n|               |                      | click (follow link, expand)                        |\n|               |                      |                                                    |\n|               | evaluation           | star-rating                                        |\n|               |                      | upvote (downvote)                                  |\n|               |                      | survey (\"did you find this worth your time?\")      |\n|               |                      | see-more                                           |\n|               |                      | dislike (see less)                                 |\n|               |                      |                                                    |\n|               | other                | report                                             |\n|               |                      | dislike                                            |\n|               |                      |                                                    |\n| user-producer | interest signal      | follow (subscribe, friend)                         |\n|               |                      | block                                              |\n|               |                      |                                                    |\n\n\n#  References\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}