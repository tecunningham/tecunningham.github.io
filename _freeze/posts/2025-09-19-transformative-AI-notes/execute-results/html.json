{
  "hash": "400c876e86ad360f052d177b62625eeb",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Notes on the Economics of Transformative AI\nbibliography: ai.bib\nreference-location: document\ncitation-location: document\nengine: knitr\nformat:\n  html:\n    other-formats: false\n---\n\n\n\n<style>\n   dl {display: grid;}\n   dt {grid-column-start: 1; width: 5cm;}\n   dd {grid-column-start: 2; margin-left: 2em;}\n</style>\n\nObservations on the economics of transformative AI.\n\n: This post will be a series of observations about the economics of AI, written partly in reflection on two great workshops: (1) Windfall Trust's [\"Economic Scenarios for Transformative AI\"](https://futureoflife.org/project/the-windfall-trust/); (2) NBER's [\"Workshop on the Economics of Transformative AI.](https://www.nber.org/conferences/economics-transformative-ai-workshop-fall-2025)\n\n##          General Observations\n\nMost economists avoided talking about transformative AI.\n\n: Both workshops were intended for discussion of a hypothetical: what would happen if AI *could* do everything that humans can do? But more than half the papers put a cap on AI abilities, many were really focused on predicting the diffusion of *existing* levels of computer-level intelligence.\n\n: Why did people avoid talking about super-AI? Some because they didn't really believe it would come, & so wanted to focus on the more likely effect of AI.\n\nWe're removing a wall and we don't know if it's load-bearing.\n\n: We write down precise equilibrium models of the economy but it's generally understood the models only roughly match aggregate statistics, and there are very many puzzles that economists argue over: about growth, the equity premium, lifecycle savings, the equilibrium size of firms, & heterogeneity in productivity. It is easy to exaggerate how accurate these models are at predicting causal effects, and in fact structural models typically underperform black-box models for forecasting. We are now going to make some drastic changes to the production function, and predict how equilibrium will change.\n\nI don't think people appreciate the speed of improvement.\n\n: People gave various examples of things the models still couldn't do: (1) ARC-AGI; (2) understand world models; (3) reliably operate a vending machine (Anthropic's vending machine). These are all good examples but I don't think there was a good sense of the rate of progress. I think it's very likely that AI will solve all these things in a year. Some general observations about the rate of progress:\n   \n      1. ARC-AGI is falling quickly: over a year performance on the original ARC-AGI went from around 10% to 80%, and over a few months performance on ARC-AGI-2 went from around 3% to 30%. It's true that a large part of the performance improvement was due to writing wrappers, but I think it's very likely the models will be pretty good at writing their own wrappers by this time next year, so they'll be able to solve the class of problems which can be addressed by LLM+wrapper.\n      2. Benchmarks are falling rapidly. I think it typically takes about 18 months for benchmark performance to go from 25% to 75%, whereas in the pre-2020s it was more like 5-10 years.\n      3. The consumer utility is growing dramatically: the Elo score of models on chatbot arena is growing at around 200 points/year. This implies about a 75% win-rate, and equivalent to the difference in skill between the top-rated chess player in the world and the player ranked 100.[^winRate]\n\nThe AI pessimism has gone away:\n: This time last year many economists confidently predicted negligible economic impacts of AI:\n\n      - Acemoglu predicted AI would cause 0.06% incremental productivity growth over the next 10 years\n      - Josh Gans said [\"I donâ€™t think it will boost growth appreciably\"](https://x.com/joshgans/status/1812492809326276786)\n      - Robert Gordon [predicted small effects](https://www.newyorkfed.org/medialibrary/media/research/conference/2024/AMEC%20US%20Productivity/Sessiontrue3_Gordon_LGAtrueNYFedtrueFuturetrueoftrueLP_240216?utm_source=chatgpt.com)\n      - Paul Romer [predicted small effects](https://finance.yahoo.com/news/nobel-laureate-paul-romer-sees-093000071.html)\n  \n   I haven't heard anyone make a claim like that this year.\n\n   [^winRate]: It likely be impossible to approach a 100% win-rate, because (1) a fraction of people give noisy answers; (2) some easy queries have a unique correct answer; (3) some queries are ambiguous, meaning .\n\nAll questions about AI are downstream of the statistical structure of the world.\n\n: I'm personally convinced that most existing work is too reduced-form, & that in a few years the way we talk about AI's effect on different domains will refer to the statistical structure of that domain -- e.g. the dimensionality of the manifold. More precisely, the closeness of the fit between the structure of the domain and the structure of the brain trying to comprehend it -- whether that brain is human or computer.\n\n\nTo add:\n:     - very emphatic opinions on regulation: Susan, Luis, Kevin\n      - Drone army and resource constraints:\n      1. Services become cheap, goods remain expensive:\n      2. A Ricardian model:\n      3. How does AI change the offense-defense balance?\n      4. How will people find meaning?\n      5. Automation and augmentation is confusing\n      6. LLMs aren't predicting next tokens. I think many economists have the m\n      - The dangers are a little surreal: responses to Chad Jones taslk.\n      - Many economists are focussed on competition. \n\n\n##          We don't have a standard model of AI capabilities.\n\nEvery conclusion depends on assumptions about machine intelligence.\n\n: All of the papers rely on some assumption on the difference between computer intelligence and human intelligence, but the assumptions differ. There is no canonical model of computer intelligence. In fact this is a famously difficult problem: for 80 years we've been struggling to come up with a good metric of computer intelligence.\n\n      (This is what I'm personally most interested in, and part of the reason why I'm leaving OpenAI and joining METR.)\n\nThere are lots of ways that machine intelligence is modelled.\n      \n: - Reduced: (1) the share of tasks that a computer perform; (2) the time horizon of tasks that computers can perform; (3) your ability to extrapolate from known information.\n\n: - Structural: (1) access to an information set - either width or depth; (2) depth of processing.\n\nMany measures of LLM capability are highly correlated.\n\n: As a consequence, AI researchers are comfortable talking just about 'model intelligence' without needing to define what they mean. The correlation of capability is consistent with the idea that LLMs are learning deep latent structures of the world, which are useful across a lot of domains.\n\nWe are making some progress in characterizing AI ability.\n\n: In 2024 people talked about AI intelligence in terms of years-of-education (Aschenbrenner and OpenAI talking about \"phd-level intelligence\"). In 2025 METR (@kwa2025longtasks) made a good argument that the most parsimonious model of intelligence is the human-time-length of tasks.\n\n\n\n##          GDP will miss a lot\n\nThere are two reasons why focussing just on GDP will miss important effects:\n\nAI will change relative prices.\n\n: It's likely the price of services will fall relative to goods. If we just talk about AI's effect on output it's much harder to see concretely how that changes income and consumption without thinking about these dramatic swings in relative prices.\n\nAI services won't show up in GDP.\n\n: AI is already providing a great deal of value (700M ChatGPT users), but the value won't show up in GDP by our normal accounting methods. In fact it's plausible that AI reduces GDP because it reduces demand for expertise: I no longer call my garage-door-repair guy, because ChatGPT tells me how to fix the door. Services are generally accounted for in GDP just by the wages paid to service-providers, if we replace those providers then GDP will fall even though the services are still being supplied.\n\n##          A concrete scenario: a resource-constrained world\n\nI found it useful to have a concrete scenario in which AI can do everything humans can do.\n\n: **TL;DR: everything but land will become dirt cheap, and those who do not own land at the time of transition will have to live off others' charity.**\n   \n      1. Assume that every service and every good can be produced by AI. The inputs are just land, energy, and raw materials.\n      2. You can now buy any good at all, price is not a relevant constraint, you just have to choose which object you like and find room for it (you can shop on Amazon with unlimited credit). You can also get any service at the highest possible quality (medical, massage, education, entertainment), price is no longer a constraint, only time.\n      3. However there will still be some scarce resources. For simplicity I'll say it's just land (minerals and energy too, but minerals are in land, and energy production requires land and minerals).\n      4. Suppose we flip a switch and plentiful robots appear, what will happen? People who already own it will be able to exchange slices of land for computer-made goods and services at very favorable rates. Those who do not have land will be stuck: their labor has become worthless, and they have nothing to exchange it for. If you are living in a rented apartment your landlord will evict you: you no longer have anything of value you can offer him in exchange for the value of the land.\n      5. Throughout human history people have been born with an endowment of labor that they could use to exchange for goods and services. This will no longer be true: people can exchange their labor for others' labor, but their labor will no longer have value for land or anything that requires scarce resources.\n      6. Taken literally this implies that people without assets suddenly become entirely dependent on charity, even for a place to sleep. Perhaps the land-rich will collectively redistribute land and resources to the land-poor, and once you have a little land then all other goods and services become effectively unlimited.\n      7. Finally we might expect political structure to follow economic structure. Democracy seems to be more common when workers have more economic power, and so if TAI causes most workers to lose their relative economic power, politics may follow.\n\n##          What is the Demand for Labor?\n\nWe're uncertain what residual value labor will have.\n\n: Many discussions ended up coming back to what value human labor will have when computers become super-capable:\n\n      1. What things can only humans do?\n      2. Will people pay a premium for human labor?\n      3. If humans have no advantage, what happens to wages?\n\nWhat things can only humans do?\n\n: Many of the discussions circled around implicit assumptions on what humans can and cannot do.\n\nWill people pay a premium for human labor?\n\n: Suppose a robot  \n\n      1. Robot chef vs real chef.\n      2. People are not willing to pay a significant premium for made-in-US vs made in the rest of the world.\n      3. People are not willing to pay a premium for in-person entertainment and services: employment as musicians, .\n\nIf AI can do everything then wages will fall below subsistence.\n\n: There's a Ricardian argument that AI will cause wages to go up even if AI can do everything: humans will just specialize in their comparative advantage (because AI will shift prices relative to the prices in autarky, giving humans gains from trade). Pascual Restrepo made this argument, similar arguments are in @caselli2019robot, @smith2024, and @korinek2024scenarios.^[@korinek2024scenarios argue that as AI progresses it will causes wages to increase then decrease, but this is a consequence of a specific assumption: that AI will have identical relative productivity across tasks to humans, and so as AI becomes more capable the equilibrium price vector first moves away from, then returns towards, the vector of human productivities.]\n: However this reassuring conclusion depends on there being no other scarce inputs which humans and computers compete for. In fact humans do have resource inputs - say 100 square feet and 2000 calories/day. If a computer can do every task at a lower resource cost than a human then there will be no humans employed in equilibrium.[^plentiful] In the hypothetical world this implies that humans who do not own land would starve: the price of their labor, denominated in energy, would fall below subsistence level. Among people who are still alive (because they own land, or from charity), would they work? Only if the incremental resource cost of working was sufficiently low that it undercut the resource cost of computers.\n\n: A thought experiment: suppose we have a stock of A100 chips, then we start introducing more powerful H100 chips. Assume the H100 can do more tasks/hour across all tasks. If A100s and H100s do not compete for other inputs then we will keep using A100s. But if both A100s and H100s require space and electricity then, once we have sufficiently many H100s, we will unplug the A100s to make room.\n\n[^plentiful]: Assuming computers are in sufficiently plentiful supply that we can ignore every other cost apart from their resource cost. If manufacturing computers requires resources then we could include their amortized manufacturing cost. If we have a scarce supply of computers for other reasons then of course this will keep human wages high.\n\n\n\n##          AI scientists will be unlike human scientists\n\nWill efficiency curves start dropping faster?\n\n: A good way of making the AI R&D question very concrete is to look at historical input-efficiency curves across a lot of different areas, and try to predict where they will go in the future. Should we expect them to start dropping faster? Which ones?\n\n: In fact I think this is a very good quantity to forecast with, and to use as an assumption for specifying scenarios.\n\n![Our World in Data: Technology Costs over time](images/2025-09-19-16-49-40.png){.column-margin}\n\nMost models of AI R&D are based on human R&D.\n: Models of AI's impact on technological discovery are often modelled on human R&D, e.g. (1) AI increases the effective supply of human scientists; or (2) AI automates one component of the R&D process. However this process is modelled on a production function fitted on data with human researchers.\n\nThere's another way to model this.\n\n: My instinct is that there's a different way of modeling this that is more structural. Suppose we have an unobserved landscape, and it can be explored either by a human brain or a computer brain. Humans have been exploring the landscape, finding successively lower local minima, and also finding general patterns in the landscape (e.g. physical laws). We wish to understand how much computers will speed up exploration of the landscape.\n\n\n\n\n::: {.cell .column-margin}\n::: {.cell-output-display}\n![**Random landscape.** Here there's no structure: every $x$ is an independent random draw.](2025-09-19-transformative-AI-notes_files/figure-html/unnamed-chunk-1-1.png){width=672}\n:::\n:::\n\n::: {.cell .column-margin}\n::: {.cell-output-display}\n![**Rugged landscape.** This shows a Weiner process (random walk). Here there's local correlation but no long-distance dependence.](2025-09-19-transformative-AI-notes_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n::: {.cell .column-margin}\n::: {.cell-output-display}\n![**Regular landscape.** Here there's some latent structure, implying that you can make long-distance predictions from local observations.](2025-09-19-transformative-AI-notes_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n\n\nThe effect of AI depends on nature of landscape.\n\n: Suppose each period we choose an $x$ to minimize $y(x)$, where $y(\\cdot)$ is unknown. This is a well-defined explore-exploit problem, and we can characterize the expected progression of efficiency over time (the decline in $y(.)$ over time) as a function of the statistical structure of the landscape:\n   \n      1. **Random landscape:** If each $y(x)$ is completely independent there's no intelligence needed in choosing $x$ (beyond keeping track of which locations you've already tried). This is just drawing balls from an urn. The growth in efficiency as a function of $N$ draws depends on the distribution of values of $y$  (@muth1986search, @kortum1997research).\n      2. **Rugged landscape:** If $y(x)$ is correlated across $x$ but the correlation is local (e.g. if $y(x)$ is a Weiner process) then the best-estimate of $y(x)$ for a new $x$ will depend only on the neighboring values of $x$. @Callander2011 and @CarnehlSchneider2025 characterize the optimal strategy. Again we are not constrained on intelligence, only on data: the extrapolation algorithm is fairly simple.\n      3. **Regular landscape:** Finally suppose the landscape has some deep latent structure. In this case the best-estimate of $y(x)$ will depend on the entire collection of previously-observed pairs $(x,y)$, and so we *do* expect that predictions could be improved with more intelligence, and so AI should have a big impact.\n\nImplications of landscape regularity.\n\n: If the world has a regular landscape then we are not primarily constrained on facts, we are constrained on intelligence. Thus if we build a sufficiently powerful pattern-matching machine our progrss might accelerate rapidly.\n\n: Random lanscapes, where we'd expect little impact of AI:\n\n     - _Discovering species._ Discovering viruses, discovering planets. When discovering new objects the observations cannot be well-predicted from first principles, we inevitably need new observations.\n      - _Plant breeding._ Suppose we breed plants just by selecting the highest-yield mutations (and suppose we don't observe anything but how well the plant grows). The statistical problem is trivial, and AI won't help at all.\n      - _Mapping a genome._ The exact base pairs in a genome require individual observations.\n\n: Regular landscapes, where we expect a large impact.\n\n      - _Folding proteins._\n      - _Discovering candidate drugs._\n      \n\n\nAI R&D has already lead to discontinuities.\n\n: Many fields which have been progressing slowly show a discontinuity when computers took over:\n\n      - Progress in solving optimization problems.\n      - Progress in proving combinatorics theorems (4-color theorem in 1976)\n      - Progress in chess strategy (frontier Elo fell quicker since 1997)\n      - Progress in protein folding.\n\n      Some of these have hit provably global minima: the 4-color them; sphere packing; Ramsey numbers; Nash equilibrium of checkers, connect 4, texas holdem, and chess endgames.\n\n      I think we can describe these discontinuities as when computer intelligence surpasses human intelligence *for a particular type of pattern-matching*. As computer intelligence becomes more general then we should expect more and more lines of progress to start accelerating.\n\n\nAristotle already had the jigsaw pieces.\n\n: There's a nice analogy for this: did Aristotle already have enough jigsaw pieces for modern science? Was he constrained on facts or intelligence? Suppose we resurrected him, is it enough to explain our theories, or does he also need to see the data we've gathered?\n\n: It seems to me plausible that we could persuade Aristotle of some of the following, just using facts he was already aware of: that the sun is a star, that the earth goes around it, that the whale is not a fish, that the human is a monkey, that force is mass times acceleration, that temperature is motion, that pitch is frequency.\n\n\nPredicting the effect of AI is *intrinsically* difficult.\n\n: The \"landscape\" model above implies that we should expect AI to have a big effect when some domain has a latent undiscovered structure. But in many cases this is very difficult to know in advance: we don't know where the floor is. It seems conceivable that there are some very simple undiscovered principles explaining cancer, fluid motion, evolution. But maybe these domains are irreducibly complex.\n\n#          Models\n\nHere are a few models that I find useful.\n\n*Labor only*: everyone is better off.\n\n: AI increases labor productivity, and makes everyone better off.\n\n: \\ \n\n*Differentiated labor*: welfare increases, GDP falls.\n\n: Suppose that each person has abilities in different areas (medicine, law, candlestick-making) and so they specialize and trade their outputs (doctor, lawyer, candlestick-maker). Suppose AI increases your productivity *outside* your area of specialty, flattening comparative advantage. In a symmetric setup with trade costs then everyone will do relatively more themselves (home production), and they will trade less, thus welfare increases but GDP falls.\n\n      Next suppose there is some asymmetry, e.g. suppose doctors are paid more because their skills are relatively more scarce. Then a flattening of comparative advantage will flatten the wage profile, raising average incomes but lowering the incomes of the highest-paid (this is the same as the effect of catch-up growth in a trade model).\n\n*Labor and land*: wages collapse.\n: Suppose land and labor are gross complements, and that AI is a perfect substitute for labor, and that we have an arbitrarily large quantity of AI. Then the marginal product of labor falls to zero and the entirety of the output will be held by the land-owner. Human labor no longer has any value and workers must live off the charity of the land-owners.\n\n      Formally:\n\n      $$Y=(\\ut{N}{land}^\\rho+{(\\ut{L}{labor}+\\ut{C}{computers})}^{\\rho})^{1/\\rho}$$\n\n      as $C\\rightarrow\\infty$ then the marginal product of labor goes to zero, and all income goes to land.\n\n      Notes:\n\n      - I assumed labor and land are gross complements ($\\varepsilon<1$). If production was Cobb-Douglas then making labor free implies we would get infinite output from a finite amount of land. As long as there is some limit on total output then land and labor must be gross complements beyond some point.\n      - I assumed AI labor is free. We could instead assume AI has some resource cost. In that case the price of labor will be driven down to the input cost of AI labor. The input cost of AI labor could be defined in a few ways but they all appear to me low compared to exiting human wages: the land cost of a GPU is a few square inches, the energy cost is 400 watts.\n      - We could distinguish between two sectors: a sector that requires land (goods) and a sector that requires only labor (services). If we introduce AI as free labor then human wages will retain the same purchasing power for services but their purchasing power for goods will collapse. Concretely: if you try to exchange your labor for goods you will have nothing to offer because the land-holder already has unlimited labor. The model predicts that workers will be not benefit from AI-produced-goods because AI requires land inputs.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}