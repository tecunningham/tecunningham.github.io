{
  "hash": "ee298c81d240c568fbfd1cb59acb3e4c",
  "result": {
    "markdown": "---\ntitle: The History of Automated Text Moderation\nauthor: \"[Integrity Institute](https://integrityinstitute.org/) collaborators: [Alex Rosenblatt](https://www.linkedin.com/in/alexrosenblatt/), [Jeff Allen](https://www.linkedin.com/in/jeff-allen-scientist/), [Ejona Varangu](https://www.linkedin.com/in/ejona-varangu/), [Dave Sullivan](https://www.linkedin.com/in/davesullivan41/), Tom Cunningham\"\ndraft: true\n# execute:\n#    echo: false\n#    cache: true # caches chunk output\n#    warning: false\n#    error: false\ndate: 2023-11-18\nengine: knitr\nbibliography: ai.bib\nformat:\n   html:\n      toc: true\n      toc-depth: 2\n      toc-location: left\n---\n\n\n<!-- https://tecunningham.github.io/posts/2023-11-18-history-automated-text-moderation.html \n\n   _freeze/posts/2023-11-18-history-automated-text-moderation/execute-results/html.json\n#\tmodified:   \n-->\n\n<span style=\"background:yellow;\">==Still a draft: not ready for general circulation.==</span>\n<!-- change again -->\n\n<!-- **Authors:** [Integrity Institute](https://integrityinstitute.org/) collaborators: [Alex Rosenblatt](https://www.linkedin.com/in/alexrosenblatt/), [Jeff Allen](https://www.linkedin.com/in/jeff-allen-scientist/), [Ejona Varangu](https://www.linkedin.com/in/ejona-varangu/), [Dave Sullivan](https://www.linkedin.com/in/davesullivan41/), Tom Cunningham.\n -->\n\n**This document describes five technologies for automated text moderation,** each roughly correspond to an historical phase.\n\n**As a working example we will use the detection of “toxic” comments.** In practice many different definitions of “toxic” have been used in the industry, and there are a variety of related concepts, e.g. “hate speech” and “offensive”.\n\n#     (1) Keywords\n\nThe simplest technology is to hard-code a list of words which are considered “toxic”, e.g. a list of curse words. This can be implemented with regular expression. This has obvious limits on the accuracy and cannot be easily maintained, however many platforms still maintain a keyword block list for some sensitive terms.\n\n#     (2) Simple classifier (“Bag of words”)\n\nWe can collect a large set of human-labeled data on whether individual messages are toxic, and then predict toxicity from the appearance of individual words e.g. using logistic regression or naive Bayes. These classifiers will find that certain words are highly predictive of toxicity. Simple classifiers often have reasonable accuracy but will have many important false positives and false negatives, and they are easy to evade by rewording or misspelling text.\n\n- 1961: Maron (1961) proposes the Naive Bayes classifier\n\n#     (3) Embedding-based classifier (2013-2018)\n\nThese models have two stages:\n\n1. Pretrain: for each word calculate an embedding (a vector of numbers) which predicts its likelihood of co-occurring with other words. Pairs of words which are nearby in embedding-space typically have similar meanings.\n2. Train: train a model to predict toxicity of a comment using the embedding of the words in a message (e.g. the average embedding).\n\nAn advantage over simple classifiers is that these models require much less labeled data for an equal performance, because the pre-training stage has already learned (crudely) the meanings of different words. Thus these models can identify words that are diagnostic of toxicity even if they never appeared in the toxicity training set.\n\nHowever embedding-based classifiers are still bad at edge cases, e.g. when a word is used inside a negation (“is an idiot” vs “is not an idiot”), or if a word is mis-spelt, or if harmless words are used to express an meaning that is toxic (“your brain is a bowl of jello”).\n\n- 2013: Word2Vec: a word embedding using a 2-layer neural network, (@mikolov2013efficient)\n- 2014: GloVe: Global Vectors for Word Representation. They say \"training is performed on aggregated global word-word co-occurrence statistics from a corpus\"  (@pennington2014glove).\n- 2015: fastText: word embedding from FAIR. They released pre-trained models for 294 languages (@joulin2016bag)\n- 2017: Jigsaw Perspective Toxicity API v1 from Google.^[I couldn't find any authoritative documentation on the architecture of this classifier: I found one reference to it using the GloVe embeddings.]\n\n#     (4) LLM-based classifiers (2018-2023)\n\nThese models have three stages:\n\n1. Embedding: Compute embedding of each token (a token is roughly equal to a word).\n2. Pretrain: Train a deep neural net to predict a token from surrounding tokens (or prior tokens), using attention (i.e. don’t weight all words equally) on an enormous training set of text from books and the internet.\n3. Train: Train a model to predict toxicity from labeled data using the top-level neurons in the net as features.\n\nConceptually these are similar to embeddings but (1) they can represent the meaning of entire sentences instead of just words, (2) have more layers so tend to have more sophisticated representations of meaning.\n\n- 2017: Transformer architecture (@vaswani2017attention)\n- 2018: BERT transformer LLM, this model has been widely used as base model for a variety of natural language tasks, including content moderation (@devlin2018bert)\n\n#     (5) Zero-shot LLMs (2023-)\n\nThese models have three stages:\n\n1. Embedding: Compute the embedding of each token.\n2. Pretrain: Train a deep net to predict the next token from previous tokens, as above.\n3. Directly ask the model whether a given message violates a given policy, e.g. “is the following sentence toxic? ___”\n\nNotably this method does not use any human-labeled data, it only needs to be told what type of text it is looking for. This is referred to as “zero shot”, meaning it needs zero training data. These models can also use “few shot” learning, where a small number of examples are given instead of the thousands of examples that had ordinarily been used.\n\nThis has big benefits: it allows you to very quickly refine policy, and the LLM can generate explanations for why it made a decision.\n\n- 2020: GPT-3: reasonable zero-shot performance (@brown2020language)\n- 2022: ChatGPT published: very good zero-shot performance on many tasks.\n- 2023: OpenAI provides GPT-4-based content moderation tools (@weng2023gpt4moderation)\n- 2023: [SafetyKit](https://www.safetykit.com), [CheckStep](https://www.checkstep.com/blog/) provide LLM-based content-moderation.\n- 2023: Stanford CoPE:an open-source LLM for moderation.\n\n\n#     Discussion\n\n**Q: now that we can use LLMs for arbitrary labeling, will we change policies?**\n\n- Proposals are coming out of Michael Bernstein’s lab, e.g. @jia2023embedding, in using LLMs to substantially change how content is ranked.\n- Dave Wilner has argued that because LLMs offer much greater flexibility then platforms will find it easier to write more complex policies and update them more frequently.\n\n**Q: what do we know about degree of accuracy across languages?**\n\n- AI typically has a strong anglophone bias. Performance in non-English languages tends to be proportional to the distance from English, e.g. European languages tend to be worse. However many also noted that there is typically a large anglophone bias in human moderation.\n<!-- - An open question: how to define appropriate moderation standards across languages. -->\n- Some literature shows that LLMs have good performance in languages with relatively little training data, e.g. @armengolestape2021multilingual.\n<!-- Papers from Jigsaw/Perspective with multilingual classifiers -->\n\n**Q: will censorship change when using LLMs instead of humans?**\n\n- Jeff noted that an advantage of human censors over machine censors is that humans might exercise their judgment to refuse to censor while machines will not.\n\n<!-- #     Recommended Further Reading\n\nAnirban Sen (2020) \"Text Classification — From Bag-of-Words to BERT\" a bigger comparison of models in predicting toxicity: bag-of-words, word2vec, fasttext, CNN, LSTM, BERT. -->",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}