{
  "hash": "5a5830b0fc863560afafe21774047ae9",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: Too Much Good News is Bad News\nauthor: Tom Cunningham\ndate: today\n# draft: true\nexecute:\n  echo: false\n  cache: true # caches chunk output\nreference-location: document\ncitation-location: document\nfontsize: 14pt\nformat: html\nengine: knitr\neditor:\n  render-on-save: true\n---\n\n\n\n#        Too Much Good News is Sometimes Bad News\n\nIn many cases a signal which is good news eventually starts to become bad news:\n\n1. The longer you wait for a bus the likelier it is to be about to arrive, until at some point it becomes more likely that you've missed it.\n2. The lower the price the better the value until it becomes suspiciously cheap.\n3. If a drug is associated with a 5% higher rate of birth defects it's probably a selection effect, if it's associated with a 500% higher rate of birth defects it's probably causal.^[Bradford-Hill: \"the mortality of chimney sweeps from scrotal cancer was some 200 times that of workers who were not specially exposed to tar or mineral oils.\"]\n<!-- 5. The fewer cars on a street the better it is for parking until there are suspiciously few cars, meaning it's probably street-cleaning day. -->\n1. If an AB test shows an effect of +2% ($\\pm$ 1%) it's very persuasive, but if it shows a an effect of +50%  ($\\pm$ 1%) then the experiment was probably misconfigured, and it's not at all persuasive.^[This case is somewhat subtle: we generally think that treatment effects are fat-tailed, and we can be confident that noise is Normal because it's the sum of many IID variables. However there's an additional source of noise from implementation error which has even fatter tails than the distribution of treatment effects.]\n2. When reading a biography each detail makes the the subject seem more impressive until you start to doubt the neutrality of the biographer.\n\n#        Usually Signal is Light-Tailed, Noise is Heavy-Tailed\n\nIn each of these cases I think it's because the noise has a fatter-tailed distribution than the signal. As a consequence when you see a very-high observation you conclude it's mostly noise, and so the very-high observation is less persuasive about the signal than a moderately-high observation.\n\n#       Outliers Have One Cause, Not Many\n\nScott Sumner argued [\"extreme events generally have multiple causes\"](https://www.themoneyillusion.com/author/ssumner/page/183/), with a few examples:\n\n1. Italy had very high COVID death rates, and this is probably the conjunction of a variety of different factors.\n2. The great depression was due to \"multiple policy errors, on both the supply side and the demand side.\"\n3. Bob Beamon's record long-jump in 1968 was probably due to a variety of factors.^[*\"While Beamon received mostly accolades, there also were detractors. The critics harped on the conditions — a following wind of 2.0 meters per second (the maximum allowable velocity for a record), a lightning fast runway and, most important, the thin air of Mexico City. Beamon’s defenders point out that the other competitors, which included the world record co-holders, had the same factors going for them and they didn’t jump close to Beamon.\"*]\n\nI think Scott Sumner is wrong in his generalization: extreme events typically have a single cause, not multiple causes. Formally (following Nair, Weierman and Zwart below), extreme draws from a sum of thin-tailed influences tend to have many causes, but extreme draws from a sum of fat-tailed influences tend to have one cause. Thus if the distribution of outcomes is fat-tailed then the outliers are likely due to single causes.\n\nI'm not sure about Italy's COVD and the great depression, but for Bob Beamon's jump in 1968 it looks like a non-Normal outlier, not the sum of orthogonal influences (see the spike on the blue line):\n\n:::{.column-margin}\n![](images/2024-12-26-07-59-52.png)\n:::\n\n#        Formalizing This\n\n**Literature.** De Finetti wrote a paper in 1961, \"The Bayesian Approach to the Rejection of Outliers\", giving a simple example where, with fat-tailed noise, a Bayesian will discount outlier observations.\n\nThere seem to be two modern strands of this literature that use somewhat different terminology:\n\n1. Anthony O’Hagan and Luis Pericchi (2012) [Bayesian heavy-tailed models and conflict resolution: a review](https://projecteuclid.org/download/pdfview_1/euclid.bjps/1341320249). They give a number of conditions under which you get \"conflict resolution\" or \"rejection of outliers\", broadly speaking when the noise has fatter tails than the signal. The critical condition is the relative speed of decline of the tails of the two distributions.\n\n2. Nair, Weierman and Zwart have a chapter \"Catastrophes, conspiracies, and subexponential distributions\" in their book [\"The Fundamentals of Heavy Tails\"](https://adamwierman.com/wp-content/uploads/2021/05/book-05-11.pdf). \n\n   - Suppose you observe the sum of a set of $N$ IID random variables. They discuss two polar ways in which your posteriors about the components will depend on the sum:\n   - If the components are heavy tailed you get the \"catastrophe principle\": the probability that the sum exceeds some value $t$ will be approximately equal to the probability that the maximum of $N$ components exceeds $t$, as $t\\rightarrow\\infty$.\n   - If the components are light-tailed you get the \"conspiracy principle\": the probability that the sum exceeds some value $t$ $t$ dominates the probability of the maximum exceeding $t$, as $t\\rightarrow\\infty$.\n   - Applications of the catastrophe principle: If a certain year has many earthquake deaths then probably there was one large earthquake, not many small ones. If a random group of people has a high average number of Twitter followers, probably one member of the group is a big outlier, and the others have an ordinary number.\n   - Applications of the conspiracy principle: if a random group of people has a high average height then probably each individual is tall.\n\n\n<!-- They talk about the Swedish mathematician Harald Cramer modelling the insurance business: the income is regular, but the payments are highly skew. We want to know what the distribution looks like as you aggregate many claims. -->\n\n**Analytic models.** I think the simplest way to get a closed-form solution which exhibits non-monotonic posteriors is with a Cauchy noise distribution:\n\n\n| signal                 | noise  | posterior shape | posterior expression |\n| ---------------------- | :----: | :-------------: | :------------------: |\n| Normal                 | Normal |    monotonic    |       analytic       |\n| Discrete               | Normal |    monotonic    |       analytic       |\n| Normal                 | Cauchy |    monotonic    |          ?           |\n| Discrete               | Cauchy |  non-monotonic  |       analytic       |\n| Spike-and-uniform slab | Normal |  non-monotonic  |     not analytic     |\n| Spike-and-uniform slab | Cauchy |  non-monotonic  |       analytic       |\n\n\n<!-- Tweedie's formula.\n: Tweedie's formula gives us the posterior from the empirical distribution as long as the noise is from an exponential family. First we'll do it with Normal noise: we observe $x=v+u$, signal $v$ is drawn from some distribution $g$ and noise is Normal:\n   $$\\begin{aligned}\n      v \\sim g(.) \\text{  and  } x|v \\sim N(v,\\sigma^2) \\\\\n   \\end{aligned}$$\n: Tweedie tells us:\n   $$E[v|x] = x + \\sigma^2 \\frac{d}{dx}\\log f(x)$$\n\n: We want to find non-monotonicities:\n   $$\\begin{aligned}\n      \\frac{dE[v|x]}{dx} = 1 + \\sigma^2 \\frac{d^2}{dx^2}\\log f(x) &< 0 \\\\\n      \\frac{d^2}{dx^2}\\log f(x) &< -\\frac{1}{\\sigma^2} \n   \\end{aligned}$$\n: So we'll get non-monotonicities in the neighborhood of highly concave parts of the empirical distribution of observations. \n\nHOWEVER -- it might be that even if g(.) is pointwise distribution there's still not a non-montonicity. \n\n-->\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}