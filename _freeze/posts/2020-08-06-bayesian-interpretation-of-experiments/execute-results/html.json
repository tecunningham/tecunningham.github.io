{
  "hash": "03ddae7a66fbd0f22fbb3260e7db572c",
  "result": {
    "markdown": "---\ntitle: The Bayesian Interpretation of Experiments\nauthor: Tom Cunningham\ndate: today\nfig-width: 2\nfig-align: center\nexecute:\n  echo: false\n  warning: false\n  error: false\n  cache: true # caches chunk output\nengine: knitr\nformat: html\neditor:\n  render-on-save: true\ndraft: true\n---\n\n\n<!-- \n   https://tecunningham.github.io/posts/2020-08-06-bayesian-interpretation-of-experiments.html\n-->\n\n<!-- https://fb.workplace.com/groups/109535843016173/permalink/345590889410666/ -->\n\n<!-- see also 2022-02-06-inference-with-experimental-data -->\n\n<!-- *(zips up flame-resistant suit)* -->\n\n#                    Summary\n\n**Once you commit the original sin of testing for statistical significance you enter upside-down world where good is bad, bad is good.** \n\n**There are lots of weird implications of significance testing:**\n\n   * The significance of a test depends on how many other tests you ran that day. \n   * It's considered bad practice to make decisions about how long to run an experiment based on early results of that experiment.\n   * It's considered bad practice to look at statistical relationships between variables that you only thought of after the data was collected.\n   * It's considered bad practice to talk about an effect which has $p=.051$ in the same way as an effect with $p=.049$. Some people even say you shouldn't use the phrase \"almost significant\" to describe things which are almost significant.<!-- (some people roll their eyes and say *\"oh ... MaRgInAlLy SiGnIfIcAnT.\"* Don't mock them, even though they're trying to mock you, they're just confused.) -->\n\n   These paradoxes confused me for a long time until I started to think of classical inference as Bayesian inference operating under a constraint. \n\n\n**Classical inference can be thought of as Bayesian inference with a hard-coded prior.** In some cases the hard-coded prior will be close to your true prior, and so it's fine, but in other cases it won't and it'll cause bad inferences. To deal with those pathological cases classical inference has built up a structure of additional rules to protect against bad inferences. The rules either specify contexts where you shouldn't do any inference at all, or they suggest transforming the problem into one where the hard-coded prior is closer to your true prior. Thus we have rules about multiple testing, about data-dependent stopping, and about testing posterior hypotheses.\n\n<!-- Once you put on Bayesian spectacles then everything comes into focus.  -->\n\n<!-- **TLDR:** (1) all these issues are much easier to think about through Bayesian lens; nevertheless (2) there is an argument for sometimes following the hypothesis-testing rules for psychological or institutional reasons. -->\n\n#                    Bayesian vs Classical Inference\n\nI'll compare two ways of interpreting experimental estimates. Suppose `posterior` represents the estimate of an effect that you'll use in decision-making, then here are two methods for calculating that:\n\n1. **Classical.** If the experiment is statistically-significantly different from zero then you use the experimental estimate (`posterior=outcome`), otherwise you assume no effect (`posterior=0`). (Technically this should be called \"Null Hypothesis Significance Testing\", and I'm giving a slightly exaggerated version of it).\n2. **Bayesian.** You set `posterior` to be a weighted average of `outcome` and of `prior`. The prior represents your best-estimate of the effect before the experiment ran. The position of the posterior between those two points depends on the relative tightness of the two distributions: if the confidence intervals from your experiment are tight relative to the uncertainty in your prior then the posterior will be closer to the outcome; if instead the confidence intervals are wide relative to your prior then the then the posterior will end up closer to your prior.\n\nGraphically we can show the three distributions:\n\n\n::: {.cell layout-align=\"center\" hash='2020-08-06-bayesian-interpretation-of-experiments_cache/html/unnamed-chunk-1_917aa8191a361af5543ea4510f40bbc2'}\n::: {.cell-output-display}\n![](2020-08-06-bayesian-interpretation-of-experiments_files/figure-html/unnamed-chunk-1-1.png){fig-align='center' width=384}\n:::\n:::\n\n\n\n#                    Classical Inference as Constrained Prior\n\nSuppose our prior was a spike at zero and otherwise uniform. This prior will cause Bayesian inference to behave similarly to classical inferences: when the outcome is small then the posterior will be heavily influenced by the spike, and so will shrink to be very near zero. When the outcome becomes larger then at some point it will escape the gravity of the central spike, and we'll have `posterior~=outcome`.\n\n\n\n::: {.cell layout-align=\"center\" hash='2020-08-06-bayesian-interpretation-of-experiments_cache/html/unnamed-chunk-2_50ef65ea2eb7ad10394d2b7a8069f40d'}\n::: {.cell-output-display}\n![](2020-08-06-bayesian-interpretation-of-experiments_files/figure-html/unnamed-chunk-2-1.png){fig-align='center' width=192}\n:::\n:::\n\n::: {.cell layout-align=\"center\" hash='2020-08-06-bayesian-interpretation-of-experiments_cache/html/unnamed-chunk-3_12030b1f39bfa846d9cf4f41e82f6522'}\n::: {.cell-output-display}\n![](2020-08-06-bayesian-interpretation-of-experiments_files/figure-html/unnamed-chunk-3-1.png){fig-align='center' width=192}\n:::\n:::\n\n::: {.cell layout-align=\"center\" hash='2020-08-06-bayesian-interpretation-of-experiments_cache/html/unnamed-chunk-4_e2552f9e985ee935a1d1faaf257cbd73'}\n::: {.cell-output-display}\n![](2020-08-06-bayesian-interpretation-of-experiments_files/figure-html/unnamed-chunk-4-1.png){fig-align='center' width=192}\n:::\n:::\n\n::: {.cell layout-align=\"center\" hash='2020-08-06-bayesian-interpretation-of-experiments_cache/html/unnamed-chunk-5_81cb88810bb819111a178650367cf751'}\n::: {.cell-output-display}\n![](2020-08-06-bayesian-interpretation-of-experiments_files/figure-html/unnamed-chunk-5-1.png){fig-align='center' width=192}\n:::\n:::\n\n\n**The point:** the two graphs at the bottom of the figure are similar: i.e., using a \"stat-sig rule\" is a not-too-bad approximation of Bayesian inference when you have a fat-tailed prior (and in most cases your prior should be fat-tailed).\n\n#                    Applications\n\nThis angle of seeing the situation makes a lot of crooked things turn straight:\n\n1. **Everything is continuous.** In the classical framework there's a sharp distinction between estimates that are significant and not significant. In reality there aren't sharp edges like this: if something is just-insignificant ($p$=0.051) you should treat it basically the same as if it's just-significant ($p$=0.049).\n\n2. **You can discount a statistically-significant outcome if it's unlikely.** Suppose you're running an experiment and you see a statistically-significant effect on an outcome nobody expected the experiment to move. Then it's perfectly reasonable to heavily discount that result: you have relatively strong reasons to expect a zero effect, AKA a tight prior around zero, and so your posterior ought to remain close to your prior.\n\n3. **Most experiments over-estimate the true effect.** In many domains the average experimental effect is zero. This implies that if you measure a positive effect in an experiment then you should expect, if you re-ran that experiment, that the effect would get closer to zero. Likewise if you measure a negative effect then, if you re-ran the experiment, you should expect the effect go up towards zero. This shrinkage towards zero is especially true for effects that are noisy (when you have wide confidence intervals), and for outcomes that are hard to move (when your prior is narrow).\n\n4. **You don't need to worry about peeking.** In the classical framework you should set and forget: it's bad practice to make decisions about how long to run the experiment after already seeing some of the results, because you can strategically choose the length of the experiment to almost guarantee a significant result even when there's no true effect. But if you're interpreting experiments in a Bayesian fashion it doesn't matter: each time you increase the sample size the experimental estimate (i) moves closer to the true effect, and (ii) gets tighter confidence intervals; but it won't cause any bias.\n\n5. **You don't need to adjust for multiple comparisons.** You should never adjust for the number of tests you're doing -- you should only adjust for the strength of your priors. The reason that people often feel they need to do multiple-comparison adjustments is because there's an *empirical* association between number-of-comparisons and strength-of-priors: when someone is testing a lot of different treatments or outcomes it's often because they have weak priors that any one of them would work, and those weak priors imply they should be relatively more conservative with the evidence. But number of tests is only a proxy, the true reason for adjustment is the strength of your priors.\n\n6. **It's OK to come up with hypotheses after the experiment has run.** It's not allowed in classical model to run extra tests you hadn't planned, but there's no problem in the Bayesian framework as long as you're rigidly honest about your priors. I.e., if you state the same prior you would've stated before seeing the data.\n\n#                    Why Does the Classical Framework Survive?\n\n**Why do we still teach & use the classical framework?**\n\nYou can think of the classical setup as a *constrained* version of the Bayesian interpretation in which you effectively use same prior for all experiments.\n\nIn the Bayesian interpretation you must supply a prior, and the shape of the prior is essentially a judgment call about what you think of as the nature of the existing evidence. The classical setup removes that judgment call and replaces it with a rigid rule.\n\n**This can have two justifications:**\n\n1. The psychological justification is that people have a weakness for misremembering their expectations, and thinking they predicted what actually happened. The classical rule constraints you from doing that.\n\n2. The organizational justification is that people will tend to misrepresent their expectation to *others*. People who run experiments face very strong incentives in interpreting their outcomes -- shipping & promotion, publication & tenure -- and so often they can't be trusted to be neutral reporters of what is a reasonable prior. \n\nLike plastic cutlery, we use significance-testing as a way of protecting you from hurting yourself and others.^[Tukey: \"The tool that is so dull that you cannot cut yourself on it is not likely to be sharp enough to be either useful or helpful.\"]\n\nI personally think this is a reasonable way of trying to soften these psychological and organizational problems -- but I think the way it's taught often confuses people. It's much easier to understand the paradoxes of hypothesis-testing if we remember:\n\n**Hypothesis testing is motivated by psychological & organizational constraints, not by statistical considerations.**\n\nYou should think of the rules of significance-testing as *etiquette*, not as statistics: i.e. we consider certain practices \"bad form\" not because they're bad inferences but because they're socially harmful.\n\n<!-- ## Coming Up\n\n1. **Q: Where do you get priors?** You can get a useful benchmark from empirical distribution of previous experiments that have been run.\n2. **Q: How to think about interpreting multiple metric movements?** and how to come up with an optimal composite/guardrail metric, and how it depends on the relative covariance of the metrics at experiment-level and user-level.\n3. **Q: how should inferences map to decisions?** (in short: you need to specify some kind of loss function). -->\n\n\n#                    Notes\n\n**Straussian reading of statistics.** Strauss says that if you observe an author contradicting themselves you should look to see if there's a deeper meaning that they are trying to convey, or if there are two audiences they're trying to talk to. I think most frequentists are operating under this constraint: they're trying to both (1) explain how to do inference from data; (2) discourage people from taking too much license in interpretation.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}